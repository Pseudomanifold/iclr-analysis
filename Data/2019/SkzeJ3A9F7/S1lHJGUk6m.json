{"title": "Review: Clear reject", "review": "Summary:\n\nThis paper proposes a novel differentiable approximation to the curiosity reward by Pathak et al. that allows a learning agent to optimize a policy for greedy exploration directly by supervised learning, rather than RL.\nThe authors motivate this work with arguments about the sample-efficiency required by real robot learning, and demonstrate basic results using a real robot.\n\nComments:\n\nThe paper has serious style and tone issues that must be addressed before publication. The rest of my review will focus solely on the technical details.\n\nThe experimental details are lacking (learning rates? rollout lengths for REINFORCE? what are the inner and outer loops and what are their sizes? what are the plots measuring - extrinsic reward? intrinsic reward? what is \"multi-step learning\" in Table 2?).\nWithout these details, the results will be difficult to validate and reproduce independently.\n\nThe approach is compared only against very weak baselines. Why vanilla REINFORCE and not any of the modern policy-gradient algorithms (A3C, PPO, TRPO, DDPG, ...)? The ability to deal with this large action space is certainly impressive, but it is likely far too large for DQN or REINFORCE to work, so the comparison is questionable to begin with: you can make any algorithm fail if you give it an unnecessarily difficult interface. Did the authors try a smaller, more traditional action space?\n\nThe authors claim the sample-efficiency improvements by many existing exploration approaches are insignificant. In what way are the results in this paper more significant? Table 1 shows very minor improvements to a MPC planning task, Appendix Figure 1 shows barely any improvement over the baseline, and Appendix Figure 4 shows that learning from extrinsic rewards using REINFORCE seems to work just fine. Why use intrinsic rewards at all in this case? It appears that maybe some of the results look significant because the baselines are so weak.\n\nThe paper contains many factual errors and unsupported claims. For example:\n- \"the field of RL was born out of need to make our robots learn\"\n- \"none of the recent advances have translated to success in the field of robotics\" (see e.g. the proceedings of CoRL 2017 and 2018)\n- \"Building a good model will require enormous number of interactions\" (see e.g. PILCO)\n- \"[our approach enables us to] for the first time ever, implement exploration on a real-world physical robot\" (PILCO and many others)\n- describing Pathak et al. curiosity as a \"gaussian density model\" in eq1; it's a deterministic forward model\n- in sec3, \"regress r^i_t to learn value estimates\", this is probably meant to be the discounted sum of rewards\n- also sec3, \"[REINFORCE] gives no signal as to what action to take\"; the signal has high variance but it works (see all policy gradient work)\n\nThese errors can be easily corrected. However, the contribution of the paper is based on a more serious error:\n- sec3.1, \"If the policy could be optimized using direct gradients, the rewarder could ... inform the agent to change its action space in the direction where forward prediction loss is high.\"\nThis is incorrect. The paper is based on using the gradient of the forward model to directly optimize the policy to produce higher prediction errors, as in Pathak et al.\nBut in order to make the prediction error differentiable, it makes the severe assumption that the next state x_{t+1} is constant and does not depend on a_t, which is false and invalidates the idea of optimizing actions for prediction error.\nAs a result, the gradient obtained does not actually move the policy toward higher prediction errors.\n\nTo understand what the author's approximation actually does, consider a perfect forward model. No matter what actions the policy produces, the prediction error is always zero, but the authors' gradient is not. So it can't be optimizing for higher prediction errors.\nInstead of optimizing for high prediction errors as the authors claim, the policy is being optimized for state transitions that are maximally different from the observed state x_{t+1}.\n\nThis is an interesting objective to optimize. I can see how it could result in interesting exploration. But it's not what the authors say they're proposing.\nIt's much more like a count-based exploration strategy, which prefers visiting states that are maximally different from the states visited so far. It is much less like the prediction-error based curiosity of Pathak et al. that the authors are motivated by.\nI would like to see focused analysis of this particular objective. For example, would this not result in the policy oscillating between different parts of the state space, since it's only optimizing for maximal difference to what it just saw, rather than long-term knowledge gain? This issue requires more discussion.\n\nFinally, the approach is not really robot-specific despite the title and arguments in the paper. I recommend pursuing a more general investigation, because if this objective is truly as effective as the authors believe, then it should be applicable in a wide variety of domains (many of which are very easy to evaluate in: Atari, OpenAI Gym, DMLab, VizDoom, Mujoco, etc.).\n\nConclusion:\n\nThe paper proposes an interesting new objective, but it is motivated by a very naive approximation that completely changes the behavior of the exploration compared to what the authors want to approximate. The idea is novel and worth exploring, but the paper should be heavily rewritten to emphasize what the authors are actually doing with this new objective, and should include thorough analysis of its behavior, before I can recommend acceptance.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}