{"title": "Review", "review": "This paper presents an interesting way to reformulate intrinsic curiosity as a differentiable function. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. For DQN this is to be expected, but it shows that backprop through this function is more efficient than reinforce in getting to unseen state spaces. I think this is an interesting method/proposal and is a somewhat novel reformulation of intrinsic error, but I do have some concerns in comparisons/claims. \n\nIn the introduction, the authors say that the intrinsic curiosity method proposed by Pathak et al. is sample inefficient and isn\u2019t tested in robots. However, to my understanding the REINFORCE baseline isn\u2019t really equivalent (though it may be possible that it is, it was unclear how exactly the loss was formulated in the baseline, did include the other components from Pathak et al.?). If the claim is that this method is more efficient, I think it should have compared against that method directly. \n\nMoreover, I think the description of the experiments doesn\u2019t provide enough information. For example, the method says that different learning rates were used for the min-max game to stabilize it, but doesn\u2019t say what they were. \nAlso, for the DQN baseline what were the parameters? Was there an epsilon greedy policy on top of the exploration reward? Was this annealed as in other work? Generally, I think more detail is needed throughout (even if it just refers to a more detailed appendix).\n\nOverall, I think this work needs to be revised to include more details on hyperaparameters, details on the baselines, and describing differences between Pathak et al.\u2019s method and the REINFORCE baseline. Moreover, feedback from other comments on this work should be addressed which reflect in more detail my comments below on opinionated claims (e.g., https://openreview.net/forum?id=SkzeJ3A9F7&noteId=HJlFlZOa2X )\n\n\nComments/Thoughts:\n\n+ I think in the introduction there are some statements that probably need citations. For example, \u201cBut the same formulation from an optimization viewpoint, it suffers from all the bad properties of extrinsic rewards. The reward is a function of environment behavior with respect to the performed action. Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.\u201d \u2014> Why is this true? Is there a citation that can back this? Do you prove it later in the paper? \n+ \u201cYes, 54 environments but no real-world physical robots\u201d \u2014> this and the intro seems like a blogpost at times. That can be fine (some would argue it\u2019s a good thing), but there seem to be some opinions without citations/backing, I suggest trying to back up statements wherever possible and avoid opinions. For example in this statement, robots aren't a requirement for evaluating intrinsic motivation.\n+ \u201cSince the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.\u201d \u2014> citation/backing? it might be nice to point to the experiment section here to back it (e.g., \"As will be shown in Section X and in \\citet{something}, REINFORCE can be quite sample inefficient\")\n+ \u201cIn practice, the existing on-policy algorithms, e.g., A3C (Mnih et al., 2016), PPO (Schulman et al., 2017) etc. are deployed off-the shelf -> This is confusing, so is this using REINFORCE or PPO/A3C? what is this statement referring to?\n+ \u201cregress to rti to learn value estimates (i.e., off-policy) as discussed in the previous section\u201d \u2014> regress to \\sum r_t{I} for a value estimate?? Value is the expected return so not sure if this is a typo or i missed something earlier\n+ What is the actual loss function used for the baseline? Is it the same as Pathak et al.?\n+ What are the hyper parameters for DQN exploration? What are all the hyper parameters for any/all the algorithms? \n+ Was a variance-reducing baseline used in REINFORCE?\n+ What is the variance representing in the graphs, std across several trials? Maybe I missed it, but how many trials represent this standard deviation?\n+ \u201cHence, we train the forward predictor slightly faster than the policy by keeping higher learning rate to stabilize the learning process. \u201c \u2014> what were the learning rates?\n\n\nLinguistic/Typos:\n\nAlso, some minor, but frequent, grammatical issues/typos that I\u2019ve added below could be fixed. I would ask that the authors please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which I\u2019ve tried to point out below. \n\n+ \u201cThis leads to a significantly sample efficient exploration policy. \u201c \u2014> significantly more (?) sample efficient ?\n\n\u201cWhy is that? To understand the reason behind sample inefficiency of curiosity or intrinsic rewards, notice how the intrinsic rewards are given by agent\u201d \u2014> by the agent?\n\n\u201cForward model f\u03b8F is trained to minimize its loss which amounts to minimizing rti with respect to \u03b8F\u201d \u2014> the forward model\n\n\u201cHowever, policy is optimized to maximize the objective\u201d \u2014> However, the policy\n\n\u201cWe can also optimize  for policy parameters \u03b8P via differentiable loss function\u201d \u2014> We can also optimize for (the) policy parameters \\theta via (a) differentiable loss function?\n\n\u201cTo optimize policy to maximize a discounted sum \u201c \u2014> To optimize the policy\n\n\u201cHow good is Forward Prediction Model\u201d \u2014> How good is the forward prediction model\n\nThere are several other spots, but basically another pass over the paper might be worth it to check for these sorts of issues. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}