{"title": "Interesting idea, but would like to see clearer set of claims with appropriate evaluation.", "review": "This work proposes an RL approach for hierarchical text classification by learning to navigating the hierarchy given a document. Experiments on 3 datasets show better performance. I'm happy to see that it was possible to \n\n1. \"we optimize the holistic metrics over the hierarchy by providing the policy network with holistic rewards\"\n\nI don't quite understand what are the \"holistic metrics\" and \"holistic rewards\". I would like the authors to answer \"what exactly does reinforcement learning get us ?\"\n - Is it optimizing F1 metric or is it the ability to fix inconsistent labeling problem ? \n- If it is the latter, what is an example of inconsistent labeling, what fraction of errors (in table 2/3) are inconsistent errors. Are we really seeing the inconsistent errors drop ?\n- If it is the former, how does this compare to existing approaches for optimizing F1 metric.\n\n2. \"the F1 score of each sample xi\"\n\na. F1 is a population metric, what does it mean to have F1 for a single sample ?\nb. I'm not aware of any work that shows optimizing per-example f_1 minimizes f_1 metric over a sample.\n\n3. with 10 roll-outs per training sample, imho, it seems unrealistic that the expected reward can be computed correctly. Would'nt most of the reward just be zero ? Or is it the case the model is initialized with an MLE pretrained parameters (which seems like it, but im not too sure).\n\nResults analysis,\n- imho, most of the rows in Table 2 does not seem comparable with each other due to pretrained word-embeddings and dataset filtering, e.g. SVM-variants, HLSTM.\n- in addition to above, there is the standard issue of using different #parameters across models which increases/decreases model capacity. This is ok as long as all parameters were tuned on held out set, or using a common well established unfiltered test set - neither of which is clear to me.\n- it is not clear how the F1 metric captures inconsistent labeling, which seems to be the main selling point for hi-lap. \n\nside comment\n- reg textcnn performance, could it be that dropout is too high ? (the code was set to 0.5)\n   ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}