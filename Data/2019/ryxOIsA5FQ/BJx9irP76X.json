{"title": "Recommend Reject as the contribution is incremental ", "review": "This paper proposed to solve the instance-based transfer learning and feature-based transfer learning by stacking with a two-phase training strategy. The source data and target data are hybrid together first to train weak learners, and then the ensembled super learner is utilized to get the final prediction. Details for the stacking process are provided. Experimental results on MNIST-USPS, COIL, and Office-Caltech datasets show the proposed method can boost the performance, compared to TrAdaboost. \n\nPros:\nThe paper proposes to using stacking or ensembling to solve the domain adaptation problem, which shows some insight for further domain adaptation research.\n\nCons:\n1. One of the main issues of this paper is the lack of novelty. The framework is incremented from the previous domain adaptation method such as TrAdaboost or BDA. For feature-based transfer learning, Equation (7)(8)(9) directly from the previous method. \n2. Some arguments in this paper are not solid. For example, in the abstract,   the authors claim that under the two-stage training architecture, the fitting capability and generalization capability can be guaranteed at the same time. However, this is not well-justified in the following literature. Another example is \"the settings of \\lamda and N should be taken into consideration, if \\lambda is too large, the performance of each learner can't be guaranteed, if \\lambda is too small, training data can't be diversified enough\" (page 7line 9~11)\n3. This paper is weakened by the experimental part. Firstly, only TraDaboost method is used as a baseline. The paper can be largely improved by comparing with the state-of-the-art ensembling method for domain adaptation, for example:\nSelf-ensembling for visual domain adaptation, Geoff French,  ICLR 2018.\nSecondly, the datasets used in this paper is small-scale and biased. It would be exciting to see how the proposed method will perform on the state-of-the-art large-scale domain adaptation dataset, for example, Office-Home dataset, Syn2Real dataset. \n\n Others:\n1. Some terminologies used in this paper are confusing: (1) the h_t and c are not defined in Equation (2). in Algorithm 2, how to construct kernel matrix K_t using k_t?   \n2. The written of this paper can be largely improved. Some sentences are grammarly mistaken. Typos examples: \nAbstract line 1: overtting -> overfitting\nSection 2.1, we use TraAdaboost -> We use TrAdaboost\n3. The citation style used in this paper is not correct.\n\nProblems:\n1. In section 2.2, what's the difference between the kernel matrix K with the unbiased estimate of MK-MMD (proposed by Gretton, NIPS 2012, also used in Deep adaptation network, Long, et al. ICML2015)?", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}