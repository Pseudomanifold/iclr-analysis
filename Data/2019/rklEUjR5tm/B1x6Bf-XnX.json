{"title": "Interesting idea but the analysis and the writing need to be improved", "review": "This paper suggests a continuous-time framework consisting of two coupled processes in order to perform derivative-free optimization. The first process optimizes a surrogate function, while the second process updates the surrogate function. This continuous-time process is then discretized in order to be run on various machine learning datasets. Overall, I think this is an interesting idea as competing methods do have high computational complexity costs. However, I\u2019m not satisfied with the current state of the paper that does not properly discuss notions of complexity of their own method compared to existing methods.\n\n1) \u201cThe computational and storage complexity for (convex) surrogates is extremely high.\u201d The discussion in this paragraph is too superficial and not precise enough.\na) First of all, the authors only discuss quadratic models but one can of course use linear models as well, see two references below (including work by Powell referenced there):\nChapter 9 in Nocedal, J., & Wright, S. J. (2006). Numerical optimization 2nd.\nConn, A. R., Scheinberg, K., & Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.\nI think this discussion should also be more precise, the authors claim the cost is extremely high but I would really expect a discussion comparing the complexity of this method with the complexity of their own approach. As discussed in Nocedal (reference above) the cost of each iteration with a linear model is O(n^3) instead of O(n^4) where n is the number of interpolation points. Perhaps this can also be improved with more recent developments, the authors should do a more thorough literature review.\nb) What is the complexity of the methods cited in the paper that rely on Gaussian processes?\n(including (Wu et al., 2017) and mini-batch (Lyu et al., 2018)).\n\n\n2) \u201cThe convergence of trust region methods cannot be guaranteed for high-dimensional nonconvex DFO\u201d\nTwo remarks: a) This statement is incorrect as there are global convergence guarantees for derivative-free trust-region algorithms, see e.g.\nConn, A. R., Scheinberg, K., & Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.\nIn chapter 10, you will find global convergence guarantees for both first-order and second-order critical points.\nb) The authors seem to emphasize high-dimensional problems although the convergence guarantees above still apply. For high-order models, the dimension does have an effect, please elaborate on what specific comment you would like to make. Finally, can you comment on whether the lower bounds derived by Jamieson mentioned depend on the dimension.\n\n3) Quadratic loss function\nThe method developed by the authors rely on the use of a quadratic loss function. Can you comment on generalizing the results derived in the paper to more general loss functions? It seems that the computational complexity wouldn\u2019t increase as much as existing DFO methods. Again, I think it would be interesting to give a more in-depth discussion of the complexity of your approach.\n\n4) Convergence rate\nThe authors used a perturbed variant of the second-order ODE defined in Su et al. 2014. The noise added to the ODE implies that the analysis derived in Su et al. 2014 does not apply as is. In order to deal with the noise the authors show that unbiased noise does not affect the asymptotic convergence. I think the authors could get strong non-asymptotic convergence results. In a nutshell, one could use tools from Ito calculus in order to bound the effect of the noise in the derivative of the Hamiltonian used in Lemma 1. See following references:\nLi, Q., Tai, C., et al. (2015). Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv preprint arXiv:1511.06251.\nKrichene, W., Bayen, A., and Bartlett, P. L. (2015). Accelerated mirror descent in continuous\nand discrete time. In Advances in neural information processing systems, pages 2845\u20132853.\nOf course, the above works rely on the use of derivatives but as mentioned earlier, one should be able to rely on existing DFO results to prove convergence. If you check Chapter 2 in the book of Conn et al. (see reference above), you will see that linear interpolation schemes already offer some simple bounds on the distance between the true gradient of the gradient of the model (assuming Lipschitz continuity and differentiability).\n\n5) Noise\n\u201cThe noise would help the system escape from an unstable stationary point in even shorter time\u201d\nPlease add a relevant citation. For isotropic noise, see\nGe, R., Huang, F., Jin, C., and Yuan, Y. Escaping from saddle points-online stochastic gradient for tensor decomposition.\nJin, C., Netrapalli, P., and Jordan, M. I. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456,\n\n6) Figure 2\nInstead of having 2 separate plots for iteration numbers and time per iteration, why don\u2019t you combine them to show the loss vs time. This would make it easier for the reader to see the combined effect.\n\n7) Empirical evaluation\na) There are not enough details provided to be able to reproduce the experiments. Reporting the range of the hyperparameters (Table 2 in the appendix) is not enough. How did you select the hyperparameters for each method? Especially step-size and batch-size which are critical for the performance of most algorithms. \nb) I have to admit that I am not extremely familiar with common experimental evaluations used for derivative-free methods but the datasets used in the paper seem to be rather small. Can you please justify the choice of these datasets, perhaps citing other recent papers that use similar datasets?\n\n8) Connection to existing solutions\nThe text is quite unclear but the authors seem to claim they establish a rigorous connection between their approach and particle swarm (\u201cIn terms of contribution, our research made as yet an rigorous analysis for Particle Swarm\u201d). This however is not **rigorously** established and needs further explanation. The reference cited in the text (Kennedy 2011) does not appear to make any connection between particle swarm and accelerated gradient descent. Please elaborate.\n\n9) SGD results\nWhy are the results for SGD only reported in Table 1 and not in the figure? Some results for SGD are better than for P-SHE2 so why are you bolding the numbers for P-SHE2?\nIt also seem surprising that SGD would achieve better results than the accelerated SGD method. What are the possible explanations?\n\n10) Minor comments\n- Corollaries 1 and 2 should probably be named as theorems. They are not derived from any other theorem in the paper. They are also not Corollaries in Su et al. 2014.\n- Corollary 2 uses both X and Z.\n- Equation 5, the last equation with \\dot{V}(t): there is a dot missing on top of the first X(t)\n\u201cSHE2 should enjoy the same convergence rate \u2126(1/T) without addressing any further assumptions\u201d => What do you mean by \u201cshould\u201d?\n- There are **many** typos in the text!! e.g. \u201cthe the\u201d, \u201cis to used\u201d, \u201cconvergeable\u201d,... please have someone else proofread your submission.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}