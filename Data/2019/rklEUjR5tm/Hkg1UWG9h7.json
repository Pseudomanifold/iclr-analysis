{"title": "Concerns about clarity, and I'm confused about the inclusion of NGD and L-BFGS as \"derivative free\"", "review": "Overall, I could potentially be persuaded to accept this paper given a relatively favorable comparison to some other blackbox optimization algorithms, but I have some serious issues about clarity and some technical details that seem wrong to me (e.g., the inclusion of L-BFGS as a \"derivative free\" baseline, and the authors' method outperforming derivative based methods at optimizing convex loss functions).\n\nI'd like to start by focusing on a few of the results in section 5.2 specifically.\nIn this section, you compare your method and several baselines on the task of training logistic regression and \nSVM models. Given that these models have convex loss functions, it is almost inconceivable to me that methods like L-BFGS and SGD (at least with decent learning rates) would perform worse than gradient free optimization algorithms,\nas both L-BFGS and SGD should clearly globally optimize a convex loss. I am also generally confused by the inclusion\nof L-BFGS as an example of a derivative free optimization problem. Are you using L-BFGS with search directions\nother than the gradient or something as a baseline? I think the exact setup here may require substantially more explanation.\n\nThe primary other issue I'd like to discuss is clarity. While I think the authors do a very good job\ngiving formal definitions of their proposed methods, the paper would massively benefit from some additional\ntime spent motivating the authors' approach. As a primary example, definition 2 is extremely confusing. I felt it wasn't as well motivated as it could have been given that it is  the central contribution of the paper. You reference an \"exploration process\" and an \"exploitation process\" that \"were shown in Eq. 4,\" but equation four is the next equation that directly jumps in to using these two processes X(t) and Y(t). These two processes are very vaguely\ndefined in the definition. For example, I understand from that definition that Y(t) tracks the current min value, but even after reading the remainder of the paper I am still not entirely sure I understand the purpose of X(t). Perhaps \nthe paper assumes a detailed understanding on the readers' part of the work in Su et al. (2014), which is cited \nrepeatedly throughout the method section?\n\nTo be concrete, my recommendation to the authors would be to substantially shorten the discussion in the paper\nbefore section 3, provide background information on Su et al., 2014 if necessary, and spend a substantially \nlarger portion of the paper explaining the derivation of SHE2 rather than directly presenting it as an ODE\nthat immediately introduces its own notation. In the algorithm block, the underlying blackbox function\nis only evaluated in the if statement on line 9 -- can the authors explain intuitively how their surrogate \nmodel evolves as a result of the Y_{t} update?\n\nIn addition to these concerns, some of the claims made in the method section seem strange or even wrong to me,\nand I would definitely like to see these addressed in some way. Here is a list of a few concerns I have\nalong this line:\n\n- A few of the citations you've given as examples of derivative free optimization are confusing.\nYou cite natural gradient methods and L-BFGS as two examples, but natural gradient descent involves preconditioning\nthe gradient with the inverse Fisher information matrix, and is therefore typically not derivative\nfree. You give Gaussian process surrogate models as an example of a convex surrogate, but GPs\nin general do not lead to convex surrogates save for with very specific kernels that are not\noften used for Bayesian optimization.\n\n- In the background, it reads to me like you define GP based Bayesian optimization as a quadratic\nbased trust region method. This seems strange to me. Trust region methods do involve quadratic surrogates,\nbut my understanding is that they are usually local optimization schemes where successive local quadratic \napproximations are made for each step. GP based Bayesian optimization, by contrast, maintains a global\nsurrogate of the full loss surface, and seeks to perform global optimization.\n\n- Equation 3 defines the squared norm \\frac{1}{2}||X-Y||^{2}_{2} as the \"Euclid[ean] distance\".\nBased on the following derivatives, I assume this is intended to be kept as \nthe squared Euclidean distance (with the 1/2 term included for derivative simplicity).\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}