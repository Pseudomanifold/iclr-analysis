{"title": "The detailed analysis of the training of DNN with the batch normalization is quite interesting. ", "review": "\nThis paper investigates the effect of the batch normalization in DNN learning.\nThe mean field theory in statistical mechanics was employed to analyze the\nprogress of variance matrices between layers. \nAs the results, the batch normalization itself is found to be the cause of gradient explosion. \nMoreover, the authors pointed out that near-linear activation function can improve such gradient explosion. \nSome numerical studies were reported to confirm theoretical findings.\n\nThe detailed analysis of the training of DNN with the batch normalization is quite interesting. \nThere are some minor comments below.\n\n- in page 3, 2line above eq(2): what is delta in the variance of the multivariate normal distribution?\n- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3. \n- The randomized weight is not very practical. Though it may be the standard approach of mean field,\nsome comments would be helpful to the readers. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}