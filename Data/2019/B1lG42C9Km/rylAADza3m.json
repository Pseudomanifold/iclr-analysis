{"title": "Review", "review": "INTRINSIC SOCIAL MOTIVATION VIA CAUSAL INFLUENCE IN MULTI-AGENT RL\n\nMain Idea: The authors consider adding a reward term to standard MARL which is the mutual information between its actions and the actions of others. They show that adding this intrinsic social motivation can lead to increased cooperation in several social dilemmas. \n\nStrong Points:\n-\tThis paper is a novel extension of ideas from single agent RL to multi agent RL, there are clear benefits from doing reward shaping in the right way to make deep RL work better.\n-\tThe paper focuses on cooperative environments which is an underfocused area in RL right now\n \nWeak Points:\n-\tThere is missing discussion of a lot of literature. The causal influence term can be thought of as a form of reward shaping. There is little discussion on the (large) literature on reward shaping to get MARL to exhibit good behavior. \n-\tThe results feel quite thin. Related to the point above: the theory of different types of reward shaping (e.g. optimistic Q-learning, prosociality, etc\u2026) are well understood. It is not clear to me under what conditions the authors\u2019 proposed augmentation to the reward function will lead to better or worse outcomes. The experiments in this paper are quite simple and only span a small set of environments so it would be good to have at least some formal theory.\n-\tSocial dilemmas don\u2019t seem like the best application. The authors define the social dilemma as: \u201cFor each individual agent, \u2018defecting\u2019 i.e. non-cooperative behavior has the highest payoff.\u201d With the intrinsic motivation, agents learn to cooperate. This is good, however, if we\u2019re thinking about situations where agents aren\u2019t trained together and have their own rewards (the authors\u2019 example: \u201cautonomous vehicles are likely to be produced by a wide variety of organizations and institutions with mixed motivations\u201d) then won\u2019t these agents be exploited by rational agents? Other solutions to this problem (e.g. recent papers on tit-for-tat by Lerer & Peysakhovich or LOLA by Foerster et al. construct agents where defectors get explicitly punished and so don\u2019t want to try exploiting). Is there something I am missing here? Do the agents learn to punish non-cooperators (if no, isn\u2019t it rational at that point to just not cooperate and won\u2019t self-driving cars trained via this method get exploited by others)? \n-\tRelate to the point(s) above: a better environment for application here seems to be coordination games/\u201dStag Hunt\u201d games where it is known that MARL converges to poor equilibria and many other methods e.g. optimistic Q-learning or prosociality have been invented to make things work better. Perhaps the method proposed here will work better than these (and it has the appealing property that it does not require the ability to observe the other agents' rewards as e.g. prosociality does)\n-\tThis paper contains some quite grandiose language connecting the proposed reward shaping to \u201chow humans learn\u201d (example: It may also have correlates in human cognition; experiments show that newborn infants are sensitive to correspondences between their own actions and the actions of other people, and use this to coordinate their behavior with others) it\u2019s unclear to me that humans experience extra reward for their actions having high mutual information (and/or causal information with others). While it\u2019s fine to argue some of these points at a high level I would suggest scrubbing the text of the gratuitous references to this.\n\nNits:\n\u201cCrawford & Sobel (1982) find that once agents\u2019 interests diverge by a finite amount, no communication is to be expected.\u201d \u2013 this is an awkward phrasing of the Crawford and Sobel result (it can be read as \u201cif interests diverge by any epsilon there can be no communication\u201d). The CS result is that information revealed in communication (in equilibrium) is proportional to amount of common interest. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}