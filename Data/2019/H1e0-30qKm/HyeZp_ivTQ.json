{"title": "Interesting idea but incomplete justifications", "review": "[Edit] I changed my rating from 4 to 5 based on the author responses.\n=======\nThis paper proposed a GAN that learns a disentangled factors of variations in unsupervised (or weakly-supervised) manner. To this end, the proposed method incorporates a contrastive loss together with Siamese network, which encourages the generator to output smaller variations in samples if they are drawn by varying the same latent factors. The proposed idea is evaluated on simple datasets such as MNIST and centered faces, and show that it is able to learn disentangled latent codes by incorporating some heuristics. \n\nAlthough the paper presents an interesting and reasonable idea, I think the paper is incomplete and in the proof-of-concept stage. In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper. In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. \n\nIn conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}