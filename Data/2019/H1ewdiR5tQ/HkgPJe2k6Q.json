{"title": "Promising but need further development", "review": "This paper proposed a new formulation of graph convolution, that is based on\ngraph wavelet transform. The convolution network is expressed in eq.(4)\nwith \\psi_s given by eq.(1). This new formulation is exciting in that\nit has numerous advantages comparing to spectral graph convolutions\nsuch as the sparsity of \\psi_s (see the items listed in section 2.3).\nThe method showed marginal improvement (<1%) on node classification\ntasks of citation networks.\n\nOverall, I am convinced with the technical novelty and that the method\ncan be promising. However, in its current form, there are several\nmajor weaknesses, hinting that this work needs further developments before\npublication.\n\n1. By the vanilla implementation without any approximations, the multiplication\nwith \\phi_s and \\phi_s^{-1} has quadratic complexity, and to compute these two\nmatrices is of cubic complexity. \nThis is not acceptable for large graphs. In section 3.2, the authors\nmentioned that Hammond et al (2011) have an approximation of\n\\phi_s and \\phi_s^{-1} with the complexity of O(|E|).\nHowever, this important technical detail is disappointingly missing\nin the main text. I have further concerns about whether the good\nproperties listed in section 2.3 are preserved by this approximation,\nwhich is not discussed in the paper.\n\nEven with such an approximation, the matrix multiplication still\nhas quadratic complexity. To reduce this complexity needs some non-trivial\ndevelopments. I suggest the author(s) dive into the expression of\n\\phi_s and write the convolution in the vertex domain and seek possibilities\nfor a linear approximation.\n\n2. Experimentally, the improvement over GCN is marginal. Taking into account\nthe implementation difficulty and complexity, the\nproposed method is more like a proof-of-concept instead of\nbeing of practical use. The authors are suggested to make the\nempirical study more comprehensive, by including node classification\nin an inductive setting, and/or including link prediction experiments. \n\n3. The hyper-parameters (scale of the heat kernel) and t (threshold\nto zero the \\phi_s matrix) has to be tuned for each data set.\nThis is not the ideal case because these parameters may not be easy\nto tune for real data sets, making the method difficult to use.\nThe authors should at least give some recipes on how to tune these parameters.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}