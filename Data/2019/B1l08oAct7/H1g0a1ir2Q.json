{"title": "Fixing Variational Bayes: Deterministic Variational Inference for Bayesian Neural Networks", "review": "This paper considers a purely deterministic approach to learning variational posterior approximations for Bayesian neural networks.  Variational lower bound gradients are obtained by approximating the lower bound using Gaussian approximations and moment propagation for network activations, and using a closed form expression for the variational expectation of the log-likelihood, the latter being available for the models considered in the paper.  \n\nThis is an interesting paper.  The Gaussian approximations and moment propagation approximations are clever and highly original although the derivation is rather heuristic.  There is some empirical support that the approximations work well.  The paper is generally well written and clearly motivated in the context of the existing literature.\n\nThe approximations work well for the examples presented in the paper.  The experiments are for rather small datasets and for the DVI method if I understand correctly only models with a single hidden layer are considered.  I wonder if the Gaussian and moment propagation approximations cause difficulty when applied repeatedly in deeper networks.  Are the problems with MCVI and high gradient variance most serious for large datasets and more complex models?  If so a comparison of DVI with MCVI in a more complex example is of interest.  The empirical Bayes approximations are interesting - I would have thought similar approximations been used in the literature before, in addition to the work you mention in Section 5?  I don't feel there is much to compare the proposed EB approximations to, although a comparison with manual tuning is given in Section 6.  \n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}