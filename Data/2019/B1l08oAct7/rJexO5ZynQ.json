{"title": "Two advances for variational Bayes on neural networks. Expectations are done deterministically (as in PBP), not by Monte Carlo, thus reducing variance. The weight prior is learned with length scales by empirical Bayes. Both should make VB training more robust, but experiments do not show that.", "review": "Summary:\n\nThis work is tackling two difficulties in current VB applied to DNNs (\"Bayes by backprop\"). First, MC approximations of intractable expectations are replaced by deterministic approximations. While this has been done before, the solution here is new and very interesting. Second, a Gaussian prior with length scales is learned by VB empirical Bayes alongside the normal training, which is also very useful.\n\nThe term \"fixing VB\" and some of the intro is not really supported by the rather weak experiments, done on small datasets and networks, where much older work like Barber&Bishop would apply without any problems. While interesting and potentially very useful novelties are presented, and the writing is excellent, both experiments and motivation can be improved.\n\n- Quality: Extremely well written paper, I learned a lot from it. Approximations are\n   tested, great figures to explain things. And the major technical novelty, the\n   expression for <h_j h_l>, is really interesting and useful.\n- Clarity: Excellent writing until it comes to the experiments. Here, important\n   details are just missing, for example what q(w) is (fully factorized Gaussian?).\n   Very nice literature review, also historical.\n- Originality: The idea of matching Gaussian moments along the network graph is\n   previously done in PBP (Lobato, Adams), as acknowledged here. Porting this from\n   ADF to VB gives dDVI. PBP also has the property that a DL system gives you the\n   gradients. Having said that, I think dDVI may be more useful than PBP.\n   While Barber&BIshop 98 is cited, they miss the expression for <h_j h_l> in\n   there. Now, what is done here, is more elegant, does not need 1D quadrature.\n- Significance: Judging from the existing experiments, the significance may be\n   rather small, *if one only looks at test log likelihood*. I'd still give this the\n   benefit of the doubt, as in particular dDVI could be really interesting at large\n   scale as well. But the authors may tone down their language a bit.\n   To increase significance, I recommend to comment beyond just test log\n   likelihood scores. For example:\n   - Does the optimization become simpler, less tuning required, more automatic?\n      Would one not expect so, given you make a big point out of reducing variance?\n      Does it converge faster?\n   - Can you do something with your posterior that normal DNN methods cannot\n      do? Better decisions (bandits, active learning, HPO)? Continual learning?\n      In the end, who really cares about test log likelihood?\n\nExperiments:\n- What is the q(w) family being used here? Fully factorized Gaussian? I\n   suppose so for dDVI. But for DVI? Not said anywhere, in main paper or\n   Appendix\n- A bit disappointing. Why not evaluate at least dDVI with diagonal q(w) on\n   some much larger models and datasets? Why not quote numbers on speed\n   and robustness of learning, etc? Show what you really gain by reducing the\n   variance.\n- Experiments are OK, but on pretty small datasets, and for single hidden\n   layer NNs. On such data and models, the Barber&Bishop 98 method could\n   be run as well\n- Was MCVI run with re-parameterization? This is really important. If not,\n   this would be an important missing comparison. Please be clear in the main\n   text\n- Advantages over MCVI are not very large. At least, dDVI should be faster to\n   converge than MCVI.\n   Can you say something about robustness of training? Is it easier to train\n   dDVI than MCVI?\n- Why not show the PBP-1 results, comparing to dDVI, in the main text? Are they\n   obtained with the same model? dDVI is doing better.\n\nOther points:\n- Please acknowledge the <h_j h_l> expression in Barber&Bishop 98. Yours is\n   more elegant and faster (does not need 1D quadrature)\n- Relation to PBP: Note that dDVI has an advantage in practice. With PBP, I need\n   to compute gradients for every datapoint. In dDVI, I can do mini-batch\n   updates.\n- I just *love* the header \"Wild approximations\". I tend to refer to this kind of work\n   as \"weak analogies\". Why do you not also compare against this, and show it really\n   does not work?\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}