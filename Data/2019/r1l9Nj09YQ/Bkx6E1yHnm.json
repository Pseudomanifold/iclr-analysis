{"title": "Good method and results overall, with a few questions on analysis", "review": "This paper introduced a GAN-based method to learn language universal representations without parallel data. The model architecture is analogous to an autoencoder. The encoder is a compound of language-universal mapper plus a language-specific LSTM. For decoding, another language-universal module first map language-universal representation back to language-specific embedding space, then another LSTM decoder generates the original sentence. The authors used GAN to encourage intermediate representation to be language-universal. The authors tested the proposed method on zero-shot semantic analysis and NLI tasks and showed nice results.\n\nOverall the proposed method is novel and nice, and experiment results are good. On both tasks the proposed method performs better than NMT methods on target languages while still achieving competitive performance on source languages. The paper is also clearly written and could be useful for future research on multilingual transfer.\n\nMy main complaint is around Figure 5, Table 3, and the corresponding analysis.\n1. In Figure 5, does it make more sense to show the perplexity of a standard LM. That is, train 7 independent LMs and report averaged perplexity. My concern is that, even with \\lambda=0.0, the model still have modules u and h that are shared across languages, and therefore I'm not sure if it implies \"representative power of UG-WGAN grows as we increase the number of languages\". It could be that the language-universal impose more constraints to model all languages, so the two variation (\\lambda=0.0 or 0.1) come closer to each other.\n\n2. In Figure 3, the perplexity difference is huge when number of languages is 2. In Table 3, however, the authors show no fundamental differences between the English and Spanish language models. I feel the two arguments contradict to each other. Is it because of the language pairs are different? The authors should provide more explanation on that.\n\nMinor:\n1. Equation 1 and 2 in page 2. Are they both compound functions? Why the first one use \\circ and the second one use parenthesis?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}