{"title": "Interesting analysis of quantifier interpretation in a VQA model, but the theoretical discussion is unsatisfying", "review": "The paper analyzes the strategy that a visual question answering model (FiLM) uses to verify statements containing the quantifier \"most\" (\"most of the dots are red\"). It finds that the model is sensitive to the ratio of objects that satisfy the predicate (that are red) to objects that do not; as the ratio decreases (e.g. 10 red dots compared to 9 blue dots), the model's performance decreases too. This is consistent with human behavior.\n\nStrengths:\n* The introduction lays out an ambitious program of comparing humans to deep neural networks.\n* The experimental results are interesting (although of modest scope) and support the hypothesis that the network is not counting the objects but rather is using an approximation that is sensitive to the ratio between the red and non-red items.\n\nWeaknesses:\n* The architecture of the particular model is described very briefly, and at multiple points there\u2019s an implication that this is an investigation of \u201cdeep learning models\u201d more generally, even though those models may vary widely. While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model. I would like to see more discussion of whether it is at all plausible for this model to acquire the pairing strategy, compared to alternative VQA models (e.g., using relation networks).\n* I found it difficult to follow the theoretical motivation for performing the work. The goal seems to be to test whether the network is performing the task in way that \"if not human-like, at least is cognitively plausible\". I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue. Later in the same paragraph, the authors argue that \"in the case of a human-centered domain like natural language, ultimately, some degree of comparability to human performance is indispensable\". This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge \"some degree of\" is really neither here nor there). In general, I don't understand why we would want a visual question answering system that returns approximate answers -- isn't it better to have it count exactly how many red dots there are compared to non-red dots?\n* The authors assume that explicit counting is not \"likely to be learned by the 'one-glance' feed-forward-style neural network\" evaluated in the paper. What is this statement based on? Why would a \"one-glance\" network have trouble counting objects? (What is a \u201cone-glance network\u201d?)\n* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can \"learn and utilize higher-level concepts than mere pattern matching\". What is \"pattern matching\" and how does it differ from \"higher-level concepts\"?\n* Why would the pairing strategy in a neural network be affected by the clustering of the objects? I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.\n\nMinor comments:\n* Is the definition of \"most\" really a central piece of evidence for \"the apparent importance of a cardinality concept to human cognition\"? Our ability to count seems sufficient to me. Perhaps I'm not understanding what the authors have in mind here.\n* Please use the terms \"interpretation\" and \"verification\" consistently.\n* \"One over the other strategy\" -> \"one strategy over the other\".\n* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}