{"title": "Paper is smooth to read, but important issues remian.", "review": "The paper addresses RL in the continuous action space. The scope is right for ICLR. The main two claimed contributions of the paper are the use of a re-parametrised policy and the use of a novel vector-based training objective (similar to solving each component of a factored MDP).\n\nI have the following concerns about the paper.\n\n(i) First, section 3 seems to re-invent the notion of a re-parametrised probability distribution. Of course you can get any well-behaved probability distribution from the uniform distribution by applying some non-linear transformation. That has, AFAIK been known since probability theory with continuous random variables has been formulated. There is a lot of work in the variational auto-encoder community right now on learning transformations like this. I do not exactly see what the purported novelty of section 3 is. The last equation on page (4) states that the derivative of the inverse function is the reciprocal of the derivative of a function, a well-known fact from calculus (and you cite a paper from 1992 to corroborate it?!). The other equations on page 4 and on top of page 5 seem to manually re-do the math of back-propagating through networks with random inputs, for a special case - this has been known for quite some time as well.\n\n(ii) I do not understand why the learned function \\hat{Q} has to be monotonic (the point of the constraint in equation (2)). Sure, you won't get a correspondence with a CDF if it's not, but why is this a problem? You would still be learning some probability distribution that does what you want?\n\n(iii) Section 4 augments the information available to the agent with the component-wise absolute value between the current position and the goal position and uses that to (purportedly) improve performance. I see two problems with this. On a practical side, this restricts the applicability of the algorithm to settings with clear goal states where the state space is Euclidean (or similar). Also, if we know the goal state, there is the question of not just feed it to the agent directly (as a part of the observation) and avoid having the complicated vector reward structure at all? I do not see a compelling reason for this. On a theatrical side, this can only really work if the state space is (a subset of) R^n. Also, if we can improve each action without regard for other actions, (as the algorithm seems to assume) doesn't this correspond to some kind of assumption that the MDP is factorised?\n\n(iv) The paper has problems with presentation. It is well-written as far as the English goes and it is superficially smooth to read, but the concepts which are crucial to understand the main point are very confounded. There is no pseudocode that explains how the algorithm works. I feel that the paper should include an equation literately states all the loss terms being optimised.\n\n(v) The paper takes a cavalier approach to formalism. It mentions arbitrary metric spaces several times, as well as the notion of dimension induced purely by metric structure, but the equations only really address the Euclidean vector space. This is OK and this is what most RL algorithm do, but you shouldn't claim that you algorithm is more general than what is actually defined in the paper.\n\nUnfortunately, as the paper stands now, I have some concerns about whether it is suitable for publication.\n\nI wanted to encourage the authors to address the following points during the discussion phase. Also, if you feel the review misunderstands the paper, please clarify - I am willing to update my score if the reply is convincing.\n\nSuggestions:\n\n1. Include an equation that mentions all the loss terms in the algorithm.\n2. Figure 2 should be augmented to include where the loss signal comes from.\n3. Contrast section 3 with variational auto-encoders [2].\n4. Contrast section 3 with re-parameterised policy gradients. \n5. Explain why the monotonicity constraint is necessary in section 3.\n5. Contrast the setting of vector reward with literature on factored MDPs [1].\n6. Remove inflated claims about applicability to general metric spaces.\n\n[1] Guestrin et al. E\ufb03cient Solution Algorithms for Factored MDPs\n[2] Doersch. Tutorial on variational autoencoders (and further references therein)\n\n\n\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}