{"title": "An attempt at exploiting the reward structure that unfortunately lacks both strong theoretical and empirical support", "review": "The algorithm presented here aims at speeding up (reinforcement) learning in situations where the reward is \u201caligned\u201d with the state space, i.e. can be decomposed into multiple components where each component depends only on one dimension of the state. Such a situation is found e.g. in tasks where the agent needs to reach a specific position in a 2D or 3D space, and the reward can be decomposed into a 2D / 3D reward vector associated to reaching the target coordinate along each axis. The algorithm consists in first pre-training (by collecting data obtained with random action distributions) a so-called \u201cPosition Change Prediction Network\u201d (PCPN) to map an action distribution (e.g. mu and sigma if the agent\u2019s policy is Gaussian) to the corresponding state change distribution along each relevant state dimension (for which there is an associated reward). The PCPN is an Implicit Quantile Network (Dabney et al, 2018), so as to model the state change distribution without assuming a specific parametric form. Once trained, the PCPN can be backpropagated through to train the agent distribution so that state changes that lead to better rewards (along each axis) become more frequent, while those leading to worse reward become less frequent (where \u201cbetter\u201d and \u201cworse\u201d are defined by the advantage, computed with a critic V). Experiments are performed on three toy tasks, and comparisons to A2C show faster learning for the proposed algorithm, especially in higher dimensional state spaces.\n\nThe proposed approach combines two interesting ideas: (1) decomposing the reward into multiple components, and (2) aligning the agent\u2019s action distribution with these components (through the PCPN). The second point is novel to the best of my knowledge. However, the first one is not since there exists a large body of literature on multi-objective reinforcement learning. The authors briefly mention some of it in the \u201cRelated Work\u201d section, but unfortunately there is no comparison being made in experiments (the only algorithm being compared against is a \u201cvanilla\u201d A2C that uses a single scalar reward). Although previous work on multi-objective RL may not necessarily take advantage of the alignment between reward components and state dimensions, it is still relevant since the decomposition of the reward function by itself might be enough to speed-up learning, regardless of whether or not it is aligned with the state space. Experiments need to show that using the alignment as proposed here actually brings some benefits over existing multi-objective techniques.\n\nIn addition to the lack of comparison to related multi-objective methods, the empirical evaluation is not very convincing since it is done on toy problems, with the most \u201crealistic\u201d one being a simple 2D task where the improvement brought by the proposed technique is not obvious (Fig. 5). Better results are obtained on high dimensional toy problems but (a) in these problems the relationship between actions and reward components is somewhat trivial, and (b) since the paper was originally motivated by spatial positioning tasks, it is not clear how often in practice one will need to go beyond 2 or 3 dimensions... Finally, since there is no theoretical analysis either (except for some intuition as to what happens in a limit case at the bottom of p. 4), and the optimized objective is essentially heuristically motivated, it is hard to tell how the algorithm would behave across a larger range of applications.\n\nI have two concerns in particular regarding the methodology:\n- Pre-training the PCPN assumes that randomly sampling action distributions will give us enough coverage of the state and action distribution spaces to generalize to those seen during training of the agent. This may not always be the case, requiring training the PCPN online: it would have been nice to verify whether this is feasible in practice.\n- I am afraid that forcing the agent to learn only from the PCPN gradient may prevent it from learning an optimal policy. As a dummy illustrative example, imagine an infinite 2D gridworld where the agent has two actions (action A = increase x coordinate by 1 only if y = 0, action B = increase both x and y by 1 only if y = 0), the reward is +1 when x increases and the agent starts at the origin. The reward is aligned with the x axis, and the optimal policy is to always take action A since as soon as action B is taken the agent can\u2019t move anymore. With the PCPN output being the change along the x axis, its gradient wrt the agent action distribution is zero since both actions A and B increase x equally. Thus the agent will never be able to learn that A is to be preferred. In other words, it seems to me that the agent is limited to learn from changes in the state coordinates aligned with the reward, even if other coordinates are important for transition dynamics that may impact future rewards.\n\nRegarding the clarity of the paper, I found it good enough overall except that it is a bit difficult to follow the narrative. In particular Section 3 breaks the flow by considering a more general setting than the one introduced previously (the note about \u201caction\u201d and \u201cpolicy\u201d meaning something else is particularly confusing). Using Q to denote the action distribution is also a poor idea since Q is pretty much a \u201creserved\u201d letter in RL. Overall the generic idea of \u201cQuantile Regression RL\u201d from Section 3 seems potentially interesting, and may be worth exploring on its own, but the current presentation (in the application to multi-objective state-aligned rewards) makes it more a distraction than an asset.\n\nMinor points:\n- The \u201cbiological insights\u201d from the introduction are difficult to grasp since a reader versed in RL is unlikely to know what are \u201cviral vector strategies\u201d or \u201ceffector specific value estimations\u201d\n- The definition of aligned spaces (\u201ctheir dimensions correlate\u201d) is pretty vague, a more precise mathematical definition would be helpful (even if the intuition is easy to grasp)\n- In the first one-line equation p. 4 there is a tau that should be a_tau\n- In the definition of R_t,n on p. 5, the sum should be up to t+n-1 (and same with the vector version at the bottom of the page)\n- I wonder if the loss L_mon is really needed, since the quantile regression loss should naturally lead to a monotonous function (assuming enough data and capacity). Are results significantly worse without it?\n- The definition of the Huber loss is incorrect \n- It is not explicitly stated that there is no backpropagation through V_psi in L_a\n- \u201cQRRL can easily be adapted to the off-policy setting\u201d: this is not obvious since you are using n-step returns bootstrapped with a critic V, which requires on-policy data (unless appropriate corrections are applied)\n- Experiments do not clearly define the rewards used by both algorithms \n- Figures would be easier to read if axes were labeled\n- I am surprised by the absence of entropy loss in A2C (beta = 0), was this hyper-parameter carefully tuned?\n- Having diagrams of the networks in Appendix A would be clearer than text explanations\n- The \u201ccosine embedding\u201d should be defined to avoid having to refer to Dabney et al\n- Note that the proposed model ends up having significantly more capacity than the single A2C network, which could raise concerns wrt the fairness of the comparison", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}