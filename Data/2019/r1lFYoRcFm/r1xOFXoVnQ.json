{"title": "The idea is interesting but the paper should backed with more complete experiments", "review": "This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles (PCPN in the paper) . The claim is that this improves the sample efficiency because the backpropagation of collected rewards takes advantage of distributional knowledge we have about the evolution of the world.\n\nI like the idea but if we have a good model of the dynamics of the world (the PCPN) then (stochastic) dynamic programming is likely to be a very hard to beat baseline. This is one of the reason why many RL approaches are avoiding an explicit model of the world.  The presence of this model is very central in the performance of the proposed approach and I do not think that the experiments are conclusive enough. More specifically I think the paper would be better if it would answer some questions \n- How the performance is affected wrt the error made by the PCPN. In the current architecture if the PCPN is bad for some reason then there is no way to recover. Thus the approach cannot be used if we are not able to fit the dynamics\n- More baselines would be added. In particular in this kind setting PPO is known to perform well (but many other could be added ACKTR, interpolated policy gradient, PPO with Stein control variate,...). I know there is possibly too many to run a reasonable experiment but with only A2C as a competitor it is not even possible to know if the extra performance comes from the distributional learning or from the use of the \"aligned states\"\n- The task is not very appealing: classical control often work better on theses tasks (with are variants of acrobot already available as Gym environments). To assert the validity of the approach it could be worth to use Atari tasks (but this is expensive) or some Mujuco/Robotics ones ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}