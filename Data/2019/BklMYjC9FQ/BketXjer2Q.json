{"title": "Interesting approach; theory and experiments not entirely convincing", "review": "The submission proposes to increase the variety of generated samples from GANs by a) using an ensemble of discriminators, and b) tasking them with distinguishing not only fake from real samples, but also their fake samples from the fake samples given to the respective other discriminators. The cost function of the minimax GAN optimization problem is changed accordingly. Experimental results suggest that this approach leads to improved results, both visually and w.r.t. FID metric.\n\nImproving on the well known problem of mode collapse in GAN training scenarios is without a doubt an important endeavor. Various methods have been proposed, as curtly summarized in Section 1.1 of the submission.\n\nWith respect to the proposed method, I am not completely clear on how it can increase sample variety, if it does. In understand the arguments brought forward in Appendix B, but:\nConsider a minimax game involving one generator G and one discriminator D, where each batch of generated samples from G(Z), is split up into two parts, A and B, via selection without replacement using uniform sampling. Z is a matrix of noise inputs, where each column corresponds to one item of the batch. D is now tasked to differentiate whether a sample came from A or B. It seems intuitive to say that in this case, D can neither win, nor provide any useful signal to G, since the sets A and B were split randomly, and there is no influence on G during training. The variety of samples in A as well as in B will be identical to the variety in the set (A and B).\n\nYet this random microbatch splitting is what seems to be happening here, if I understood Section 3 correctly; just with an ensemble of discriminators, and not just with one.\nWhile it is thus not completely clear to me *why* the proposed additional term seems to bring increased variety, experiments strongly suggest that it does.\nAs described in Section 3.1, choice of the weighting parameter alpha seems crucial, and additionally alpha needs to depend on the iteration index. Different schedules are demonstrated, but optimality of either is not guaranteed. This makes the actual influence of the additional loss term even harder to judge and evaluate.\n\nSection 4.1 seems to confirm the increase in variety via the self-defined \"Intra FID\" measure. I would have liked to see this measure evaluated on conventionally trained GANs as a baseline, as well on the methods compared to in Section 5.\nIn Table 1, both min and mean FID are given over 50k iterations. Instead of reporting the minimum, it might be fairer to compare FIDs after a fixed number of iterations (i.e. 50k in this case).\n\nThe method comparison in Section 5 is generally appreciated, but I think some of its flaws are:\n- Datasets, including ImageNet, are all downsampled to 32x32 pixels. We have seen generators in recent work that produce interesting high-resolution output in even megapixel size; the tiny size seems like a pessimization of overall approaches.\n- The proposed method is compared to other methods using only 2 discriminators, although Section 4.3 suggest a larger number is better.\n- MicroGAN does not compare favorably to many of the compared to methods in Table 2. This may not necessarily by a flaw of the MicroGAN contributions, but is rather a problem of an apples-to-oranges comparison, as the authors readily admit (\"the use of more powerful architectures [...] plays a big role\"). I question the value of such a comparison, if not only the method differ, but also implementation details such as network architectures.\n\nOverall the submission is quite interesting, but not without the above-mentioned flaws.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}