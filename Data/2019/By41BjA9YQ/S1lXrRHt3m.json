{"title": "Theory and experiments can be improved", "review": "This paper proposed a variant of gradient descent that can be approximately understood as gradient descent on a smoothed version of the objective function. The motivation of this work is finding flat minima which could imply better generalization ability of a machine learning model.\nCompared with gradient, the proposed algorithm uses gradient multiplied by a special constant square matrix as update direction. The complexity of the matrix vector multiplication is brought down from O(d^2) to O(d*logd) by exploiting special structure of the matrix using FFT. It is proved that the new update vector has smaller variance and amplitude compared to gradient. \nExperiments on different applications showed that the proposed algorithm may have better generalization ability compared with SGD.\nThis is a clearly written paper, but I have a few questions about theoretical gaps and simulation results in the paper.\n\na). It seems the smoothing explanation at the beginning of section 2 is for implicit scheme (equation (3)). However, the explicit scheme used in practice (the first unnumbered equation in section 2.1) uses a heuristic relaxation which makes the smoothing explanation \u201capproximate\u201d for the explicit scheme. Since the implicit scheme is much more complicated than the explicit scheme, I don\u2019t know if the argument for the implicit scheme will \u201capproximately\u201d hold for the explicit scheme used in practice.\n\nb). The concept flat minimum is only useful in nonconvex optimization, but the convergence of the algorithm is only proved in convex setting. Since the main motivation of the algorithm is finding flat minima, the lack of convergence proof for nonconvex setting concerns me.\n\nc). In the neural net experiment in section 4.1, both gradient descent and smooth gradient descent use the same stepsizes. It is known that the performance of gradient descent is sensitive to the choice of stepsizes, for a fair comparison, one should compare the performance of the two algorithms using optimized stepsizes.\n\nd). In the experiment in section 4.2, the proposed algorithm is only used for the first 40 epochs during training and SGD is used for the later phase of training. Why switching to SGD later? \n\nOverall, I feel the idea of this paper is interesting, but the theory and experiments in the paper are not very strong.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}