{"title": "Interesting work", "review": "The authors propose a new network architecture for multi-agent reinforcement learning. The new architecture addresses three issues: (1) the applicability of existing algorithms to semi-cooperative or competitive settings; (2) the ability to use local rewards during agent training; (3) the credit assignment problem with global multi-agent rewards. The authors address these issues with a new architecture that is comprised of several LSTM controllers with tied weights that transmit a continuous vector to each other, and that are augment with a gating mechanism that allows them to abstain from communicating.\n\nI think that this paper makes a solid contribution over the existing literature. My main comments are the following:\n* I feel like the paper can be strengthened by comparing to additional baselines. The authors compare mainly to Sukhbataar et al., but I think a more detailed comparison to other approaches (e.g. Foerster et al.)\n* One of the advantages of this method is that it can be used in non-cooperative settings. I am not familiar with this regime, and I would like a better explanation about why we would train competing agent with the same controller, rather than using a different controller for each team.\n* In several experimental results, the proposed method seems to have significantly higher variance than the baselines. I would like to see some discussion about why it is the case.\n* Also, in some places (e.g. Table 1), the method is highlighted in bold, even though it doesn\u2019t actually outperform the baseline. Please correct this and only highlight the best method (if several methods are tied, either highlight them one, or don\u2019t highlight any).\n* Also, in some cases when the error bars contain the previous best result, I am not sure if we can say that the proposed method is obviously better.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}