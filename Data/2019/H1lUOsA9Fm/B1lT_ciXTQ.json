{"title": "SoundFont-rendered audio is too restricted a domain on which to draw conclusions about music generation", "review": "This paper proposes several architecture changes to a WaveNet-like dilated convolutional audio model to improve performance for MIDI-conditioned single-instrument polyphonic music generation.\n\nThe experimental results and provided samples do clearly show that the proposed architecture does well at reproducing the sounds of the training instruments for new MIDI scores, as measured by CQT error and human preference.  However, the fact that the model is able to nearly-exactly reproduce CQT is contrary to intuition; given only note on/off times, for most instruments there would be many perceptually-distinct performances of those notes.  This suggests that the task is too heavily restricted.\n\nIt isn't clearly stated until Section 4 that the goal of the work is to model SoundFont-rendered music.  (The title \"SynthNet\" is suggestive but any music generated by such an audio model could be considered \"synthesized\".)  Using a SoundFont instead of \"real\" musical recordings greatly diminishes the usefulness of this work; adding and concatenating outputs from the single-note model of Engel et al. removes any real need to model polyphony, and there's no compelling argument that the proposed architecture changes should help in other domains.\n\nOne change that could potentially increase the paper's impact is to train and evaluate the model on MusicNet (https://homes.cs.washington.edu/~thickstn/musicnet.html), which contains 10+ minutes of recorded audio and aligned note labels for each of ~5 single instruments (as well as many ensembles).  This would provide evidence that the proposed architecture changes improve performance on a more realistic class of polyphonic music.\n\nAnother improvement would be to perform an ablation study over the many architecture changes.  This idea is mentioned in 4.2 but seemingly dismissed due to the impracticality of performing listening studies, which motivates the use of RMSE-CQT.  However, no ablation study is actually performed, so it's not obvious what readers of the paper should learn from the new architecture even restricted to the domain of SoundFont-rendered music generation.\n\n\nMinor points / nitpicks:\n\nOne of the claimed contributions is dithering before quantization to 8-bits.  How does this compare to using mixtures of logistics as in Salimans et al. 2017?\n\nS2P3 claims SynthNet does not use note velocity information; this is stated as an advantage but seems to make the task easier while reducing applicability to \"real\" music.\n\nS4P1 and S4.1P4 state MIDI is upsampled using linear interpolation.  What exactly does this mean?  Also, the representation is pianoroll if I understand correctly, so what does it mean to say that each frame is a 128-valued vector with \"note on-off times\"?  My guess is it's a standard pianoroll with 0s for inactive notes and 1s for active notes, where onsets and offsets contain linear fades, but this could be explained more clearly.\n\nWhat is the explanation of the delay in the DeepVoice samples?  If correcting this is just a matter of shifting the conditioning signal, it seems like an unfair comparison.\n\nS1P2 points (2) and (3) arguing why music is more challenging than speech are questionable.  The timbre of a real musical instrument may be more complex than speech, but is this true for SoundFonts where the same samples are used for multiple notes?  It's not clear what the word \"semantically\" even means with regard to music.\n\nThe definition of timbre in S3.2P2 is incomplete.  Timbre is not just a spectral envelope, but also includes e.g. temporal dynamics like ADSR.\n\nSpelling/grammar:\nS1P3L4 laborius -> laborious\nS1P3L5 bypassses -> bypasses\nS3.2P2L-1 due -> due to\nS4.4P2L1 twice as better than -> twice as good as\nS4.4P2L3 basline -> baseline\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}