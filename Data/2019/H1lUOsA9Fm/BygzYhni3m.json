{"title": "Learning a neural music synthesizer from a software MIDI synthesizer seems hard to do while having a strong ceiling on performance", "review": "This paper describes the use of a wavenet synthesizer conditioned on a piano-roll representation to synthesize one of seven different instruments playing approximately monophonic melodic lines.  The system is trained on MIDI syntheses rendered by a traditional synthesizer.\n\nWhile the idea of end-to-end training of musical synthesizers is interesting and timely, this formulation of the problem limits the benefits that such a system could provide.  Specifically, it would be useful for learning expressive performance from real recordings of very expressive instruments.  For example, in the provided training data, the trumpet syntheses used to train this wavenet sound quite unconvincing and unexpressive.  Using real trumpet performances could potentially learn a mapping from notes to expressive performance, including the details of transitions between notes, articulation, dynamics, breath control, etc.  MIDI syntheses have none of these, and so cannot train an expressive model.\n\nWhile the experiments show that the proposed system can achieve high fidelity synthesis, it seems to be on a very limited sub-set of musical material.  The model doesn't have to learn monophonic lines, but that seems to be what it is applied on.  It is not clear why that is better than training on individual notes, as Engel et al (2017) do.  In addition, it is trained on only 9 minutes of audio, but takes 6 days to do so.  This slow processing is somewhat concerning.  In addition, the 9 minutes of audio seems to be the same pieces played by each instrument, so really it is much less than 9 minutes of musical material.  This may have implications for generalization to new musical situations and contexts.\n\nOverall, this is an interesting idea, and potentially an interesting system, but the experiments do not demonstrate its strengths to the extent that they could.\n\n\nMinor comments:\n\n* The related work section repeats a good amount of information from the introduction. It could be removed from one of them\n\n* Table 1: I don't understand what this table is describing.  SynthNet is described as having 1 scalar input, but in the previous section said that it had 256-valued encoded audio and 128-valued binary encoded MIDI as input.\n\n* The use of the term \"style\" to mean \"timbre\" is confusing throughout and should be corrected.\n\n* Figure 1: I do not see a clear reason why there should be a discontinuity between L13 and L14, so I think it is just a poor choice of colormap.  Please fix this.\n\n* Page 5: MIDI files were upsampled through linear interpolation.  This is a puzzling choice as the piano-roll representation is supposed to be binary. \n\n* Page 7: \"(Table 3 slanted)\" I would either say \"(Table 3, slanted text)\" or \"(Table 3, italics)\".\n\n* Page 8: \"are rated to be almost twice as better\" this should be re-worded as \"twice as good\" or something similar.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}