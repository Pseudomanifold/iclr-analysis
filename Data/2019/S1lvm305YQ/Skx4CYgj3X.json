{"title": "Timbre can be tranferred pretty well using a constant-Q transform for features, followed by a CycleGAN to do the transfer, followed by a Wavenet to resynthesize it to audio.", "review": "Main Idea: The authors use multiple techniques/tools to enable neural timbre transfer (converting music from one instrument to another, ex: violin to flute) without paired training examples. The authors are inspired by the success of CycleGANs for image style transfer, and by the success of Wavenet for generating realistic audio waveforms. Even without the CycleGAN, the use of CQT->WaveNet for time stretching and pitch shifting of a single piece is an interesting and valuable contribution.\n\nMethodology: Figure 1 captures the overall timbre-conversion methodology concisely. In general the details of the methodology look sound. The lengthy appendices offer additional implementation details, but without access to a source code repository, it is hard to say if the results are perfectly reproducible.\n\nExperiment and Results: Measuring the quality of generated audio is challenging.  To do so, subjective listening tests are conducted on Amazon mechanical turk, but without a comparison to a baseline system except for another performance of the target piece. Note that there are few published timbre-transfer methods (see Similar Work).\n\nOne issue with the AMT survey is that the total number of workers is not reported, and as such the significance of the results can be questioned.\n\nSignificance: In my mind, the paper offers validation of the three techniques used. CycleGANs, originally designed for images,  are shown to work for style transfers on audio spectrograms. Wavenet's claim to be a generic technique for audio generation is tested and validated for this domain (CQT spectrogram to audio). That CQT outperforms STFT on musical data seems to be a well established result already, but this offers further validation.\n\nThis paper also offers practical advice for adapting the techniques/tools (Wavenet, CycleGAN, CQT) to the timbre-transfer task.\n\n\nSimilar Work:\n\nI have only found 2 papers dedicated to timbre transfer in the field of Learning Representations.\n\nBitton, Adrien, Philippe Esling, and Axel Chemla-Romeu-Santos. \"Modulated Variational auto-Encoders for many-to-many musical timbre transfer.\" arXiv preprint arXiv:1810.00222 (2018).\n\nwhich was published on sept 29th 2018, so less than 30 days ago, which is fine according to the reviewer guidelines.\n\n\nVerma, Prateek, and Julius O. Smith. \"Neural style transfer for audio spectograms.\" arXiv preprint arXiv:1801.01589 (2018).\n\nwhich is a short 2 page exploratory paper.\n\n \nIt could be useful to cite:\n\nShuqi Dai, Zheng Zhang, Gus G. Xia.  \"Music Style Transfer: A Position Paper.\" arXiv preprint arXiv:1803.06841 (2018)\n\n\nWriting Quality\n\nOverall the paper is written well with clear sentences.\n\nCertain key information would be useful to move from the appendices to the main body of the paper.  This includes the number of AMT workers, the size of the CQT frame/hop over which they are summarized, and the set of instruments that are being used in the experiments.\n\n\nSome minor nitpicks: \n\nsection 6.3, sentence 2 needs to be reworked. ('After moving on to real world data, we noticed that real world data is harder to learn because compared to MIDI data it\u2019s more irregular and more noisy, thus makes it a more challenging task.') \n\nsection 3.2 sub-section 'Reverse Generation', sentence 1 uses the word 'attacks' for the first time. Please explain this for those not familiar.\n\nsection 3.1, sentence 3 has a typo, 'Thanks' is wrongly capitalized.\n\ntable 1 (and other tables in appendix), 'Percentage' (top left) does not add anything to the table.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}