{"title": "Interesting approach but an important question about why acceleration occurs is missing", "review": "The paper introduces an ordinary differential equation (ODE) with a term involving the Hessian and shows that Nesterov's method can be derived from a direct discretization of the proposed ODE. The perspective of connecting Nesterov's method and ODE is not new, but to the best of my knowledge, this is the first work where Nesterov's method can be derived directly from the given ODE. \n\nThe proposed approach is interesting but I find the contribution a bit weak. A large part of the paper is devoted to deriving Nesterov's method from the proposed ODE, which is basically applying Euler method. However, the intuition behind the ODE is missing. In particular, it will be interesting to explain why acceleration is attained from discretizing the given ODE. \n\nI agree that the acceleration is followed by the convergence analysis, but the convergence analysis in the paper is not new, which follows the standard proof of Nesterov's method using Lyapunov functions. The inexact version is not new neither, which can be viewed as a direct application of inexact accelerated gradient method in [1]. As a result, it will be important to discuss why the proposed framework intuitively provides acceleration, which is missing in the current version. \n\nOverall, I find the approach interesting, but a large portion of the paper is spent to reformulate existing result in accelerated gradient descent literature instead of explaining why the proposed ODE helps understanding acceleration. \n\n[1] M. Schmidt,  N. Roux, and F. Bach, Convergence rates of inexact proximal-gradient methods for convex optimization\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}