{"title": "A special interpretation of Dropout. Unfortunately, not convincing.", "review": "Different from an existing variational dropout method which used variational inference to explain Dropout, this paper proposes to interpret Dropout from the MAP perspective. More specifically, the authors utilize the Jensen inequality to develop a lower bound for log-posterior, which is used as training objective for dropout. They then exploit the power mean to develop the conditional power mean model family, which provide additional flexibility for evaluation during validation.\nEven though the way how the proposed method is analyzed/generalized is interesting, the proposed method is not convincing, but I am not absolutely sure. Besides the paper is hard to follow, some other concerns are listed below.\n(1) \u201c\u2026the original/usual dropout objective\u201d and \u201cthe dropout rate\u201d are not defined in the paper, even though they appear many times in the paper.\n(2) In the last paragraph of Sec. 2, the authors argue that utilizing their MAP objective \u201csidestep any questions about whether variational inference makes sense.\u201d However, the presented MAP lower bound has its own problem, since it is derived using the Jensen inequality.  For example, as shown in Appendix C, the equality becomes true only when p(w|\\Theta) is a delta function.\n(3) How to tune the hyperparameters (alpha, lambda) of the extended dropout family in practice?\n(4) The current experiments might be weak. Additional experiments on popular image datasets are recommended.\n\nMinors:\n(1) In Eq. (3), is p(w_r|\\Theta) of the second formula identical to p(w|\\Theta) of the third formula?\n(2) In the second row below Eq. (6), E_w p(w|\\Theta) p(y|x,w) is a typo.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}