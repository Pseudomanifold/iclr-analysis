{"title": "Try to provide new perspectives on the problem. But they are not sound. No convincing approach. ", "review": "This paper studies the problem of making predictions with a model trained using dropout. Authors try to provide a theoretical foundation for using dropout when making predictions. For this purpose, they show that when using dropout training we are maximizing a common lower bound on the objectives of a family of models, including most of the previously used methods for prediction with dropout. \n\nI find that the paper addresses a relevant problem and try to apply a novel approach. But, in general, I find the paper is not easy to follow and to grasp the main ideas. \n\nHere I detail my main concerns:\n\n\n1. This is one of my main concerns. The contraposition between the geometric and the average model. I don't like this contraposition. The average model is just the standard marginalization operation over the weights, $p(y|x) = \\int p(y|x,w)p(w|\\Theta)dw$. This is the natural solution for the prediction problem to the problem if we accept the generative model given in Eq (3). \n\nIn the case of the variational dropout, we depart from the same generative model, but we employ an approximation. It is the variational approximation the one that induces the geometric mean provided in eq (6). I.e. if we want to compute the posterior over the label y* for a sample x*, after training, we should compute the associated lower bound\n$\\ln p(y*|x*) >= E_q[\\ln p(y*|x*,w)] - KL(q|p)$\nIn this case, q(w) = p(w|\\Theta), as stated Eq (3) and in the corresponding equation provided in page 2 (the q(w) is not learnt because it only depends on the dropout rate, while the $\\Theta$ are learnt by maximum log-likelihood and do not have a $q$ associated).  This gives rise to the geometric mean approximation provided in Eq (6).  I.e. the geometric mean prediction is simply the result of using a variational approximation at prediction time.   \n\nMy problem here is that authors employ convoluted arguments to introduce this geometric mean prediction and the average prediction, without making the connection discussed above. \n\n3. Section 3.3 and 3.4 introduces new arguments for modifying the dropout rate (and the alpha) parameter at test time. But, again, I find the arguments convoluted. We consider the dropout rate a hyper-parameter of the model, the standard learning theory tells us to fix the parameters with the training data and evaluate them later when making predictions. Why should we use different dropout rates at training and testing? Authors arguments about the tightness of the bound of Eq (8) and Eq(9). are not convincing to me. \n\nSo, I don't find authors provide convincing answers to the raised questions at the beginning of the paper about the use of dropout when making predictions. \n\nMinor comments:\n\n1. The generative model for Variational dropout is the same than the generative model for the \"conditional model\", eq. (3). \n\n2. In Eq. (7) authors are defining the weighted power mean. I think it would be clearer to directly introduce the weighted power mean instead of the standard power mean in Section 3.2.\n\n3. Section 3.3. I find some parts are difficult to understand. \"suppose we pick a base model from the power mean family and have a continuum of subvariants with gradually reduced variance in their predictions but the same expectation.\" Later, I can understand authors are referring to the possibility of reducing the dropout rate. ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}