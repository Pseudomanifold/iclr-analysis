{"title": "new loss function for set autoencoders; experiments are not sufficient", "review": "This paper proposes an objective function for sets autoencoders such that the loss is permutation invariant with respect to the order of reconstructed inputs. I think that the problem of autoencoding sets is important and designing custom loss functions is a good way to approach it. Thus, I quite like the idea of SCE  from that point of view. However, I find the experiments not convincing for me to accept the paper. \n\nWhile reading Section 3, I found it hard to keep in mind that x and y are discrete probability distributions and the notation like P(x=y) is not making things easier. Actually, I\u2019ve never seen cross entropy written with P(x=y). Though is my personal opinion and I don\u2019t have a suggestion on how to improve the explanations in Eq. 1-8. However, I\u2019m glad there is an example at the end of Section 3.\n\nI have some comments on the Experiments section. \n\n* Puzzles:\n(1) Figure 1 could have been prettier. \n(2) The phrase \u201cThe purpose of this experiment is to reproduce the results from (Zaheer et al., 2017)\u201d makes little sense to me.  In Deep Sets, there are many experiments and it\u2019s not clear which experiment is meant here.\n(3) Table 1 gives test error statistics for 10 runs. What is changed in every run? Does the test set stay the same in every run or is a kind of a cross-validation? Or is it just a different random seed for the initial weights? I could not find an explanation in the text, so there is no way I can interpret the results.\n\n* Blocksworld: the reconstructions are nice, but the numbers in Table 2 are difficult to interpret. \nFor example, I cannot estimate how important the difference of 10 points in SH scores is.\n\n* Rule learning ILP tasks: I don\u2019t know enough about learning logic rules tasks to comment on those experiments, but Table 3 seems overwhelming and the concept of 10 runs is still unclear.\n\n--- General comment on the experiments ---\n\nI think an important goal of any autoencoder is to learn a representation that can be useful in other tasks. There is even an example in the paper: \u201cset representation of the environment is crucial in the robotic systems\u201d. Thus, the experiments I would like to see are about evaluating the quality of a representation from an SCE-trained autoencoder compared to other training methods.  Without those experiments, I cannot estimate how valuable the SCE loss function is.  ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}