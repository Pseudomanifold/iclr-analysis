{"title": "This paper proposes a new framework that generalizes previous algorithms such as AMSGrad and PAdam.", "review": "Pros:\n1. The algorithm saves about 1/3 memory consumption compared with AMSGrad.\n2. The authors give the proof that the generalized algorithms have the same convergence rate with weaker assumptions.\n\nCons:\n1. All the experiments are based on CNN. There are no results based on modern deep neural networks such as Residual Nets and Dense Nets, where it is obvious to see Adam suffers from poor generalization. \n\n2. Algorithms like SGD with momentum and Adam should be included for comparison.\n\n3. This framework introduces two more hyper-parameters p and q, which makes it more difficult for practitioners to tune.\n\nAlthough this framework has proven convergence in both convex and non-convex smooth cases, the experimental evidence is limited. In addition, the proof strategy is not novel enough, Theorem 1 is similar to Theorem 4 in AMSGrad paper and Theorem 2 is similar to Theorem 3.3 in Zhou et al's paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}