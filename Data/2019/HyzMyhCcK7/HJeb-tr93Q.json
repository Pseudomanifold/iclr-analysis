{"title": "An interesting addition to the (large) literature on methods to learn deep networks with quantized weights.", "review": "This paper proposes a new approach to learning quantized deep neural networks, which overcome some of the drawbacks of previous methods, namely the lack of understanding of why straight-through gradient works and its optimization instability. The core of the proposal is the use of quantization-encouraging regularization, and the derivation of the corresponding proximity operators. Building on that core, the rest of the approach is reasonably standard, based on stochastic proximal gradient descent, with a homotopy scheme.\n\nThe experiments on benchmark datasets provide clear evidence that the proposed method doesn't suffer from the drawbacks of  straight-through gradient, does contributing to the state-of-the-art of this class of methods.\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}