{"title": "Review: Local SGD converges fast and communicates little", "review": "The authors analyze the local SGD algorithm, where $K$ parallel chains of SGD are run, and the iterates are occasionally synchronized across machines by averaging. For sufficiently short intervals between synchronization, the algorithm achieves the same convergence rate in terms of the number of gradient evaluations as parallel minibatch SGD, but with the advantage that communication can be significantly reduced.\n\nThe algorithm is simple and practical, and the analysis is concise and seems like it could be applicable more generally to other parallel SGD variants.\n\nI am curious about what happens for the analysis of the algorithm when $H$ becomes large. As the authors point out, when $H=T$, this is one-shot averaging which is known to converge. The authors mention not working too hard to optimize the bounds for extreme values of $H$, which is fine, but I wonder if this is possible using their analysis technique, or whether new tools would be necessary.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}