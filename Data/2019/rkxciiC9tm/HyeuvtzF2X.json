{"title": "A nice paper on temporally consistent exploration", "review": "This paper proposed to use dropout to randomly choose only a subset of neural network as a potential way to perform exploration. The dropout happens at the beginning of each episode, and thus leads to a temporally consistent exploration. The paper shows that with small amount of Gaussian multiplicative dropout, the algorithm can achieve the state-of-the-art results on benchmark environments. And it can significantly outperform vanilla PPO for environments with sparse rewards.\n\nThe paper is clearly written. The introduced technique is interesting. I wonder except for the difference of memory consumption, how different it is compared to parameter space exploration. I feel that it is a straightforward extension/generalization of the parameter space exploration. But the stochastic alignment and policy space constraint seem novel and important.\n\nThe motivation of this paper is mostly about learning with sparse reward. I am curious whether the paper has other good side effects. For example, will the dropout cause the policy to be more robust? Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores. In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.\n\nOverall, I like this paper. It is well written. The method seems technically sound and achieves good results. For this reason, I would recommend accepting this paper.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}