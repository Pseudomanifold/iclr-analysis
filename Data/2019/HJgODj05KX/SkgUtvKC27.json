{"title": "Combining Adam and Nesterov's momentum, but why?", "review": "Authors propose combining Adam-like per-feature adaptative and Nesterov's momentum. Even though Nesterov's momentum is implemented in major frameworks, it is rarely used, so there's an obvious question of practical relevance of proposed method.\n\nSignificant part of the paper is dedicated to proof of convergence, however I feel that convergence proofs are not interesting to ICLR audience unless the method is shown to be useful in practice, hence experimental section must be strong. Additionally, there's a veritable zoo of diagonal preconditioning methods out there already, this puts an onus on the authors to show an advantage in terms of elegance or practicality.\n\nExperimental section is weak:\n- There are 2 tunable parameters for each method. PA-SGD seems to be a bit better in the tail of optimization for convex problem, but I'm not confident that this is not due to better tuning of parameters (ie, due to researcher degrees of freedom). Authors state that \"grid search was used\", but no details on the grid search.\n- PA-SGD seems quite sensitive to choice of alpha. \n- No comparison against momentum which is probably the most popular method for neural network training nowadays (ie, ImageNet training is done using momentum and not Adam)\n- non-linear optimization experiments are shown using logarithmic scale for y for a huge number of epochs. This amplifies the tail behavior. More relevant is measure like \"number of steps to reach xyz accuracy\", or wall-clock time\n- it seems to perform equivalent to Adam for non-linear problem\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}