{"title": "see review", "review": "The paper talks about a method to combine preconditioning at the per feature level and Nesterov-like acceleration for SGD optimization.\n\nThe explanation of the method in Section 3 should be self-contained.  The main result, computational context, etc., are poorly described, so that it would not be easily understandable to a non-expert.\n\nWhat was the reason for the choice of the mini batch size of 128.  I would guess that you would actually see interesting differences for the method by varying this parameter.\n\nHow does this compare with the FLAG method of Chen et al from AISTATS, which is motivated by similar issues and addresses similar concerns, obtaining stronger results as far as I can tell?\n\nThe figures and captions and inserts are extremely hard to read, so much so that I have to trust it when the authors tell me that their results are better.\n\nThe empirical evaluation for \"convex problems\" is for LS regression.  Hmm.  Is there not a better convex problem that can be used to illustrate the strength and weaknesses of the method.  If not, why don't you compare to a state-of-the-art least squares solver.\n\nFor the empirical results, what looks particularly interesting is some tradeoffs, e.g, a slower initial convergence, that are shown.  Given the limited scope of the empirical evaluations, it's difficult to tell whether there is much to argue for the method.  But those tradeoffs are seen in other contexts, e.g., with subsampled second order methods, and it would be good to understand those tradeoffs, since that might point to where and if a methods such as this is useful.\n\nThe conclusions in the conclusion are overly broad.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}