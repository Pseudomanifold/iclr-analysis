{"title": "Interesting analysis of STE used in activation bianrized networks but not well written.", "review": "This paper provides theoretical analysis for two kinds of straight-through estimation (STE) for activation bianrized neural networks. It is theoretically shown that the ReLU STE has better convergence properties the identity STE,  by studying the properties of the orientation and norm of the course gradients for STE.\n\nWhile the paper presents many theoretical results which might be useful for the community, they are not organized very well.  It is a bit hard for readers to quickly find the most important theoretical results.  Moreover, some symbols are used without definition, e.g. g_{relu} is used before being defined in sec 3.1. The discussions for most theoretical results are very short or not organized well, making the whole paper hard to follow,  e.g., \"the key observation ...\" after Lemma 4 is actually not about the Lemma 4 above, but Lemma 5 in the next Lemma.  Another major concern is that activation quantization is usually used in combination with weight quantization.  It would be more useful if weight and activation quantizations can be analyzed together.\n\nClarity in the experiment part can also be further improved. From Table 1, the clipped ReLU STE has the best performance, however, there is no theoretical analysis for it. For ResNet-20 with 2-bit activation, the training loss/accuracy results of vanilla ReLU is much worse than clipped ReLU, is there any explanation for this?  For the discussion in sec 4.2, what information does it want to convey?  What is the \"normal schedule of learning rate\"? What if the small learning rate 1e-5 is kept after 20 epochs?\n\nTypo: The last sentence on page 3, the definition of y*.\n\n------------------------\n\nThe author response have addressed most of my concerns. Thus I have increased my score. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}