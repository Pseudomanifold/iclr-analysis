{"title": "Interesting approach to correlate STE updates with true loss however implications are weak and assumptions are strong", "review": "The paper examines the use of STE for learning simple one-layer convolutional networks with binary activations and non overlapping patches. In this setting, the gradients are 0 almost everywhere (gradient of sign(x)) hence it is not clear how to use gradient descent. The approach studied here is to instead use the gradient of an alternative function such as ReLU or identity which is not always 0. The authors prove that if the ReLU's gradient is used then under gaussian distribution, the algorithm will converge to the local minimas/saddle points of the expected squared loss. they also show that the same does not hold for the identity's gradient.\n\nThe proof technique is interesting and the results do show the validity of the STE approach. The fact that the loss is provably monotonically decreasing is a strong validation. The paper is clearly written. However, I do have the following concerns/questions:\n- The authors claim that their analysis is the first to analyze STE however I would like to point out that [1] studies the same setting (they allow overlapping patches and other distributions) with ReLU activation and show convergence guarantees to the global optima with using identity gradient instead of the ReLu gradient. Also for a single binary output case, using STE as identity equals the perceptron algorithm which is very well studied in literature.\n- Restrictive setting: gaussian input, no label noise, non-overlapping architecture. Not clear what the motivation for this setting is. Also binary activations are rarely used in practice. Analysis also seems tied to the gaussian distribution.\n- Infinite sample assumption is strong.\n- No guarantees for convergence to the optimal solution unlike prior work.\n- Assumptions on the weights being lower and upper bounded by a constant at each iteration seems strong unless an explicit projection step is used. Could the authors explain why this is a valid assumption to make?\n- In the experimental section, momentum is used whereas it is not mentioned in the analysis. Does the STE perform well without the momentum? It is unclear why quantized ReLU is used.\n\n[1] Surbhi Goel, Adam Klivans, and Raghu Meka. \"Learning One Convolutional Layer with Overlapping Patches.\" ICML 2018.\n\n----------\nApart from one concern (refer to comment), the authors have responded to most of my other comments. Based on this, I think the paper does offer an interesting analysis of the STE approach used for training binary networks, hence I'm increasing my score.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}