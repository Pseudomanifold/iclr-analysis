{"title": "Review of \"Modeling Uncertainty with Hedged Instance Embeddings\"", "review": "While most works consider embedding as the problem of mapping an input into a point in an embedding space, paper 1341 considers the problem of mapping an input into a distribution in an embedding space. Computing the matching score of two inputs (e.g. two images) involves the following steps: (i) assuming a Gaussian distribution in the embedding space, computing the mean and standard deviation for each input, (ii) drawing a set of samples from each distribution, (3) computing the normalized distances between the samples and (iv) averaging to obtain a global score.\n\nThe proposed approach is validated on a new benchmark built on MNIST.\n\nOn the positive side:\n-\tThe topic of injecting uncertainty in neural networks should be of broad interest to the ICLR community.\n-\tThe paper is generally clear.\n-\tThe qualitative evaluation provides intuitive results.\n\nOn the negative side:\n-\tThe whole idea of drawing samples to compute the distance between two Gaussian distributions seems unnecessarily complicated. Why not computing directly a distance between distributions? There exist kernels between distributions, such as the Probability Product Kernel (PPK). See Jebara, Kondor, Howard \u201cProbability product kernels\u201d, JMLR\u201904. The PPK between two distributions p(x) and q(x) writes as: \\int_x p^a(x) q^a(x) dx, where a is a parameter. When a=1, it is known as the Expected Likelihood Kernel (ELK). When a=1/2, this is known as the Hellinger or Bhattacharyya kernel (BK). In p and q are Gaussian distributions, then the PPK can be computed in closed form. If p and q are mixtures of Gaussians, then the ELK can be computed in closed form. \n-\tThe Mixture of Gaussians embedding extension is lacking in details. How does the network generate C Gaussian distributions? By having 2C output branches generating C means and C standard deviation vectors? \n-\tIt might be useful to provide more details about why the self-similarity measure makes sense as an uncertainty measure. In its current state, the paper does not provide much intuition and it took me some time to understand (I actually understood when I made the connection with the ELK). Also, why not using a simpler measure of uncertainty such as the trace of the covariance matrix?\n-\tThe experiments are lacking in some respects:\no\tIt would be useful to report results without the VIB regularization.\no\tThe focus on the cases D=2 and D=3 (embedding in a 2D or 3D space) shades some doubt on the practical usefulness of this framework in a higher-dimensional case.\n\nMiscellaneous:\n-\tIt seems there is a typo between equations (4) and (5). It should write z_1^{(k_1)} \\sim p(z_1|x_1)\n\n--- \n\nIn their rebuttal, the authors satisfyingly addressed my concerns. Hence, I am upgrading my overall rating.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}