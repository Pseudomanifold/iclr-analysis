{"title": "It is an interesting paper which explores multi-task model for simultaneously improving both monolingual and cross-lingual tasks. However, due to missing information and lacking of clarity makes it hard to accept at this point of time.", "review": "Summary\n----------\nIn this paper, authors explore learning of cross-lingual sentence representations with their proposed dual-encoder model. Evaluation conducted with learned cross-lingual representations on several tasks such as monolingual, cross-lingual, and zero-shot/few-shot learning show the effectiveness of the proposed approach. Also, they show provide a graph-based analysis of the learned representations.\n\nThree positive and negative points of the paper is presented as follows:\n\npros\n------\n\n1. cross-lingual representation learning by combinining ideas from learning sentence representations and cross-language retrieval.\n2. Multi-task setup of different tasks for improving cross-language and monolingual tasks.\n3. Lot of experimental results.\n\n\ncons\n-----\n1. Claim it works for monolignual tasks in target language such as zero-shot learning for sentiment classification and NLI. Also, for cross-lingual STS and eigen-similarity metric is hard to retrieve from the paper.\n\n2. Many terms, datasets are used without being referenced.\n\n3. Usage of existing approaches to build a single model for many tasks.\n\ncomments to authors\n-----------------------\n\n\n1. Dual-encoder architecture is inspired from Guo et al (2018) which uses encoding of source and target sentence with Deep neural network. However, it is here replaced into multi-task dual-encoder model.\n\n2. What are the tasks that are very specific to source language? \n\n3. Equation-1 is basically a logistic regression or softmax over \\phi. However \\phi is dot product of encodings as similar to Deep averaging networks (Iyyer et al. 2015) ?\n\n4. In Section-2, it is unclear what does symmetric tasks mean? They use parallel corpora?\n\n5. In Section-2.1, it is mentioned that Word embeddings are learned end-to-end. Does this mean they are not initialized with pretrained ones?\n\n6. In Section-2.1, it is mentioned that word and character embeddings are learned in a computationally efficient way, what does it represent? They use less parameters, parallelizable?\n\n7. Why only three layers of transformer, It is understood that 6-12 layers is required for effective encoding of sentences (Al-Rfou et al., 2018)\n\n8. In model configuration, how is convergence decided. Any stopping criterion?\n\n9. What are the splits for reddit, wikipedia datasets?\n\n10. In Table-1, what does MR,CR etc., refer to? They are never mentioned before. Does all tasks only use only English ?\n\n\n\nOverall it is an interesting paper which explores multi-task model for simultaneously improving both monolingual and cross-lingual tasks. However, due to missing information and lacking clarity in some details it is hard to accept at this point of time.\n\nMinor issues\n--------------\n\n1. Sentences are very long and not easily comprehensible.\n2. Target language and repsonse are used without referencing each other. Better to use one of them for better tracking.\n3. No common notation for the model. It is been referenced with different names (cross-lingual multi-task model, multi-task dual-encoder model).", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}