{"title": "ACCEPTABLE", "review": "\nThe authors proposed to exploit hyperbolic geometry in computing the attention mechanisms for neural networks. Specifically, they break the attention read operation into two parts: matching and aggregation. In matching step, they use the hyperbolic distance to quantify the macthing between a query and a key; in the aggregation step, they use the Einstein midpoint. Their experiments results based on synthetic and real-world data shows the new method outperforms the traditional method based on Euclidean distance. This paper is acceptable.\n\n\nQuestion: In Figure 3(Center), the number of nodes 1000 and 1200 are pretty close. How about the results on 500 nodes and 2000 nodes? It seems the accuracy difference increases as the number of nodes increases. Is this true? \n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}