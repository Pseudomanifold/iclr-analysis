{"title": "Promising performance", "review": "In the paper, the authors proposed to solve the learning problem of adversarial examples from Riemannian geometry viewpoint. More specifically, the Euclidean metric in Eq.(7) is generated to the Riemannian metric (Eq.(8)). Later, the authors built the correspondence between the metric tensor and the higher order of Taylor expansions.  Experiments show the improvement over the state-of-the art methods.\n\nSome questions:\nFirst of all, the idea of introducing Riemannian geometry is appealing.\nIn the end, a neural network can be roughly viewed as a chart of certain Riemannian manifold.\nThe challenging part is how can you say something about the properties of the high dimensional manifold, such as curvature, genus, completeness etc.\nUnfortunately, I didn't find very insightful analysis about the underlying structure.\nWhich means, hypothetically, without introducing Riemannian geometry we can still derive Eq.(14) from Eq.(12), Taylor expansion will do the work.\nSo more insights about metric tensor G determined manifold structure can be very helpful.\n \nSecond, Lagrange multipliers method is a necessary condition, which means the search directions guided by the constraint may not lead to the optimal solutions.\nIt would be better if the authors can provide either theoretical or experimental study showing certain level of direction search guarantee.\n \nLast, the experiment results are good, though it lacks of detailed discussion, for example could you decompose the effect achieved by proposed new Riemannian constraint and neural network architecture? Merely demonstrating the performances does not tell the readers too much.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}