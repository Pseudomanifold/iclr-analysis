{"title": "Review Number 3 (So sorry for the delay!)", "review": "The authors propose a new regularizer for policy search in a multi-goal RL setting. The objective promotes a more efficient exploration strategy by encouraging the agent to learn policies that depend as little as possible on the target goal. This is achieved by regularizing standard RL losses with the negative conditional mutual information I(A;G|S). Although this regularizer cannot be optimize, the authors propose a tractable bound. The net effect of this regularizer is to promote more effective exploration by encouraging the agent to visit decision states, in which goal-depend decisions play a more important role. The idea of using this particular regularizer is inspired by an existing line of work on the information bottleneck.\n\nI find the idea proposed by the authors to be interesting. However, I have the following concerns, and overall I think this paper is borderline.\n\n1. The quality of the experimental validation provided by the authors is in my opinion borderline acceptable. Although the method performs better on toy settings, it seems barely better on more challenging ones. Experiments in section 4.5 lack detail and context.\n2. The clarity of the presentation is also not great.\n    2.1. The two-stage nature of the method was confusing to me. I didn\u2019t understand the role of the second stage. Most focus is on the first stage, and only very little on the second stage. For example, I was confused about why the sign of the regularizer was flipped.\n    2.2. I was confused by how exactly the bounds (3) and (4) we applied and in what order.\n    2.3. I think the intuition of the method could be better explained and better validated by experiments.\n\nI also have the following additional comments:\n* How is the regularizer applied with other policy search algorithms besides Reinforce? Was it done in the paper? I can\u2019t say for sure. Specifically, when comparing to PPO, was the algorithm compared to a version of PPO augmented with this regularizer? Why yes or why no?\n* More generally, experiments where more modern policy search algorithms are combined with the regularizer would be helpful. In particular, does it matter which policy search algorithm we use with this method?\n* Experimental plots in section 4.4 are missing error bars, and I can\u2019t tell if the results are significant without them.\n* I thought the motivation for choosing this regularizer was lacking. The authors cite the information bottleneck literature, but we shouldn\u2019t need to read all these papers, the main ideas should be summarized here.\n* The argument for how the regularizer improves exploration seemed to me very hand-wavy and not well substantiated by experiments.\n* I would love to see a better discussion of how the method is useful when he RL setting is not truly multi-goal.\n* The second part of the algorithm needs to be explained much more clearly.\n* What is the effect of the approximation on Q?\n\n---\n\nI have read the response of the authors, and they have addressed a significant numbers of concerns that I had. I am upgrading my rating to a 7.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}