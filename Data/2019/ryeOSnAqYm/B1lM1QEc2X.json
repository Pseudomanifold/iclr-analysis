{"title": "Lacking arguments why the proposed method generalizes well to other problem settings", "review": "The paper presents a methodology for improved program synthesis by generating datasets for program induction and synthetic tasks from uniform distributions. This method is evaluate on two problem settings. \n\nThe methodology is presented in section 3. Even though the outline does not seem to be complicated, the presentation in section 3 leaves me puzzled. The the second paragraph two sets of silent variables are introduced X_1,...,X_n and Z_1,...,Z_m but never used again the rest of the paper. In the third and forth paragraph details about the Karel domain are presented without the Karel domain having been introduced. It seems you are using rejection sampling to sample from a uniform distribution. Why can you not sample from a uniform distribution directly? What do you mean with the notation X(s)? What are you proving in Appendix? Would maybe be clearer if you presented it as a theorem/lemma.\n\nThe remaining part of the paper evaluates this methodology on two specific problem settings, the Karel domain and Calculator domain. The generalization performance is increased when trained on datasets generated by the method presented in the paper. However, I cannot find and strong arguments in the paper why this property should generalize to other problem settings. To me the analysis and experimental results seems to be tailored to the two problems settings used in the paper.\n\n==== After revision ====\n\nThe authors have done a great job addressing the concerns I had about the clarity. Consequently, I have raised my score, whereas my fairly low confidence still remains.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}