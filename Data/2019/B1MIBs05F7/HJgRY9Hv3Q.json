{"title": "Interesting investigation of the applicability of SVGD to modern neural networks", "review": "This work investigates the applicability of SVGD to modern neural networks and shows the naive application of SVGD typically fail. The authors find that the naive application of batch norm, dropout, and data augmentation deviate significantly from the assumptions of SVGD and variance reduction can fail easily in large nets due to the weight moves quickly in the training.\n\nThis is a thorough exploration of a well-known algorithm - SVGD in deep neural networks. Although most results indicate that SGVD fails to reduce the variance in training deep neural networks, it might still provide insights for other researchers to improve SVGD algorithm in the context of deep learning. Therefore, I'm inclined to accept this paper.\n\nOne weakness of this work is that it lacks instructive suggestion of how to improve SVGD in deep neural networks and no solution of variance reduction is given.\n\nFinally, I'd like to pose a question: Is it really useful to reduce variance in training deep neural networks? We've proposed tons of regularizers to increase the stochasticity of the gradient (e.g., small-batch training, dropout, Gaussian noise), which have been shown to improve the generalization. I agree optimization is important, but generalization is arguably the ultimate goal for most tasks.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}