{"title": "The usefulness of graph spectral regularizer is shown, but the key points in practice are not considered.", "review": "Authors highlight the contribution of graph spectral regularizer to the interpretability of neural networks. Specifically, authors consider the Laplacian smoothing regularizer to enhance the local consistency/smoothness between a neuron and its neighbors. Furthermore, by extending the graph Fourier transformation to overcomplete dictionary representation, authors further propose a spectral bottleneck regularizer. Experimental results show that when suitable structural information and corresponding regularizers are imposed, the interpretability of the intermediate layers is improved.\n\nMy main concern is that the power of Graph-based regularizer has been well-known in the ML community for a long time. It is not surprising that adding such regularizers to the training process of neural networks can help to get more structural activations. The key points are \n\n1) How to define the Laplacian graph for the neurons? For the simple case shown in Figures 1 and 2, the topology of the neurons has been predefined and the functionality of them is predefined implicitly. For more challenging cases, how to build the Laplacian graph reasonably? \n\n2) How to add the regularizers with good scalability? The complexity of the proposed regularizers is O(N^2) where N is the number of neurons. When the layers contains thousands of neurons or more, how to add the regularizers efficiently?\n\n3) Which regularizer should be selected? Authors propose a class of graph spectral regularizers and their performance is different in different tasks. Is there any strategy helping us to select suitable regularizers for specific tasks?\n\nUnfortunately, authors provide little analysis on these key points.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}