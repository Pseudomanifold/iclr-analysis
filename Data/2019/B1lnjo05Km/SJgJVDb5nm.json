{"title": "Latent structure through spectral regularization.", "review": "The paper introduces a spectral regularization with the aim of obtaining representations\nthat are easier to interpret.\n\nSome sentences are often confusing and, in general, clarity needs to be improved.\n\nThe motivation of the work is not very strong in my opinion, in particular by adding such\na prior the space of possible solutions greatly shrinks and I am afraid\nthat interesting solutions will be lost. I think one should focus on properties\nrather than visual inspection.\nAlso, isn't it that if we can clearly see the pattern, perhaps that pattern is\nlinear and of easy discovery also by simpler models?\n\nMore importantly, it seems that all experiments are performed on tasks where the\nunderlying structure is known, however this is almost never the case in practice.\nAssuming one uses the proposed spectral regularization, how would one interpret\nit in such cases?\n\nIn section 2 please clarify the paragraph on bounded Lp norm.\n\nI am sorry but why isn't there a relation, for convolutional nets,\nbetween neurons in different channels? Each element in the feature map represents\nthe input surrounding that location in a k dimensional space.\n\nThe authors state that the usual bottleneck for autoencoders is composed of 2/3\nneurons, this is simply not true. There has been extensive work on\novercomplete representations that shows that is better to have many more dimensions\nbut only few degrees of freedom.\n\nThe spectral bottleneck should cite VQVAE as the approach is very similar and the \nauthors should compare to it.\n\nFor the topological inference experiment it is assumed that one knows the structure,\nbut how to address the more general problem?\nMore practically, the regularization enforces smoothing (if few eigenfunctions\nare used, which is never explained in the paper) between connected nodes, did\nthe authors try to have a simple L2 penalty instead? E.g. minimize the difference\nbetween activations in the group.\n\nRegarding the capsule network example, when you write that without regularization\neach digit responds differently to perturbation of the same dimension, isn't it\npossibly true only up to a, unknown, permutation of the neurons?\n\nTo summarize, while the idea sounds interesting, I miss to find the easy interpretability\nof results and also the overall motivation sounds a bit weak. \nMore importantly the selection of W, crucial for defining structure, is not discussed at all in the paper.\nExperiments are performed on toy examples only whereas here, given that we can\npossibly interpret the results I would have liked something more involved to\nbetter show that this kind of interpretability is needed.\n\nMissing cites:\n[1] van den Oord et al, Neural Discrete Representation Learning.\n[2] Koutnik et al, Evolving neural networks in compressed weight space.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}