{"title": "Good and significant work, paper could be improved", "review": "Paper summary:\n\nThe paper presents two distinct contributions in text-to-speech systems:\na) It describes a method for distilling a Gaussian WaveNet into a Gaussian Inverse Autoregressive Flow that uses an analytically computed KL between their conditionals.\nb) It presents a text-to-speech system that is trained end-to-end from text to waveforms.\n\nTechnical quality:\n\nThe distillation method presented in the paper is technically correct. The evaluation is based on Mean Opinion Score and seems to follow good practices.\n\nThe paper makes three claims:\na) A WaveNet with Gaussian conditionals can model speech waveforms equally well as WaveNets with other types of conditionals.\nb) Analytically computing KL divergence stabilizes distillation.\nc) A text-to-speech system trained end-to-end from text to waveforms outperforms one that has separately trained text-to-spectrogram and spectrogram-to-waveform subsystems.\n\nClaims (a) and (c) are clearly demonstrated in the experiments. However, there is nothing in the paper that substantiates claim (b). I think the paper would be strengthened if the performance of sample-based KL distillation was added into Table 2, and if learning curves were reported that evaluate the amount of stabilization that an analytical KL may offer vs a sample-based KL.\n\nFurther points about the experiments:\n- It wasn't clear to me whether distillation happens at the same time as the autoregressive WaveNet is trained on data, or after it has been fully trained. I think the paper should make this clear.\n- The paper says that distillation makes generation three orders of magnitude faster. I think it would be good if actual generation times (e.g. in seconds) were reported.\n\nClarity:\n\nThe paper is generally well-written. Sections 1 and 2 in particular are excellent.\n\nHowever, section 3 contains several notational errors and technical inaccuracies, that makes it rather confusing to read. In particular:\n- q(x_t | z_{<=t}) is used in several places to mean the Gaussian conditional q(x_t | z_{<t}) (e.g. in Eqs (6) and (7), and elsewhere). This is confusing, as q(x_t | z_{<=t}) is actually a delta distribution.\n- q(x | z) is used in several places to mean q(x) (e.g. in Eq. (7), in Alg. 1 and elsewhere). This is confusing, as q(x | z) is also a delta distribution.\nI believe that section 3, especially subsections 3.2 and 3.3.1, should be reworked to be made clearer, and the notation should be carefully revised.\n\nI don't think the paper needs to span 9 pages. Section 3 is rather wordy, and should be compressed to the important points.\n\nOriginality:\n\nDistilling a Gaussian autoregressive model to another Gaussian autoregressive model by matching their Gaussian conditionals with an analytical KL is rather straightforward, and, methodologically speaking, I wouldn't consider it an original contribution on its own. However, I think its application and demonstration in text-to-speech constitutes an original contribution.\n\nSignificance:\n\nThe paper contains a substantial amount of significant work that I think is important to be communicated to the ICLR community, especially the text-to-speech community.\n\nReview summary:\n\nPros:\n+ Substantial amount of good work.\n+ Significant improvement in text-to-speech end-to-end software.\n+ Generally well-written (with the exception of section 3 which needs work).\n\nCons:\n- Some more experiments would be good to substantiate the claim that analytical KL is better.\n- Notational errors and confusion in section 3.\n- Too wordy, no need for 9 pages.\n\nNitpicks:\n- As I said above, I wouldn't consider distillation of models with Gaussian conditionals using analytical KLs methodologically novel, so I think the phrase \"novel regularized KL divergence\" should be moderated.\n- Eq. (1) should contain theta on the left hand side too.\n- Page 3: \"at Appendix B\" --> \"in Appendix B\".\n- Page 4: In flows we don't just \"suppose z has the same dimension as x\"; rather, it's a necessary condition that must hold.\n- Footnote 5: It's unclear to me what it means to \"make the loss less sensitive\".\n- References: Real NVP, Fourier, Bayes, PixelCNN, WaveNet, VoiceLoop should be properly capitalized.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}