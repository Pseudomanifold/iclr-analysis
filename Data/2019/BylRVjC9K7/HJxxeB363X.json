{"title": "An interesting method, but the motivation should be clarified and the comparisons to the state-of-the-art should be improved", "review": "The paper discusses two ways of constructing adversarial examples (images) using PCA+knn in the input space. Compared to the litterature on adversarial examples, the modifications proposed by the authors are clearly visible to the human eye and the resulting images do not seem natural (see Figure 4 and 5). The authors acknowledge this difference between their work and the state-of-the-art (e.g., \"Modified images are sometimes visible but still can keep original structures and information, this shows that adversarial images can be in more forms than small perturbations\", Section 4.1), but it remains unclear why generating such images would be interesting in practice.\n\nThe algorithm for generating adversarial examples from nearest neighbors and PCA is reasonable. It seems simple and fairly easy to implement. However, it does not seem to be competitive with the current litterature for generating adversarial examples. An important point of the authors is that their method constructs \"adversarial\" samples without taking into account the specific structure of neural networks (more generally, without any knowledge of the classifier). This claim would have more practical impact if the method was shown to fool more algorithms/types of models than usual approaches (e.g., fast gradient sign). But there is no comparison to the state-of-the-art, so it is unclear in what situation the method should be interesting.\n\nI found the motivation based on knowledge representation rather confusing, and I found no clear arguments for the PCA nor the k-nn approach. The write-up uses statements that are vague or not properly justified such as \"For human beings, both abstraction and sparsity can be achieved with hierarchical storage of knowledge. This can be similar to object-oriented programming\" (why is object-oriented programming relevant here?, is there any justification and formal statement for the first claim (e.g., a reference)?), \"Neural networks store learned knowledge in a more hybrid way\", \"In summary, human beings can detect different objects as well as their transformations at the same time. CNNs do not separate these two.\" (there is no clear experiment proving that there is no \"separation\" because it is unclear what the DeepDream visualization of Figure 1 effectively proves). The equations do not really help (e.g., X_2 is supposed to be a vector if I understand correctly, what is X_2^{-1}?). Overall, I think the paper would gain a lot by making the motivation part more formal.\n\nIn conclusion, the authors seem to depart from the idea that adversarial examples should a) fool the network but b) still feel natural to humans. There is no clear motivation for generating such unnatural adversarial examples, and there is no clear application scenario where the algorithm would generate \"better\" adversarial examples than usual methods.\n\nminor comments:\n\n* please add spaces before \\cite commands \n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}