{"title": "Inspiring thought, though lack of sufficient proofs", "review": "This paper explored the means of tuning the neural network models using less parameters. The authors evaluated the case where only the batch normalisation related parameters are fine tuned, along with the last layer, would generate competitive classification results, while using very few parameters comparing with fine tuning the whole network model. However, several questions are raised concerning the experiment design and analysis:\n1. Only MobilenetV2 and InceptionV3 are evaluated as classification model, while other mainstream models such as ResNet, DenseNet are not included. Would it be very different regarding the conclusion of this paper?\n2. It seems that the only effective manner is by fine tuning the parameters of both batch normalisation related and lasts layer, while fine tuning last layer seems to be having the main impact on the final result. In Table 4, authors do not even provide the results fine tuning last layer only.\n3. The organisation of the paper and the order of illustration is a bit confusing. e.g. later sections are frequently referred in the earlier sections. Personally I would prefer a plain sequence than keep turning pages for confirmation.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}