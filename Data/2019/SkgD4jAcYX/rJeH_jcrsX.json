{"title": "Simple, intuitive idea. There are some issues.", "review": "The authors present a parametrized version of ReLU. It is similar to PReLU except the trainable parameter causes the flat region of ReLU to shift up and down instead of causing it to change its slope. Their method outperforms PReLU and leaky ReLU in their experiments.\n\nSince leaky ReLU / PReLU is seeing some practical use, I believe any improvement upon these methods is welcome. However, there are a significant number of issues regarding this paper. I don't view any of them as major. Nonetheless, they add up.\n\nThe experiments should be made stronger in a few ways:\n- include at least a few large-scale experiments (ImageNet or larger). Common wisdom suggests that methods that improve performance on CIFAR-sized datasets don't necessarily improve performance on larger datasets \n- your method ConvReLU is composed of three building blocks: (1) the novel parametrization of ReLU (AdaReLU), (2) the parameter sharing scheme, (3) L2 regularization on the trainable parameters. You should run experiments that isolate the contribution of each building block to the performance gains you observed. I.e. you should run AdaReLU with a simpler parameter sharing scheme and ConvReLU without L2 regularization, for example. \n- specifically, I think you should run experiments withe AdaReLU where you only have a single threshold parameter per channel, i.e. x^l_{i,j,d} = \\sum_{c=0}^{n_{out}-1} \\sum_{a=0}^{k-1} \\sum_{b=0}^{k-1} w_{a,b,c,d} \\max(x^{l-1}_{i+a,j+b,c}, \\theta_c) to use your notation. I would consider this the \"default\" parameter sharing scheme because this is what is used for the trainable parameters of PReLU, for the trainable biases of convolutional layers, as well as the trainable scaling and bias parameters of batch normalization, layer normalization etc. An important difference between having only a single parameter per channel and using your scheme is that your computation can no longer be efficiently divided into two steps. I.e. if we use a single parameter per channel we can divide the computation x^l_{i,j,d} = \\sum_{c=0}^{n_{out}-1} \\sum_{a=0}^{k-1} \\sum_{b=0}^{k-1} w_{a,b,c,d} \\max(x^{l-1}_{i+a,j+b,c}, \\theta_c) into two steps: x^l_{i,j,d} = \\sum_{c=0}^{n_{out}-1} \\sum_{a=0}^{k-1} \\sum_{b=0}^{k-1} w_{a,b,c,d} z^{l-1}_{i+a,j+b,c} and z^{l-1}_{i,j,c} = max(z^{l-1}_{i,j,c}, \\theta_c). The first step is the \"ReLU step\" and the second is the \"convolutional step\". However, the same decomposition is not possible with your parameter sharing scheme without wasting memory. Hence, your scheme is somewhat more complicated than having one threshold per channel. And this additional complexity has to be justified explicitly with experimental gains.\n- Looking at table 6, I find that PreLU only has 12 more parameters than ReLU. This looks like PReLU's trainable parameter was shared between every single unit in each layer. Firstly, this is unrealistic as PReLU would ordinarily have a single trainable parameter per channel, not per layer. Second, this makes the comparison to ConvReLU unfair. You should compare AdaReLU and PReLU with the same parameter sharing scheme. Once you have shown that ConvReLU outperforms PReLU in that scenario, then you can compare various parameter sharing schemes. Also allowing ConvReLU the benefit of L2 regularization but not PReLU further waters down the validity of the comparison.\n- compare ConvReLU also against ELU/SELU/Swish. The authors claim that \"ELU/SELU are only for fully-connected layers\", but I don't see a reason that prevents them from being applied to convolutional layers. From the anonymous comment below, it appears SELU has actually been used for convolutional layers with some success. \n\nPlease provide more details on how the hyperparameters are tuned, including the strength of L2 regularization for the thresholds. Are the same hyperparameters used for all methods / architectures? The reviewers should be able to verify that ConvReLU was not tuned excessively relative to the baseline.\n\nPlease provide some practical guidance as to which layers ConvReLU should be applied to in an architecture other than the architectures you used. It appears in your experiments that you apply ConvReLU to roughly the bottom third of layers. You should formulate a clear hypothesis regarding which layers ConvReLU should be applied to in an arbitrary network and then test that hypothesis explicitly.\n\nI find figures 2 and 3 unclear and needlessly complicated. What is s/x/y/m/n? Why does the depicted network get narrower with depth? Without the formula you provided in the comment below, I would not be 100% sure how your method works. I would simply replace both figures with the formula you provided below.\n\nI am not entirely clear on what you mean by the phrases \"input neuron/unit\" and \"output neuron/unit\". \"input neuron\" generally refers to neurons in the input layer to which the data is clamped. \"output neuron\" generally refers to neurons of the output layer which are fed into the loss function. I would suggest using different terms for the concepts you are describing with those phrases, and be more clear on what those concepts are.\n\nI don't think you show that the observed gains of your method are due to solving the \"dying ReLU problem\". While technically any ReLU where the threshold value is not exactly 0 cannot be \"dead\", in figure 6 you show that most of your thresholds are in fact very close to 0. Further, you do not show that the \"aliveness\" of the ReLU units is what causes the performance gain. One could just fix all thresholds at -0.01 to avoid deadness, yet it's not clear whether one would still observe the performance gains you observed. Also, you fail to mention that in practice, the majority of the time, dying ReLU is solved by employing batch normalization, as is done e.g. in the DenseNet architecture you use.\n\nRegarding threshold initialization: First, it's called \"Glorot initialization\", after the author of the 2010 paper, not \"Glot initialization\". Second, I'm not sure why you would consider Glorot initialization at all. The goal of Glorot initialization is to stabilize the overall magnitude of neuron activations in the forward pass and backward pass as much as possible. However, Glorot initialization is specifically designed for linear operations (fully-connected / convolutional) based on how those operations impact the magnitude of neuron activations. If you wanted to choose the initial thresholds in order to achieve the same stability, you would have to analyze what impact the threshold values have on the magnitude of neuron activations and then choose the initialization variance accordingly. Simply copying the Glorot variances makes no sense.\n\nWhat variance do the thresholds have under your \"Random initialization\"?\n\n\"Despite the possibility of resulting in dying neurons in the network, ReLUs have the advantage of inducing higher sparsity for the computed features as compared to other activation functions. This may consequently reduce the risk of over-fitting (Glorot et al., 2011). Although these activation functions can overcome the dying ReLU problem, the sparsity of the outputs is significantly reduced, especially for the final linear classifier layer. This may increase the risk of over-fitting.\" I have never come across this over-fitting hypothesis and I don't see it in the Glorot paper. Feel free to let me know which papers talk about this over-fitting hypothesis in your rebuttal.\n\n\"We believe this marginal increase in parameter number is negligible and will not cause the overfitting problem.\" Total parameter number is not generally known to be related to overfitting in deep networks.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}