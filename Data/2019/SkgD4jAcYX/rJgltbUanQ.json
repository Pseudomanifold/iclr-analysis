{"title": "Simple idea, overall well-structured paper, but fairly incremental and unconvincing improvement over alternatives given the extra parameters, little to no exploration of learned thresholds, writing needs work. ", "review": "\nThe most important criticism I have is about the claim that the marginal increase in parameters is negligible. I think it's _probably_ true, but given that the increases in performance are quite small across the board (<0.5 gain in accuracy), and that all of the learned thresholds are so close to 0 (i.e. not much different from pure relu), I think it would have been important and pretty easy just to remove a few thousand parameters from the convrelu architecture to equalize the number of parameters in each network. I think would need to see accuracies for at least one experiment like this to increase my rating, or to be somehow otherwise convinced that the number of parameters does not make the difference. \n\nThe background and related work section covers most of the recent ReLU alternatives, but doesn't not clearly explain what an activation function is, and misses some references. I find the way the biological motivation is presented to be a bit misleading/overstated. It's fine and interesting to say that the idea was inspired by something about biological neurons, but I think it's a stretch to claim to \"mimic brain function\". The related work section is pretty good, but reads a bit like a list of papers - it would be nice to have a bit more discussion of how related methods are similar/different to the proposed method.\n\nThe experimental sections, tables, and plot of learned threshold values are good, but impression from the swish paper and others was that ReLU variants do not perform much differently on smaller datasets, and only make a difference in very deep networks. If my impression is correct, it would be important to do an experiment with a deep resnet or something like that, but I'm not very sure about this impression.\nThe exploration of the partial replacement strategy is interesting and worthwhile, and again might be even more interesting in much deeper networks.\n\nMost hyperparameter settings are mentioned, but what about the learning rate? Was this tuned for each experiment? I seem to remember there being a paper that showed learning rate made a pretty big difference for different activation functions' performance.\nThe tinyimagenet numbers seem a little off; I don't have personal recent experience with this dataset but this blog post claims 56.4 with normal ReLUs in a VGGish architecture. https://learningai.io/projects/2017/06/29/tiny-imagenet.html\n\nSome other questions:\n - How similar are the distributions of learned thresholds across datasets? \n - How much do the thresholds change over the course of training (do they quickly reach the values shown in the histogram and stay that way, or are they very noisy?\n - An interesting experiment could be to use fixed thresholds according to the distribution shown in Fig. 6. Especially if the distributions are similar across datasets, it may be that this performs as well as learning the thresholds, without introducing extra parameters to learn. This would also help disentangle the effect of having different thresholds vs. the possible implicit regularization from the noise of the thresholds changing.\n\nOther refs to mention:\nRandomized relu (randomly set the leak threshold) https://arxiv.org/pdf/1505.00853.pdf\nNoisy activation functions\n\nQuality: Decent (6.5/10), but I think the experiments, level of analysis, and quality of writing are more like a very good blog post than an academic paper (although I think that that line is becoming very blurry)\n\nClarity: Decent (7/10), but there are many small grammar errors, especially to do with pluralization (e.g. \"activation functions .... its importance\" should be \"their importance\") and  incorrect verb tense (e.g. \"was used\" should be \"is used\" or \"has been used\"; past tense implies it no longer is used). I pointed out the ones that were particularly confusing in the \"specific comments\" section, but the paper should be thoroughly reviewed for writing quality. \nThe idea is simple and its implementation and the experiments are well explained. \n\nOriginality: (6.5/10) As far as I know the idea of parameterized thresholds for relus is original, as is the partial replacement strategy, but I am not an expert in this area. There is not much/any originality or creativity in the analysis of the results or probing of why variable thresholds are better; I think this could make the paper much stronger.\n\nSignificance: (5.5/10) For anyone looking to squeeze a little extra performance out of a model for some reason, I think even the <0.5 improvement is useful to know about. This seems more like an engineering problem than a research problem though, and I think that the paper is lacking in significant research insight / exploration. The gains might be more significant in deeper networks, but this is difficult to assess without experiments.\n\nPros: \n - marginal improvement across the board\n - partial replacement strategy might be useful for other methods\n\nCons:\n - Writing contains grammatical errors / change of tense / confusing sentence construction which make the paper somewhat hard to get through\n - Not much exploration or insight about the learned functions or the activation function performs well\n - Numbers seem quite incremental to me, and the method is not tested on larger/deeper networks where the gains might be mode significant.\n\n\nSpecific comments/nits (in order reading through paper):\n1. \"ReLUs can solve the gradient vanishing and accelerate training convergence\" <- cite this claim\n2. it suffers -> they suffer\n3. \"suffer from the dying relu problem\" <- cite this claim\n4. \"For the parameters involved in ...\" this sentence makes it sound like you don't tryi anything other than L2, and it does not explain why you want the trainable thresholds close to zero.\n5. \"we propose a ...method known as adaptive ReLUs\" -> \"method we call adaptive ReLUs\" or something like that (it can't be known as it yet; you've just proposed it).\n6. \"due to its importance in deep neural networks\" this is not an explanation of _why_ activations are a popular research field; it just raises the further question of why they are important in DNNs.\n7. tanh: describe why the zerocentering property is good if you want to mention it being preferred for that reason\n8. mention that motivation for elu etc. is to be soft/differentiable\n9. \"the use in convolution is not clear\"  what do you mean by this? that it hasn't worked well in practice or that they don't work by construction for some reason... explain\n10. Not clear what the swish function is or how it is related to ELUs and SELUs.\n11. \"well known that neural networks mimic computational activities in the brain\" I would not say this is well known. It's a contentious claim; \"inspired by\" or \"modeled after\" would be more accurate in my opinion.\n12. \"which is also called\" this is confusing/misleadingly worded; the process of the brain inputting and outputting signals is NOT \"also called feedforward network with relus\"\n13. \"which can better mimic brain functions\" most would consider neuron firing is a process that occurs in the brain, not a \"brain function\" which would be something like \"seeing\"\n14. \"share the same outgoing weight\" this is confusing; makes it sound like you dynamically check the value of the weight and group units based on this value.\n15. \"Glot initialization\" -> \"Glorot initialization\"\n16. \"zero init was used\" -> \"has been used\" \n17. Cite densenet in Fig.4 caption\n18. \"with minor adjustments to accomodate diferent datasets\" \"standard data augmentation\" explain what these are (in appendix if necessary)\n19. \"VGG like network\" cite VGG and explain what is different about yours", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}