{"title": "Exploration using Distributional RL and UCB", "review": "Summary:\n========\nThe paper presents an RL method to manage exploration-exploitation trade-offs via UCB\ntechniques. The main idea appears to be that quantile regression can be used to construct\ntighter upper confidence bounds which give rise to better performance. The authors\ndemonstrate their method on a synthetic multi-armed bandit problem and the Atari games.\n\nI don't think the methodological and technical contributions are significant enough to\nwarrant acceptance. Moreover, the presentation of the paper needs to be improved.\n\n\n\nDetailed Comments\n=================\n\nI see two main issues with the proposed method.\n1. First, the fact that the authors are attempting to recover the entire pdfs of the\nreward distributions - the sample complexity for estimating pdfs (or quantilies) is\nsignificantly more than what is required to recover the optimal arm. In that sense, the\nproposed method runs counter to theoretical work in the MAB literature.\n2. Once you estimate the quantiles/pdfs, there really is no reason to use the\n \"mean + constant * std\" form for the UCB anymore - you can directly use the pdf to\n discard a low probability region.\n\n\nMany design choices are made in a very ad hoc manner with only (if any) speculative\njustification. Some examples,\n- Many of the statements in the para starting \"In the case of UCB type ...\"\n- The quantity \\sigma^2_+ and in particular, the use of the median and not the mean\n\n\nOther:\n- Page 1: In the RL setting, an arm corresponds to a state-action pair ... : this\n  statement needs justification. The naive way to treat n RL problem as a MAB problem is\n  to treat all sequences of actions as an arm.\n- If you are repeating algorithms for other work (e.g. Algorithm 1), include the citation\n  there.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}