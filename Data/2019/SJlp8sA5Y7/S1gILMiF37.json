{"title": "An Efficient Network for Predicting Time-Varying Distributions ", "review": "This paper is on regressing over probability distributions, extending from previous work, this paper studies time varying distributions in a recurrent neural network setting. Given state-of-the-art in this topic of regressions over networks, embedding the idea into a recurrent neural setting is novel, but only marginally so. The paper is well-written and easily readable. Four problems are used to illustrate the idea: (a) a synthetic Gaussian data in which the mean of a fixed-variance Gaussian is allowed to wander; (b) an Ornstein-Uhlenbeck process arising in a climate model; (c) image evolution data of images of cars; and (d) stock (distribution) prediction. That is a good coverage of different types of problems which add strength to the idea explored in the manuscript. I have a couple of issues with the empirical work reported in the paper: (i) in the first example of the roving-mean Gaussian, with the variance fixed, the task is simply tracking a sine wave. How then are the distribution networks able to outperform the vanilla RNN (or even the MLP) - the fact that there is a distribution should not matter (what am I missing?). (ii) for the stock prediction problem, it looks like the RDRN shows worse performance than DRN both in terms of mu and sigma^2 in Fig. 4. [0.37 and 0.45; and 0.23 (RDRN) and 0.36(DRN)] Why is this so? Moreover, the problem itself makes me uncomfortable and needs better motivation. The distributions are constituent stocks (returns on them) of DOW, Nikkei and FTSE. I would think there is not much overlap between the constituent assets, and, when viewed as distributions, there is no natural ordering among them. So just by re-ordering the list of assets, we get a different-looking distribution we will attempt to fit. Is this not an issue? ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}