{"title": "interesting observations; but what insights to get out of it?", "review": "This paper proposes a surprisingly simple technique for improving the robustness of neural networks against black-box attacks. The proposed method creates a *fixed* random mask to zero out lower layer activations during training and test. Extensive experiments show that the proposed method without adversarial training is competitive with a state-of-the-art defense method under blackbox attacks.\n\nPros:\n -- simplicity and effectiveness of the method\n -- extensive experimental results under different settings\n\nCons:\n -- it's not clear why the method works besides some not-yet-validated hypotheses.\n -- graybox results seem to suggest that the effectiveness of the method is due to the baseline CNNs and the proposed CNNs learning very different functions; source models within the same family still produce strong transferable attacks. It would have been much more impressive if different randomness could result in very different functions, leading to strong defense in the graybox setting.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}