{"title": "An inspiring study on adversarial attacks for natural language", "review": "The authors provide a natural definition of adversarial examples for natural language transduction (meaning-preserving on source side while meaning-destroying on target side) and a human judgment task to measure it. They then investigate three different ways of generating adversarial examples and show that a metric based on character n-gram overlap (chrF) has a stronger correlation with human judgment. Finally, they show that adversarial training with the attack most consistent with the introduced meaning-preservation criteria results in improved robustness to this type of attack without degradation in the non-adversarial setting.\n\nOverall this is a strong paper. It is well structured, the problem studied is highly interesting and the proposed meaning-preserving criteria and human judgement will be useful to anyone interested in adversarial attacks for natural language. While the studied attack methods are fairly primitive, the empirical results are still interesting.\n\nComments\n---------------\nI wish the authors would include experiments with CharSwap where OOV is not forced as I'm not sure the assumption that OOV is more meaning-destroying in the target side is necessarily true (one could also argue that since the models are already trained with OOV words, they may be more robust to OOV words than in-vocabulary words in the wrong context).\n\nIt would be nice to add correlation for each type of constraint as well to Table 2. The result would be even stronger if the experiment was replicated in the opposite direction or for another language pair as well.\n\nI don't understand why the adversarial output in the second example in table 4 has a RDchrF of zero (the word July is completely dropped).\n\nFrom Table 6 it looks like random sampling is actually slightly better than adversarial training in terms of robustness to CharSwap attacks in the Transformer model. Moreover, the benefit of adversarial rather than random sampling is quite small in the LSTM model as well. This could be made more clear in the text.\n\nIt would be interesting to see how adversarial training with the CharSwap method fares against the unconstrained and kNN attacks in table 6.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}