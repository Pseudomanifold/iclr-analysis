{"title": "No real distinction from classical One vs. All classification", "review": "The paper discusses a method to increase accuracy of deep-nets on multi-class classification tasks by what seems to be a reduction of multi-class to binary classification following the classical one-vs-all mechanism. I fail to see any novelty in the paper. The problem of reducing multi-class to binary classification has been studied thoroughly with many classical papers like:\n\n1. Usual one-vs-all - this paper does the same thing as one vs all in my opinion even though this technique is known for a decade or so. \n2. Weighted One-Against-All - http://hunch.net/~jl/projects/reductions/woa/woa.pdf\n\nand more sophisticated techniques like:\n\n3. Alina Beygelzimer, John Langford, Pradeep D. Ravikumar. Error-Correcting Tournaments. CoRR, abs/0902.3176, 2009.\n4. Erin L. Allwein, Robert E. Schapire, Yoram Singer, Pack Kaelbling. Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers. Journal of Machine Learning Research, 113\u2014141, 2000.\n5. Thomas G. Dietterich, Ghulum Bakiri. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research, 2:263\u2014286, 1995.\n\nIn my opinion the methods in [1,2,3,5] above can be used with any binary learners and therefore deep-networks. This paper makes no effort in comparing with any of these well-known papers. Moreover the experiments do not show any gain in state of the art performances in the data-sets used, as experiments are done with toy-networks. Further some rules for selecting the class is discussed in Section 3. There are many known rules for generating probability scores in one-vs-all classification and the relations to these are not discussed. \n\nTherefore, I fail to see any novelty in this paper (theoretical or empirical). ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}