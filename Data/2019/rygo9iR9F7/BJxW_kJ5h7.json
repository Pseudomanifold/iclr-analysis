{"title": "The paper proposes a progressive pruning technique which imposes structural sparsity constraint on the weight parameter. Since solving the minimization with sparsity constraint is hard in general, the paper rewrites the optimization as an ADMM framework. While ADMM method suffers from slow convergence, a progressive weight pruning approach is proposed, which falls into curriculum learning.", "review": "The authors argue that ADMM-based approach achieves higher accuracy than projected gradient descent. However, experimental evidence is lacking. The authors should compare to a trivial variant of Adam that a projection step is followed by the gradient update.\n\nExperimental results are weak. It seems that the proposed method only works on small networks such as AlexNet and LeNet. On larger networks such as VGG-16 and ResNet, the proposed method achieves higher compression rates at the expense of lower accuracies compared to the related works. Thus, the authors should compare with other methods with the same compression rates.\n\nAs ADMM is sensitive to the penalty parameter, the authors should also conduct more experiments to show robustness of the choice of the penalty parameter across different experiments.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}