{"title": "Nice and diverse experiments, slightly limited novelty", "review": "# 1. Summary\nThis paper presents a novel spatio-temporal attention mechanism. The spatial attention is decomposed from the temporal attention and acts on each frame independently, while the temporal attention is applied on top of it on the temporal domain. The main contribution of the paper is the introduction of regularisers that improve performance and interpretability of the model.\n\nStrengths:\n* Quality of the paper, although some points need to clarified and expanded a bit more (see #2)\n* Nice diversity of experiments, datasets and tasks that the method is tested on (see #4)\n\nWeaknesses:\n* The paper do not present substantial novelty compared to previous work (see #3)\n\n\n# 2. Clarity and Motivation\nThe paper is in general clear and well motivated, however there are few points that need to be improved:\n* How is the importance mask (Eq. 1) is defined? The authors said \u201cwe simply use three convolutional layers to learn the importance mask\u201d, however the convolutional output should be somehow processed to get out the importance map, in order to match the same sizes of X_i. The details of this network are missing to be able to reproduce the model.\n* The authors introduced \\phi(H) and \\phi(X) which are feedforward networks, but their definition and specifics are not mentioned in the paper.\n* It is not clear how Eq. 9 performs regularization of the mask. Can the authors give an intuition about the definition of L_{contrast}? What does it encourages? In which cases might it be useful?\n* Why does L_{unimodal} need to encourage the temporal attention weights to be unimodal? It seems that the assumption is valid because of the nature of the dataset, i.e., the video clips contain only a single action with some \u201cbackground\u201d frames in the beginning and the end. This is not valid in general. Can the authors discuss about this maybe with an example?\n\n\n# 3. Novelty\nThe main concern of the proposal in this paper is its novelty. Temporal attention pooling have been explored in other papers; just to cite a popular one among others:\n* Long, Xiang, et al. \"Attention clusters: Purely attention based local feature integration for video classification.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n* Other paper from the youtube8m workshops explore the same ideas: https://research.google.com/youtube8m/workshop2017/ \nSec. 2.2 should be expanded by including papers and discuss how the presented temporal attention differs from that.\n\nMoreover spatio-temporal attention has been previously explored. For example, the following paper also decouple the spatial and temporal component as the proposal:\n* Song, Sijie, et al. \"An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data.\" AAAI. Vol. 1. No. 2. 2017.\nThis is just an example, but there are there are other papers that model the spatio-temporal extent of videos without attention for action recognition. The authors should expand Sec. 2 by including such relevant literature.\n\n\n# 4. Experimentation\nThe experiments are carried on video action recognition task on three public available datasets, including HMDB51, UCF101 and Moments in Time. The authors show a nice ablation study by removing the main components of the proposed method and show nice improvements with respect to some baseline (Table 1). Although the results are not too close to the state of the art for video action recognition on HMDB51 and UCF101, the authors first show nice accuracy on Moments in Time (Table 2).\n\nMoreover the authors show that the model can be useful on the more challenging task of weakly supervised action localization (UCF101-24, THUMOS). Specifically, spatial attention is used to localize the action in each frame by thresholding, showing competitive results (Table 3). Although some more recent references are missing, see the following paper for example:\n* G. Singh, S Saha, M. Sapienza, P. H. S. Torr and F Cuzzolin. \"Online Real time Multiple Spatiotemporal Action Localisation and Prediction.\" ICCV, 2017.\nThen the authors tested also for temporal action localization (Table 4).\n\nIn general, the paper is not showing state-of-the-art results, however the diversity of experiments, datasets and tasks that are presented makes it pretty solid and interesting.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}