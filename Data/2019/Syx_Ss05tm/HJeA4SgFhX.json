{"title": "Review for \"Adversarial Reprogramming of Neural Networks\" -- A Good paper with an interesting novel approach to adversarial attacks", "review": "Summary:\nThe authors present a novel adversarial attack scheme where a neural net is repurposed or \"reprogrammed\" to accomplish a different task than it the one it was originally trained on. This reprogramming from task1 to task2  is done through a given image from task2 additively enhanced with an adversarial program which is trained given the knowledge of the models parameters. A mapping from the repurposed output from task1 to relevant output for taks2 is also necessary (h_g function).\n\nReview:\nThis approach seems quite novel as it enables the repurposing of ImageNet classifiers to be used for counting dots in images, MNIST and CIFAR10 classifications. This new type of \"adversarial attack\" by repurposing a model shows surprising efficacy at allowing an attacked models to change its task at hand. Some tasks being more difficult (CIFAR10) than MNIST or counting dots.\n\nThe paper is well-written and explains clearly the proposed technique. The proposed technique is simple in its formulation.\nThe assumption it is based on (access to model parameters) is acceptable for the sake of proof of concept.\nOverall it is an interesting paper to read and seems of significance for the community working on adversarial attacks.\n\nFew comments/questions come to mind though:\n- The adversarial images are quite different from a common image as they embed the program around the new task images. This makes the technique itself quite susceptible to detection (just look at the statistics of the input images).\n- How do you handle front end processing? Usually for ImageNet classification, a system will (for instance) resize its input to 256x256, center crop to 224x224 and renormalize the RGB features to match the statistics from the training data. It looks like the images generated are passed as inputs to the system. Do you assume that the front-end steps are not applied or do you assume it is (by including them in the network while training your program W).  My assumption is that you include those steps in the training network for W.\n- The size of the program is disproportionately big compare to the task2 embedded image. This begs the question: what happens when you limit the size of the program to a smaller percentage of the whole image? When do you see a break in the reprogramming? Do you need that much extra programming W in your adversarial images?\n- As the adversarial images seem to be quite easy to detect, would it be easy to integrate it into some task1 images? The equation (2) gives X_{adv} = \\tilda{X} + P, could you use X_{adv} + w * X_{task1}, basically finding a way to hide the program and task2 image within a task1 image. This seems difficult, but have you thought of such approach?\n\nOverall this is a paper that is a pleasant read and should be considered for publication.\n\nPost Rebuttal: The draft paper improves on the original paper and demonstrates possible concealment of the program. I adjusted my rating upward to 8.  ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}