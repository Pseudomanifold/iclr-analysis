{"title": "Interesting idea but not sufficiently convincing", "review": "This paper presents a variation of dropout, where the proposed method drops with higher probability those neurons which contribute more to decision making at training time. This idea is evaluated on several standard datasets for image classification and action recognition.\n\nPros:\n1. This paper has interesting idea related to dropout, and shows some benefit.\n2. Paper is well-written and easy to understand.\n\nCons:\n1. There are many variations in dropouts and they all claim superiority to others. Unfortunately, most of them are not justified properly. Excitation dropout looks interesting and has potential, but its validation is not strong enough. Use of Cifar10/100, Caltech256, and UCF 101 may be okay for concept proofing, but not be sufficient for thorough validation. Also, the reported results are far from the state-of-the-art performance of each dataset. I would recommended to add the idea to the network \n to achieve the state-of-the-art performance because it will show real extra benefit of \"excitation\" dropout. \n\n2. There are many variations of dropouts including variational dropout, L0-regularization, and adaptive dropout, and the paper needs to report their accuracy in addition to curriculum dropout.\n\n3. Dropout does not exist in many modern deep neural networks and its usability is a bit weak. It would be better to generalize this idea and make it applicable to ResNet-style networks.\n\n4. There is no clear (theoretical) justification and intuition why excitation dropout improves performance. More ablation study with internal analysis would be helpful.\n\nOverall, this paper has interesting idea but needs more efforts to make the idea convincing.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}