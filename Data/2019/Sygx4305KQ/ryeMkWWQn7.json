{"title": "Interesting research direction but the paper needs a lot more work before publication", "review": "This paper proposes an approximate second-order method with low computational cost. A common pitfall of second-order methods is the computation (and perhaps inversion) of the Hessian matrix. While this can be avoided by instead relying on Hessian-vector products as done in CG, it typically still requires several iterations. Instead, the authors suggest a simpler approach that relies on one single gradient step and a warm start strategy. The authors points out that the resulting algorithm resembles a momentum method. They also provide some simple convergence proofs on quadratics and benchmark their method to train deep neural networks.\n\nWhile I find the research direction interesting, the execution is rather clumsy and many details are not sufficiently motivated. Finally, there is a lot of relevant work in the optimization community that is not discussed in this paper, see detailed comments and references below.\n\n1) Method\nThe derivation of the method is very much driven on a set of heuristics without theoretical guarantees. In order to derive the update of the proposed method, the authors rely on three heuristics:\na) The first is to reuse the previous search direction z as a warm-start. The authors argue that this might be beneficial if If z does not change abruptly. In the early phase, the gradient norm is likely to be large and thus z will change significantly. One might also encounter regions of high curvature where the direction of z might change quickly from one iteration to the next.\nThe \"warm start\" at s_{t-1} is also what yields the momentum term, what interpretation can you give to this choice?\n\nb) The second step interleaves the updates of z and w instead of first finding the optimum z. This amounts to just running one iteration of CG but it is rather unclear why one iteration is an appropriate number. It seems one could instead some adaptive strategy where CG with a fixed accuracy. One could potentially see if allowing larger errors at the beginning of the optimization process might still allow for the method to converge. This is for instance commonly done with the batch-size of first-order method. Gradually increasing the batch-size and therefore reducing the error as one gets close to the optimum can still yield to a converging algorithm, see e.g. \nFriedlander, M. P., & Schmidt, M. (2012). Hybrid deterministic-stochastic methods for data fitting. SIAM Journal on Scientific Computing, 34(3), A1380-A1405.\n\nc) The third step consists in replacing CG with gradient descent.\n\"If CG takes N steps on average, then Algorithm 2 will be slower than SGD by a factor of at least N, which can easily be an order of magnitude\".\nFirst, the number of outer iterations may be a lot less for the Hessian-free method than for SGD so this does not seem to be a valid argument. Please comment.\nSecond, I would like to see a discussion of the convergence rate of solving (12) inexactly with krylov subspace methods. Note that Lanczos yields an accelerated rate while GD does not. So the motivation for switching to GD should be made clearer.\n\nd) The fourth step introduces a factor rho that decays z at each step. I\u2019m not really sure this makes sense even heuristically. The full update of the algorithm developed by the author is:\nw_{t+1} = w_t - beta nabla f + (rho I - beta H) (w_t - w_{t-1}).\nThe momentum term therefore gets weighted by (rho I - beta H). What is the meaning of this term? The -beta H term weights the momentum according to the curvature of the objective function. Given the lack of theoretical support for this idea, I would at least expect a practical reason back up by some empirical evidence that this is a sensible thing to do.\nThis is especially important given that you claim to decay rho therefore giving more importance to the curvature term.\nFinally, why would this be better than simply using CG on a trust-region model? (Recall that Lanczos yields an accelerated linear rate while GD does not).\n\n2) Convergence analysis\na) The analysis is only performed on a quadratic while the author clearly target non-convex functions, this should be made clear in the main text. Also see references below (comment #3) regarding a possible extension to non-convex functions.\nb) The authors should check the range of allowed values for alpha and beta. It appears the rate would scale with the square root of the condition number, please confirm, this is an important detail. I also think that the constant is not as good as Heavy-ball on a quadratic (see e.g. http://pages.cs.wisc.edu/~brecht/cs726docs/HeavyBallLinear.pdf), please comment.\nc) Sub-sampling of the Hessian and gradients is not discussed at all (but used in the experiments). Please add a discussion and consider extending the proof (again, see references given below).\n\n3) Convergence Heavy-ball\nThe authors emphasize the similarity of their approach to Heavy-ball. They cite the results of Loizou & Richtarik 2017. Note that they are earlier results for quadratic functions such as \nLessard, L., Recht, B., & Packard, A. (2016). Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1), 57-95.\nFlammarion, N., & Bach, F. (2015, June). From averaging to acceleration, there is only a step-size. In Conference on Learning Theory (pp. 658-695).\nThe novelty of the bounds derived in Loizou & Richtarik 2017 is that they apply in stochastic settings.\nFinally, there are results for non-convex functions such convergence to a stationary point, see\nZavriev, S. K., & Kostyuk, F. V. (1993). Heavy-ball method in nonconvex optimization problems. Computational Mathematics and Modeling, 4(4), 336-341.\nAlso on page 2, \"Momentum GD ... can be shown to have faster convergence than GD\". It should be mentioned that this only hold for (strongly) convex functions!\n\n4) Experiments\na) Consider showing the gradient norms. \nb) it looks like the methods have not yet converged in Fig 2 and 3.\nc) Second order benchmark:\nIt would be nice to compare to a method that does not use the GN matrix but the true or subsampled Hessian (like Trust Region/Cubic Regularization) methods given below.\nWhy is BFGS in Rosenbrock but not in NN plots?\nd) \"Batch normalization (which is known to improve optimization)\" \nThis statement requires a reference such as\nTowards a Theoretical Understanding of Batch Normalization\nKohler et al\u2026 - arXiv preprint arXiv:1805.10694, 2018\n\n5) Related Work\nThe related work should include Cubic Regularization and Trust Region methods since they are among the most prominent second order algorithms. Consider citing Conn et al. 2000 Trust Region,  Nesterov 2006 Cubic regularization, Cartis et al. 2011 ARC.\nRegarding sub-sampling: Kohler&Lucchi 2017: Stochastic Cubic Regularization for non-convex optimization and Xu et al.: Newton-type methods for non-convex optimization under inexact hessian information.\n\n6) More comments\n\nPage 2\nPolyak 1964 should be cited  where momentum is discussed.\n\"Perhaps the simplest algorithm to optimize Eq. 1 is Gradient Descent\". This is technically not correct since GD is not a global optimization algorithm. Maybe mention that you try to find a stationary point\nrho (Eq. 2) and lambda (Eq. 4) are not defined\n\nPage 4: \nAlgorithm 1 and 2 and related equations in the main text: it should be H_hat instead of H.\n\nBackground\n\u201cMomemtum GD exhibits somewhat better resistance to poor scaling of the objective function\u201d\nTo be precise the improvement is quadratic for convex functions. Note that Goh might not be the best reference to cite as the article focuses on quadratic function. Consider citing the lecture notes from Nesterov.\n\nSection 2.2\nThis section is perhaps a bit confusing at first as the authors discuss the general case of a multivalue loss function. Consider moving your last comment to the beginning of the section.\n\nSection 2.3\nAs a side remark, the work of Dauphin does not rely on the Gauss-Newton approximation but a different PSD matrix, this is probably worth mentioning.\n\nMinor comment: The title is rather bold and not necessarily precise since the stepsize of curveball is not particularly small e.g. in Fig 1.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}