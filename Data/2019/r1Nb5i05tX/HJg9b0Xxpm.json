{"title": "The effectiveness of layer-by-layer training using the information bottleneck principle", "review": "\nThis work is about layer-wise training of networks by way of optimizing the IB cost function, which basically measures the compression of the inputs under the constraint that some degree of information with respect to the targets must be preserved. Both terms of the IB cost function are formalized as mutual informations, but since in neural nets, the latent \"compression\" is a deterministic function of the inputs, a severe technical problems arises: the joint distribution between p-dimensional inputs X and the q-dimensional latent compression L is degenerate in that  its support lies in a space of dimension p (and not p+q as it would be in the non-degenerate case). As a consequence, no p.d.f. exists (with respect to the Lebesgue measure of R^{p+q}). Thus, defining mutual information is cumbersome. The paper attempts to overcome this problem by using a noisy version of the latent compression, i.e. L' = L + \\epsilon, which can be seen as an \"ad hoc\" fix of this problem. Not too surprising, this additive noise works as a ridge-type (or weight-decay) regularizer, just as a Gaussian prior in regression.\n\nOn one hand, I find this paper interesting, because it aims at carefully studying the proposed link between DNN training and IB optimization, thereby showing that layer-wise IB training indeed seems to work very well in practice. Such results are certainly interesting, both from a theoretical and from a practical point of view. On the other hand, I honestly think that on the conceptual side, this work does not make that many really interesting contributions. The observation that additive noise works as a weight-decay regularizer is in my opinion almost trivial, and any claims about experimental results \"validating(!) the IB theory\" seem to contain some degree of over-selling. In summary, I think that this is a paper that certainly contains some interesting ideas, but on the other hand I am not fully convinced about the significance and relevance of the findings.       ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}