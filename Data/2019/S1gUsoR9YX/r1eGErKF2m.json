{"title": "Solid experimentation but...", "review": "... I would have liked to see some more insights.\n\nThe authors present a method for distilling knowledge from individual models to train a multilingual model. The motivation stems from the fact that while most s-o-t-a multilingual models are compact (as compared to k individual models) they fall short of the performance of the individual models. The authors demonstrate that using knowledge distillation, the performance of the multilingual model can actually be better than the individual models. \n\nPlease find below my comments and questions.\n\n1) The authors have done a commendable job of validating their hypothesis on multiple datasets. Solid experimentation is definitely the main strength of this paper.\n\n2) However, this strength also makes way for a weakness. The entire experimental section is just filled with tables and numbers. The same message is repeated across these multiple tables (multi+distill > single > multi). Beyond this message there are no other insights. For example, \n\n- How does the performance depend on the divergence between source and target language?\n- Why is there more important on some languages and less on others ?\n- Why are the improvements on the TED dataset so much higher as compared to the other 2 datasets.\n- What happens when the target language is something other than English? All the experiments report results from X-->English, why not in the other direction? The model then is not really \"completely\" multilingual. It is multi-source-->single target. \n- Can you comment on the total training time ?\n- What happens when you do not stop the distillation even when the accuracy of the student crosses that of the teachers ? What do you mean by accuracy here? Only later when you mention that \\threshold = 1 BLEU it became clear that accuracy means BLEU in this context ?\n\n3) Is it all worth it? One disappointing factor is that end of all this effort where you train K individual models and one monolithic model with distillation, the performance gain for most language pairs is really marginal (except on the TED dataset). I wonder if the same improvements could have been obtained by even more carefully fine tuning the baseline models itself.\n\n4) On a positive note, I like the back-distillation idea and the experiments on top-K distillation\n\n+++++++++++++++++++\nI have updated my rating after reading author's responses\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}