{"title": "Dnn compression and acceleration", "review": "REVISED: I am fine accepting. The authors did make it a bit easier to read (although it is still very dense). I am also satisfied with related work and comparisons\nSummary: \nThis paper proposes to activate only a small number of neurons during both training and inference time, in order to speed up training and decrease the memory footprint. This works by constructing dynamic sparse graph for each input, which in turn decides which neurons would be used. This happens at each iteration and it does not permanently remove the neurons or weights. To construct this dynamic sparse graph, authors use dimensionality reduction search which estimates the importance of neurons\n\nClarity:\nOverall I found it very hard to follow. Lots of accronyms, the important parts are skipped (the algorithm is in appendix) and it is very dense and a lot of things are covered very shallowly. It would have been better for clarity to describe the algorithm in more details, instead of just one paragraph, and save space by removing other parts.  I would not be able to implement the proposed solution by just reading the paper\n\nDetailed comments.\nThis reminds me a lot of a some sort of supervised dropout. \n\nMy main concern, apart from clarity, is that there is no experimental comparison with any other method. How does it compare with other methods of dnn compression or acceleration?\n\nAlso i found the literature review is somewhat lacking. What about methods that induce sparsity via the regularization, or those that use saliency criterion, hessian based approaches like Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. NIPS, 2015. , pruning filters Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for, efficient convnets. ICLR, 2017.  etc. \nBasically i don't understand how it compares to alternative methods at all.\n\nQuestions:\nHow does it run during inference? does inference stay deterministic (there is a random projection step there)\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}