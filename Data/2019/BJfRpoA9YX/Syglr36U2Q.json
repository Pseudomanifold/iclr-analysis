{"title": "Interesting idea, Too complex model", "review": "This paper proposed a generative model to learn the representation which can separates the identity of an object from an attribute. Authors extended the autoencoder adversarial by adding an auxiliary network. \n\nStrength\nThe motivation of adding this auxiliary network, which is to distinguish the information between latent code z and attribute vector y, is clean and clear.\nExperiments illustrate the advantage of using auxiliary network and demonstrating the role of classify. Experimental results also show the proposed model learning to factor attributes from identity on the face dataset.\n\nWeakness \nThe proposed model seem to be unnecessarily complex. For example, the loss of  in (6) actually includes 6 components (5 are from L_enc) and 4~5 tuning hyper-parameters. The L_gan also includes 3 parts. The reason of adding gan loss lacks either theoretical or empirical analysis. So as L_KL. In addition, the second term in L_gan is unnecessary since you already have a reconstruction loss. It also make it to be unclear what we obtain if the equilibrium of the GAN objective achieved.\n\nThe written of this paper can be improved to make it more clear. \nIt looks \\hat_y and \\tilde_y are same thing. \nHow do you get \\hat_z? Do you assume the posterior distribution is Gaussian and use the reparameterization trick? What are \\hat_y and \\hat_\\hat_y? Are they binary or a scalar between 0 and 1?  How do you generate \\hat_x? When generating \\hat_x, do you sample \\hat_z and \\hat_y? If so, how do treat the variance problem of \\hat_y? \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}