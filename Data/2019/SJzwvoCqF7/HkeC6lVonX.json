{"title": "Different Take on Generalization - Size Dependent Bounds", "review": "The paper provides a generalization bound for multi-layered deep neural networks in terms of dimensions rather than norms. The bound is derived by controlling Rademacher complexity of the Ramp loss under the Lipschitzness of the network as a parametric function in Depth * Width ^2 number of parameters, and then using standard L-2 covering and Dudley Integral. They extend this technique for CNNs, Resnets, Hyper-spherical Networks, etc and provide specialized bounds for each case. In the end, the authors provide comparisons to the existing bounds. \n\nAlthough intended, the bound in Theorem-1 depends on the number of parameters and hold only if m > d * (p^2) = number of parameters (from the last line of proof of Lemma 3, we need \\beta < \\alpha and thus m > h). Such bounds are already know in the literature (see Anthony and Bartlett, 1999). Adaptive (completely norm dependent, like Bartlett et. al. 2017) bounds will be better than explicit dimension dependent bounds. The comparison in Figure-1 which suggest their bound to be better is unfair because they are comparing their specialized bounds for CNN to generic bounds for standard feedforward networks. Same for comparison in Table-2. \n\nIt was already established in Theorem 3.4 (Bartlett et al. 2017) that spectral norms are necessary for any generalization bounds for Deep Neural Networks, thus voiding the claims made in the paper (and discussion) about the importance of spectral norms. \n\nTypos / Errors : \n1. Statement of Lemma 2 does not contain the spectral norms terms. \n2. The third equation in Page 13 should be K <= \\sqrt{pD} max B_{d, 2}; and this changes the bound further. \n\nThe paper introduces some new techniques on mathematical analysis of specialized neural networks. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}