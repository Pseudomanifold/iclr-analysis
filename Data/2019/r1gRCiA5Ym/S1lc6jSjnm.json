{"title": "not well explained and not rigorously tested", "review": "This paper proposes jumpout, which is a 3 step modification based on dropoout\nthat is designed to work better with batch normalization. Unfortunately, I did not understand the arguments on locally linear regions and ReLu and its relationship with the monotone dropout scheme,\nor why the half Gaussian is chosen.\n\nStill, jump out the procedure is fairly clear in Algorithm 1, and the results seems good.\nHowever, I could not make out much of why each step is done, and could not find empirical tests of the value of each step.\n\nI think the paper needs more work. All the proposals seem very heuristic, and it is important to test their separate effects. It should be easy to perform a ablation analysis since the 3 proposed steps are pretty independent and can be tested separately. Since two of these have to do with modifying the dropout rate, it would be important to compare with carefully cross-validated dropout rates, which I also do not see.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}