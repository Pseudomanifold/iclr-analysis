{"title": "needs further empirical evidence", "review": "Authors propose three modifications to dropout, specifically in context of dropout applied to deep networks utilizing the ReLU non-linearity.  The three modifications seem independently motivated and aim to overcome separate potential shortcomings of the current dropout approach.  These three modifications are combined into a new approach termed Jumpout.\n\nOverall I find this to be a weak paper requiring further work, for the following main reasons:\n\n* The proposed modifications are intuitively motivated and then empirically supported.  However, I find the intuitive reasoning unclear and have to lean much more on empirical evidence.  For instance, the motivation for modification 2 \u201cdropout rate adapted to number of active neurons\u201d, is that in case ReLU causes a large number of neurons to \u2018shut down\u2019 then the dropout rate in that layer should be reduced (or increased, depending on how it is defined) causing fewer neurons to further dropout.  However, if preventing co-adaptation is a reason to dropout neurons then the issue of conditional correlation (or co-activation given related inputs) will remain regardless of number of active neurons in a layer, thus changing the dropout rate as a function of ReLU activation is not fully justified.  Similarly, modification 3 \u201crescale outputs to work with batch normalization\u201d proposes exponentiation by -0.75 with weak justification as a compromise.\n\n* I find the empirical evidence and support for the three modifications lacking in detail.  The authors provide results of the combined Jumpout technique on a number of tasks, but do not demonstrate effectiveness and contribution of individual modifications on error rates on the tasks they evaluated.\n\n* I also find the baseline systems to be on the weaker side (e.g. on CIFAR100 many systems now have higher than 82% accuracy with best being over 84, on STL-10 many systems now are well above 85%).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}