{"title": "A simple extension of multi-level attention, but needs more extensive comparison to existing methods", "review": "The paper introduces hierarchical attention, where they propose to weighted combine all the intermediate layers of multi-level attention. The idea is simple and seems to be promising, however originality seems incremental.\n\nIn order to fully demonstrate the significance of the proposed algorithm, the authors should conduct more comparisons, for example, to multi-level attention. Just comparing with one-level attention seems unfair given the significant increase of computation. Another aspect of comparison may be to consider computation and performance improvements together and discuss the best trade-off. The authors should also include some standard benchmark datasets for comparisons. The current ones are good but it is not so clear what is the best state-of-the-arts results on them when compared with all other methods.\n\nThe analysis on the network's representation and convergence is nice but it does not bring much insights. The argument for decreasing global minimal of the loss function in terms of increasing parameter size can be made for nearly all models but it is of little practical use since there is no guarantee one can reach the global optimal of these models.\n\nI recommend the authors to analyze/demonstrate how effective this weighted combination is. For example, the paper can benefit from some clear examples that show the learned weights across the layers and which ones are more important.\n\nThe presentation of the paper needs some polishing. For example, there are numerous typos, grammatical errors everywhere.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}