{"title": "Lacks Novelty , incomplete results", "review": "Overall, this is an incremental paper.\nThe authors propose a hierarchical attention layer, which computes an aggregation of self attention layer outputs in the multi level attention model. This seems like a small improvement.\n\nThere are results using this hierarchical attention layer instead of the vanilla attention layers on Machine Reading Comprehension and Chinese Poem Generation. The authors should have also included results on more tasks to show the clear improvement of the proposed method.\n\nThe issues with this paper are:\n- Aggregating weights of different layers has been an idea explored before (Elmo, Cove, etc.). So the model improvement itself seems small.\n- Lack of strong experimental evidence. In my regard, the experiments are somewhat incomplete. In both the tasks, the authors compare only the vanilla model (BIDAF, MatchLSTM, R-NET) and the model with HAM layers. It is not clear where the improvement is coming from. It would have made sense to compare the number of parameters and also, using the same number of vanilla attention layers  which outputs the last layer and compare it to the one proposed by the authors.\n- Since the argument is towards using weighted average rather than the last layer, there should have been a more detailed analysis on what was the weight distribution and on how important were representations from different layers.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}