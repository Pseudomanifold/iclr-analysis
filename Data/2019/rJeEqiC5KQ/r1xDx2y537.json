{"title": "The paper adresses the problem of incremental learning when data from new classes are available as a stream and one wants to be able to update to learn new observed classes without forgetting the older ones.  This is a relevant problem but the paper strongly lacks precision about the proposed method that looks like an incremental work with limited innovation but that the reader can only try to guess.", "review": "The paper adresses the problem of incremental learning when data from new classes are available as a stream and one wants to be able to update to learn new observed classes without forgetting the older ones. There is a budget issue here and one does not want to just keep the whole training set of all known previously observed classes but rather one wants to consider a maximum memory budget allowed to store what is necessary for an optimal incremental learning (typical examples, statistics etc). There is also a privacy issue preventing from storing original training samples.\n\nThis is a relevant problem that is has gain interest in the last few years. It is related to topics such as few shot learning and meta few shot learning (with respect ti the number of examples per class that are kept, which is limited) and somehow to budget learning . Yet these topics and associated references are surprisingly not evoked in the text.\n\nThe paper is rather well written but it strongly lacks precision about the proposed method. A description of the ICARL state of the art method is missing and would have been mandatory since the proposed work appears to build on iCARL method. Actually the description of the method is very short since the dedicated section (\u00a74) is mainly used to describe a rather standard convolutional auto encoder architecture. At the end one tries to guess what the proposed method consists in. As far as i understand it is based on iCARL method where selected examples of past observed classes are not stored as is but in their encoded form (by the convolutional autoencoder). At the end my understanding of the proposed approach is that it consists in an incremental progress of a state of the art method, then an incremental work with limited innovation.\n\nBy the way i am not sure of the meaning of pseudo exemplar as used in the proposed method. Are these drawn following a distribution computed on training samples ? Or are these pseudo exemplar because you use reconstructed samples from encodings (by the CAE).  \n\nWhen looking at experimental results the proposed method seem to bring some benefit but it does not look fully convincing. As written in the paper the proposed system outperforms iCARL in case the examples are encoded in the same dimension as original examples (hence no benefit on the storage side) but reaches similar performance when using less storage capacity. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}