{"title": "An interesting idea for disentangled patch representation learning as a drop-in module for UDA. Method effective but relative weak results compared to SOTA", "review": "This paper proposes a drop-in module of disentangled patch representation learning for adversarial learning-based domain adaptation. The main idea is to encourage the source patch level representation to be disentangled, by creating certain intermediate pseudo-ground truths via clustering the label patch histograms using k-means. This basically creates an alternative, additional view of prediction target of the network outputs. And similar to global network output alignment by Tsai et al., the authors impose an adversarial loss on the additionally introduced view.\n\nClarity: The paper is well-written with good clarity. \n\nResults: This paper has a good experimental validation of proposed module.\n\nConcerns: \n- The idea of using patches in domain adaptation is not completely new. ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes, CVPR 2018 also uses the patch level information to help domain adaptation. Although the ideas are not entirely identical, this paper should at least cite and compare this work.\n\n- The disentangled patch feature learning introduces two additional loss, L_d and L_adv^l, which require three extra parameters, including K in K-means, lambda_d and lambda_adv^l. It will be great if a formal sensitivity analysis on the parameters can be conducted. There are some details missing in the paper too. For example, what is the performance of the VGG source model without adaptation? I am also curious about the learning behavior of the proposed method. Could you show the mIoU v.s. epoch curve for GTA2Cityscapes, or any other benchmarks?\n\n- Although consistently improving over Tsai et al., CVPR18, the introduced methods does not show very significant gain in multiple experiments. On SYNTHIA-to-City, only 0.4 mIoU gain is obtained. In addition, while the proposed method is empirically effective, it is largely task-specific and restricted to domain adaptation for scene parsing only. It seems difficult to generalize the same method to other domain adaptation tasks. The limitation on the performance gain and generalizability somehow reduced the contribution from this work to the community.\n\n- A major concern of this work is the lack of citation and direct comparison to multiple previous SOTAs. For example, the paper should compare the end-system performance with several published works such as:\n1. Zhang et al., Fully convolutional adaptation networks for semantic segmentation, CVPR2018\n2. Zhu et al., Penalizing top performers: conservative loss for semantic segmentation adaptation, ECCV2018\n3. Zou et al., Domain adaptation for semantic segmentation via class-balanced self-training, ECCV2018\nAnd according to the results reported by these works, the proposed joint framework in this paper does not seem very competitive in terms of the UDA performance in multiple settings", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}