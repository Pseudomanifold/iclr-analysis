{"title": "Review", "review": "The authors tackle the unsupervised domain adaptation problem on tasks with structured output (in this case, semantic segmentation) by performing adversarial alignment at two levels: globally, using the entire image, and locally, using patches of the image. Their global alignment method matches previous adversarial adaptation approaches, so the primary contribution appears to be their patch-level alignment method. They cluster source image patches by histogramming the corresponding label patches, then performing K-means clustering on the histogrammed label features. A new model is trained to reproduce the cluster labels from the source image patches, and this model is adversarially optimized so that target image patches produce a matching feature distribution.\n\nThe paper is well-written and concise. It's organized well, and I had very little trouble following the description of their method. The various components of their model are straightforward and well-motivated. They validate their model on multiple synthetic-to-real segmentation tasks, demonstrating strong performance relative to existing baselines, and they also provide a thorough ablation study showing that each of the components of their proposed model is an important part of their final product, which further convinces the reader that the model is sound.\n\nOne quibble is that the authors mention disentanglement quite a bit in this paper, including in the title, though it isn't clear to me what is being disentangled. They claim the use of of label information is a disentangling factor, but that seems to be true of domain adaptation approaches in general, which all attempt to disentangle semantic information from domain-specific details in some form or other. Further clarification on precisely what is being disentangled would be helpful.\n\nAnother question that lingers is whether or not the additional classification module $H$ and the clustering are truly necessary. A baseline I would like to see would be to remove $H$ entirely and simply train another adversarial discriminator similar to $D_g$ directly on patches of $O$ instead of the full output. This sounds similar to the ablation experiment mentioned in 4.2 where $L_d$ is removed, but my understanding is that ablation experiment still uses an additional featurizer $H$. A more rigorous exploration of the clustering process, such as visualizations of learned clusters and a study of how the number of clusters affects performance would serve to further validate the model.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}