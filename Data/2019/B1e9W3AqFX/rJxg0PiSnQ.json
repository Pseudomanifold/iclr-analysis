{"title": "Review", "review": "This paper proposes that models for different tasks in multi-task learning cannot only share hidden variables but also gradients.\n\nPros:\n- The overall framework is theoretically motivated and intuitive. The idea of passing gradients for multi-task learning is interesting and the execution using fast weights is plausible.\n- The experiments are extensive and cover three different task combinations in different domains.\n- The results are convincing and the additional analyses are compelling.\n\nCons:\n- I would have liked to see a toy example or at least a bit more justification for the \"pretend-to-share\" problem that models \"collect all the features together into a common space, instead of learning shared rules across different tasks\". As it is, evidence for this seems to be mostly anecdotal, even though this forms the central thesis of the paper.\n- I found the use of Read and Write ops confusing, as similar terminology is widely used in memory-based networks (e.g. [1]). I would have preferred something that makes it clearer that updates are constrained in some way as \"writing\" implies that the location is constrained, rather than the update minimizing a loss.\n\nQuestions:\n- How is the weight list of task similarities \\beta learned when the tasks don't share the same output space? How useful is the \\beta?\n- Could you elaborate on what is the difference between pair-wise gradient passing (PGP) and list-wise gradient passing (LGP)\n\n[1] Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}