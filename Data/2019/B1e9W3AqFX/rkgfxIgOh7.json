{"title": "just packing exsiting algorithms", "review": "This paper tries to address the\"pretend-to-share\" problem by designing the gradient passing schemes in which the gradient updates to specific parameters of tasks are passed to the shared parameters. Besides, the authors summarize existing multitask learning algorithms in a framework called Parameters Read-Write Networks (PRAWN). \n\nPros:\n- The view of putting existing multi-task learning algorithms in a read-write framework is quite intriguing and inspiring.\n\nCons:\n- Motivation: The whole paper is assumed to address the \"pretend-to-share\" problem, while the authors never provide any evidence that such problem really exists for any other algorithm. It seems to be an assumption without any support.\n- Method:  \n   - Though the read-write framework is very interesting, the authors do not clearly present it, so that the readers can be totally get lost. For example, what do you mean by writing {\\Theta^{*r}_k - \\theta^{swr}_k}? In the line of structural read-op, where are \\theta_3 and \\theta_4  in the column of the constituent para. ? What do you mean by writing the equation (4)? How do you define g() in equation (8)? This is a research paper which should be as clear as possible for the readers to reproduce the results, rather than a proposal only with such abstract and general functions defined. \n   - In the list-wise communication scheme, you define the task relationship in equation (11). The problem is how do you evaluate the effectiveness of such definition, since massive works in multitask learning pursue to learn the task relationship automatically to guarantee the effectiveness instead of such heuristic definition. \n- Related works: The authors do not clearly and correctly illustrate the connections between this work and meta-learning/domain adaptation. To my best knowledge, meta-learning, including MAML (Finn et al. 2017), can obviously solve both in-task setting and out-task setting. In some sense, I think this work is almost equivalent to MAML. \n- Experiments: \n   - First, several state-of-the-art baselines including MAML and cross-stitch networks should be compared. Specifically, for the text classification dataset, there have been a lot of domain adaptation works discovering the transferable pivots (shared features) and non-pivots (specific features), which the authors should be aware of and compare in Table 3.  \n   - The Figure 5 is not clear to me, and so is the discussion. The authors try to explain that the updating direction of shared parameters for PGP-SR is an integration of two private updating directions. I tried hard to understand, but still think that Figure 5(a) is even better than Figure 5(b). The updating direction of the shared parameters is almost the same as the cyan line.\n- Presentation: there are so many grammatical errors and typos. For example,\n   - In the introduction, \"...datasets, range from natural\" -> \"...datasets, ranging from natural\"\n  - In the related work, \"and they propose address it with adversarial\" -> \"and they propose to address it with adversarial\"\n - In the beginning of Section 4, \" an general\" -> \"a general\"", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}