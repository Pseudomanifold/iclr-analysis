{"title": "A good paper with interesting contributions that requires improved motivations and some clarifications", "review": "Summary: the paper proposes a method for Deep Neural Networks (DNN) that identifies automatically relevant features of the set of the classes, enriching the predictions made with the visual features that contributed to that class, supporting, thus, interpretation (understanding what the model has learned) and explanation (justification of the predictions/classifications made by the model). This scheme does not rely on additional annotations, like earlier techniques do.\n\nThe contributions of this paper are relevant to, I would say, a large segment of the AI community, since interpretability and explainability of AI (XAI) is the focus of many current works in the area, and there are still many unresolved issues. I consider this paper suitable for ICLR 2019, in particular, it fits the call for papers topic \u201cvisualization or interpretation of learned representations\u201d.\n\nThe authors also present a new dataset (am8Flower) that can be used by the community for future evaluations of explanation methods for DNN. From my point of view, this is a significant contribution, since there is a lack of datasets that can be used for evaluation.\n\nThe authors motivate properly the need for this research/study, addressing the main weakness of the two more common strategies for interpreting DNN, (1) manually inspecting visualizations of every single filter or (2) comparing the internal activations produced by a given model w.r.t. a dataset with pixel-wise annotations of possibly relevant concepts.\n\nI would encourage the authors to write the limitations and weakness of their proposal w.r.t. similar approaches they reviewed. I am aware that the space is limited, but in p.8, section 4.3, when Table 1 is introduced and the authors confirm that their proposal has higher IoU than other methods, the authors could explain, in brief, what are the weaknesses of their method w.r.t. the other approaches analyzed.\n\nAnother clarification concerns the initialization of input parameters, such as sparsity; e.g., p.6 sparsity is initialized with 10 for all datasets, why? How has this value been selected and how sensitive is the performance regarding variations of this value?\n\nOnce again, I know that the space is limited, but I would like to be able to see some of the figures better (since this is an essential part of the paper). The additional material complements very well the paper and shows larger figures, but I think that the paper itself should be self-sufficient, and figures like Fig. 5 should be enlarged so it is easier to see some details.\n\nJust a concern or something that I quite did not understand about one of the arguments the authors use to justify the evaluation carried out: the authors claim that they want to avoid the subjectivity introduced by humans (citing Gonzalez-Garcia et al. 2017), and prefer to avoid user studies, presenting a more objective approach in their evaluation. Ok, but then, the analysis presented in, for example, page 7, is based mainly in their interpretation of the results, a qualitative analysis of the images (we can see fur patterns, this and that, etc.). So aren\u2019t they interpreting the results obtained as users? So after all, aren\u2019t the visual explanations and feedback intended for users? Why should we claim that we want to avoid the subjectivity introduced by humans in the evaluation when the method proposed here is actually going to be used by users \u2013with their inherent subjectivity? I do not mean that the evaluation carried out is not interesting per se, but it could be motivated differently, or it could be complemented later on with future user studies (that would make an interesting addition to the paper). Moreover, I also wonder whom the authors see as intended users for the proposed scheme.\n\nSmall comments:\nP.1 \u201cuseful insights on the internal representations\u201d \uf0e0 insights into the internal representations.\nP. 2: space needed in \u201cback-propagation methods.Third,\u201d\nP. 3: Remove \u201cs\u201d in verb (plural authors): \u201cSimilarly, Bach et al. (2015) decomposes the classification\u201d \uf0e0 decompose or decomposed\nP.3: n needed \u201cChattopadhyay et al. (2018) exteded\u201d \uf0e0 extended\nP.3: \u201cThis saliency-based protocol assume that\u201d \uf0e0 protocol assumes\nP.3: \u201chighlighted by the the explanations\u201d \uf0e0 remove one \u201cthe\u201d\nP. 5: \u201cspace. As as result we get\u201d \uf0e0 remove one \u201cas\u201d\nP. 5: \u201cand compensate this change\u201d \uf0e0 compensate for this change\nP. 6: \u201cIn this experiment we verify\u201d \uf0e0 In this experiment, we verify\nP. 6: \u201cTo this end, given a set of identified features we\u201d \uf0e0 To this end, given a set of identified features, we\nP. 6: \u201cNote that the OnlyConv method, makes the assumption\u201d \uf0e0 remove \u201c,\u201d after method\nP. 7: \u201cIn order to get a qualitative insight of the type of\u201d \uf0e0 insight into the\nP. 7: I would write siamese and persian cat with capital \u201cS\u201d and \u201cP\u201d (Siamese, Persian)\nP. 7: others/ upper \u201cSome focus on legs, covered and uncovered, while other focus on the upped body part.\u201d \uf0e0 while others focus on the upper body part\nP. 7: \u201cThese visualizations answers the question\u201d \uf0e0 answer\nP. 7:  \u201cIn this section we assess\u201d \uf0e0 In this section, we\nP. 7: Plural \u201cWe show these visualization for different\u201d \uf0e0 these visualizations\nP. 7: In \u201cHere our method reaches a mean difference on prediction confidence\u201d \uf0e0 difference in prediction \u2026\nP. 7: \u201cThis suggest that our method is able\u201d \uf0e0 This suggests that\nP. 8: state-of-the-art\nP. 8: \u201chas higher mean IoU\u201d \uf0e0 has a higher mean IoU\nWhole document: when using \u201ci.e.\u201d add \u201c,\u201d after: i.e.,\n\nReferences: Some of the references in the list have very little information to be able to find it/proper academic citation, e.g. , Yosinski et al. 2015; Vedaldi and Lenc, 2015:\n\nJason Yosinski, Jeff Clune, Anh Mai Nguyen, Thomas J. Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. 2015.\n\nA. Vedaldi and K. Lenc. Matconvnet: Convolutional neural networks for matlab. In MM, 2015.\n\nRef Doersch et al.: What makes paris look like paris? \uf0e0 Paris\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}