{"title": "Ok to accept after discussion", "review": "Verifying the properties of neural networks can be very difficult.  Instead of\nfinding a formal proof for a property that gives a True/False answer, this\npaper proposes to take a sufficiently large number of samples around the input\npoint point and estimate the probability that a violation can be found.  Naive\nMonte-Carlo (MC) sampling is not effective especially when the dimension is\nhigh, so the author proposes to use adaptive multi-level splitting (AMLS) as a\nsampling scheme. This is a good application of AMLS method.\n\nExperiments show that AMLS can make a good estimate (similar quality as naive\nMC with a large number of samples) while using much less samples than MC, on\nboth small and relatively larger models.  Additionally, the authors conduct\nsensitivity analysis and run the proposed algorithm with many different\nparameters (M, N, pho, etc), which is good to see.\n\n\nI have some concerns on this paper:\n\nI have doubts on applying the proposed method to higher dimensional inputs. In\nsection 6.3, the authors show an experiments in this case, but only on a dense\nReLU network with 2 hidden layers, and it is unknown if it works in general.\nHow does the number of required samples increases when the dimension of input\n(x) increases? \n\nFormally, if there exists a violation (counter-example) for a certain property,\nand given a failure probability p, what is the upper bound of number of samples\n(in terms of input dimension, and other factors) required so that the\nprobability we cannot detect this violation with probability less than p?\nWithout such a guarantee, the proposed method is not very useful because we\nhave no idea how confident the sampling based result is. Verification needs\nsomething that is either deterministic, or a probabilistic result with a small\nand bounded failure rate, otherwise it is not really a verification method.\n\nThe experiments of this paper lack comparisons to certified verification\nmethods. There are some scalable property verification methods that can give a\nlower bound on the input perturbation (see [1][2][3]).  These methods can\nguarantee that when epsilon is smaller than a threshold, no violations can be\nfound.  On the other hand, adversarial attacks give an upper bound of input\nperturbation by providing a counter-example (violation). The authors should\ncompare the sampling based method with these lower and upper bounds. For\nexample, what is log(I) for epsilon larger than upper bound?\n\nAdditionally, in section 6.4, the results in Figure 2 also does not look very\npositive - it unlikely to be true that an undefended network is predominantly\nrobust to perturbation of size epsilon = 0.1. Without any adversarial training,\nadversarial examples (or counter-examples for property verification) with L_inf\ndistortion less than 0.1 (at least on some images) should be able to find. It\nis better to conduct strong adversarial attacks after each epoch and see what\nare the epsilons of adversarial examples.\n\nIdeas on further improvement:\n\nThe proposed method can become more useful if it is not a point-wise method.\nIf given a point, current formal verification method can tell if a property is\nhold or not.  However, most formal verification method cannot deal with a input\ndrawn from a distribution randomly (for example, an unseen test example). This\nis the place where we really need a probabilistic verification method. The\nsetting in the current paper is not ideal because a probabilistic estimate of\nviolation of a single point is not very useful, especially without a guarantee\nof failure rates.\n\nFor finding counter-examples for a property, using gradient based methods might\nbe a better way. The authors can consider adding Hamiltonian Monte Carlo to\nthis framework (See [4]).\n\nReferences: \nThere are some papers from the same group of authors, and I merged them to one.\nSome of these papers are very recent, and should be helpful for the authors\nto further improve their work.\n\n[1] \"AI2: Safety and Robustness Certification of Neural Networks with Abstract\nInterpretation\", IEEE S&P 2018 by Timon Gehr, Matthew Mirman, Dana\nDrachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, Martin Vechev \n\n(see also \"Differentiable Abstract Interpretation for Provably Robust Neural\nNetworks\", ICML 2018. by Matthew Mirman, Timon Gehr, Martin Vechev.  They also\nhave a new NIPS 2018 paper \"Fast and Effective Robustness Certification\" but is\nnot on arxiv yet)\n\n[2] \"Efficient Neural Network Robustness Certification with General Activation\nFunctions\", NIPS 2018. by Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui\nHsieh, Luca Daniel.  \n\n(see also \"Towards Fast Computation of Certified Robustness for ReLU Networks\",\nICML 2018 by Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh,\nDuane Boning, Inderjit S. Dhillon, Luca Danie.)\n\n[3] Provable defenses against adversarial examples via the convex outer\nadversarial polytope, NIPS 2018. by Eric Wong, J. Zico Kolter.\n\n(see also \"Scaling provable adversarial defenses\", NIPS 2018 by the same authors)\n\n[4] \"Stochastic gradient hamiltonian monte carlo.\" ICML 2014. by Tianqi Chen,\nEmily Fox, and Carlos Guestrin.\n\n============================================\n\nAfter discussions with the authors, they agree to revise the paper according to our discussions and my primary concerns of this paper have been resolved. Thus I increased my rating.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}