{"title": "Overall interesting idea, unclear on technical details, missing an important baseline.", "review": "This paper proposes to apply MAML to a multi-agent setting. In this formulation each opponent corresponds to a task and two separate parts of the policy are learned via meta-learning: \n1) the opponent modelling network that predicts the value function for a given opponent based on past actions and states. \n2) the policy network which takes in the state and the predicted value function of the opponent. \nThe main concern with this paper is the lack of technical detail and an important missing baseline. The paper also suffers from lacking clarity due to a large number of grammatical mistakes. \n\nTechnical detail and concerns: \nThe paper mentions Duelling DQN as the RL algorithm in the inner loop. This is very unusual and it's a priori unclear whether MAML with DQN in the inner loop is a sensible algorithm. For example, DQN relies both on a target network and an argmax operator which seem to violate the differentiability requirements needed for MAML regarding higher order gradients. The authors entirely miss this and fail to address possible concerns. \n\nThe authors also fail to provide any details regarding the exploration scheme used. In fact, a value function is never mentioned, instead the authors talk about a policy pi^a_i, leaving it unclear how this policy is derived from the value function. When the Q-function takes as input the true opponent, there is no need for meta-learning of the policy: Given a known opponent, the tuple (s_t, opponent) defines a Markov state. As far as I could gather from the paper, the authors are missing a baseline which simply learns a single Q-function across all opponents (rather than meta-learning it per opponent) that takes as input the predicted opponent. \nMy expectation is that this is more or less what is happening in the paper. The authors also fail to compare and contrast their method to a number of recent multi-agent algorithms, eg. MADDPG, COMA and LOLA. \n\nFurthermore, the results are extremely toy and seem to be for single runs , rendering them insignificant. \n\nWhile the idea itself is interesting, the above concerns render the paper unsuitable for publication in it's current form.\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}