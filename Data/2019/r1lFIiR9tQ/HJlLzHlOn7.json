{"title": "An interesting paper with weak experiments", "review": "This paper proposed a novel variational upper bound for f-divergence, one example of which is the famous evidence lower bound for max-loglikelihood learning. The second contribution might be the spread f-divergence for distributions having different supports. Even though theoretically sound, I believe that the presented experimental results are not strong enough to support the effectiveness of the proposed techniques. Detailed comments are listed below.\n\n1) Notations are confusing, especially in Section 3 when introducing the SPREAD f -DIVERGENCE.\n2) I cannot find on arXiv the reference \u201cD. Barber, M. Zhang, R. Habib, and T. Bird. Spread divergences. arXiv preprint, 2018.\u201d So I am not sure whether you can take credit from the \u201cspread f-divergence\u201d or not.\n3) Important analysis/experiments on several key points are missing, for example, (i) how to specify the variance of the spread divergence in practice? (ii) how to estimate log p(y)? What is the influence?\n4) In the paragraph before Sec 4.2, how the sigma of the spread divergence is annealed?\n5) Despite the toy experiment in Sec 4.4, what are the advantages of the proposed f-divergence upper bound over the Fenchel-conjugate f-divergence lower bound? The current experimental results barely show any advantage.\n\nMinors:\n1) Page 6, under Figure 3. The statements of \u201cKL (moment matching)\u201d and \u201creverse KL (mode seeking)\u201d are not consistent with what\u2019s stated in Sec 2.2 (the paragraph under Eq (3)). \n2) \u201cRKL\u201d and \u201cJS\u201d are not defined. Forward KL and standard KL are both used in the paper.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}