{"title": "interesting conjecture, needs experiments on larger dataset and better presentation and explanation about the result", "review": "It was believed that sparse architectures generated by pruning are difficult to train from scratch. The authors show that there exist sparse subnetworks that can be trained from scratch with good generalization performance. To explain the difficulty of training pruned networks from scratch or why training needs the overparameterized networks that make pruning necessary,  the authors propose a lottery ticket hypothesis: unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy.  They also present an algorithm to identify the winning tickets.\n\nThe conjecture is interesting and it is still a open question for whether a pruned network can reach the same accuracy when trained from scratch. It may helps to explain why bigger networks are easier to train due to \u201chaving more possible subnetworks from which training can recover a winning ticket\u201d. It also shows the importance of both the pruned architecture and the initialization value. Actually another submission (https://openreview.net/forum?id=rJlnB3C5Ym) made the opposite conclusions.\n\nThe limitations of this paper are several folds:\n\n- The paper seems a bit preliminary and unfinished.  A lot of notations seems confusing, such as \u201cwhen pruned to 21%\u201d. The author defines a winning lottery ticket as a sparse subnetwork that can reaching the same performance of the original network when trained from scratch with the \u201coriginal initialization\u201d. It is quite confusing as there is no definition anywhere about the \u201coriginal initialization\u201d. It would be clearer if the author can use some math notations.\n\n- As identified by the authors themself, lacking of supporting experiments on large-scale dataset and real-world models. Only MNIST/CIFAR-10 and toy networks like LeNet, Conv2/Conv4/Conv6 are used. The author has done experiments on resnet, I would be better to move it to the main paper.\n\n- There is no explanation about why the \u201clottery ticket\u201d can perform well when trained with the \u201coriginal initialization\u201d but not with random initialization. Is it because the original initialization is not far from the pruned solution? Then this is a kind of overting to the obtained solution.\n\n- The other problem is that the implications are not clearly useful without showing any applications. The paper could be stronger if the authors can provide more results to support the applications of this conjecture.\n\n- The authors only explore the sparse networks. Model compression by sparsification has good compression rate, especially for networks with large FC layers. However, the acceleration relies on specific hardware/libraries. It would be more complete if the author can provide experiments on structurally pruned networks, especially for CNNs.\n\n- The x-axis of pruning ratios in Figure 1/4/5 could be uniformly sampled and make the figure easier to read.\n\nQuestions:\n- Does the winning tickets always exist?\n- What is the size of winning tickets for a very thin network? Would it also be less than 10%?\n\n\n------update----------\n\nI appreciate the author\u2019s efforts on providing detailed response and more experiments. After reading the rebuttal and the revised version, though the paper has been improved, my concerns are not fully addressed to safely accept it.\n\nIt can be summarized that there exists a sparse network that can be trained well only provided with certain weight initialization.The winning tickets can only be found via iterative pruning of the trained network. This is a chicken-egg problem and I failed to see how it can improve the network design. It still feels incomplete to me by just providing a hypothesis with limited sets of experiments. The implications are actually the most valuable/attractive part, such as \u201cImprove our theoretical understanding of neural networks\u201d, however, they are very vague with no clear instructions even after accepting this hypothesis. I would expect analysis of the reason behind failure and success. I understand that it could be left for another paper, but the observations/experiments only are not strong enough for confirming the the hypothesis.\n\nSpecifically, the experiments are conducted on relatively wide and shallow CNNs. Note that VGG-16/19 and ResNet-18 are designed for ImageNet but not CIFAR-10, which are much wider than normal CIFAR-10 networks, such as ResNet-56. Even \u201cresnet18 has 16x fewer parameters than conv2 and 75x fewer than VGG19\u201d, it is mainly due to the removal of FC layers with average pooling and cannot be claimed as \u201cmuch thinner\u201d networks. As increasing the wideness usually ease the optimization, and the pruned sparse network still enjoy this property unless significantly pruned. Thus, I still doubt whether the conclusion can hold for much thinner network, i.e., \u201cwinning tickets near or below 10-20%, depending on the level of overparameterization of the original network.\u201d\n\nThe observation of \u201cwinning ticket weights tend to change by a larger amount then weights in the rest of the network\u201d in Figure 19 seems natural and the conjecture of the reason \u201cmagnitude-pruning biases the winning tickets we find toward those containing weights that change in the direction of higher magnitude\u201d sounds reasonable. It would be great if the authors can dig into this and make more comparison with the distribution of random weights initialization.\n\nThe figures could also be improved and simplified as the lines are hard to read and compare.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}