{"title": "This paper conducts a large series of experiments on curiosity based rewards for RL agents, discuss different setups for the intrinsic reward, and, experiment on a wide range of tasks.", "review": "This paper studies the dynamics-based curiosity intrinsic reward where the agent is rewarded highly in states where the forward dynamic prediction errors are high in an embedding space (either due to complexity of the state or unfamiliarity).\n\nOverall I like the paper, it's systematic and follows a series of practical considerations and step-by-step experimentations.\n\nOne of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search. While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods. In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.\n\nAnother area of improvement is the experiments around VAE. While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance. \n\nAlso it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).\n\nAn interesting area for future work could be on early stopping techniques for embedding training - it seems that RFs perform well without any training while in some scenarios the IDFs work overall the best. So it would be interesting to explore how much training is needed for the embedding model. RFs are never trained and IDFs are continuously trained. So maybe somewhere in between could be the sweet spot with training for a short while and then fixing the features.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}