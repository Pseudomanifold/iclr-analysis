{"title": "Very interesting idea with weak evaluation", "review": "In the paper, the authors try to improve the generation of clarification questions using reinforcement learning against a discriminator, creating a GAN-like setting. They train two sequence-to-sequence models that i) generate questions from context, and ii) answers from (context, question) pairs. They also train a discriminator model on (question, answer, context) triples.\n\nI believe the task, and the setup created by the authors is very interesting and novel, and the paper is very well written. They show that the additional training against the discriminator leads to a (very small) increase in diversity and question specificity.\n\nOn the negative side:\n  - It is not clear to me that the discriminator acts as a utility function as they claim, at least in the way they define utility. It is only trained to distinguish real questions about a context from random ones.\n  - The results presented in Table 1 and 2 show only very small differences to the other approaches. I wonder why that is, and how much the model actually changes in the reinforcement learning tuning step.\n  - The automated metrics do not seem suitable for the task, since they can only measure how close a generated example is to some gold example. \n  - The only significant improvement is on specificity, with the much more important goal of creating useful questions not achieved. I am actually not sure if increased utility (i.e. identifying missing information and asking about it) can be achieved with a setup like this.\n\nBut despite these weaknesses I still think this is a very interesting contribution.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}