{"title": "Needs more work and clarity", "review": "This paper addresses an interesting task of clarification question generation by proposing a GAN-based approach. It mainly builds on the ideas of Rao & Daum\u00b4e III (2018) to understand the usefulness of generated questions via a utility function that acts as the discriminator while a simple seq-to-seq model is used to generate questions in the generator module. The proposed GAN model is inspired by the sequence GAN model of Yu et al. (2017) with simple variations such as using MIXER (Ranzato et al., 2015) as the generator and not using a CNN-based discriminator. Experiments were conducted on two datasets, and the obtained results were mixed and not conclusive. Overall, due to the lack of novelty and unconvincing results, I feel the paper needs more work before it is ready for publication. My detailed comments are below:\n\n- \"As a running example, we will use the Amazon setting: ...\" --> The example in Figure 1 is only referred once and not even in Table 3 to show the related model predictions. I would suggest to truly consider it as a running example to clarify the training and testing procedure better. Also, an example of the StackExchange dataset would be helpful.\n\n- The utility function (Section 2.3) seems to be simple. Did you evaluate the effectiveness of this function solely in predicting the usefulness of a question? How would a binary classifier work instead? I also wonder why simple seq-to-seq models were used as question/answer generators while there exist a lot of work that already outperform these models for similar text generation tasks. \n\n- \"In our model, the answer is an latent variable: we do not actually use it anywhere except to train the discriminator. Because of this, we train our discriminator using (context, true question, generated answer) triples as positive instances and (context, generated question, generated answer) triples as the negative instances.\" --> This part is not clear. Did you use generated answers or the true answers as part of the positive instances? Please clarify across the paper when you used generated answer/question and when you used true answer/question.\n\n- \"Unlike the question generator, the parameters of the answer generator are kept fixed during the adversarial training\" --> please explain why.\n\n- I like that the experiments were carried out on multiple datasets. What are the lengths of the contexts, questions, and answers for both datasets on average number of words? What are the impacts of the length restrictions of 100, 20, and 20 you set for context, question, answer on the evaluation results? How did you come up with these numbers? I would suggest to include an analysis of impacts of variable lengths of context, question, answer on the model performance.  \n\n- It's not clear how the Lucene system was built with human generated questions. Please clarify.\n\n- Table 2 shows mixed results, what should we conclude from this? \n\n- How many crowdworkers were used for human judgements? What was the inter-annotator agreement? How did you convert the human answers into the numeric scores of Table 2? Without these information, it is not possible to judge the utility of the human evaluation. StackExchange results could have been annotated via other crowdsourcing venues e.g. upwork.   \n\n- The related work should be better compared and contrasted with the proposed work, especially the main contributions of the paper should be clearly highlighted. \n\n- Table 3 is not referred in text. I would suggest to include the name of the products also for better context. The human evaluation scores look very subjective, hence, the inter-annotator agreement is an essential factor. \n\n- There are a lot of grammatical mistakes and inconsistencies across the paper that need to be corrected.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}