{"title": "Interesting approach with bad evaluation setup", "review": "The paper proposes a new approach for the problem of clarification question generation. It is based on training question-answer generator model jointly with a utility function (or discriminator) using a GAN-like objective. The system is compared to a variety of baselines and shows to generate slightly more diverse answers than competing baselines, but is otherwise quite comparable. \n\nThe paper is overall well presented and introduces an interesting approach that combines SeqGANs with MIXER training and a self critical baseline. The authors also took care to establish reasonable baselines given the novelty of the task. The evaluations are carried out on a very artificial task setup, however, that is overall not very usefull for evaluating clarification questions. Therefore, I believe that this paper needs a completely different evaluation setup and I can unfortunately not recommend it for acceptance without that.\n\nDetailed comments:\nIt is unclear to me whether it is really possible to evaluate a model trained in such a setup, because it is impossible to say what is a useful clarification question without establishing an information need first. Just asking random questions about a product, which is the best we can hope to learn, is not a very interesting task. Clarification is usually a means to an end goal, but in this paper it is established as the final goal, which doesn't make too much sense to me. This leads to the rather artificial treatment of \"Utility\", which is impossible to define without a clear down-stream task. The paper relabels generating human-like questions as the utility to optimize towards in order to ultimately fool a discriminator. I don't see how this can be viewed as defining utility. So I strongly suggest to evaluate the approach on a task that might actually require asking clarification questions, in which case utility is naturally defined. GAN training could still be used to make the generated questions more diverse.\n\n\nStrengths:\n- clearly written and well presented\n- learning to generate clarification questions is an important topic\n- interesting combination of SeqGANs, MIXER and self-critical baseline for policy gradient updates\n- a range of good baselines for this novel task setup\n\n\nWeaknesses:\n- minor: automatic evaluations are kind of useless here and the datasets are rather artificial for this task\n- major: generating clarification questions cannot be the end goal in and of itself (see above explanation)\n\n\nOther comments:\n- section pretraining, paragraph question generator: I do not understand the reference to answer generator in this paragraph. I think something got mixed up in this section.\n- needs some proof reading: some spelling mistakes (eg: p3 thier->their), missing spaces (e.g., p.4 \"model\u00a72.1\"), \n\n\nQuestions:\nWhy is specificity such an important aspect if we mainly care about usefulness? In other words, is usefulness not capturing specificity to a certain degree?\n\nThe goal of this paper is to train clarification questions, so I do not really understand why also synthetic answers are being generated? Why not just training the system on (context, question) tuples? ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}