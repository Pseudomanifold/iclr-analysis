{"title": "A paper proposing a quite complex system (with no explicit probabilistic factorisation) which seems to obtain good experimental results.", "review": "As a reviewer I am expert in learning in structured data domains. \nThe paper proposes a quite complex system, involving many different choices and components, for obtaining chemical compounds with improved properties starting from a given corpora. \nOverall presentation is good, although some details/explanations/motivations are missing. I guess this was due to the need to keep the description of a quite complex system in the given space limit. Such details/explanations/motivations could, however, have been inserted in the appendix. As an example, let consider the description of the decoding of the junction tree. In that section, it is not explained when the decoding process stops. My understanding is that this is when, being in the root node, the choice is to go back to the parent (that does not exist). In the same section, it is not explicitly discussed that the probability to select between adding a node or going back to the parent should have a different distribution according to \"how many\" nodes have been generated before, i.e. we do not want to have a high probability to \"go back\" at the beginning of the decoding, while I guess it is desirable that such probability increases proportionally with the number of generated nodes. This leads to an issue that I personally think is important: the paper does lack an explicit probabilistic modelling of the different involved components, which may help for a better understanding of all the assumptions made in the construction of the proposed system. \nThe complexity of the proposed system is actually an issue since the author(s) do not attempt (except for  the presence or absence of the adversarial scaffold regularization and the number of trials in appendix) an analysis of the influence of the different components (and corresponding hyper-parameters). \nReference to previous relevant work seems to be complete.\nI think the paper is relevant for ICLR (although there is no explicit analysis of the obtained hidden representations) and of interest for a good portion of attendees.\n\nMinor issues:\n- Tree and Graph Encoding: asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph. A discussion about that would be needed.\n- eq.(6): \\mathbb{u}^d is not defined.\n- Section 3.3:\n   - first paragraph is not clear. An example and/or figure is needed to understand the argument, which is related to the presence of cycles.\n  - the definition of f(G_i) involves  \\mathbb{x}_u. I guess they should be  \\mathbb{x}_u^G.\n  - not clear how the log-likelihood of ground truth subgraphs is computed given that the predicted junction tree, especially at the beginning of training, may be way different from the correct one. Moreover, what is the assumed bias of this choice ?\n- Table I: please provide an explanation of why using a larger value for \\delta does provide worst performance than a smaller value. From an optimisation point of view it should provide at least an as good performance. This is a clear indication that the used procedure is suboptimal.\n- diversity could be influenced by the cardinality of the sample. Is this false ? please discuss why diversity is (not) biased versus larger sets.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}