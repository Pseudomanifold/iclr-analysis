{"title": "Interesting contribution to multiagent RL ", "review": "The paper considers an actor-critic scheme for multiagent RL, where the critic is specific to each agent and has access to all other agents' embedded observations. The main idea is to use an attention mechanism in the critic that learns to selectively scale the contributions of the other agents. \n\nThe paper presents sufficient motivation and background, and the proposed algorithmic implementation seems reasonable. The proposed scheme is compared to two recent algorithms for centralized training of decentralized policies, and shows comparable or better results on two synthetic multiagent problems. \n\nI believe that the idea and approach of the paper are interesting and contribute to the multiagent learning literature. \n\nRegarding cons: \n- The critical structural choices (such as the attention model in section 3.2) are presented without too much justification, discussion of alternatives, etc. \n- The experiments show the learning results, but do not provide a peak \"under the hood\" to understand the way attention evolved and contributed to the results. \n- The experiments show good results compared to existing algorithms, but not impressively so. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}