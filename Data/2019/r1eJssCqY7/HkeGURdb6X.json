{"title": "Review of submission 585", "review": "Summary: This paper introduces a new Neural Network training procedure, designed for tabular data, that seeks to leverage feature clusters extracted from GBDTs.\n\nStrengths: The idea of leveraging feature groups in a neural network structure; the novelty of the RESE model;\n\nWeaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.\n\nMinor typos:\n(abstract)\n- \"NN has achieved\" => \"Neural Networks have achieved\"\n- \"performances\" => performance\n- \"explicitly leverages\" => \"explicitly leverage\"\n\nQuestions:\n- (top of p. 2) What exactly is the difference between \"implicit feature combinations\" and \"explicit (?), expressive feature combinations\"\n- (top of p. 2) \"encourage parameter sharing\" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]\n- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.\n\n\nRecommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}