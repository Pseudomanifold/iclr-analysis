{"title": "Misleading title", "review": "This paper discusses conditions under which  the convergence of training models with low-precision weights do not rely on model dimension. Extensions to two kinds of non-linear quantization methods are also provided. The dimension-free bound of the this paper is achieved through a tighter bound on the variance of the quantized gradients.  Experiments are performed on synthetic sparse data and small-scale image classification dataset MNIST.\n\nThe paper is generally well-written and structure clearly. However, the bound for linear quantization is not fundamentally superior than previous bounds as the \"dimension-free\" bound in this paper is achieved by replacing the bound in other papers using l2 norm with l1 norm. Note that l1 norm is related to the l2 norm as: \\|v\\|_1 <= \\sqrt{d}\\|v\\|_2, the bound can still be dependent on  dimension, thus the title may be misleading. Moreover, the assumptions  1 and 2 are much stronger than previous works, making the universality of the theory limited. The analysis on non-linear quantization is interesting, which can really theoretically improve the bound. It would be nice to see some more empirical results on substantial networks and  larger datasets which can better illustrate the efficacy of the proposed non-linear quantization.\n\nSome minor issues:\n1. What is HALP in the second contribution before Section 2?\n2. What is LP-SVRG in Theorem 1?\n3. What is \\tilde{w} in Theorem 2?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}