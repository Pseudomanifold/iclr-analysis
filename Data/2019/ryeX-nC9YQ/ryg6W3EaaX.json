{"title": "An in-depth study of quantization errors and quantized convex optimization in low-precision training", "review": "This paper provides an in-depth study of the quantization error in low-precision training and gives consequent bounds on the low-precision SGD (LP-SGD) algorithm for convex problems under various generic quantization schemes. \n\n[pros]\nThis paper provides a lot of novel insights in low-precision training, for example, a convergence bound in terms of the L1 gradient Lipschitzness can potentially be better than its L2 counterpart (which is experimentally verified on specially designed problems). \n\nI also liked the discussions about non-linear quantization, how they can give a convergence bound, and even how one could optimally choose the quantization parameters, or the number of {exponent, significance} bits in floating-point style quantization, in order to minimize the convergence bound.\n\nThe restriction to convex problems is fine for me, because otherwise essentially there is not a lot interesting things to say (for quantized problems it does not make sense to talk about \u201cstationary points\u201d as points are isolated.)\n\nThis paper is very well-written and I enjoyed reading it. The authors are very precise and unpretentious about their contributions and have insightful discussions throughout the entire paper.\n\n[cons]\nMy main concern is that of the significance: while it is certainly of interest to minimize the quantization error with a given number of bits as the budget (and that\u2019s very important for the deployment side), it is unclear if such a *loss-unaware* theory really helps explain the success of low-precision training in practice.\n\nAn alternative belief is that the success comes in a *loss-aware* fashion, that is, efficient feature extraction and supervised learning in general can be achieved by low-precision models, but the good quantization scheme comes in a way that depends on the particular problem which varies case by case. Admittedly, this is a more vague statement which may be harder to analyze or empirically study, but it sounds to me more reasonable for explaining successful low-precision training than the fact that we have certain tight bounds for quantized convex optimization. \n\n[a technical question]\nIn the discussions following Theorem 2, the authors claim that the quantization parameters can be optimized to push the dependence on \\sigma_1 into a log term -- this sounds a bit magical to me, because there is the assumption that \\zeta < 1/\\kappa, which restricts setting \\zeta to be too large (and thus restricts the \u201cacceleration\u201d of strides from being too large) . I imagine the optimal bound only holds when the optimal choice of \\zeta is indeed blow 1/\\kappa?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}