{"title": "Promising unsupervised approach, but clarity issues", "review": "Overall and positives:\n\nThe paper investigates the problem of multidocument summarization\nwithout paired documents to summary data, thus using an unsupervised\napproach. The main model is constructed using a pair of locked\nautoencoders and decoders. The model is trained to optimize the\ncombination of 1. Loss between reconstructions of the original reviews\n(from the encoded reviews) and original the reviews, 2. And the\naverage similarity of the encoded version of the docs with the encoded\nrepresentation of the summary, generated from the mean representation\nof the given documents.\n\nBy comparing with a few simple baseline models, the authors were able\nto demonstrate the potential of the design against several naive\napproaches (on real datasets, YELP and AMAZON reviews). \nThe necessity of several model components is demonstrated\nthrough ablation studies. The paper is relatively well structured and\ncomplete. The topic of the paper fits well with ICLR. The paper\nprovides decent technical contributions with some novel ideas about\nmulti-doc summary learning models without a (supervised) paired\ndata set.\n\nComments / Issues\n\n[ issue 6 is most important ]\n\n1.  Problem presentation. The problem was not properly introduced and\nelaborated. In fact, there is not a formal and mathematical\nintroduction of the problem, input, output, dataset and model\nparameters. The notations used are not very clearly defined and are\nquite handwavy, (e.g. what is V, dimensions of inputs x_i was not\nmentioned until much later in the paper). The authors should make\nthese more precise. Similar problem with presentations of the models,\nparameters, and hyperparameters.\n\n3.  How does non-equal weighted linear combinations of l_rec and l_sim\nchange the results? Other variation of the overall loss function? How\ndo we see the loss function interaction in the training, validation\nand test data? With the proposed model, these could be interesting to\nobserve.\n\n4.  In equation two, the decoder seems to be very directly affecting\nthe quality of the output summary. Teacher forcing was used to train\nthe decoder in part (1) of the model, but without ground truth, I\nwould expect more discussions and experiments on how the Gumbel\nsoftmax trick affect or help the performance of the output.\n\n5.  Baseline models and metrics\n\n(1) There should be more details on how the language model is trained,\nsome examples, and how the reviews are generated from the language\nmodel as a base model (in supplement?).\n\n(2). It is difficult to get a sense of how these metrics corresponds\nto the actual perceived quality of the summary from the\npresentation. (see next)\n\n(3). It will be more relevant to evaluate the proposed design\nvs. other neural models, and/or more tested and proved methods.\n\n6. The rating classifier (CLF) is intriguing, but it's not clearly\nexplained and its effect on the evaluation of the performance is not\nclear: One of the key metrics used in the evaluation relies on the\noutput rating of a classifier, CLF, that predicts reader ratings on\nreviews (eg on YELP).  The classifier is said to have 72%\naccuracy. First, the accuracy is not clearly defined, and the details\nof the classifier and its training is not explained (what features are\nits input, is the output ordinal regression).  Equation 4 is not\nexplained clearly: what does 'comparing' in 'by comparing the\npredicted rating given the summary rating..' mean?  The classifier may\nhave good performance, but it's unclear how this accuracy should\naffect the results of the model comparisons.\n\nThe CLF is used to evaluate the rating of output\nreviews from various models. There is no justification these outputs\nare in the same space or generally the same type of document with the\ntraining sample (assuming real Yelp reviews).  That is probably\nparticularly true for concatenation of the reviews, and the CLF classifier\nscores the concatenation very high (or  eq 4 somehow leads to highest value\nfor the concatenation of reviews )... It's not clear whether such a classifier is \nbeneficial in this context.\n\n7. Summary vs Reviews. It seems that the model is built on an implicit\nassumption that the output summary of the multi-doc should be\nsufficiently similar with the individual input docs.  This may be not\ntrue in many cases, which affects whether the approach generalizes.\nDoc inputs could be covering different aspects of the review subject\n(heterogeneity among the input docs, including topics, sentiment etc),\nor they could have very different writing styles or length compared to\na summary.  The evaluation metrics may not work well in such\nscenarios.  Maybe some pre-classification or clustering of the inputs,\nand then doing summarization for each, would  help?  In the conclusions section, the\nauthors do mention summarizing negative and positive reviews\nseparately.\n\n\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}