{"title": "RL based method for pruning a pre-trained network", "review": "This paper proposes a layer-based pruning method based on reinforment learning for pre-train networks.\n\nThere are several major issues for my rating:\n\n- Lack of perspective. I do not understand where this paper sits compared to other compression methods. If this is about RL great, if this is about compression, there is a lack of related work and proper comparisons to existing methods (at least concenptual)\n- Claims about the benefits of not needed expertise are not clear to me as, from the results, seems like expertise is needed to set the hyperparameters.\n\n- experiments are not convincing. I would like to see something about computational costs. Current methods aim at minimizing training / finetuning costs while maintaining the accuracy. How does this stands in that regard? How much time is needed to prune one of these models? How many resources?\n\n- Would it be possible to add this process into a training from scratch method?\n\n- how would this compare to training methods that integrate compression strategies?\n- Table 1 shows incomplete results, why? Also, there is a big gap between accuracy/number of parameters trade-of between this method and other presented in that table. Why?\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}