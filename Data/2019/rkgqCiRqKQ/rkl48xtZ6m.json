{"title": "Interesting topic and approach, needs work and careful evaluation", "review": "Not all examples in the introduction are necessarily biases but can be modeled with reward functions, where reward is given to specific states other than finishing work by the deadline. It would be helpful for the reader to get examples that  correspond to the investigated biases. \n\nIt would be good if the authors could at least mention that \u201cBoltzmann rational\u201d is a specific model of \u201csystematic\u201d bias for which much experimental support eith humans and animals exists. \n\nThe authors are strongly encouraged to review the literature on IRL, which includes other examples of modeling explicitly suboptimal agents, e.g.:\n- Rothkopf, C. A., & Dimitrakakis, C. (2011). Preference elicitation and inverse reinforcement learning. ECML.\nSimilarly, the idea to learn an agent\u2019s reward functions across multiple tasks has also appeared in the literature before, e.g.:\n- Dimitrakakis, C., & Rothkopf, C. A. (2011). Bayesian multitask inverse reinforcement learning. EWRL.\n- Choi, J., & Kim, K. E. (2012). Nonparametric Bayesian inverse reinforcement learning for multiple reward functions. NIPS\n\nThe authors state:\n\u201cThe key idea behind our algorithms is to learn a model of how the demonstrator plans, and invert the model\u2019s \"understanding\" using backpropagation to infer the reward from actions.\u201d\nIt would be also important in this case to relate this to prior work, as several authors have proposed a very similar idea, in which a particular parameterization of the agent\u2019s planning given the rewards and the transition function are learned, including Ziebart et al. and Dimitrakakis et al. This is also related to \n- Neu, G., & Szepesv\u00e1ri, C. (2007). Apprenticeship learning using inverse reinforcement learning and gradient methods. UAI.\n\nIt would be great if the authors could also discuss how assumption 3 is a necessary for accurately inferring reward functions and biases and how deviations from this assumption interfere with the goal of this inference. This seems to be a central and important point for the viability of the approach the authors take here.\n\nCurrently, the evaluation of the proposed method is in terms of the loss incurred by a planner between the inferred reward function and the true reward function, figure 3. It would be important for the evaluation of the current manuscript to know what the inferred biases are. That using a wrong model of how actions are generated given values, e.g. myopic vs. Boltzmann-rational, results in wrong inferences, should not be too surprising. Therefore, the main question is: does the proposed algorithm recover the actual biases?\n\nMinor points:\n\u201clike they naive and sophisticated hyperbolic discounters\u201d\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}