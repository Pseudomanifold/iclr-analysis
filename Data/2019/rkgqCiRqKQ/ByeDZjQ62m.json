{"title": "Paper studies a relevant and interesting problem but needs extended empirical evaluation", "review": "This paper addresses the interesting and challenging problem of learning the reward function from demonstrators which have unknown biases. As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. They propose algorithms for both cases and evaluate these in basic experiments.\n\nThe studied problem is relevant as many/most demonstrators have unknown biases and we still need methods to effectively learn from those.\n\nAs far as I am aware of the related literature, the problem has not been studied in that explicit form although there is related work which targets the problem of learning from suboptimal demonstrators or demonstrators that can fail, e.g. [1] (I suggest to discuss this and other relevant papers in a related work section).\n\nThe main shortcomings of the paper are a lack of clarity at certain points and a limited experimental validation:\n* For instance, the formalization of \u201eAssumption 1\u201c is unclear. In which sense does this cover similarity in planing? As far as I understand, the function D could still map any combination of world model and reward function to any arbitrary policy. What does it mean that the planning algorithm D is \u201efixed and independent\u201c?\n* A crucial point requiring more investigation in my opinion is Assumption 3 (well-suited inductive bias). Empirically the chosen experimental setup yields expected results. However, to better understand the problem of learning with unknown biases it would be important to see how results change if the model for the planner changes. A small step in that direction would have been to provide results for value iteration networks with different number of iterations and number neurons, etc. \n* If you use the differentiable planner instead of the VIN, how many iterations do you unroll?\n* Is there any evidence that the proposed approach can work effectively in larger scale domains with more difficult biases? Also in the case in which the biases are inconsistent among demonstrations?\n\nFurther suggestions:\n* Test how algorithm 1 performs if first initialized on simulated optimal demonstrations.\n* Improve notation for the planning algorithm D by using brackets.\n\n[1] Shiarlis, K., Messias, J., & Whiteson, S. (2016, May). Inverse reinforcement learning from failure. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems (pp. 1060-1068). International Foundation for Autonomous Agents and Multiagent Systems.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}