{"title": "Excellent motivation of work but lacks technical merit; results not convincing", "review": "This paper has proposed algorithms for inferring reward functions from demonstrations with unknown biases. To achieve this, the authors have proposed to learn planners from demonstrations in multiple tasks via value iteration networks to learn the reward functions.\n\nThis paper has provided an excellent motivation of their work in Sections 1 & 2 with references being made to human behaviors and heuristics, though the authors can choose a more realistic running example that is less extreme than making orthogonal decisions/actions. The paper is well-written, up till Section 4. \n\nOn the flip side, there does not seem to be any significant technical challenges, perhaps due to some of the assumptions that they have made. Like the authors have mentioned, I do find assumption 3 to be overly strong and restrictive, as empirically demonstrated in Section 5.2. Arguably, is it really weaker than that of noisy rationality? At this moment, it is difficult to overlook this, even though the authors have argued that it may not be as restrictive in the future when more sophisticated differentiable planners are developed.\n\nThe experimental results are not as convincing as I would have liked. In particular, Algorithm 2 (learning a demonstrator's model) does not seem to outperform that of assuming an optimal demonstrator for the noiseless case and a Boltzmann demonstrator for the noisy case (Fig. 3). This was also highlighted by the authors as well: \"The learning methods tend to perform on par with the best of two choices.\" It begs the question whether  accounting for unknown systematic bias can indeed outperform the assumption of a particular inaccurate bias when we know a priori whether the demonstrations are noisy or not.\n\n\n\nOther detailed comments are provided below:\n\nI would have preferred that the authors present their technical formulations in Section 4 using the demonstrator's trajectories instead of policies.\n\nThe authors say that \"In some cases, like they naive and sophisticated hyperbolic discounters, especially the noisy ones, the learning methods outperform both optimal and Boltzmann assumptions.\" But, Fig. 3 shows that Algorithm 2 does not perform better than either that of the optimal or Boltzmann demonstrator.\n\nIn Section 5.2, the authors have empirically demonstrated the poor approximate planning performance of VIN, as compared to an exact model the demonstrator. What then would its implications be on the adaptivity of Algorithms 1 and 2 to biases?\n\nThe following references on IRL with noisy demonstration trajectories would be relevant:\n\nBenjamin Burchfiel, Carlo Tomasi, and Ronald Parr. Distance Minimization for Reward Learning from Scored Trajectories. In Proc. AAAI, 2016.\n\nJ. Zheng, S. Liu, and L. M. Ni. Robust Bayesian inverse reinforcement learning with sparse behavior noise. In Proc. AAAI, 2014.\n\n\n\nMinor issues:\nOn page 4, the expression D : W \u00d7 R -> S -> A -> [0, 1] can be more easily understood with the use of parentheses.\n\nFor Assumption 2b, you can italicize \"some\".\n\nIn the first paragraph of section 4.1, what are you summing over?\n\nLine 3 of Algorithm 1: PI_W?\n\nPage 7: For the learning the bias setting?\n\nPage 7: figure 3 shows?\n\nPage 7: they naive?\n\nPage 7: so as long as?\n\nPage 8: adaption?\n\nPage 8: predicated?\n\nPage 8: figure 4 shows?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}