{"title": "Nice intuitive paper", "review": "In this paper, the authors propose a compression technique to reduce the number of parameters to learn in a neural network without losing expressiveness.\nThe paper nicely introduces the problem of lack in espressiveness with low-rank factorizations, a well-known technique to reduce the number of parameters in a network.\nThe authors propose to use a linear combination of low-rank factorizations with coefficients adaptively computed on data input. Through a nice toy example based on XNOR data, they provide a good proof of concept showing that the accuracy of the proposed technique outperforms the classical low-rank approach.\nI enjoyed reading the paper, which gives an intuitive line of reasoning providing also extensive experimental results on multilayer perceptron, convolutional neural networks and recurrent neural networks as well.\nThe proposal is based on an intuitive line of reasoning with no strong theoretical founding. However, they provide a quick theoretical result in the appendix (Proposition 1) but, I couldn\u2019t understand very well its implications on the expressiveness of proposed method against classical low-rank approach.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}