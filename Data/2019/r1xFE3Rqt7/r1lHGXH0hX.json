{"title": "The paper introduces a low rank factorization strategy for neural network compression. They propose a data adaptive model to approximate the weights as a learned mixture of low rank factorizations. The method is novel and the results look promising. A limitation of the method presented is that its is applicable only to weights arising as mode-2 tensors (matrices).", "review": "Some suggested improvements follow below\n\n1. It is claimed (page 2, last paragraph) that the proposed method leads to a 3.5% and 2.5% improvement in top-1 accuracy over the mobilenet v1 and v2 models. However the results in table 2 indicate 2.5% and 1.4% improvement. This should be corrected.\n2. The authors should include the performance of the full rank CNN for the toy example in Figure 1. A Neural Net with 2 neurons in the hidden layer can not learn the XOR/XNOR efficiently . So its rank-1 factorization can only perform as good as the original CNN.\n3. In (1), the dimensions of U^k and V^k should be mentioned explicitly.\n4. The choice of \u201ck\u201d in (1) should be discussed. How does it relate to the overall accuracy / compression of the CNN?\n5. The paper addresses low rank factorization for \u201cMLP\u201d, RNN/LSTM and \u201cpointwise\u201d convolutions. All of these have weights in the form of matrices (mode 2 tensors). The extension to mode-3 and and mode-4 tensors which are more common in CNNs is not straightforward.\n6. In the imagenet experiment, the number of mixtures (k) is set to the rank (d). How is the rank computed for every layer?\n7. In Fig 7, row 0 and row 8 look identical. Is this indicative of something?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}