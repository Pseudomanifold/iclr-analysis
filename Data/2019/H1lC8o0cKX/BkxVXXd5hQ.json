{"title": "Interesting topic, contribution questionable, better write-up required", "review": "The paper proposes that agents can extract the spatial structure of the world if consistently explored solely from sensorimotor predictions.\n\nThe topic of the paper is relevant and fits the conference. However, I have doubts about the generality of the claims made.\n\nStrong aspects:\nAbstract makes the reader curious and excited about the content\nInteresting and important topic for representation learning.\n\nWeak aspects:\nThe paper is written in a way that it is not easy to follow. \nImportant details are in the Appendix and some more formal description would be helpful. A clearer presentation of the three different exploration types for instance would help the reader. \nThe paper is not using standard notation and deviates from the POMDP description for not clear reason. \nIt is hard to compare and the results are in a very restricted setup.\nNo baseline or comparison to other methods are given.\nWhy does space emerge in the motor-state (h) not the sensor state?\nNot enough details given to reproduce the paper. It is nothing mentioned about code to be made public.\n\nDiscussion:\nThe argument of the paper is that it is sufficient to do sensor prediction when the world is consistently explored and changing in a consistent manner. There is no need for additional losses or the like. However, I am not convinced about the generality of this statement. What if the world is much richer and the agent and the environment undergo the typical transformations. Rotation and movement in 3D space + non-linear distortions by sensors (e.g. a camera). I am not seeing why a representation of space should emerge without any pressure for minimality or the like. The paper also assumes that the sensor is moved by the motor command in space directly and not, for instance, as a double integrator equations where the motor command is a force that accelerates a body. \nIn the current form, what it really says, is that the network figures out the actual effect of the motor command in moving the sensor around. The sensor is moved in a 2D space by the actions such that the network recovers this 2D manifold. And this can only happen if it actually observes movements and the dataset is rich enough. This distinguishes the 3 different exploration settings. \n\nDetails:\nSec 2:  Prior work: Which priors in Jonschkowski and Brock 2015 do you mean that are specific to the emergence of space? Slowness?\nSec 3: what do you really mean by motor state? In MDPs there is the notion of action that effects the state of the system. There is not really a state of the motor system? Do you mean proprioceptive sensors or actually the motor command/action.\nThe definition s = \\phi_e(m) is kind of nonstandard as one would expect a dependency on s_{t-1} which is in your notation hidden in \\epsilon.\nThe logic is not so straight forward to me. Isn't the logic such that: Given a rigid metric space: when the agent moves around the same type of movements lead to the same kind of transformations independently of there the agent is located?\nI believe this section could be streamlined and illustrated by some examples to drive the message home. Also, making a clear and potentially more explicit statement of these invariances (e.g. by an equation) and why they will be revealed by learning predictions.\n\nSec 4.2:\nImprove the description of the settings. In the first setting (MTM) subsequent sensor/motor values are independent right? So it is the same as having randomly selected isolated data points from a normal interaction. \n\nMMT setting: you write ... which can translate randomly after each transition.  This is almost the same as in MTM where you write ...translates randomly between t and t+1\nIn the Appendix you write that in MMT that environment moves only every 100 steps? \n\nBTW:\nIs the env-movement a smooth movement or a jump?\n \nFig 2: why does the green curve end so early? Is it because of your stopping criteria for training. I would like to see the same training time for all settings.\n\nFig 3 and text: Should one not expect a torus in (a)? The world is not a square but a torus, as you have cyclic boundary conditions. I am surprised to not see this in the plots. The current result somehow violates the smoothness because there is a big jump between the boundaries although in environment there is none.\n \nSec 6: par 2: ...these invariants represents for the predictive model?\n\nOut of my curiosity:\nyou write that ... casts some doubts on purely passive and observational approaches....\nIn which sense did the actions help here? Do you mean that the agent needs to know its own actions right? \nSo when it is to be done from a video (just sensor information) than the actions would need to be inferred first?\n\nTypos:\np1 par2: approache\np8 last par: could be merge\n\nupdated score after authors revision\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}