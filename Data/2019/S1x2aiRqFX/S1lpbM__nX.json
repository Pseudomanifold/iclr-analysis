{"title": "official review", "review": "The paper describes a differentiable expected BLEU objective which computes expected n-gram precision values by ignoring the brevity penalty. \n\nClarity: \nSection 3 of the paper is very technical and hard to follow. Please rewrite this section to be more accessible to a wider audience by including diagrams and more explanation.\n\nOriginality/signifiance: the idea of making BLEU differentiable is a much researched topic and this paper provides a nice idea on how to make this work.\n\nEvaluation: \nThe evaluation is not very strong for the following reasons:\n\n1) The IWSLT baselines are very weak. For example, current ICLR submissions, report cross-entropy baselines of >33 BLEU, whereas this paper starts from 23 BLEU on IWSTL14 de-en (e.g., https://openreview.net/pdf?id=r1gGpjActQ), even two years ago baselines were stronger: https://arxiv.org/abs/1606.02960\n\n2) Why is policy gradient not better? You report a 0.26 BLEU improvement on IWSLT de-en, which is tiny compared to what other papers achieved, e.g., https://arxiv.org/abs/1606.02960, https://arxiv.org/abs/1711.04956\n\n3) The experiments are on some of the smallest translation tasks. IWSLT is very small and given that the method is supposed to be lightweight, i.e., not much more costly than cross-entropy, it should be feasibile to run experiments on larger datasets.\n\nThis makes me wonder how significant any improvements would be with a good baseline and on a larger datasets.\n\nAlso, which test set are you using?\n\nFinally, in Figure 3, why is cross-entropy getting worse after only ~2-4K updates? Are you overfitting? \nPlease reference this figure in the text.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}