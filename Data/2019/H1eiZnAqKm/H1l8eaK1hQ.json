{"title": "Review for the expressive power of GRUs as a continuous dynamical system.", "review": "Here the authors convert the GRU equations into continuous time and use theory and experiments to study 1- and 2-dimensional GRU networks. The authors showcase every variety of dynamical topology available in these systems and point out that the desirable line and ring attractors are not achievable, except in gross approximation.  The paper is extremely well written.\n\nI am deeply conflicted about this paper.  Is the analysis of 1 or 2 dimensional GRUs interesting or significant? That\u2019s a main question of this paper.  There is no question of quality, or clarity, and I am reasonably certain nobody has analyzed the GRU in this way before.\n\nOn the one hand, the authors bring a rigor and language to the discussion of recurrent networks that is both revealing (for these examples) and may to bear fruit in the future.  On the other hand, the paper is exclusively focused on 1- and 2-dimensional examples which have precisely no relevance to the recurrent neural networks as used and studied by machine learning practitioners and researchers, respectively. If the authors have proved something more general for higher dimensional (>2) cases, they should make it as clear as possible.\n \nA second, lesser question of relevance is studying a continuous time version.  It is my understanding that discrete time dynamics may exhibit significantly more complex dynamical phenomenon and again practitioners primarily deploy discrete time GRUs.  I understand that theoretical progress often requires retreating to lower dimensionality and (e.g. linearization, etc.) but in this case it is not clear to me that the end justifies the means.  On the other hand, a publication such as this will not only help to change the language of RNNs in the deep learning community, but also potentially bring in more dynamical systems specialists into the deep learning field, which I thoroughly endorse.\n\nModerate concern\n\n\u201cIn order to show this major limitation of GRUs \u2026\u201d but then a 2-gru is used, which means that it\u2019s not a general problem for GRUs with higher dim, right?  Also, won\u2019t approximate slow points would also be fine here? I think this language needs to be more heavily qualified.\n\nMinor\n\nGRU almost always refers to the network, even though it is Gated Recurrrent Unit, this means that when you write \u2018two GRUs\u2019, the naive interpretation (to me) is that you are speaking about two networks and not a GRU network with two units.\n\nSide note requiring no response: It might be interesting to study dynamical portrait as a function of training for the two-d GRU.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}