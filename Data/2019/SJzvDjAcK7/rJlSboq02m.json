{"title": "Too few experiments for many analyses", "review": "This paper presents a way to induce low-rank representations in a deep neural network and study its effect on adversarial attacks.\n\nQuality\n\nThe analyses are conducted on several types of problems, first classification tasks for confirmation of the low-rank structure, and then on adversarial attacks. Unfortunately, there is only a very few number of experiments per analysis, making it virtually impossible to infer reliably any trends in the data. Table 1, 2, 3 and 4 contains at best enough information for a proof of concept, but it is not possible to make any conclusion out of them. Also, VGG results only appear in figure 2 where they appear to contradict the conclusions held by the authors. Is it difficult to understand why results from VGG do not appear in any table. \n\nClarity\n\nThe paper is difficult to follow. Introduction gives too much details and even contains methodological information, all of which obfuscates the main message which does get more clear in the latter sections. The sections 2 and 3 are confusing because they do not follow the logic presented in the abstract. The latter states that observations on the low-rank structure of the representations will be done prior to experimentally impose low-rank. However, section 2 presents the low-rank structure imposed on models while section 3 presents the observations of low-rank-representations jointly with the results of imposed low-rank structure.\n\nThere is no clear definitions of what the \"intriguing properties\" are beside the fact that forced low-rank representations yield similar results on classification and are more robust to adversarial attacks on a very limited number of experiments.\n\nOriginality\n\nUsing low-rank representation is not something new and has already been explored in [1] for instance.\n\nSignificance\n\nThere would be an important contribution to make if the author would analyze the effect of low-rank by varying the constraint. However, the current analyses are not pushed far enough to get any useful insight using only a fixed rank and making a minor modification by adding one or two LR-layers. Experiments in table 2 is a good step in this direction nonetheless.\n\n[1] Luo, Ping. \"Learning deep architectures via generalized whitened neural networks.\" In International Conference on Machine Learning, pp. 2238-2246. 2017.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}