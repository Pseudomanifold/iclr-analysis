{"title": "ultimately, I am not sure there is anything \"Wasserstein\" going on in this new GAN algorithm.", "review": "The authors propose a new GAN procedure. It's maybe easier to reverse-engineer it from the simplest of all places, that is p.16 in the appendix which makes explicit the difference between this GAN and the original one: the update in the generator is carried out l times and takes into account points generated in the previous iteration. \n\nTo get there, the authors take the following road: they exploit the celebrated Benamou-Brenier formulation of the W2 distance between probability measures, which involves integrating over a vector field parameterized in time. The W2 distance which is studied here is not exactly that corresponding to the measures associated with these two parameters, but instead an adaptation of BB to parameterized measures (\"constrained\"). This metric defines a Riemannian metric between two parameters, by considering the resulting vector field that solve this equation (I guess evaluated at time 0). The authors propose to use the natural gradient associated with that Riemannian metric (Theorem 2). Using exactly that natural gradient would involve solving an optimal transport problem (compute the optimal displacement field) and inverting the corresponding operator. The authors mention that, equivalently, a JKO type step could also be considered to obtain an update for \\theta. The authors propose two distinct approximations, a \"semi-backward Euler formulation\", and, next, a simplification of the d_W, which, exploiting the fact that one of the parameterized measures is the push foward of a Gaussian, simplifies to a simpler problem (Prop. 4). That problem introduces a new type of constraint (Gradient constraint) which is yet again simplified.\n\nIn the end, the metric considered on the parameter space is fairly trivial and boils down to the r.h.s. of equation 4. It's essentially an expected squared distance between the new and the old parameter under a Gaussian prior for the encoder.  This yields back the simplification laid out in p.16.\n\nI think the paper is head over heels. It can be caricatured as extreme obfuscation for a very simple modification of the basic GAN algorithm. Although I am *not* claiming this is the intention of the authors, and can very well believe that they found it interesting that so many successive simplifications would yield such a simple modification, I believe that a large pool of readers at ICLR will be extremely disappointed and frustrated to see all of this relatively arduous technical presentation produce such a simple result which, in essence, has absolutely nothing to do with the Wasserstein distance, nor with a \"Wasserstein natural gradient\".\n\nother comments::\n\n*** \"Wasserstein-2 distance on the full density set\": what do you mean exactly? that d_W(\\theta_0,\\theta_1) \\ne W(p_{\\theta_0},p_{\\theta_1})? Could you elaborate where this analogy breaks down? \n\n*** It is not clear to me why the dependency of \\Phi in t has disappeared in Theorem 2. It is not clear either in your statement whether \\Phi is optimal at all for the problem in Theorem 1.\n\n*** the \"semi-backward Euler method\" is introduced without any context. The fact that it is presented as a proposition using qualitative qualifiers such as \"sufficient regularity\" is suspicious. ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}