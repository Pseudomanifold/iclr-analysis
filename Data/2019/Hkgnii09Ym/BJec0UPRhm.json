{"title": "A good paper but need some clarifications and improvements", "review": "This paper presented an attention-based neural network, namely set transformer, a new neural model \nbased on original transformer designed for set inputs. The basic idea is to introduce the attention\nmechanism in both learning the feature embeddings of the set inputs during \u201cencoding\u201d and aggregating \nthese embeddings during \u201cdecoding\u201d. The paper is written clearly and well motivated. The extensive \nset of experiments were conducted to demonstrate the effectiveness of the proposed method. In general, \nI like reading this paper but there are some limitations or unclear parts I need authors to clarify\nand explain. \n\ni) The proposed architecture is mainly adopted from the original transformer but it is highly related\nto the baselines used in the experiments. For instance, it seems like that the current set \ntransformer is a simple combination of Yang et al.(2018) and Mishra et al.(2018) (using Stack of\nSABs) in encoder side and of Ilse et al.(2018) (using PMA and stack of SABs) in the decoder side. \nThis simple combination makes the novelty of this paper unclear. I would like authors to clarify \nmore on the originality w.r.t. these previous works. \n\nii) Although authors proposed a variant of SABs - ISABs using landmark points to accelerate the \ncomputation, there are no any runtime comparisons between SABs and ISABs by fixing other components. \nIt would be interesting to see that ISABs can approach the performance SABs and how it approaches it. \nFor instance, shall we expect that ISABs approach the performance of SABs when increasing the number\nof landmark points (inducing points)? Since in practice most of datasets are relatedly large, I think\nunderstanding the behavior of ISABs is a more interesting problem. \n\niii) After seeing the results in table 6, I have quite concerned about the practical performance of\nset transformer on relatively large datasets (like 1000 points each class in the settings.) It looks\nto me that not only set transformer may have computational issues to scale up, but more importantly\nthat when encoder learned really expressive embeddings with a relatively large number of the set \ninputs it might be little need to leverage attention in pooling anymore. I would like authors to \nconduct some other experiments on relatively large datasets to verify this hypothesis, which is \nimportant for the practical applications of the proposed model. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}