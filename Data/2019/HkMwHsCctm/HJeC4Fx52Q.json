{"title": "Review", "review": "This paper studies the problem of proper learning of deep neural network. In particular, the focus is on doing\napproximate empirical risk minimization over the class of neural networks of a fixed architecture. The main \nresult of the paper is that approximate ERM can be formulated as an LP problem that is of size exponential in the\nnetwork parameters and the input dimensionality. The paper uses a framework of Bienstock and Munoz that shows how to \nwrite a binary optimization problem as a linear problem with size dependent on the treewidth of an appropriate graph\nassociated with the optimization problem. In order to apply the framework, the authors first discretize the parameter\nspace appropriately and then apply analyze the treewidth of the discretized space. The authors also provide treewidth\nanalysis of specific architectures including fully connected networks, and CNNs with various activations.\n\nMost of the technical work in the paper involves analyzing the treewidth of the resulting discretized problem. The nice \nfeature of the result is that it holds for worst case data sets, and hence, the exponential dependence on various\nparameters is unavoidable. On the other hand, it is unclear to me as to how these ideas might eventually lead to \npractical algorithms or shed light on current training practices in the deep learning community. For instance, it would\nbe very interesting to investigate if under certain assumptions on the data generation process, one can get small LPs\nthat depend exponentially only in the depth, as opposed to the input dimensionality.  \n\nI also feel that section 5 does not add much to the main results of the paper and can be skipped or moved entirely to the appendix. On a technical note, I don't see where the dependence on the input dimensionality appears in Theorem 5.1. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}