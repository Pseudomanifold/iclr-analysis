{"title": "exponential complexity; practical relevance unclear", "review": "This work reformulates the neural network training as an LP with size that is exponential in the size of the architecture and data dimension, and polynomial in the size of the data set. They further analyze generalization properties. It extends previous works on 1-hidden-layer neural-nets (say, In Arora et al. (2018)). \n\nPros: Establish new time complexity (to my knowledge) of general neural-nets. \n\nCons: It seems far from having a practical implication. Exponential complexity is huge (though common in TCS and IP communities). No simulation was presented. Not sure which part of the approach is useful for practitioners.\n    My feeling is that the paper is a bit too theoretical and less relevant to ICLR audience. More theoretical venues may be a better fit. \n\nOther questions:\n--The authors mentioned \u201cthat is exponential in the size of the architecture (and data dimension)  and polynomial in the size of the data set;\u201d and \"this is the best one can hope for due to the NP-Hardness of the problem \". \na)\tThe time complexity is exponential in both the size of neural-net and the data dimension (the latter seems to be ignored in abstract). Is there a reference that presents results on NP-hardness in terms of both parameters, or just one parameter? \nb)\tThe NP-hardness reduction may give an exp. time algorithm. Is there a simple exponential time algorithm? If so, I expect the dependence on the size of the data set is exponential, and the contribution of this paper is to improve to polynomial. The authors mentioned one discretization method, but are there others? More explanation of the importance of the proved time complexity will be helpful. \n\n-- Novelty in technical parts: The idea of tree-width graph was introduced in Bienstock and Mun\u0303oz (2018). The main theorem 3.1 is based on explicit construction for Theorem 2.5, and Theorem 2.5 is an immediate generalization of a theorem in Bienstock and Mun\u0303oz (2018) as mentioned in the paper. Thus, this paper looks like an easy extension of Bienstock and Mun\u0303oz (2018) --intuitively, minimizing polynomials by LP seems to be closely related to solving neural-nets problems by LP. Could the authors explain more on the technical novelty? \n\nUpdate after rebuttal: I'd like to thank the authors for the detailed response. It addressed most of my concerns.\n    The analogy with MIP makes some sense, as huge LPs are indeed being solved every day. However, an important difference is that those problems cannot be solved in other better ways till now, while for deep learning people are already successfully solving the current formulations. I still think this work will probably not lead to a major empirical improvement.\n     I just realize that my concern on the practical relevance is largely due to the title \"Principled Deep Neural Network Training through Linear Programming\". It sounds like it can provide a better \"training\" method, but it does not show a practical algorithm that works for CIFAR10 at this stage. The title should not sound like \"look, here is a new method that can change training\", but \"hey, check some new theoretical progress, it may lead to future progress\". I strongly suggest changing the title to something like \"Reformulating DNN as a uniform LP\" or \"polynomial time algorithm in the input dimension\", which reflects the theoretical nature. \n    That being said, the MIP analogy makes me think that there might be some chance that this LP formulation is useful in the future, maybe for solving some special problems that current methods fail miserably.  In addition, it does provide a solid theoretical contribution. For those reasons (and assuming the title will be changed), I increase my score. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}