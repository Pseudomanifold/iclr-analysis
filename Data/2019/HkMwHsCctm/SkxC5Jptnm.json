{"title": "Solid work that could discuss the implications for the ICLR community better", "review": "This is very solid work and the framework allows one to plug-in existing complexity measures to provide complexity upper bounds for (some) DNNs. The main idea is to rephrase an empirical risk minimization problem in terms of a binary optimization problem \nusing a discretization of the continuous variables. Then this formulation is used to provide a as a moderate-sized linear program of its convex hull. \n\nIn my opinion, every paper that provides insights into the complexity and generalization of deep learning is an important contribution. Moreover, the present paper is based on \na recent insight of the authors, i.e., it is based on solid grounds. However, it would have been nice to also show some practical insights. The main take-aways message is that we need exponential time. Is this practical for networks with with millions of parameters? Or does this imply that deep learning is hopeless (in theory)? To be fair, the authors touch upon this in the conclusions, but only 1-2 sentences. This discussion should be extended. Nevertheless, I agree that the bridge built is important and may indeed trigger some very important future contributions. \n\nThe authors should, however, also review other work on linear programming for deep networks coming from the machine learning community such as \n\nBrandon Amos, Lei Xu, J. Zico Kolter:\nInput Convex Neural Networks. \nICML 2017: 146-155\n\nGiven the background of the average ICLR reader, the authors should also introduce (at least the intuitions) improper and proper learning setups in the introduction before using them.   This also holds for other terminology from complexity theory. Indeed, the authors cannot introduce/review all complexity theory. However, they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic. Without, while important for the ICLR community, the authors run the risk that the paper would better be suited by a learning theory venue. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}