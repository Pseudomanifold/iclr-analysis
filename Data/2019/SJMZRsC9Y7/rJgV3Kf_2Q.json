{"title": "This paper is not positioned well with respect to the literature. I am not sure what are its key contributions and how significant they are.  The technical exposition also appears somewhat incoherent and not well-justified (see my specific comments below).", "review": "PAPER SUMMARY:\n\nThis paper introduces a non-extensive statistic random walk model to generate sentence embedding while accounting for \nhigh non-linearity in the semantic space.\n\nNOVELTY & SIGNIFICANCE:\n\nI am not sure what the main focus of this paper is. It seems accounting for non-linearity in the semantic space while generating sentence embedding has already been achieved by existing LSTM models -- the goal seems to be more about interpretability and computational efficiency but the paper did not really discuss these in detail (more on this later).\n\nIn terms of the proposed solution, I am also not sure what is the significance of using non-extensive statistic in this context. In fact, the background section gave the impression that the non-linear form of q-exponential is the main reason to advocate this approach. But, if it is only about handling non-linearity, there are plenty of alternatives and it is important to point out exactly what advantages non-extensive statistic has over the existing literature (e.g., why is it more interpretable than LSTM). Please expand the respective background section to clarify this. \n\nTECHNICAL SOUNDNESS:\n\nThere are parts of the technical exposition that appear confusing and somewhat incoherent. For instance, what exactly is this confounding effect of vector length & why do we need to address this issue if according to the Section 2.2, it has already been addressed in the same context?\n\nSection 2.2 seems to discuss this effect but the exposition is unclear to me. The authors start with an example and a bunch of assumptions that lead to a contradiction. \n\nIt is then concluded that the cause of this is due to the linearity assumption (what about the other assumptions?) in estimating the discourse vector. \n\nI do not really follow this reasoning and it would be good if the authors can elaborate more on this.\n\nCLARITY:\n\nThe paper seems to focus too much on technical details and does not give enough discussion on its positioning. The significance of the proposed solution with respect to the literature remains unclear. \n\nEMPIRICAL RESULTS:\n\nI am not an expert in this field and cannot really judge the significance of the reported results. I do, however, have a few questions: in all benchmarks, are the algorithms tested on a different domain than the domain it was trained on? \n\nHave the authors compared the proposed sentence embedding framework with the LSTM literature mentioned in the introduction? I noticed there was a LSTM AVG in the comparison table.\n\nIs that the simple averaging scheme mentioned in the introduction when the authors discussed transferrable sentence embedding?\n\nIs there any reason for not comparing with RNN (Cho et al., 2014)? \n\nIn terms of the computation processing cost, how efficient is the proposed method (as compared to existing literature)?", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}