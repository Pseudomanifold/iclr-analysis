{"title": "Dynamic graph representation learning with self-attention", "review": "This paper describes learning representation for dynamic graphs using structural and temporal self-attention layers. They applied their method for the task of link-prediction. However, I have serious objections to their experimental setup. I have seen people used sets of edges and pairs of vertices without an edge for creating examples for link-prediction on a static graph, however, working with a real-world dynamic graph, you can compute the difference between G_t and G_{t+1} as the changes that occur in G_t+1 1) Why are you not trying to predict these changes?  Moreover, 2) why do you need examples from snapshot t+1 for training when you have already observed t snapshots of the graph? \n3) The selected graphs are very small comparing to the dynamic graphs available here http://konect.uni-koblenz.de/networks/.  \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}