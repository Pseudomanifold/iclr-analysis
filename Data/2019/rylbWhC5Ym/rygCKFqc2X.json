{"title": "A new td method", "review": "This paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.\n\nThis paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.\n\nMain argument\n\nThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paper\u2014that is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case. \n\nThe resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how it\u2019s fixed point relates to TD?\n\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:\n1) What was the policy used in exp1 for policy evaluation in MC?\n2) Why Fourier basis features?\n3) In MC with DQN how did you adjust the parameters and architecture for the MC task?\n4) Was the reward in MC and Acrobot -1 per step or something else\n5) How did you tune the parameters in the MC and Acrobot experiments?\n6) Why so few runs in MC, none of the results presented are significant?\n7) Why is the performance so bad in MC?\n8) Did you evaluate online learning or do tests with the greedy policy?\n9) How did you initialize the value functions and weights?\n10) Why did you use experience replay for the linear experiments?\n11) IN MC and Acrobot why only a one layer MLP?\n\n\nIgnoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains. \n\nThe differences in the \u201caverage change in value plots\u201d are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methods\u2014this is not a valid comparison. Your method may just work better with alpha = 0.1. \n\n\nThe paper has many imprecise parts, here are a few:\n1) The definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation\n2) equation 1 of section 2.1 is neither an algorithm or a loss function\n3) TD does not minimize the squared TD. Saying that is the objective function of TD learning in not true\n4) end of section 2.1 says \u201cIt is computed as\u201d but the following equation just gives a form for the partial derivative\n5) equation 2, x is not bounded \n6) You state TC-loss has an unclear solution property, I don\u2019t know what that means and I don\u2019t think your approach is well justified either\n7) Section 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear\n8) treatment of n_t in alg differs from appendix (t is no time episode number)\n9) Your method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.\n10) Over-claim not supported by the results: \u201cwe see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is \u201c. The results do not show this.\n11) Section 4.4 does not seem to go anywhere or produce and tangible conclusions\n\nThings to improve the paper that did not impact the score:\n0) It\u2019s hard to follow how the prox operator is used in the development of the alg, this could use some higher level explaination\n1) Intro p2 is about bootstrapping, use that term and remove the equations\n2) Its not clear why you are talking about stochastic vs deterministic in P3\n3) Perhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization\n4) Section 2: \u201ccan often be a regularization term\u201d >> can or must be?\n5) update law is a odd term\n6)\u201d tends to alleviate\u201d >> odd phrase\n7) section 4 should come before section 3\n8) Alg 1 in not helpful because it just references an equation\n9) section 4.4 is very confusing, I cannot follow the logic of the statements \n10) Q learning >> Q-learning\n11) Not sure what you mean with the last sentence of p2 section 5\n12) where are the results for Acrobot linear function approximation\n13) appendix Q-learning with linear FA is not DQN (table 2)", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}