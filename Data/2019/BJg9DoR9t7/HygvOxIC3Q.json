{"title": "Generally well-written paper on a new information theoretical approach for learning from crowdsourced data.", "review": "Top pros:\n- Well motivated approach with good examples from clinical setting\n- Sound proof on why information theoretical approach is better than MLE based approaches\n- Experiments on diversified data sets to show their approach's performance, with good implementation details. \n\nTop cons:\n- Fairly strong assumption on the existence of mutually independent senior experts in the labeling process\n- Hard-to-check assumption for Theorem 3.4 for real world problems, on the sufficiency of senior expert's info to predict the true class label\n\nThe paper is in general well written, and builds upon existing work on crowdsourced data mining and co-training. I believe this line of work will benefit the community in taking a more information theoretical approach with relaxed assumptions on the data collection process. My main feedback is how to check the existence of senior experts in real-world applications. In particular,\n- If the labels are collected from an unknown setup (e.g. on AMT), where it is hard to establish the dependency structure of the experts, how can we use such approaches effectively? \n- Even if there exists a clear line between senior/junior experts in the labeling process, how do we know or check that the senior experts' opinion can sufficiently estimate the true labels? \n\nIn the experiment section, the label data was collected with a build-in assumption of senior/junior labelers, and we also know exactly who are senior/junior experts. So it is not surprising that the proposed approach outperforms other approaches. It's also interesting to see that AggNet isn't that bad in general compared to the proposed approach (except on LUNA16). What if we combine all experts in one setting and apply the proposed approach without prior knowledge of who are senior/junior? Also, did you require all experts to label ALL the data points or only a subset of training data points? \n\nMinor points:\n- I don't believe \"Naive majority\" is an interesting setting - we can easily detect those junior experts that always label cases with one class, and remove these experts from the system, in practice. \n- I wouldn't call this an \"early\" algorithm as it indicates it's somewhat pre-mature. Just call this a novel approach that is in the early phase, and more sophisticated approach can be further developed. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}