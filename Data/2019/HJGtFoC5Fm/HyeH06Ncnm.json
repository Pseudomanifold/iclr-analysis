{"title": "Interesting insights on inductive bias of two-layer ReLU networks", "review": "This paper studies the implicit bias of minimizers of a regularized cross entropy loss of a two-layer network with ReLU activations. By combining several results, the authors obtain a generalization upper bound which does not increase with the network size. Furthermore, they show that the maximum normalized margin is, up to a scaling factor, the l1 svm margin over the lifted feature space of an infinite-size network. Finally, in a setting of infinite-sized networks, it is proved that perturbed Wasserstein gradient flow finds a global minimum in polynomial time.\n\nI think that the results are interesting and relevant to current efforts of understanding neural networks. The techniques and ideas seem promising and may be applied in more general settings. The paper is mostly clearly written, but there are some issues which I outline below.\n1.\tIt is not clear what is the novelty in sections 2 and 3.1 except the combination of all the results to get a generalization bound which does not increase with network size (which on its own is non-trivial). Specifically, \na.\tWhat is the technical contribution in Theorem 2.1 beyond the results of the two papers of Rosset et al. (journal paper and the NIPS paper which was mentioned in the comment on missing prior work)?\nb.\tHow does Theorem 3.1 compare with previous Rademacher bounds for neural networks which are based on the margin? In Neyshabur et al. (2018), it is shown that margin-based generalization bounds empirically increase with network size. Does this hold for the bound in Theorem 3.1?\n\n2.\tIn the work of Soudry et al. (2018) section 4.3, they consider deep networks with an unregularized loss and show that gradient descent converges to an l2 max margin solution under various assumptions. What is the connection between this result and the l1 max margin result in section 3.3?\n\n3.\tWhat are the main proof ideas of Theorem 4.3? Why is the perturbation needed?\n\n4.\tWhat is the size of the network that was trained in Section 5 in the experiments of Figure 3? Only the size of the ground truth network is mentioned.\n\n\n---------Revision------------\n\nI have read the author's response and other reviews. I am not changing the current review.\nI have one technical question. In the new generalization bound (Proposition 3.1), the authors claim that the product of Frobenius norms is replaced with a sum. However, I don't see any sum in the proof. Could the authors please clarify this?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}