{"title": "Review of 'On the Margin Theory of Feedforward Neural Networks'", "review": "UPDATE: after revisions and discussion. There seems to be some interesting results presented in this paper which I think would be good to have discussed at the conference. This is conditional on further revisions of the work by the authors.\n\n\nThis paper studies margin theory for neural nets.\n\n1. First it is shown that margin of the solution to regularized problem approaches max margin solution.\n2. Then a bound is given for using approximate solution to above optimization problem instead of exact one. Note that the bound depends on size of the network via parameter a.\n3. Then 2-layer relu networks are studied. It shown that max margin is monotonically increasing in size of the network. Note however, it is hard to relate this results to inexact solutions since the bound in that case as was pointed out also depends on the size of the network.\n4. Paper also provides comparison with kernel methods, simulations and shows that perturbed wasserstein flows find global optimiziers in poly time.\n\nThe paper argues that over-parameterization is good for generalization since margin grows with the number of parameters. However, it should be also noted the radius of data may also grow (and in case of the bounds it seems to be the radius of data in lifted space which increases with the size of the network). I hope authors can clarify this and points 2 and 3 above in their response. In the current form the paper is below the acceptance threshold for me. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}