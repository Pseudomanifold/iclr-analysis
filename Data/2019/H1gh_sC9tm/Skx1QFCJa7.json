{"title": "Several inconsistencies", "review": "This paper proposes a new detection method for adversarial examples, based on a prior network, which gives an uncertainty estimate for the network's predictions.\n\nThe idea is interesting and the writing is clear. However, I have several major concerns. A major one of these is that the paper considers \"detection of adversarial attacks\" to mean detecting adaptive and non-adaptive attacks, while the latter are (a) unrealistic and (b) a heavily explored problem, with solutions ranging from clustering activations, to denoising the image via projection (in particular, once can use any of the circumvented ICLR 2018 defenses which all work in the non-adaptive sense, and check the prediction of the denoised image vs the original). Thus, the paper should focus on the regime of adaptive attacks. Within this regime:\n\nMotivation:\n- This work seems to suggest that dropout-based detection mechanisms are particularly successful. While Carlini & Wagner finds that the required distortion (using a specific attack) increases with randomization, the detection methods which used dropout were still completely circumvented in this paper.\n\n- The claim that adversarial examples are \"points off of the data manifold\" is relatively unmotivated, and is not really justified. Justification for this point is needed, as it forms the entire justification for using Prior Networks.\n\n- Detecting adversarial examples is not the same problem to detecting out-of-distribution samples, and the writing of the paper should be changed to reflect this more.\n\nEvaluation:\n- 100 iterations is not nearly enough for a randomization-based or gradient masking defense, so the attacks should be run for much longer. In particular, some of the success rate lines appear to be growing at iteration 100.\n\n- There is no comparison to any other method (in particular, just doing robust prediction via Madry et al or something similar); this should be added to contextualize the work.\n\n- The term \"black-box\" attacks can take on many meanings/threat models. The threat models in the paper need to be more well-defined, and in particular \"black-box attacks\" should be more accurately defined. If black-box attack refers to query-based attacks, the success rate should be equal to those of white-box attacks (or very close to it), as then the attack can just estimate the gradient through the classifier via queries.\n\n- The fact that the attacks do not reach 100% on the unprotected classifier is concerning, and illustrates the need for stronger attacks.\n\nSmaller comments:\nPage 1: Abstract: Line 4: missing , at the end of the line\nPage 1: Abstract: Line 5: \u201cHowever, system can\u201c missing a before system\nPage 1: Abstract: Line 10: \u201chave been shown\u201d should be \u201chas\u201d instead of \u201chave\u201d\nPage 1: Abstract: Line 13: \u201cIn this work\u201d missing a , after\nPage 1: Last paragraph: Line 2: \u201cinvestigate\u201d missing an s and should be \u201cinvestigates\u2019\nPage 2: Section 2: Line 5: \u201cin other words\u201d missing a , after\nPage 2: Section 2: Line 8: \u201cIn order to capture distributional uncertainty\u201d  missing a , after\nPage 2: Last paragraph: Line 2: \u201cTypically\u201d missing a , after\nPage 3: Paragraph 2: Line 1: \u201cIn practice, however, for deep,\u201d no need for the last ,\nPage 3: Section 2.2: paragraph 1: Line 2: \u201ca Prior Network p(\u03c0|x\u2217; \u03b8\u02c6), \u201c no need for the last ,\nPage 3: Section 2.2: paragraph 1: Line 3: \u201cIn this work\u201c missing a  , after\nPage 3: second last paragraph: Line 1: refers to figure as fig and figure (not consistent)\nPage 3: second last paragraph: Line 3: \u201cuncertainty due severe class\u201c missing \u201cto\u201d before \u201csevere\u201d\nPage 4: Paragraph 1: Line 2: \u201cto chose\u201d should be \u201cto choose\u201d\nPage 4: Paragraph 2: Line 1: \u201cGiven a trained Prior Network\u201c missing a , after\nPage 4: Paragraph 2: two extra ,\nPage 6: paragraph 1: Last line: 5 needs to be written in words and same for 10 in the next paragraph\nPage 7: section 4.2: paragraph 3: \u201cFor prior networks\u201d need a , after\nPage 8: Paragraph 1: Line 6: \u201cand and\u201d\nPage 8: Conclusion: Line 4: \u201cIn section 4.2\u201d needs a , after\nPage 8: Conclusion: Line 7: \u201cit difficult\u201d missing \u201cis\u201d\nPage 8: Conclusion: Line 9: \u201cis appropriate\u201d should be \u201cif\u201d instead of \u201cis\u201d", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}