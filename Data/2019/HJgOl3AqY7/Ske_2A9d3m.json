{"title": "Nice idea but falls short of what it promises", "review": "This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. Contrary to previous work, a single (conditioned) decoder is used for all instrument domains, which means a single model can be used to convert any source domain to any target domain.\n\nUnfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts. The instruments are often unrecognisable, although with knowledge of the target domain, some of its characteristics can be identified. The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.\n\nI have several further concerns about this work:\n\n* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present. I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.\n\n* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?\n\n* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE? This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.\n\nI appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.\n\nOverall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.\n\n\n\nOther comments:\n\n* In the introduction, an adversarial criterion is referred to as a \"discriminative objective\", but \"adversarial\" (i.e. featuring a discriminator) and \"discriminative\" mean different things. I don't think it is correct to refer to an adversarial criterion as discriminative.\n\n* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.\n\n* Some turns of phrase like \"recently gained a flourishing interest\", \"there is still a wide gap in quality of results\", \"which implies a variety of underlying factors\", ... are vague / do not make much sense and should probably be reformulated to enhance readability.\n\n* Introduction, top of page 2: should read \"does not learn\" instead of \"do not learns\".\n\n* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a \"domain confusion loss\"), contrary to what is claimed in the introduction.\n\n* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model. I think all claims about running time should be corroborated by controlled experiments.\n\n* I think Figure 1 is great and helps a lot to distinguish the different domain translation paradigms.\n\n* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. \"matching samples\").\n\n* Section 3.1, \"amounts to optimizing\" instead of \"amounts to optimize\"\n\n* Higgins et al. (2016) specifically discuss the case where beta in formula (1) is larger than one. As far as I can tell, beta is annealed from 0 to 1 here, which is an idea that goes back to \"Generating Sentences from a Continuous Space\" by Bowman et al. (2016). This should probably be cited instead.\n\n* \"circle-consistency\" should read \"cycle-consistency\" everywhere.\n\n* MMD losses in the context of GANs have also been studied in the following papers:\n- \"Training generative neural networks via Maximum Mean Discrepancy optimization\", Dziugaite et al. (2015)\n- \"Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy\", Sutherland et al. (2016)\n- \"MMD GAN: Towards Deeper Understanding of Moment Matching Network\", Li et al. (2017)\n\n* The model name \"FILM-poi\" is only used in the \"implementation details\" section, it doesn't seem to be referred to anywhere else. Is this a typo?\n\n* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?\n\n* The descriptor distributions in Figure 3 don't look like an \"almost exact match\" to me (as claimed in the text). There are some clearly visible differences. I think the wording is a bit too strong here.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}