{"title": "Review of \"Interpreting Adversarial Robustness: A View from Decision Surface in Input Space\"", "review": "This paper argues that analyzing loss surfaces in parameter space for the purposes of evaluating adversarial robustness and generalization is ineffective, while measuring input loss surfaces is more accurate. By converting loss surfaces to decision surfaces (which denote the difference between the max and second highest logit), the authors show that all adversarial attack methods appear similar wrt the decision surface. This result is then related to the statistics of the input Jacobian and Hessian, which are shown to differ across adversarially sensitive and robust models. Finally, a regularization method based on regularizing the input Jacobian is proposed and evaluated. All of these results are shown through experiments on MNIST and CIFAR-10.\n\nIn general, the paper is clear, though there are a number of typos. With respect to novelty, some of the experiments are novel, but others, including the improved training method, have been explored before (see specific comments for references). Finally, regarding significance, many of the insights provided this paper are true by definition, and are therefore unlikely to have a significant impact. \n\nWhile I strongly believe that rigorous empirical studies of neural networks are essential, this paper is lacking in several key areas, including framing, experimental insights, and relation to prior work, and is therefore difficult to recommend. Please see the comments below for more detail. \n\nMajor comments:\n\n1) In the beginning of the paper, adversarial robustness and generalization are equated. However, adversarial robustness and generalization are not necessarily equivalent, and in fact, several papers have provided evidence against this notion, showing that adversarial inputs are likely to be present even for very good models [5, 6] and that adversarially sensitive models can often generalize quite well [2]. Moreover, all the experiments within the paper only address adversarial robustness rather than generalization to unperturbed samples.\n\n2) One of the main results of this paper is that the loss surface wrt input space is more sensitive to adversarial perturbations than the loss surface wrt parameter space. Because adversarial inputs are defined in input space, by definition, the loss surface wrt to the input must be sensitive to adversarial examples. This result therefore appears true by definition. Moreover, [3] related the input Jacobian to generalization, finding a similar result, but is not discussed or cited.\n\n3) The main result of Section 3 is that all adversarial attacks \u201cutilize the decision surface geometry properties to cross the decision boundary within least distance.\u201d While to my knowledge the decision surface visualization is novel and might have important uses, this statement is again true by definition, given that adversarial attack methods try to find the smallest perturbation which changes the network decision. As a result, all methods must find directions which are short paths in the decision surface. It is therefore unclear what additional insight this analysis presents. \n\n4) How does measuring the loss landscape as an indicator for adversarial robustness differ from simply trying to find adversarial examples as is common? If anything, it seems it should be more computationally expensive as points are sampled in a grid search vs optimized for. \n\n5) The proposed regularizer for adversarial robustness, based on regularizing the input Jacobian, is very similar to what was proposed in [1], yet [1] is not discussed or cited. \n\nMinor comments:\n\n1) The paper\u2019s first sentence states that \u201cIt is commonly believed that a neural network\u2019s generalization is correlated to ...the flatness of the local minima in parameter space.\u201d However, [4] showed several years ago that the local minima flatness can be arbitrarily rescaled and has been fairly influential. While [4] is cited in the paper, it is only cited in the related work section as support for the statement that local minima flatness is related to generalization when this is precisely opposite the point this paper makes. [4] should be discussed in more detail, both in the introduction and the related work section.\n\n2) The paper is quite lengthy, going right up against the hard 10 page limit. While this may be acceptable for papers with large figures or which require the extra space, this paper does not currently meet that threshold. \n\n3) Throughout the figures, axes should be labeled. \n\n4) In section 2.2, it is stated that both networks achieve optimal accuracy of ~90% on CIFAR-10. This is not optimal accuracy and hasn\u2019t been for several years [7].\n\n5) Why is equation 2 calculated with respect to the logit layer vs the normalized softmax layer? Using the unnormalized logits may introduce noise due to scaling.\n\n6) In Figure 8, the scales of the Hessian are extremely different. Does this impact the measurement of sparseness? \n\n\nTypos:\n\n1) Introduction, second paragraph: \u201cFor example, ResNet model usually converges to\u2026\u201d should be \u201cFor example, ResNet models usually converge to\u2026\u201d\n\n2) Introduction, second paragraph: \u201c...defected by the adversarial noises...\u201d should be \u201c...defected by adversarial noise\u2026\u201d\n\n3) Introduction, third paragraph: \u201c...introduced by adversarial noises...\u201d should be \u201c...introduced by adversarial noise\u2026\u201d\n\n4) Section 3.1, first paragraph: \u201ccross entropy based loss surface is\u2026\u201d should be \u201ccross entropy based loss surfaces is\u2026\u201d\n\n[1] Jakubovitz, Daniel, and Raja Giryes. \"Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization.\" arXiv preprint arXiv:1803.08680 (2018). ECCV 2018.\n[2] Zahavy, Tom, et al. \"Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms.\" arXiv preprint arXiv:1602.02389 (2016). ICLR Workshop 2018\n[3] Novak, Roman, et al. \"Sensitivity and generalization in neural networks: an empirical study.\" arXiv preprint arXiv:1802.08760 (2018). ICLR 2018.\n[4] Dinh, Laurent, et al. \"Sharp Minima Can Generalize For Deep Nets.\" International Conference on Machine Learning. 2017.\n[5] Fawzi, Alhussein, Hamza Fawzi, and Omar Fawzi. \"Adversarial vulnerability for any classifier.\" arXiv preprint arXiv:1802.08686 (2018). NIPS 2018.\n[6] Gilmer, Justin, et al. \"Adversarial spheres.\" arXiv preprint arXiv:1801.02774 (2018). ICLR Workshop 2018.\n[7] http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}