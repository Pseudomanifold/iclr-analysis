{"title": "Improving InfoVAE", "review": "This paper presents a new regularization technique for VAEs similar in motivation and form to the work on InfoVAE.  The basic intuition is to encourage different training samples to occupy different parts of z-space, by maximizing the expected KL divergence between pairwise posteriors, which they call Mutual Posterior-Divergence (MPD).  They show that this objective is a symmetric version (sum of the forward and reverse KL) of the Mutual Info regularization used by the InfoVAE.  In practice however, they do not actually use this objective.  They use a different regularization which is based on the MPD loss but they say is more stable because it's always greater than zero, and ensures that all latent dimensions are used.  In addition to the MPD based term, they also add another term which encouraging the pairwise KL-divergences to have a low standard-deviation, to encourge more even spreading over the z-space rather than the clumpy distribution that they observed with only the MPD based term.\n\nThey show state of the art results on MNIST and Omniglot, improving over the VLAE.  But on natural data (CIFAR10), their results are worse than VLAE.  \n\nPros:\n\t1. The technique has a nice intuitive (but not particularly novel) motivation which is kinda-sorta theoretically motivated if you squint at it hard enough.\n\t2. The results on the simple datasets are solid and encouraging.\n\nCons:\n\t1.  The practical implementation is a bit ad-hoc and requires turn two additional hyper parameters (like most regularization techniques).\n\t2. The basic motivation and observations are the same as InfoVAE, so it's not completely novel.\n\t3. The CIFAR10 results are bit concerning, and one can't help but wondering if the technique really only helps when the data has simpler shared structure.\n\nOverall:  I think the idea is interesting enough, and the results encouraging enough to be just above the bar for acceptance at ICLR.\n\nI have the following question for the authors:\n\n\t1. Why do you use the truncated pixelcnn on CIFAR10?  Did you try it with the more expressive decoder (as was used on the binary images) and got worse results?  or is there some other justification for this difference?\n\nI would have like to see the following modifications to the paper:\n\n\t1. The paper essentially presents two related but separate regularization techniques.  It would be nice to have ablation results to show how each of these perform on their own.\n\t2. Bonus points for showing results which combine VLAE (which already has a form of the MPD regularization) with the smoothness regularization.\n\t3. It would be nice to see samples from VLVAE in Figure 3 next to the MAE samples to more easily compare them directly.\n\t4. There are many grammatical and English mistakes.  The paper is still quite readably, but please make sure the paper is proofread by a native English speaker.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}