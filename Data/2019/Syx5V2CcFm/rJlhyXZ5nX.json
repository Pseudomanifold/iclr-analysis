{"title": "Novel idea, Like the paper", "review": "Summary:\nThe paper presents an analysis and numerical evaluation of stagewise SGD, ADAGRAD and Stochastic momentum methods for solving stochastic non-smooth non-convex optimization problems. \n\nComments:\nI find the ideas presented in this paper very interesting. The convergence analysis seems correct and the paper is reasonably well written, and tackles an important problem. \n\nThe analysis holds for \u03bc-weekly convex functions. This assumption is really important for the development of the algorithm and the proposed analysis. I like the fact that the authors provide two examples showing that popular objective functions in machine learning satisfy this assumption.\n\nThe numerical evaluation is adequate showing the effectiveness  of the proposed stagewise algorithms.  However i have the follow suggestions/minor comments:\n\n1) It will be nice to have also some plots showing the performance of the proposed method on the ImageNet dataset. \n2) Another possible nice experiment will be a comparison of the four stagewise methods (SGD,ADAGRAD,SHB,SNAG) on the same dataset. Which one behaves better? \n\nMinor Comments:\n1) The captions of the figures can be more informative (mention also the division by column). First column is SGD, Second column Adagrad, etc.\n2) Typos: \nSection 1, last bullet point, second line: \"stagwise\"\nSection 5, second paragraph , first line :\"their their\"\npage 8, 3 line from the bottom:  \"seems, indicate\"\n\n2) Missing reference.\nIn the area of stochastic gradient methods with momentum many papers have been proposed recently for the case of convex optimization that worth to be mentioned:\nGadat, S\u00e9bastien, Fabien Panloup, and Sofiane Saadane. \"Stochastic heavy ball.\" Electronic Journal of Statistics 12.1 (2018): 461-529.\nLoizou, Nicolas, and Peter Richt\u00e1rik. \"Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods.\" arXiv preprint arXiv:1712.09677 (2017).\nLan, Guanghui, and Yi Zhou. \"An optimal randomized incremental gradient method.\" Mathematical programming (2017): 1-49.\n\nOverall, I suggest to accept this paper.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}