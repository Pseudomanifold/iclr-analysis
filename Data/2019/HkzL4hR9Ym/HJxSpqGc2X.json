{"title": "The paper presents a community based autoencoder framework. It tries to address co-adaptation of encoders and decoders. And aims at constructing better representations. Overall, the paper needs work.", "review": "1) Can we think of this setup as sparse + desoising AE? As opposed to being similar to dropout? \nFor better clarity, describe what the setup reduces to for length 1 AE setting in the CbAE? \n\n2) What is the influence of regularization in AEs, including denoising or contractive criteria? And jumping to experiments, these varieties of AEs are not evaluated? Since the criterion of encoder-decoder selection corresponds to noising, this evaluation is critical.\n\n3) The natural baseline here is an ensemble of AEs. Should be compared.\n\n4) How similar are the representations across the decodes in the community (not the RSA setup used in page 8)? This gives clear info on how big the community should be? And more critically, whether the cross-decoder representations are repetitive/redundant (i.e., an intrinsic fitting issue for CbAE)? \n\n5) Is the statement following eq 3 the other way around? Positive SCG is good or bad? Confusing, explain? \n\n6) What about non-monotonic learning curves? How is SCG formulated for such a setting? A windowed average of learning curve thresholds is a better criterion. A more broader question is why using this criterion makes sense for the goal of better representation building? \n\n7) Why is iteration 64, community size 16 consistently -ve SCG in CIFAR? And for the same community size the RSA is going down after increasing initially in Table 3? Something going on here. Intuitively, this implies bounds on the community size (like with random forests ensemble setting -- a tradeoff of number of trees vs. depth). \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}