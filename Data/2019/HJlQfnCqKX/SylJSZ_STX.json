{"title": "An empirical study towards the prediction power based on the margin distribution at each layer.", "review": "The author(s) suggest using geometric margin and layer-wise margin distribution in [Elsayed et al. 2018] for predicting generalization gap.\n\npros,\na). The author shows large experiments to support their argument.\n\ncons,\na). No theoretical verification (nor convincing intuition) is provided, especially for the following questions,\n    i) what benefit can be acquired when using geometric margin defined in the paper.\n    ii) why does normalization make sense beyond the simple scaling-free reason. For example, spectral complexity as a normalization factor in [Bartlett et al. 2017] is proposed from the fact, that the Lipschitz constant determines the complexity of network space.\n    iii) why does the middle layer margin can help? \n    iv) why a linear (linear log) relation between the statistic and generalization gap.\n\nFurther question towards experiment,\ni) I don't think your comparison with Bartlett's work is fair. Their bounds suggest the gap is approximately Prob(0<X<\\gamma) + Const/\\gamma for a chosen \\gamma, where X is the normalized margin distribution. I think using the extracted signature from margin distribution and a linear predictor don't make sense here.\nii) If you do regression analysis on a five layers cnn, can you have a good prediction on a nine layers cnn (or even residue cnn)?\n\nFinally, I'm not sure the novelty is strong enough since the margin definition comes from [Elsayed et al. 2018] and the strong linear relationship has been shown in [Bartlett et al. 2017, Liao et al. 2018] though in different settings.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}