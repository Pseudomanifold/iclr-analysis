{"title": "Experiments and novelty need improvement", "review": "This paper presents some experiments using random projections instead of embeddings from a 1-of-V encoding.  Experiments on the Penn TreeBank benchmark data set show that in a feed-forward language modeling architecture similar to that of (Bengio, 2003), the random projections substantially reduce the number of parameters of the model while not harming perplexity too much.\n\nThe paper would need to be improved substantially in order to appear at a conference like ICLR.  First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.\n\nSecond, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.  First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.  Second, the paper needs to use more state-of-the-art architectures.  Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.  Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.  Changing the number/sizes of the network layers or using sparse weight matrices (perhaps with sparsity-inducing regularization) would be natural ways to reduce the parameter space.\n\nIn my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.\n\nMinor\nIn the start of Section 3, it is not clear why having the projection be sparse is desired.  Later, space (and time) efficiency is revealed as the motivation for the sparsity, but it would be helpful if the paper said this earlier.\nEquation 6 seems to have an error, the probability should be P(w_t | w_t-1...) instead of P(w_t , w_t-1...) if this is to represent the standard LM objective (the probability of the corpus).\nSec 3.3: \"all models sare\"", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}