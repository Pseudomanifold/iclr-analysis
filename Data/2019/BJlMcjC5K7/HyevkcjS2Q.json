{"title": "Preliminary work on using random projections for word embeddings in language models", "review": "The main idea behind the paper is to use random projections as the initial word representations, rather than the vocab-size 1-hot representations, as is usually done in language modeling. The benefit is that the matrix which projects words into embedding space can then be much smaller, since the space of random projections can be much smaller than the vocab size. The idea is an interesting one, but this work is at too much of a preliminary stage for a top-tier conference such as ICLR. In its present state it would make for a potentially interesting paper at a targeted workshop.\n\nMore specific comments\n--\n\nThe initial description of the language modeling problem assumes a particular decomposition of the joint probability, according to a particular application of the chain rule, but of course this is a modeling choice and not the only option (albeit the standard one).\n\nThe main problem with the paper is the use of simple baseline setups as the only experimental configuration:\n\no feedforward rather than recurrent network;\no use of the Penn Treebank dataset only;\no use of a small n for the n-grams.\n\nAll or at least some of these decisions would need to be relaxed to make a convincing paper.\n\nThe reasons for the use of the energy-based formulation are not clear to me. Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?\n\nJust before equation 6 it says that the resulting vector representation is the *sum* of all the non-zero entries. But there are some minus ones in the random projection? \n\nThe PPL expression at the bottom of p.5 doesn't look right. The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.\n\nIt looks like all the results are given on the test set. Did you not do any tuning on the validation data?\n\nThe plots in figure 4 are too small. It would be useful to have a table, like the one on the last page, which clearly shows the baseline vs. the random-projection model, with some description of the results in the main body of the text.\n\nThe overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission. For example, there are lots of typos such as \"instead of trying to probability of a target word\".\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}