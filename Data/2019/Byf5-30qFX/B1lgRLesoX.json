{"title": "Simple and nice idea, but very unclear description and some serious flaws", "review": "In this paper, the authors extend the HER framework to deal with dynamical goals, i.e. goals that change over time.\nIn order to do so, they first need to learn a model of the dynamics of the goal, and then to select in the replay buffer experience reaching the expected value of the goal at the expected time. Empirical results are based on three (or four, see the appendix) experiments with a Mujoco UR10 simulated environment, and one experiment is successfully transfered to a real robot.\n\nOverall, the addressed problem is relevant (the question being how can you efficiently replay experience when the goal is dynamical?), the idea is original and the approach looks sound, but seems to suffer from a fundamental flaw (see below).\n\nDespite some merits, the paper mainly suffers from the fact that the implementation of the approach described above is not explained clearly at all.\nAmong other things, after reading the paper twice, it is still unclear to me:\n- how the agent learns of the goal motion (what substrate for such learning, what architecture, how many repetitions of the goal trajectory, how accurate is the learned model...)\n- how the output of this model is taken as input to infer the desired values of the goal in the future: shall the agent address the goal at the next time step or later in time, how does it search in practice in its replay buffer, etc.\n\nThese unclarities are partly due to unsufficient structuring of the \"methodology\" section of the paper, but also to unsufficient mastery of scientific english. At many points it is not easy to get what the authors mean, and the paper would definitely benefit from the help of an experienced scientific writer.\n\nNote that Figure 1 helps getting the overall idea, but another Figure showing an architecture diagram with the main model variables would help further.\n\nIn Figures 3a and 5, we can see that performance decreases. The explanation of the authors just before 4.3.1 seem to imply that there is a fundamental flaw in the algorithm, as this may happen with any other experiment. This is an important weakness of the approach.\n\nTo me, Section 4.5 about transfer to a real robot does not bring much, as the authors did nothing specific to favor this transfer. They just tried and it happens that it works, but I would like to see a discussion why it works, or that the authors show me with an ablation study that if they change something in their approach, it does not work any more.\n\nIn Section 4.6, the fact that DHER can outperform HER+ is weird: how can a learn model do better that a model given by hand, unless that given model is wrong? This needs further investigation and discussion.\n\nIn more details, a few further remarks:\n\nIn related work, twice: you should not replace an accurate enumeration of papers with \"and so on\".\n\np3: In contrary, => By contrast, \n\nwhich is the same to => same as\n\ncompare the above with the static goals => please rephrase\n\nIn Algorithm 1, line 26: this is not the algorithm A that you optimize, this is its critic network.\n\nline 15: you search for a trajectory that matches the desired goal. Do you take the first that matches? Do you take all that match, and select the \"best\" one? If yes, what is the criterion for being the best?\n\np5: we find such two failed => two such failed\n\nthat borrows from the Ej => please rephrase\n\nwe assign certain rules to the goals so that they accordingly move => very unclear. What rules? Specified how? Please give a formal description.\n\nFor defining the reward, you use s_{t+1} and g_{t+1}, why not s_t and g_t?\n\np6: the same cell as the food at a certain time step. Which time step? How do you choose?\n\nThe caption of Fig. 6 needs to be improved to be contratsed with Fig. 7.\n\np8: the performance of DQN and DHER is closed => close?\n\nDHER quickly acheive(s)\n\nBecause the law...environment. => This is not a sentence.\n\nMentioning in the appendix a further experiment (dy-sliding) which is not described in the paper is of little use.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}