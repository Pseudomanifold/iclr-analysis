{"title": "Novel approach", "review": "This paper shows that significant speed-up gains can be achieved by using KL-regularization with information asymmetry in sparse-reward settings.  Different from previous works, the policy and default policy are learned simultaneously.  Furthermore, it demonstrates that the default policy can be used to perform transfer learning.\n\nPros:\n\n- Overall the paper is well-written and the organization is easy to follow.  The approach is novel and most relevant works are compared and contrasted.  The intuitions provided nicely complements the concepts and experiments are thorough.\n\nCons:\n\n- The idea of separating policy and default policy seems similar to having high and low level controller (HLC and LLC) in hierarchical control -- where LLC takes proprioceptive observations as input, and HLC handles task specific goals.  In contrast, one advantage of the proposed method in this work is that the training is end-to-end.  Would have liked to see comparison between the proposed method and hierarchical control.\n\n- As mentioned, the proposed method does not offer significant speed-up in dense-reward settings.  Considering that most of the tasks experimented in the paper can leverage dense shaping to achieve speed-up over sparse rewards, it'd be nice to have experiments to show that for some environments the proposed method can out-perform baseline methods even in dense-reward settings.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}