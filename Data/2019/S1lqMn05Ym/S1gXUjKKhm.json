{"title": "Review", "review": "This is a very interesting piece of work. We know from cognitive science literature, that there are 2 distinct modes of decision making - habit based and top-down control (goal directed) decision making. The paper proposes to use this intuition by using information theoretic objective such that the agent follows \"default\" policy on average and agent gets penalized for changing its \"default\" behaviour, and the idea is to minimize this cost on average across states.\n\nThe paper is very well written. I think, this paper would have good impact in coming up with new learning algorithms which are inspired from cognitive science literature as well as mathematically grounded. But I dont think, paper in its current form is suitable for publication. \n\nThere are several reasons, but most important:\n\n1) Most of the experiments in this paper use of the order of  10^9 or even 10^10 steps. Its practically not possible for anyone in academia to have such a compute. Now, that said, I do think this paper is pretty interesting. Hence, Is it possible to construct a toy problem which has similar characteristics, and then show similar results using like 10^6 or 10^7 steps ? I think it would be easy to construct a 2D POMPD maze navigation env and test similar results. This would improve the paper, as well as could provide a baseline which people in the future can compare to.\n\n2) It becomes more important to compare to stronger baselines like maximum entropy RL ( for ex. Soft Actor Critic). And spend some good of amount time getting these baselines right on these new environments. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}