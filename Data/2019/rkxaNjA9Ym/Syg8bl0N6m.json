{"title": "the introduction of criteria 2-5 are kind of heuristic and lack of clarity", "review": "This paper proposes a FX (fixed point) framework to calculate the reduced bit numbers, which can (I) use float numbers with the reduced bits to represent each NN layer's weight values W_l, activation values A_l, gradients of weights G_l^W, gradients of nodes G_{l+1}^A, and the cumulated (updated) weight W_l^(acc); (II) with the reduced representations, the training loss and testing loss will not be sacrificed much comparing with the original FL framework, where each float number is 32-bit.\n\nSome positive points:\n(a) The proposed FX framework can reduce cost for both inference and training.\n(b) The experimental results looks promising.\n(c) The Criteria 1-5 seems systematic and the conditions in Claim 1 can be used to calculate the required bit numbers. The author proposed an implementation of Claim 1.\n\nSome negative points / questions:\n\n(a) The most important part of the paper is Criteria 1-5. Criteria 1 generalizes the idea from Sakr et al. 2017, to force the contributions of weight and activation almost at the same order, which seems reasonable to make the mismatch budget p_m smallest. But the other criteria (with their corresponding notions, e.g., Criterion 2 and the concept of clipping rate \\beta) are introduced in a way which is not clear enough and make the audience confused. For example, why is clipping rate \\beta and relative quantization bias \\eta is needed here and what is their relationship with the usual weight gradient clip norm (5% target \\beta and 1% \\eta target correspond to what order of clip norm)? Criteria 4 & 5 are introduced in the same way with one sentence explained like heuristics. They seem to me are introduced just for reducing corresponding bit numbers. More motivation and explanation of introducing these criteria and notions are needed.\n\n(b) For W_l and A_l, the necessary bit numbers are calculated using Criteria 1 & EFQN condition. But for gradients, their PDRs and Deltas are calculated using other criteria & conditions. Then how to calculate their bit numbers from PDRs and Deltas?\n\n(c) The proof of Lemma 2 & 3 directly used CLT for mini-batch average gradient items. CLT is for asymptotic case and in finite sample case it is not true. So it is heuristic calculation rather than lemma with proof (If seeking proof then some finite-sample argument like Berry-Esseen theorem is needed to quantify the probability of the average is not Gaussian). And what is the mini-batch size used here? If it is too small then probably the error of taking the mini-batch SGD as Gaussian will be large.\n\n(d) The importance/minimality of each individual bit number in C_o is not investigated, and the claim of the near minimality of C_o cannot hold according to the current experiments. The experiments of C_{+1} and C_{-1} do not exclude the possibility that some items (not the whole C_o) are minimal. More experiments (changing one or more items while fixing the others) are needed to show every item in C_o is minimal (or not). And which one of them is the most important for training/testing (how sensitive the training/testing performance is to each bit number)?\u0010 Also in C_{-1} and C_{+1}, are target \\beta, \\eta changed? What are these changed values?\n\n(e) From the Claim 1, it seems that these bit numbers are sufficient, and smaller than necessary to get the same training/testing performance (e.g., from the proof of Lemma 3, with the result \\eta = 0.4% < 1%). So one question is what kind of \\beta and \\eta target are necessary for preserving the performance and what corresponding bit numbers are necessary to achieve the necessary \\beta and \\eta values?\n\n(f) The computational cost definition looks different with Eq (3) in Sakr et al. 2017. Why?\n\nSome typos:\n1.Figure 2(a), B_{A_l} is 8 for Layer 1, but 9 in Appendix Table.\n2. In the last third paragraph of Page 7, is it 2.6 = (148/56.5) instead of 2.6 * (148/56.5)? Same for the following numbers.\n\n========================================\nRevision: I have read the reply from the authors and it clarified several matters. I adjusted my rating of this paper (from 6 to 7).", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}