{"title": "Paper needs to be re-written", "review": "This papers introduces a quantization scheme for the back-propagation algorithm to reduce the bit size in the target neural networks. While the paper introduces one way to bring the quantization inside the training procedure and shows the tradeoff between number of bits and the accuracy, the paper is poorly written so it is hard to understand the paper's main proposal.\nSo I would recommend to re-organize the paper and introduce one toy example to illustrate how the proposed method works in the training time and the inference time.\nCurrently the important part, the overall architecture, is explained in the appendix, not in the main paper.\nThe main idea is rather simple, to introduce a quantizer in various components in the back-propagation algorithm.\nI think we need a clear explanation on \"how to\" quantize each tensor in each quantizer, instead of many obscure terms in the section 2 and 3. Also the important numbers are in the appendix C, but their meanings are hard to understand.\n\nAlso in general, quantization is one way of reducing training and inference computational complexity. There are other ways of achieving the same purpose such as distillation to a smaller network (less parameters), etc, so in order to argue the computational gains over this obvious approach, we need a training time and inference time benchmark. \n", "rating": "3: Clear rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}