{"title": "A good idea, but suffers from lack of clarity", "review": "The paper suggests a method for generating representations that are linked to goals in reinforcement learning. More precisely, it wishes to learn a representation so that two states are similar if the policies leading to them are similar.\n\nThe paper leaves quite a few details unclear. For example, why is this particular metric used to link the feature representation to policy similarity? How is the data collected to obtain the goal-directed policies in the first place? How are the different methods evaluated vis-a-vis data collection?  The current discussion makes me think that the evaluation methodology may be biased. Many unbiased experiment designs are possible. Here are a few:\n\nA. Pre-training with the same data\n\n1. Generate data D from the environment (using an arbitrary policy).\n2. Use D to estimate a model/goal-directed policies and consequenttly features F. \n3. Use the same data D to estimate features F' using some other method.\n4. Use the same online-RL algorithm on the environment and only changing features F, F'.\n\nB. Online training\n\n1. At step t, take action $a_t$, observe $s_{t+1}$, $r_{t+1}$\n2. Update model $m$ (or simply store the data points)\n3. Use the model to get an estimate of the features \n\nIt is probably time consuming to do B at each step t, but I can imagine the authors being able to do it all with stochastic value iteration. \n\nAll in all, I am uncertain that the evaluation is fair.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}