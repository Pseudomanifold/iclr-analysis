{"title": "Interesting idea, but needs a stronger experimental justification", "review": "The proposed approach aims to mitigate catastrophic forgetting in continual learning (CL) problems by structure learning: determining whether to reuse or adapt existing parameters, or initialise new ones, when faced with a new task. This is framed as an architecture search problem, applying ideas from Differentiable Architecture Search (DARTS). The approach is verified on the Permuted MNIST dataset and evaluated on the Visual Decathlon, showing an improvement.\n\nI think this is an interesting idea with potential, and is worth exploring, and the paper is well-structured and easy to follow.\n\nUnfortunately, I feel the paper fails to consider recent work on CL, both in terms of discussion and benchmarking. The only previous work that is compared is EWC, on permuted MNIST, and the Visual Decathlon performance is only compared to simple baselines (such as adding an adapter or fine tuning) which makes it difficult to gauge the contribution.\nThere are recent works, some with better results on more difficult problems, such as Variational Continual Learning [1], Progress and Compress [2], or (Variational) Generative Experience Replay [3][4].\nGiven the approach is based on dynamically adding parameters or modules, Progressive Networks and Dynamically Expandable Networks (both cited) are especially relevant and should be compared (I believe the former may be related to the \u201cadapter\u201d baseline, but this should be made explicit).\n\nI have some questions / discussion points:\n- What's the intuition behind implementing the \u201cadapt\u201d operator as additive bias over the previous weights, rather than just copying the previous weights and fine tuning?\n- In the general case, if the architecture search is a continuous relaxation (softmax combination of operators), why is the \"adapt\" operator necessary? Wouldn't this already be a linear combination of new and old parameters? (In the example case of a 1\u00d71 adaptor it makes sense, but this is a special restricted case which adapts with a smaller set of parameters)\n- How is the structure regulariser backpropagated into the parameters of each layer? As I understand, it is composed of a constant discrete term z (number of parameters in each option), multiplied by architecture softmaxes alpha; the gradient with respect to each alpha is a constant, and so this has the effect of scaling the gradients of each operator.\n- For the \"reuse - tuned\" case, isn\u2019t the model effectively maintaining a new network for each task?\n\nI also have a number of other comments:\n- Reference to figure in page 6 should be figure 4, not 5.\n- I think the readability of the paper would benefit from another few proofreads; there are a number of grammatical issues throughout, and several sentence fragments, eg. in the top para of page 2: \u201c..., it has the potential to encourage information sharing. Since now the irrelevant part can be handled\u2026\u201d.\n\nI would encourage the authors to strengthen the experimental comparison by incorporating stronger, external baselines, and improving some of the minor writing issues.\n\n[1] Nguyen, Cuong V., et al. \"Variational Continual Learning.\" ICLR, 2018.\n[2] Schwarz, Jonathan, et al. \"Progress & Compress: A scalable framework for continual learning.\" ICML, 2018.\n[3] Shin, Hanul, et al. \"Continual learning with deep generative replay.\" NIPS, 2017.\n[4] Farquhar, Sebastian, and Yarin Gal. \"Towards Robust Evaluations of Continual Learning.\" arXiv, 2018.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}