{"title": "LEARNING NEURON NON-LINEARITIES WITH KERNEL-BASED DEEP NEURAL NETWORKS", "review": "The paper investigates the problem of designing the activation functions of neural networks with focus on recurrent architectures. The authors frame such problem as learning the activation functions in the space of square integrable functions by adding a regularization term penalizing the differential properties of candidate functions. In particular, the authors observe that this strategy is related to some well-established approaches to select activation functions such as ReLUs. \n\n\nThe paper has some typos and some passages are hard to read/interpret. The write-up needs to be improved significantly.  \n\nWhile some of the observations reported by the authors are interesting it is in general hard to evaluate the contributions of the paper. In particular the discussion of Sec. 2 is very informal, although ti describes the key technical observations used in the paper to devise the model (Sec. 3) that is then evaluated in the experiments (Sec. 4). In particular, it is unclear whether the authors are describing some known results - in which case they should add references - or original contributions - in which case they should report their results with more mathematical rigour. Indeed, in the abstract, the authors state that a representation theorem is given, but in the text they provide only an informal discussion of such result. \n\nOverall, it is hard to agree with the authors' conclusion that \"the KBRN architecture exhibits an ideal computational structure to deal with classic problems of capturing long-term dependencies\": the theoretical discussion does not provide sufficient evidence in this sense.\n\nSome minor points: \n\nConfusing notation: why were the alpha^k replaced with the \\chi^k between Sec. 2 and Sec. 3?\n\nUnclear motivation for some design choices. For instance 1) the justification given by the authors to neglect the linear terms from both g(x) and k(x) in Sec. 3 is unclear. 2) why was the \\ell_1 norm used as penalty for the regularizer R(\\chi) in Sec.3? One could argue that \\ell_1 is used to encourage sparse solutions, but the authors should explain why sparsity is desirable in this setting. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}