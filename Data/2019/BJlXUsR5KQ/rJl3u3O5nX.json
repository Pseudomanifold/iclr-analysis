{"title": "Unclear presentation and weak results.", "review": "Summary: the authors propose a method for learning activation functions in neural networks using kernels. Each activation function is modeled as a weighted set of kernels, where (as I understand it) the weights are learned simultaneously with the linear weights in the network. The authors apply their method to learn the XOR function and to a simple sequence memory task.\n\nMy main concern with the paper is that the presentation is very hard to follow. The motivation, background, and contributions of this paper are all mixed in the abstract and introduction, making it hard to understand what is the current state of learned activations, what this paper introduces, and how the work in this paper relates to prior work. Instead, I found the presentation in Scardapane et al much clearer (that paper has a clear separation of background, related work, and their contributions). By contrast, this paper has one large paragraph in the introduction that muddles together multiple threads of thought, making it hard to digest. Before the reader has had time to digest the main ideas, the paper launches into a highly technical description of the method, without a clear high level explanation of what the main technique is. A lot of the mathematical notation introduced in Sec2 is not clearly motivated.\n\nMy other main concern is with the results. The paper solves two simple tasks with their method, and it is unclear what their kernel methods really buy them. For the XOR problem, it is such a simple task that it is hard to judge the method (it seems like complex activation functions are not required to solve the task, and it is not clear what including them gets you). For the sequence memory task, it seems unfair to compare their results to recurrent networks with a *single* hidden unit. If you have networks with 2 or 4 or 8 hidden units, do they solve the task? These experiments do not shed much light on the advantages of using kernel activations in recurrent networks.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}