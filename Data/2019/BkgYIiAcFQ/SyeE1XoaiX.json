{"title": "motivation and experiment are not convincing enough", "review": "This paper provide a modification on the classical LSTM structure. Specifically, it reformulate the forget gate with a monotonically decreasing manner, using sinusoidal function as the activation function. \n\nHowever, both the motivation and experimental results on such modification are not convincing enough. \n\n1. While there are many heuristic guesses in sec3, important supports of these guesses are missed. For example, Figure 2 is designed to provide supports for the claim that we need controlled forget gates.  However, all the values of forget gates and input gates in Figure 2 are manually set as *conceptual observations*, which provides limited insight on what will happen in the real cases. While the reformulation in sec4 is based on the observations in Figure 2, it is important to plot the real cell propagation after the reformulation, and see whether the real observation meets the conceptual observations in Figure 2.\nBTW, Plots in Figure 2 only account for LSTMs' propagation within 3 steps, but in real cases there are way more steps. \n\n2. The authors claim monotonic propagation in the constant forget gates is more interpretable than those of the vanilla-LSTM, as no abrupt shrinkage and sudden growth are observed. But it isn't straightforward to get the relations between abrupt shrinkage and sudden growth on forget gates and the expressive power of the vanilla-LSTM. Also, it's hard to say the monotonic propagation is more interpretable because we don't know what's the meaning of such propagation on the behaviors of LSTMs in applications. \n\n3. The reformulation in sec 4, especially the formula for the forget-polar input p_k, looks heavily hand-crafted, without experimental supports but statements such as \"we ran numerous simulations\", which is not convincing enough. \n\n4. Experiments are applied on MNIST and Fashion-MNIST. While both datasets are not designed in nature for sequential models like LSTMs. There are better datasets and tasks for testing the proposed reformulation.   e.g. sentence classification, text generation, etc.  No explanation on the choice of datasets.  In addition, the difference between vanilla-LSTM and DecayNet-LSTM is small and it's hard to say it isn't marginal. Maybe larger-scale datasets are needed. \n\n5. Lacking of explanation on specific experimental settings. E.g. training all methods for *only one epoch*, which is very different from the standard practice.  \n\n6. More qualitative interpretations for real cell states in both vanilla LSTM  and DecayNet-LSTM are needed. Only conceptual demonstration is included in Figure 2. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}