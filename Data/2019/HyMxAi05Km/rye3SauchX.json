{"title": "Overlooked assumption?", "review": "This paper provides a theoretical perspective of the dual learning tasks and proposes two generalizations (multipath/cycle dual learning) that utilize multiple language sets. Through experiments, the paper discusses the relationship between theoretical perspective and actual translation quality.\n\nOverall, the paper is well written and discussed enough. My concern is about Theorem 1 that could be a critical problem.\nIn the proof of Theorem 1, it discussed that the dual learning can minimize Case 2. This assumption is reasonable if the vanilla translator is completely fixed (i.e., no longer updated) but this condition may not be assumed by the authors as far as I looked at Algorithm 2 and 3 that update the parameters of vanilla translators directly. The proof is constructed by only the effect against Case 2. However, if the vanilla translator is directly updated through dual training, there should exist some random changes in also Case 1 and this behavior should also be included in the theorem.\n\nCorrection and suggestions writing:\n* It is better to introduce an additional $\\alpha$, $\\beta$ and $\\gamma$ for the vanilla translation accuracy (e.g., $\\alpha_0 := p_{ij}p_{ji}^r$) so that most formulations in Section 3 can be largely simplified.\n* In Justification of Assumption1 ... \"the probability of each cluster is close to $p_max$\" ->  maybe \"greater than $p_max$\" to satisfy the next inequality.\n* Eq. (3) ... $T_{ji}^d(T_{ij}^d(x^{(i)})$ -> $T_{ji}^d(x^{(j)})$ to adjust other equations.\n* Section 3 Sentence 1: \"translatorby\" -> \"translator by\"\n* Section 4.2: ${\\rm Pr}_{ X^{(3)} \\sim T_{23}^d (X^{(1)}) }$ -> $... (X^{(2)}) }$", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}