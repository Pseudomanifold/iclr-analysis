{"title": "Official Review", "review": "Summary:\nThe paper proposes LEMONADE, an evolutionary-based algorithm the searches for neural network architectures under multiple constraints. I will say it first that experiments in the paper only actually address to constraints, namely: log(#params) and (accuracy on CIFAR-10), and the method as currently presented does not show possible generalization beyond these two objectives, which is a weakness of the paper.\n\nAnyhow, for the sake of summary, let\u2019s say the method can actually address multiple, i.e. more than 2, objectives. The method works as follows.\n\n1. Start with an architecture.\n\n2. Apply network morphisms, i.e. operators that change a network\u2019s architecture but also select some weights that do not strongly alter the function that the network represents. Which operations to apply are sampled according to log(#params). Details are in the paper.\n\n3. From those sampled networks, the good ones are kept, and the evolutionary process is repeated.\n\nThe authors propose to use operations such as \u201cNet2WiderNet\u201d and \u201cNet2DeeperNet\u201d from Chen et al (2015), which enlarge the network but also choose a set of appropriate weights that do not alter the function represented by the network. The authors also propose operations that reduce the network\u2019s size, whilst only slightly change the function that the network represented.\n\nExperiments in the paper show that LEMONADE finds architecture that are Pareto-optimal compared to existing model. While this seems like a big claim, in the context of this paper, this claim means that the networks found by LEMONADE are not both slower and more wrong than existing networks, hand-crafted or automatically designed.\n\nStrengths:\n1. The method solves a real and important problem: efficiently search for neural networks that satisfy multiple properties.\n\n2. Pareto optimality is a good indicator of whether a proposed algorithm works on this domain, and the experiments in the paper demonstrate that this is the case.\n\nWeaknesses:\n1. How would LEMONADE handle situations when there are more than one $f_{cheap}$, especially when different $f_{cheap}$ may have different value ranges? Eqn (8) and Eqn (9) does not seem to handle these cases.\n\n2. Same question with $f_{exp}$. In the paper the only $f_{exp}$ refers to the networks\u2019 accuracy on CIFAR-10. What happens if there are multiple objectives, such as (accuracy on CIFAR-10, accuracy on ImageNet) or (accuracy on CIFAR-10, accuracy on Flowers, image segmentation on VOC), etc.\n\nI thus think the \u201cMulti-Objective\u201d is a bit overclaimed, and I strongly recommend that the authors adjust their claim to be more specific to what their method is doing.\n\n3. What value of $\\epsilon$ in Eqn (1) is used? Frankly, I think that if the authors train their newly generated children networks using some gradient descent methods (SGD, Momentum, Adam, etc.), then how can they guarantee the \\epsilon-ANM condition? Can you clarify and/or change the presentation regarding to this part?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}