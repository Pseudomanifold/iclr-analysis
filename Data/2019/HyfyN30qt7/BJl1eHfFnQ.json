{"title": "Network quantization on ResNets", "review": "The article presents a method for quantization of deep neural networks for classification and regression, using three key parts: (i) noise injection to model the effect of quantization during forward inference, (ii) clamping with learned maximum activations to reduce the quantization bin size, and (iii) gradual quantization of blocks of the network, while previously quantized blocks remain unchanged. The method is evaluated on ImageNet, CIFAR-10, and a regression task, showing performance on-par or better than state-of-the-art methods for particular quantization bit size. Finally, the method is used for porting network onto a FPGA.\n\nThe paper addresses an important topic, because there is increasing interest in hardware-efficient implementations of deep neural networks. The method could be interesting for practitioners, because it does not interfere with the original training of the full-precision method, and can be applied later on.\n\nThe main weakness is that none of the proposed methods are entirely original, and the combination is rather ad-hoc than well-justified. For example, quantization noise has been considered in several previous articles, e.g. already in BinaryConnect (Courbariaux et al. 2015), although the novelty here is that the noise is explicitly added during the forward path. However, the choice of the Bernoulli mask with p=0.05 is not justified and might not work best for other tasks. The authors admit that gradual quantization has been proposed before, and clamping a ReLU is also not new, although here a new way to learn and initialize the clamping parameters is presented.\n\nThe article would be OK if the empirical results were really strong, but unfortunately they are not entirely convincing:\n1. The classification results are only for ResNet architectures, it remains unclear whether results would hold also for other architectures.\n2. The numerical results in Table 2 are very close to each other, and no error bars are available, so it is not possible to judge whether differences are significant. Also, the advantage of the NICE method vanishes for 3-bit models.\n3. The results for CIFAR-10 come without any comparison.\n4. The results for regression are only compared to a single method, which is re-implemented by the authors, and might therefore not be fully optimized. Thus there is no strong baseline to judge the results.\n5. No results are shown for the hardware implementation.\n\nOverall, the paper is not a particularly interesting read for people interested in a deeper understanding of network quantization, but the method could still be valuable for applications. Is this sufficient for ICLR? Since the experimental results do not entirely convince me I will put my grade slightly below acceptance threshold.\n\nMinor points:\n- The abstract has a pretty long introduction before it begins to tell what the contributions of the article are.\n- Occasional grammar mistakes.\n- Tables 1 and 2 are misplaced\n\nPros:\n+ important topic (network quantization)\n+ good empirical results\n+ easy to apply\n\nCons:\n- combination of previously proposed methods\n- no convincing justification \n- no strong advantage over previous methods", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}