{"title": "Low novelty and weak experiments.", "review": "[Summary]\nNeural network quantization is can enable many practical applications for deep learning, therefore it is an important research problem. The paper claims two contributions: 1. Injecting noise during training to make it more robust to quantization errors. 2. Clamping the parameter values in a layer as well as the activation output, where the clamping interval is some multiple of the standard deviation about the mean, and the clamping interval is updated using the Straight through estimator. The main strength of the paper lies in the empirical results where the combination of techniques employed by the authors outperforms the SOTA methods in a compute of scenarios.\n\n[Pros]\nThe paper is working on an important problem area and as a technical report this work can be valuable in the industry. There is novelty in the particular combination of techniques that the authors have employed and some of the empirical results show the strength of the technique.\n\n[Cons]\nthe main contribution of the paper is a careful combination of existing techniques and the associated empirical results, therefore the experiments need to be strong. I noticed some strange omissions in the  results, and asked the authors for a reply via a public comment but they did not reply. Specifically,\n\na) On RESNET 34 the results for PACT 5,5 are not shown and JOINT and PACT on 3,3 on ResNet-34 are also not shown. Why are these results omitted?\n\nb) The noise+gradual training decreases performance on (layer-weight bitwidth, activation bitwidth)  =  (3, 3). But further experiments for table 2 where the nets do not use noise+gradual training is not shown. Currently the proposed recipe for quantizing nets does not seem to be all that better than existing methods and it hard to guess exactly what was the reason for the improved results in situations where the results were infact better. Why were these experiments omitted ?\n\nOverall the experimental results in the paper are weak and the novelty of the proposed methods is low.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}