{"title": "Interesting paper", "review": "This paper proposes a graph to sequence transducer consisting of a graph encoder and a RNN with attention decoder. \n\nStrengths:\n- Novel architecture for graph to sequence learning.\n- Improved performance on synthetic transduction tasks and graph to text generation. \nWeaknesses:\n- Experiments could provide more insight into model architecture design and the strengths and weaknesses of the model on non-synthetic data. \n\nTransduction with structured inputs such as graphs is still an under-explored area, so this paper makes a valuable contribution in that direction. Previous work has mostly focused on learning graph embeddings producing outputs. This paper extends the encoder proposed by Hamilton et al (2017a) by modelling edge direction through learning \u201cforward\u201d and \u201cbackward\u201d representations of nodes. Node embeddings are pooled to a form a graph embedding to initialize the decoder, which is a standard RNN with attention over the node embeddings. \n\nThe model is relatively similar to the architecture proposed by Bastings et al (2017) that uses a graph convolutional encoder, although the details of the graph node embedding computation differs. Although this model is presented in a more general framework, that model also accounted for edge directionality (as well as edge labels, which this model do not support). \n\nThis paper does compare the proposed model with graph convolutional networks (GCNs) as encoder experimentally, finding that the proposed approach performs better on shortest directed path tasks. However the paper could make difference between these architectures clearer, and provide more insight into whether different graph encoder architectures might be more suited to graphs with different structural properties. \n\nThe model obtains strong performance on the somewhat artificial bAbI and Shortest path tasks, while the strongest result is probably that of strong improvement over the baselines in SQL to text generation. However, very little insight is provided into this result. It would be interesting to apply this model to established NLG tasks such as AMR to text generation.  \n\nOverall, this is an interesting paper, and I\u2019d be fine with it being accepted. However, the modelling contribution is relatively limited and it feels like for this to be a really strong contribution more insight into the graph encoder design, or more applications to real tasks and insight into the model\u2019s performance on these tasks is required. \n\nEditing notes:\nHamilton et al 2017a and 2017c is the same paper. \nIn some cases the citation format is used incorrectly: when the citation form part of the sentence, the citation should be inline. E.g. (p3) introduced by (Bruna et al., 2013)  -> introduced by Bruna et al. (2013). \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}