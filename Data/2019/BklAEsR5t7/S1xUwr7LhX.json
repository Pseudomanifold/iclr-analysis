{"title": "weak contribution and experiments", "review": "This paper tackles the problem of estimating pairwise potentials when the number of labels is large. Two modifications are proposed: one is to factorize the matrix for pairwise potentials, and the other is to approximate the log likelihood objective with the MEMM objective.\n\nThe problem and the proposed approach are well motivated. It is particularly useful to draw the connections between MEMM and piecewise-pseudolikelihood.\n\nThe major weakness of the paper is whether the approximations are necessary. It is hard to see why approximating the log likelihood with MEMM is necessary, because inference and computing the gradients of the log likelihood have the same computational complexity. So the authors could have trained the model with the log likelihood.\n\nRegardless, it is still valuable to compare MEMM and log likelihood for training CRFs. However, the authors fail to show how well MEMM approximates the log likelihood. For example, the authors can compare the solutions when optimizing with the gradients of log likelihood and the with the gradients of MEMM. It is especially important to compute the training log likelihood for the two solutions, as it tells us how well MEMM approximates the log likelihood. This is also true for the low-rank approximation of the pairwise potentials. The authors fail to compare the case with low-rank approximation and the case without. It is important to evaluate the training error first with both methods as they share the same objective. This type of comparison should be apply to batch normalization as well.\n\nApproximating the pairwise potentials with matrix factorization is also not novel.  See the list below. (The list is by no means exhaustive. Please see the citations therein.)\n\nDense and low-rank Gaussian CRFs using deep embeddings\nChandra et al., ICCV 2017\n\nEfficient SDP inference for fully-connected CRFs based on low-rank decomposition\nWang et al., CVPR 2015\n\nNeural CRF parsing\nDurrett and Klein, ACL 2015\n\nFinally, some of the claims made in the paper (listed below) should be more careful.\n\np.4\n\nthe likelihood function, therefore, is log-linear and concave.\n--> concave in what?\n\nthe scoring function is still concave, ...\n--> concave in what?\n\nthe objective function is no longer linear or concave with respect to the network parameters, ...\n--> what are the network parameters?\n\nbut deep learning training techniques have been shown to yield good results ...\n--> this argument is weak. the key is point out that SGD is used, plus SGD has been shown to work well on many matrix factorization problems. see the paper below.\n\nOnline learning for matrix factorization and sparse coding\nMairal et al., JMLR 2010\n\np.5\n\nthe test time inference uses a global normalization ... avoids the label bias problem.\n--> the partition function is not even computed when using Viterbi. I'm also not sure how this avoids the label bias problem.\n\nwhitening the inputs to each layer may also prevent converging into poor local optima.\n--> this is a hand-wavy claim. it would be best if the authors can provide citations to the claim.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}