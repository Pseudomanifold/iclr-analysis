{"title": "Reasonable paper on an interesting topic", "review": "The paper deals with the expressibility of quantized neural network, meaning where all weights come from a finite and small sized set. It proves that functions satisfying standard assumptions can be represented by quantized ReLU networks with certain size bounds, which are comparable to the bounds available in prior literature for general ReLU networks, with an overhead that depends on the level of quantization and on the target error.\n\nThe proofs generally go by simulating non-quantized ReLU networks with quantized ones, by means of replacing their basic operations with small quantized networks (\"sub-networks\") that simulate those same operations with a small error. Then the upper bounds follow from known results on function approximation with (non-quantized) ReLU networks, with the overhead incurred by introducing the sub-networks.\nNotably, this approach means that the topology of the network changes. As such it not compatible with quantizing the weights of a given network structure, which is the more common scenario, but rather with choosing the network structure under a given level of quantization. This issue is discussed directly and clearly in the paper.\n\nOverall, while the paper is technically quite simple, it forms an interesting study and blends well into recent literature on an important topic. It is also well written and clear to follow.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}