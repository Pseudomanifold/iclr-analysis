{"title": "Revisit old image processing idea, add parameters, make differentiable. Show that it can be used to ignore background textures. Extensive experiments on domain adaptation.", "review": "Summary:\nThe paper proposes an unsupervised approach to identify image features that are not meaningful for image classification tasks. The goal is to address the domain adaptation (DA)/domain generalization (DG) issue. The paper introduces a new learning task where the domain identity is unavailable during training, called unguided domain generalization (UDG). The proposed approach is based on an old method of using gray level co-occurence matrix, updated to allow for differentiable training. This new approach is used in two different ways to reduce the effect of background texture in a classification task. The paper introduces a new dataset, and shows extensive and carefully designed experiments using the new data as well as existing domain generalization datasets.\n\nThis paper revisits an old idea from image processing in a new way, and provides an interesting unsupervised method for identifying so called superficial features. The proposed block seems to be very modular in design, and can be plugged into other architectures. The main weakness is that it is a bit unclear exactly what is being assumed as \"background texture\" by the authors.\n\n\nOverall comments:\n- Some more clarity on what you mean by superficial statistics would be good. E.g. by drawing samples. Are you assuming the object is centered? Somehow filling the image?  Different patch statistics? How about a texture classification task?\n- please derive why NGLCM reduces to GLCM in the appendix. Also show the effect of dropping the uniqueness constraint.\n- Section 3.2: I assume you are referring to an autoencoder style architecture here. Please rewrite the first paragraph. The current setup seems to indicate that you doing supervised training, since you have labels y, but then you talk about decoder and encoder.\n- Section 3.2: Please expand upon why you use F_L for training but F_P during testing\n\n\nMinor typos/issues:\n- Last bullet in Section 1: DG not yet defined, only defined in Section 2.\n- page 2, Section 2, para 1: data collection conduct. Please reword.\n- page 2, Section 2, para 2: Sentence: For a machine learning ... There is no object in this sentence. Not sure what you are trying to define.\n- page 2, Section 2, para 2: Is $\\mathcal{S}$ and $\\mathcal{T}$ not intersecting?\n- page 2, Section 2.1: Heckman (1977), use \\citep\n- page 2, Section 2.1: Manski, citep and missing year\n- page 3, Section 2.1: Kumagai, use citet\n- page 3, Section 3.1: We first expand ... --> We first flatten A into a row vector\n- page 4, Section 3.1: b is undefined. I assume you mean d?\n- page 4, Section 3.1: twice: contrain --> constraint\n- page 4, Section 3.2: <X,y> --> {X,y} as used in Section 3.1.\n- page 4, Section 3.2, just below equation: as is introduced in the previous section. New sentence about MLP please. And MLP not defined.\n- page 4, Section 3.2, next paragraph: missing left bracket (\n- page 4, Section 3.2: inferred from its context.\n- page 5, Section 4: popular DG method (DANN)\n- page 7: the rest one into --> the remaining one into\n- page 8: rewrite: when the empirical performance interestingly preserves.\n- page 8, last sentence: GD --> DG\n- A2.2: can bare with. --> can deal with.\n- A2.2: linear algebra and Kailath Variant. Unsure what you are trying to say.\n- A2.2: sensitive to noises --> sensitive to noise.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}