{"title": "Interesting paper, but does it really need RL?", "review": "This paper focuses on the alignment of different Knowledge Graphs (KGs) obtained from multiple sources and languages - this task is similar to the Link Prediction setting, but the objective is learning a mapping from the entities (or triples) in one Knowledge Graph to another. In particular, the paper focuses on the setting where the number of available training alignments is small.\n\nThe model and training processes are slightly convoluted:\n- Given a triple in the KG A, the model samples a candidate aligned triple from a KG B, from a (learned) alignment distribution.\n- The objective is a GAN loss where the discriminator needs to distinguish between real and generated alignments.\nThe objective is non-differentiable (due to the sampling step), and it's thus trained via policy gradients.\n\nQuestion: to me it looks like the whole process could be significantly simpler, and end-to-end differentiable. For instance, the loss may be the discrepancy between the alignment distribution and the training alignments. As a consequence, the whole procedure would be significantly more stable; there would be no need of sampling; or tricks for reducing the variance of the gradient estimates. What would happen with such a model? Would it be on par with the proposed one?\n\nThe final model seems to be better than the considered baselines.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}