{"title": "The biggest strength of the paper is the novelty of the proposed method but we are not happy with the experimental evaluation. ", "review": "The authors propose KAGAN, a novel method for knowledge graph (KG) alignments using GANs. In contrast to most other methods, KAGAN does not rely on a supervised setting where a set of already aligned triples is used as seed. In addition, the authors propose modifications such that their method can also integrate information about aligned triples.\n\nIn my opinion the biggest strength of the paper is the novelty of the proposed method. The standard framework of GANs is adjusted elegantly to the KG alignment setting. Further, the basic approach as well as the proposed modification to deal with practical issues such as mode collapse are well motivated and comprehensible. The ability to perform well in the unsupervised/weakly supervised setting is another plus. Up to my knowledge, this paper constitutes the first approach using GANs for the KG alignment task. However, there are several methods that use GANs directly to produce KG embeddings (e.g. KBGAN). These methods are related to KAGAN since they employ TransE as an underlying embedding method. Therefore, I strongly believe the authors should include [1] and [2] in their references and discuss these methods briefly. \n\nThe authors conduct experiments on benchmark datasets for the KG alignment tasks. Thereby, they aim to show that their KAGAN outperforms other state of the art methods. However, I have one major concern: In the subsection 'Parameter Settings' the authors state without further explanation that they set the embedding size to 512 for all compared methods. I do not understand the rationale behind this decision. In particular, since the embedding sizes in the original publications (e.g., in Chen et al. (2017a)) are very different. For KG embedding methods the dimensionality of the embeddings is probably the most important hyperparameter. In my opinion, picking a global value for all methods instead of tuning it for each method individually does not guarantee a fair comparison. \n\nOn a side note, the authors might also consider the mean rank as performance measure which is frequently employed in the KG alignment setting.   \n\nOverall, the paper is well written and organized. A minor point: I would get rid of the formulas in the introduction; in particular, since the notation is not introduced at this point.\n\nWhile the novelty and good structure of the paper are reasons to accept it, I have doubts concerning the soundness of the results due to the experimental setup. \n\nReasons to accept the paper\n- novelty\n- works in an unsupervised setting\n- well written/structured\n\nReasons to reject\n-  Doubts concerning the experimental setting\n- (Minor) related work is not complete\n- (Minor) not all common performance measures are reported\n\n\n----------------------------\n\n[1] Wang, Peifeng, Shuangyin Li, and Rong Pan. \"Incorporating GAN for Negative Sampling in Knowledge Representation Learning.\" (2018).\n[2] Cai, Liwei, and William Yang Wang. \"KBGAN: Adversarial Learning for Knowledge Graph Embeddings.\" Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Vol. 1. 2018.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}