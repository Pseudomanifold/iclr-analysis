{"title": "Need experiments on more challenging tasks", "review": "The paper proposes learning Restricted Boltzmann Machines for solving small computational tasks (e.g., 1-bit addition) and composing those RBMs to form a more complex computational module (e.g., 16-bit addition). The claim is that such an approach can be more data efficient than learning a single network to directly learn the more complex module. Results are shown for addition and factoring tasks.\n\n- The paper is somewhat easy to follow and the figures are helpful. But the overall organization and flow of ideas can be improved significantly.\n- The term \"combinatorial optimization\" is used in a confusing way -- addition would not usually be called a combinatorial optimization problem.\n- It would be good to understand what benefit does the stochasticity of RBMs provide. How do deterministic neural networks perform on the addition and factoring tasks? The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across as arbitrary.\n- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.  After all, the former approach gets a lot more knowledge about the target function built into it. It's good that the paper empirically confirms the intuition, but doesn't feel like a significant contribution on its own.\n- The paper would be stronger if it includes more complex tasks, e.g., TSP, and show that the same ideas can be applied to improve the learning a solver for such tasks. The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}