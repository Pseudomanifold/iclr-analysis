{"title": "Good cocktail of ideas, limited analysis", "review": "This work introduces a framework for learning implicit models that is robust to mode collapse. It consists in learning an explicit model of the implicit model through maximum likelihood while the later is used to teach the explicit model to better match the data distribution. The resulting bi-level optimization is carried out with truncated unrolled stochastic gradient descent.\n\n# Quality\n\nThe method combines an interesting set of ideas. It is validated on some reasonable experiments. \n\nHowever after reading the paper, I remain with too many unanswered questions:\n- Why should the method avoid mode collapse? Experiments clearly show that it indeed is resilient to mode collapse, but I have would have been curious in seeing some more discussion regarding this point. What is the exact mechanism that solves the issue?\n- What is the effect of K? Is mode collapse solved only because of the unrolled gradients?\n- What is the effect of M? How does the method behave for M=1, as usually done in GANs?\n- What if the explicit model has not enough capacity?\n- The original Unrolled GAN paper presents better results for the ring problem. Why are results worse in the experiments?\n\nMore fundamentally what is the main benefit of this approach with respect to models that can be trained straight with maximum likelihood? (e.g., flow-based neural generative models; and as required for the explicit model) Is it only to produce generative models that are fast (because they are implicit)? Why not training only the explicit model directly on the data?\n\n# Clarity\n\nThe paper is in general well-written, although some elements could be removed to actually help with the presentation.\n- The development around influence functions could be removed, as the method ends up instead making use of truncated unrolled gradients.\n- The theoretical analysis is straightforward and could be compressed in a single paragraph to motivate the method.\n\n# Originality\n\nThe method makes use of several ideas that have been floating around and proposed in different papers. As far as I know, the combination proposed in this work is original.\n\n# Significance\n\nResults show clear resistance to mode collapse, which is an improvement for implicit models. However, other types of generative models generally do not suffer from this issue. Significance is therefore limited.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}