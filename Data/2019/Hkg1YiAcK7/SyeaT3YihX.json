{"title": "Well-written,  interesting experiments, doubts about scalability", "review": "The authors present a novel architecture of  an implicit unsupervised learning architectures using\na teacher student approach.  In particular the main advantage to me seems to be the mode-collapse property,  an important drawback in standard\nGAN approaches.\n\nThe paper is written very well and is easy to follow. The methodology is presented in a clear way and the experiments make sense given the research question.  I particular like that the authors define clear metrics to evaluate success, which is often the weak point in unsupervised learning problems. \n\nI believe the work is interesting, but the results still preliminary and  possibly limited by  scalability.  As the authors put it \n\n\"The main bottleneck of LBT is how to efficiently solve the bi-level optimization problem. On one\nhand, each update of LBT could be slower than that of the existing methods because the computational\ncost of the unrolling technique grows linearly with respect to the unrolling steps.\"\n\nOn the other hand, I appreciate the honesty in discussing possible scalability constraints.\n\nI was a bit surprised that the method the authors propose seems to work well in the  \"Intra-mode KL divergence\".  My expectation was  that the main advantage of your method is capturing the global, holistic shape of the distribution\nof the data, whereas classical methods would, because of mode collapse, only capture specific  sub-spaces.  Therefore, i would expect these classical methods to perform better in intra-mode KL divergence,  which is a metric to measure local\n, not global, approximation quality.\n\nTypos: \n-  In practise (Introduction) -> in practice\n- 3.1 accent -> ascend\n- Conclusion: on one hand / other hand is used for two opposite ways of thinking", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}