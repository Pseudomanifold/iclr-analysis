{"title": "Interesting empirical analysis", "review": "This is an experimental paper that investigates how spatial ordering of patches influences the classification performances of CNNs. To do so, the authors design CNNs close to ResNets that almost only consist in a simple cascade of 1x1 convolutions, obtaining relatively small receptive field. It is an interesting read, and I recommend it as a valuable contribution to ICLR, that might lead to nice future works.\n\nI have however several comments and questions, that I would like to be addressed.\n\n1) First I think a reference is missing. Indeed, to my knowledge, it is not the first work to use this kind of techniques. Cf [1]. This does not alterate however the novelty of the approach.\n\n2) \u00ab We construct a linear DNN-based BoF \u00bb : I do not like this formulation. Here, you assume that you build a ResNet-50 with 1x1 as a representation and have a last final linear layer as a classifier. One could also claim it is a ResNet-48 as a representation followed by 2 layers of 1x1 as a classifier.\n\n3) \u00ab our proposed model architecture is simpler \u00bb this is very subjective because for instance the FV models are learned in a layer-wise fashion, which makes their learning procedure more interpretable because each layer objective is specified. Furthermore, analyzing these models is now equivalent to analyze a cascade of fully connected layers, which is not simple at all.\n\n4) Again, the interpretability mentioned in Sec. 3  is in term of spatial localization, not mapping. I think it is important to make clear this consideration. Indeed, this work basically leaves the problem of understanding general CNNs to the problem of understanding MLPs.\n\n5) The graphic of the Appendix A is a bit misleading : it seems 13 downsampling are performed whereas it is not the case, because the first element of each group of block is actually only done once.(if I understood correctly)\n\n6) I think the word feature is sometimes mis-used: sometimes it seems it can refer to a patch, sometimes to the code for a patch. (\u00ab Surprisingly, feature sizes assmall as 17 \u00d7 17 pixels \u00bb)\n\nI got also few questions:\nQ1 : I was wondering if you did try manifold learning on the patches ? Do you expect it to work ?\nQ2 : Is there a batch normalization in the FC or a normalization? Did you try to threshold the heat maps before feeding them to the linear layer? I'm wondering indeed if the amplitude of those heatmaps is really key.\nQ3 : do you think it would be easy to exploit the non-overlapping patches for a better parallelization of computations ?\n\nFinally, I find very positive the amount of experiments to test the similarity with standard CNNs. Of course, it\u2019s far from being a formal proof, but I think it is a very nice first step.\n\n[1] Oyallon, Edouard, Eugene Belilovsky, and Sergey Zagoruyko. \"Scaling the scattering transform: Deep hybrid networks.\" International Conference on Computer Vision (ICCV). 2017.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}