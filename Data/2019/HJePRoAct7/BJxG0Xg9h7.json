{"title": "interesting problem of pooling/upsampling graphs, experimental validation and literature review could be significantly improved", "review": "This paper proposes pooling and upsampling operations for graph structured data, to be interleaved with graph convolutions, following the spirit of fully convolutional networks for image pixel-wise prediction. Experiments are performed on node classification benchmarks, showing an improvement w.r.t. architectures that do not perform any downsampling/upsampling operations.\n\nGiven that the main contribution of the paper is the introduction of a pooling operation for graph structured data, it might be a good idea to evaluate the operation in a task that does require some kind of downsampling, such as graph classification / regression. Moreover, authors should compare to other graph pooling methods.\n\nAuthors claim that one of the motivations to perform their pooling operation is to increase the receptive field. It would be worth comparing pooling/upsamping to dilated convolutions to see if they have the same effect on the performance when dealing with graphs. \n\nSome choices in the method seem rather arbitrary, such as the tanh non-linearity in \\tilde y. Could the authors elaborate on that? How important is the gating?\n\nIt would be interesting to analyze which nodes where selected by the pooling operators. Are those nodes close together or spread out in the previous graph?\n\nThe proposed unpooling operation seems to be the same as unpooling performed to upsample images, that is using skip connections to track indices, by recovering the position where the max value comes from and setting the rest to 0. Have the authors tried other upsampling strategies analogous to the ones typically used for images (e.g. upsampling with nearest neighbors)?\n\nWhen skipping information from the downsampling path to the upsampling path, is there a concatenation or a summation? How do both operations compare? (note that concatenation introduces many more parameters) How about only skipping only the indices (no summation nor concatenation)? This kind of analysis, as it has been done in the computer vision literature, would be interesting.\n\nWhat is the influence of the first embedding layer to reduce the dimensionality of the features?\n\nHow do the models in Table 2 compare in terms of number of parameters?\n\u2028What's the influence of imposing larger weights on self loop in the graph?\n\nWhat about experiments in inductive settings?\n\nPlease add references for the following claim \"U-Net models with depth 3 or 4 are commonly used...\"\n\nPlease double check your references, e.g. in the introduction, citations used for CNNs do not always correspond to CNN architectures.\n\nThe literature review could be significantly improved, missing relevant papers to discuss include:\n- Gori et al. A new model for learning in graph domains, 2005.\n- Scarselli et al. The graph neural network model, 2009.\n- Bruna et al. Spectral networks and locally connected networks on graphs, 2014.\n- Henaff et al. Deep convolutional networks on graph-structured data, 2015.\n- Niepert et al. Learning convolutional neural networks for graphs, 2016.\n- Atwood and Towsley. Diffusion-convolutional neural networks, 2016.\n- Bronstein et al. Geometric deep learning: going beyond Euclidean data, 2016.\n- Monti et al. Geometric deep learning on graphs and manifolds using mixture model cnns, 2017.\n- Fey et al. SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels, 2017.\n- Gama et al. Convolutional Neural Networks Architectures for Signals Supported on Graphs, 2018.\nAs well as other pixel-wise architecture for image-based tasks such as:\n- Long et al. Fully Convolutional Networks for Semantic Segmentation, 2015.\n- Jegou et al. The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation, 2016.\n- Isola et al. Image-to-image translation with conditional adversarial networks, 2016.\n- Zhao et al. Stacked What-Where auto-encoders, 2015.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}