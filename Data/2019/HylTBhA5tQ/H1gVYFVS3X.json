{"title": "Reviewer's summery: interesting idea/findings but with questions", "review": "In this paper, the authors associated with the generalization gap of robust adversarial training with the distance between the test point and the manifold of training data. A so-called 'blind-spot attack' is proposed to show the weakness of robust adversarial training.  Although the paper contains interesting ideas and empirical results, I have several concerns about the current version. \n\na) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\". Can authors provide more details, e.g., empirical results, about it? What is its rationale?\n\nb) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\ngenerative models like in Song et al. (2018). For the MNIST dataset which Madry et al. (2018) demonstrate the strongest defense results so far, we propose a simple transformation to find the blind-spots in this model.\" Can authors provide empirical comparison between blind-spot attacks and the work by Song et al. (2018), e.g., attack success rate & distortion? \n\nc) The linear transformation x^\\prime = \\alpha x + \\beta yields a blind-spot attack which can defeat robust adversarial training. However, given the linear transformation, one can further modify the inner maximization (adv. example generation) in robust training framework so that the $\\ell_infty$ attack satisfies  max_{\\alpha, \\beta} f(\\alpha x + \\beta) subject to \\| \\alpha x + \\beta \\|\\leq \\epsilon. In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training. \n\nd) \"Because we scale the image by a factor of \\alpha, we also set a stricter criterion of success, ..., perturbation must be less\nthan \\alpha \\epsilon to be counted as a successful attack.\" I did not get the point. Even if you have a scaling factor in x^\\prime = \\alpha x + \\beta, the universal perturbation rule should still be | x - x^\\prime  |_\\infty \\leq \\epsilon. The metric the authors used would result in a higher attack success rate, right? \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}