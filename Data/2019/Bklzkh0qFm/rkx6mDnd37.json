{"title": "A slight reformulation of RGCN with attention mechanisms with mixed results on graph classification and node classification tasks.", "review": "The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations.\n\nUnfortunately the paper falls short in two main areas:\n\n- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)\n- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)\n\nHowever, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}