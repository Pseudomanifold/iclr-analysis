{"title": "A very well written paper with solid technical contribution.", "review": "A very well written paper with solid technical contribution. The impact to the community might be incremental.\n\nPros:\n1. I enjoyed reading the paper, very well written, clean, and organized.\n2. Comprehensive literature survey, the authors provided both enough context for the readers to digest the paper, and well explained how this work is different from the existing literature.\n3. Conducted extensive experiments.\n\nCons (quibbles):\nExperiments:\nThe authors didn't compare the proposed method against topic model (vanilla LDA or it\u2019s derivatives discussed in related work). Because most topic models could generate vector representation for document too, and it's interesting to learn additional benefit of local context provided by the word2vec-like model.\n\nMethodology:\n1. About hyperparameters:\na. Are there principled way/guideline of finding sparsity parameters k in practice?\nb. How about the upper bound m (or K, the authors used both notation in the paper)?\n\n2. About scalability:\nHow to handle such large sparse word vectors, as it basically requires K times more resource compared to vanilla word2vec and it's many variants, when it\u2019s used for other large scale downstream cases? (see all the industrial use cases of word vector representations)\n\n3. A potential alternative model: The motivation of this paper is that each word may belong to multiple topics, and one can naturally extend the idea to that \"each sentence may belong to multiple topics\". It might be useful to apply dictionary learning on sentence vectors (e.g., paraphrase2vec) instead of on word vectors, and evaluate the performance between these two models. (future work?)\n\nTypos:\nThe authors mentioned that \"(Le & Mikolov, 2014) use unweighted averaging for representing short phrases\". I guess the authors cited the wrong paper, as in that paper Le & Mikolov proposed PV-DM and PV-DBOW model which treats each sentence as a shared global latent vector (or pseudo word).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}