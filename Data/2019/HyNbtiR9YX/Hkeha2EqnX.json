{"title": "review of Unsupervised Document Representation using Partition Word-Vectors Averaging ", "review": "Paper overview: The paper extends the method proposed by Arora 2017 for sentence embeddings to longer document embeddings. The main idea is that, averaging word embedding vectors mixes all the different topics on the document, and therefore is not expressive enough. Instead they propose to estimate the topic of each word (using dictionary learning) through the $\\alpha$ weights (see page 4).These weights give \"how much\" this word belongs to a certain topic. For every topic we compute the $\\alpha$-weighted vector of the word and  concatenate them (see word topic vector formation). Finally, we apply SIF (Arora 2017) using these word embeddings on all the document.   \n\nQuestions and remarks:\n     1) How sensitive is the method to a change in the number of topics (k)?\n    2) Please provide also the std instead of just the average performance, so that we can understand if the differences between methods are significantly meaningful\n \nPoints in favor: \n   1) Good results and thorough tests \n    2) Paper is easy to read and follow \n\nPoints against:\nA very similar method was already proposed by Mekala 2017, as the authors acknowledge in section 7. The main difference between the two methods is that Mekala et al use GMM and the authors of the present paper use sparsity method K-SVD to define the topics. \n\n\nThe novelty of the paper is not enough to justify its acceptance at the conference.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}