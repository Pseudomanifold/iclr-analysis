{"title": "Do not have enough comparison to existing works; need to improve writing", "review": "Focus on navigation problems, this paper proposes Q-map, a neural network that estimates the number of steps (in terms of the discount factor gamma) required to reach any position on the observable screen/window. Moreover, it is shown that Q-map can be applied for exploration, by trying to reach randomly selected goal.\n\nPros\n1. Novel goal-based exploration scheme\n\nCons\n1. Similar idea has been proposed before\nFor example, Dayan (1993) estimates the number of steps to reach any position on the map using successor representations. Discussion about this field (successor representations/features) is completely missing in the paper.\nRef:\n- Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613\u2013624, 1993.\n- Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, David Silver, and Hado van Hasselt. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4058\u20134068, 2017.\n- Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning, pp. 510\u2013519, 2018.\n\n2. Comparison to existing methods is only vaguely discussed\nFor example, it is claimed multiple times that UVFA requires the goal coordinates, but Q-map also requires coordinates when doing the exploration.\n\n3. The network architecture is not clearly presented\nFor example, the output of the network needs to be clipped, which suggests that there is no output transform. Since the predicted output is in [0,1], it would make sense to use Sigmoid transform for each pixel and use logistic loss.\n\n4. The proposed exploration scheme could be unnecessarily complicated\nSec.3.1 provides lengthy discussion about the drawback of eps-greedy exploration. Then in Sec.3.2, \\epsilon_r is basically the same as the eps-greedy algorithm, using to randomly select an action. Isn't this a \"bad\" thing as suggested in Sec.3.1? Moreover, the new exploration scheme requires two more hyper-parameters (min/max distance threshold), which will add more complication to the already very complicated deep RL learning procedure.\n\n5. Experiment results are limited\nFor the toy experiment in Sec.2.3, the map are relatively simple. The example of Dayan (1993) with an agent surrounded by walls is an interesting scenario and should be included. The proposed Q-map (ConvNet) could fail because it is hard to learn geodesic distance with only local information. More importantly, there is no comparison to similar methods in Sec.3. UVFA can replace Q-map to do similar exploration.\n\n6. Writing can be greatly improved\nThere are many grammar errors. To name a few, \"agent capable to produce\", \"the gridworld consist of\", \"in the thrist level\".\n\nMinors\n- UFV should be UVF in the introduction\n- Citation in Sec.3 is not consistent with the rest of the paper. Use \\citep or \\citet properly.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}