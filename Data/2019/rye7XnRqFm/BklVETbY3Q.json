{"title": "Review", "review": "The main idea in the paper is to use on-screen locations as goals for an RL agent. Using a de-convolutional network to parameterize the Q-function allows all goals to be updated at once and correlations between nearby or similar goal locations could be modelled. The paper explores how this type of goal space can be used for better exploration showing modest improvement in scores on Super Mario.\n\nClarity - The paper is well written and easy to follow. The Q-map architecture is well motivated and intuitive and the exploration strategy based on Q-maps is interesting.\n\nNovelty - The idea of using spatial goals combined with a de-convolutional architecture is not new and goes back at least to \u201cReinforcement Learning with Unsupervised Auxiliary Tasks\u201d by Jaderberg et al.. The UNREAL agent used the same type of de-convolutional \u201cQ-map\u201d to update a spatial grid of goals all at once. The main difference is that the UNREAL agent learns about spatial goals as an auxiliary task and does not execute/act on the goals like the Q-map agent. Nevertheless, the type of architecture and algorithm (called 3D Q-learning in this paper) is essentially the same.\n\nSignificance - The Q-map architecture requires access to the position of the avatar on the screen at training time. I would expect that using such a significant part of the agent\u2019s true state during training should lead to a significant improvement in performance at test time. Why not evaluate the proposed exploration strategy on well known hard exploration tasks? The results on Montezuma\u2019s Revenge are only qualitative. There Q-map agent did outperform an epsilon-greedy DQN baseline on Super Mario but the improvement does not seem very significant given how much prior knowledge Q-map was given compared to the baseline. It is also not clear how much of the improvement comes from training the Q-map as an auxiliary task and how much of it comes from better exploration.\n\nOverall quality - Given that the architecture is not very novel and requires the avatar\u2019s position to train I did not find the qualitative or quantitative results compelling enough. Perhaps the authors could show that the exploration strategy works well on several difficult exploration games. Another possibility would be to showcase other ways to use the Q-map, for example in an HRL setup.\n\nMinor comment - Some sections seem to be missing references. For example, the second paragraph of the introduction discusses GVFs and the Horde architecture without any references.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}