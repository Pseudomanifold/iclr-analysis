{"title": "Interesting Idea, but not well evaluated", "review": "Authors propose to overcome the sparse reward problem using an exploration strategy that incentivizes the agent to visit different parts of the game screen. This is done by building Q-maps, a 3D tensor that measures the value of the agent's current state (defined as the position of the agent) and action in reaching other (x, y) locations in the map. Each 2D slice of the Q-map measures the value at different (x, y) locations for one action. Such 2D slices (i.e. channels) are stacked together to form the Q-map. Taking the max across the channels, thus, provides the Q-value for the optimal action. \n\nA policy for maximizing the rewards is trained using DQN. The Q-map based exploration is used as a replacement for \\epsilon-greedy exploration. \n\nThe Q-map is used for exploration in the following way:\n(a) Chose a random action with probability \\epsilon_r. \n(b) If neither a random action nor a \"goal\" is chosen, a new goal is chosen with probability \\epislon_g. The goal is a (x, y) location, chosen so that is not too hard or too easy to reach it (i.e. Q-map values are neither too high or low; intuitively [1 - Q-map(x, y, a)] (for normalized/clipped Q) is a measure of distance of the goal).  \n     -- If a \"goal\" is chosen, the greedy action to go towards the goal is chosen. \n(c) If neither a goal or random action is chosen, DQN is used to chose the greedy exploration. \n\nAuthors also bias the goal selection to match DQN's greedy action. This is done as following -- from a set of goals that satisfy (b) above; chose the goal for which Q-map selected action matches the DQN's greedy action. \n\nResults are presented on simple 2D maze environments, Mario and Montezuma's revenge. \n\nI have multiple concerns with the papers:\n(i) The writing is informal and the ideas are not well explained. It would really benefit -- if authors introduce an algorithm box or talk about the method as a sequence of points. Right now, the ideas are scattered throughout the paper. I am still confused by figure 3 -- when are random goals chosen? Do random goals correspond to (b) above? Also, when the Horde architecture, GVF and UVF are mentioned, the references are missing -- I would love for the authors to include the corresponding  references.  \n\n(ii) The idea of reaching as many states as possible has been explored in count based visitation (Bellemare et al, Tang et al) \u2014 but no comparisons have been made to any previous work. Its always good to put a new work in the perspective of old work with similar ideas. \n\n(iii) The authors propose biased and random goal sampling \u2014 I would love to see how much improvement does biased goal sampling offer over random goal sampling. \n\n(iv) \u201c\u2026compare the performance of our proposed agent and a baseline DQN with a similar proportion of exploratory actions\u201d .. I don\u2019t agree with this a metric \u2014 I think the total number of steps is a good metric. Exploration is part of the agent\u2019s algorithm to find the goal, we shouldn\u2019t compare against DQN by matching the number of exploratory actions. \n\n(v) \u201cThe Q-map is trained with transitions generated by randomly starting from any free locations in the environment and taking a random action.\u201d Does this mean that when the agent is trained with Mario \u2014 the game is reset after every episode and the agent is placed a random starting location? If yes, then this is not a realistic assumption. \n\n(vi) I would like to see \u2014 how do Q-maps generalize across levels of Mario or Montezuma\u2019s revenge? Does Q-map trained on level-1 help in good exploration on future levels without any further fine-tuning? \n\nOverall, I like the idea of incentivizing exploration without changing the reward function as is done in multiple prior works. However, I think more thorough quantitative evaluation is required and it will be interesting to see transfer of Q-maps outside the 2D-domains. I am happy to increase my score if such evidence is provided. \n\nOther references worth including:\n(a) Strategies for goal generation: Automatic Goal Generation for Reinforcement Learning Agents (https://arxiv.org/abs/1705.06366) ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}