{"title": "Interesting idea, but insufficient comparison to baselines", "review": "This paper proposes a model-based reinforcement learning approach, called SOLAR, \nwhich consists of mapping complex, high-dimensional observations to low-dimensional \nrepresentations where transition dynamics between consecutive states are approximately linear. \nIn this low-dimensional space, local models can easily be fit in closed form and then used to optimize a policy, using a similar method to Guided Policy Search (GPS). The method is evaluated in 4 different settings (3 simulated, 1 on a real robot). \n\n*Quality: the method seems to work well in the experiments. However, there are issues with the experimental evaluation (detailed below) which make it unclear whether the method is better than standard baselines.\n\n*Clarity: the paper is well-written and clear overall.  \n\n*Originality: the paper proposes an extension of GPS, which to my knowledge is novel. \n\n*Significance: the idea of learning representations where transitions are linear seems well-founded and potentially useful. However the merits of this method are not yet clear from the experiments. \n\n\nSpecific Comments:\n\n- Please include an illustration of the 2D navigation task in Figure 3a\n- I'm confused by the poor performance of E2C in the 2D navigation task. \nThe previous works of [Watter et. al, 2015] and [Banijalami et. al, 2017] report close to 100% accuracy using similar methods. Is the task formulated differently here? \n- I would think a global action-conditional forward model (represented as convnet+deconvnet, and trained unrolled on its own predictions to reduce model errors) would perform quite well on the 2D navigation task, and possibly on the reacher task. Even though these are represented as images, they are very simple images with little distracting information, no changes in illumination/perspective, etc. It seems the model essentially just needs to learn a pixel translation for each action for the navigation task, and some rotations for the reacher. It already seems to work quite well for the non-holonomic car, which requires learning similar transformations. This baseline should be included for all the tasks. \n- Although it does seem that the method performs well on the stacking tasks for the real robot, there are no baselines included. However, there are many works which have explored representation learning and control for robotics using neural networks. A couple examples (+see references within):\n\n\"Learning to poke by poking: Experiential learning of intuitive physics\" Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, Sergey Levine. NIPS 2016\n\"Deep Visual Foresight for Planning Robot Motion\" Chelsea Finn, Sergey Levine ICRA 2017\n\nAt the very least, the method should be compared to pixel-based global models and representations learned with some kind of autoencoder or forward model for the robot task. \n\nThe paper proposes what seems to be a good idea, but it is not yet demonstrated by the current experiments. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}