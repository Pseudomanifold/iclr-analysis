{"title": "Unclear writing and contributions", "review": "This paper considers a new learning paradigm for Bayesian Neuron Networks (BNN): learning distribution in the functional space, instead of weight space. A new SG-MCMC variant is proposed in Algorithm 1, and applied to sampling in a \n\"functional space\". The approach is demonstrated on various tasks.\n\nQuality: Low, due to the low clarity detailed below.\n\n\nClarity: I do not fully follow the core algorithm:  The posterior is U_D(\\theta) = \\sum_{i=1}^D  \\lambda_i * u_i, where  \\lambda_i is represented as MCMC samples,  what is u_i then? I guess u_i is defined in (2), which is approximated in (3) if weight sample is used. However, how is u_i represented in the functional approach? I guess it is similar to the weight-based approach. If this is true, how could we distinguish between a functional approach and weight-based approach?\n\nThe proposed SGFuncRLD is essentially Adam plus Gaussian noise, but performed in a so-called \"functional space\"? It is therefore not surprise to me that SGFuncRLD performs better than pSGLD (RMSprop plus Gaussian noise), just as Adam performs better than RMSprop. If we only focus on the new SG-MCMC approach itself, the authors need to justify: (1) the smoothed gradient is an unbiased gradient estimator, how does it effect convergence? Does it guarantee to  true posterior? this should be done in theory. (2)  The SGFuncRLD  algorithm itself is the same with pSGLD except the smoothed gradient part.  This makes  the clear comparison even important. Does SGFuncRLD  perform better just because the proposed smoothed gradient, or because the sampling is done in the functional space?\n\nMy suggestions: Please disentangle the contributions clearly. There are two things: (1) smooth gradient, (2) sampling in a functional space. Which one really contributes the performance improvement?\n\nTo demonstrate (1),  the authors could at least conduct on a toy distribution, to demonstrate the difference with pSGLD, regardless it is to the functional space or the weight space. \nTo demonstrate (2), the authors  could apply the same SG-MCMC variant to the functional space and to the weight space, and see the difference. \n\nOriginality: To me, the idea of learning uncertainty of BNN in the functional space appeared in Prof.  Yee Whye Teh's NIPS 2017 presentation. The motivation in his presentation is very clear. However, how to implement this abstract idea in practice is unclear yet. This submission is the first attempt. However, I am concerned about the real contribution.\n\nSignificance: It is a very interesting research direction. The paper could have been significant if every part is clearly motivated and demonstrate. At this point, I am not fully convinced. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}