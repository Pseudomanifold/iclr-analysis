{"title": "Relation to prior work is very unclear", "review": "The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective. They also provide experimental results on the OLS objective as well as small scale neural networks. First of all, understanding the properties of batch normalization is an important topic in the machine learning community so in that sense, contributions that tackle this problem are of interest for the community. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work. Please address this point clearly in your rebuttal.\n\n1) Overlap with Kolher et al. 2018: The authors erroneously state that Kolher et al. considered the convergence properties of BNGD on linear networks while after taking a close look at their analysis, they first derive an analysis for least-squares and then also provide an extension of their analysis to perceptrons. The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. I will come back to this aspect multiple times below.\n\n2) Properties of the minimizer\nThe authors should clearly state that Kolher et al. first proved that a^* and w^* have similar properties to Eq. 8. If I understand correctly, the difference seem to be that the algorithm analyzed in Kohler relies on the optimal a^* while the analysis presented here alternates between optimizing a and w. Is this correct? Is there any advantage in not using a^*? I think this would be worth clarifying.\n\n3) Scaling property\nI find this section confusing. Specifically,\na) The authors say they rely on this property in the proof but it is not very clear why this is beneficial. Can you please elaborate?\nb) It seems to me this scaling property is also similar to the analysis of Kolher et al. who showed that the reparametrized OLS objective yields a Rayleigh quotient objective. Can you comment on this?\nc) The idea of \u201crestarting\u201d is not clear to me, are you saying that one the magnitude of the vector w goes above a certain threshold, then one can rescale the vector therefore going back to what you called an equivalent representation? I don\u2019t see why the text has to make this part so unclear. Looking at the proof of Theorem 3.3, this \u201cproperty\u201d seem to be used to simply rescale the a and w parameters.\nd) The authors claim that \u201cthe scaling law (Proposition 3.2) should play a significant role\u201d to extend the analysis to more general models. This requires further explanation, why would this help for say neural networks or other more complex models?\n\n4) Convergence rate\nIt seems to me that the results obtained in this paper are weaker than previous known results, I would have liked to see a discussion of these results. Specifically,\na) Theorem 3.3 is an asymptotic convergence result so it is much weaker than the linear rate of convergence derived in Kolher et al. The authors require a sufficiently small step size. Looking at the analysis of Kolher et al., they show that the reparametrized OLS objective yields a Rayleigh quotient objective. Wouldn\u2019t a constant step size also yield convergence in that case?\nb) Proposition 3.4 also only provides a local convergence rate. The authors argue BNGD could have a faster convergence. This does seem to again be a weaker result. So again, I think it would be very beneficial if the authors could clearly state the differences with previous work.\n\n5) Saddles for neural nets\nThe authors claim they \u201chave not encountered convergence to saddles\u201d for the experiments with neural networks. How did you check whether the limit point reached by BNGD was not a saddle point? This requires computing all the eigenvalues of the Hessian which is typically expensive. How was this done exactly?\n\n6) Extension of the analysis to deep neural networks\nThe analysis provided in this paper only applies to OLS while Kolher et al. also derived an analysis for neural networks. Can the authors comment on extending their own analysis to neural nets and how this would differ from the one derived in Kolher et al.?\n\n7) Experiments\nHow would you estimate the range of suitable step sizes (for both a and w) for BNGD for a neural network?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}