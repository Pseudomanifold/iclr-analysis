{"title": "Interesting idea of casting one-class classification/set beloning problem onto 4 player game", "review": "This paper describes a new form of one-class/set beloning learning, based on definition of 4 player game:\n- Classifier player (c), which is a typical one-class classifier model\n- Comparator player (h), which given two instances answers if first is \"not smaller\" (wrt. set belonging) than the other\n- Classifier adversary player (Gc), which tries to produce hard to distinguish samples for (c)\n- Comparator adversary player (Gh), which tries to produce hard to classify samples for (h)\nThis way authors end up with cooperative-competitive game, where c and h act cooperatively to solve the problem, while Gc and Gh constantly try to \"beat\" them. \n\nOverall I find this paper to be interesting and worth presenting, however I strongly encourage authors to rethink the way story is presented so that it is more approachable by people who do not have much experience with viewing typical classification problems as games. In particular, one could completely avoid talking about \"sets of local maxima\" and just talk about the density estimation problem, with c being characteristic function (of belonging to the support) and h being comparator of the pdf.\n\nStrong points:\n- Novel, multi-agent in nature, approach to one-class classification\n- Proposed method build a complex system, which can be used in much wider class of problems than just classification (due to joint optimisation of classifier and comparator)\n- Extensive evaluation on 4 problems\n- Nice ablation study showing that most of the benefits come from pure c/Gc game (on average 68.8% acc vs 65.2% of just c, and 69.8% of entire system) but that h/Gh players do indeed still improve (an extra 1%). It might be interesting to investigate what exactly changed in c due to existance of h in training. Are there any identifiable properties of the model that can now be analysed?\n\nWeak points:\nIn general I believe that theoretical analysis is the weakest part of the paper, and while interesting - it is actually a minor point, and shows interesting properties, but not the ones that would guarantee anything in \"practical setup\". I would suggest \"downplaying\" this part of the paper, maybe moving most of it to the appendix. \nTo be more specific:\n- Theorem 1 shows that representation can be more compact, however existance of compactness does not rely imply that this particular solution can ever be learned or that it is a good thing (number of parameters is not correlated with generalisation capabilities of the model).\n- Lemma 1 seems a bit redundant for the story. While it is nice to be able to show generalisation bounds in general, this paper is not really introducing new class of models (since in the end c is going to be used for actual classification), but rather training regime, and generalisations bounds do not tell us anything about the emerging dynamical system. The fact that adding v does not constrain c too much seems quite obvious, and as a result I would suggest moving this section to appendix.\nInstead, if possible, the actual tricky mathematical bit for methods like this would be, in reviewers opinion, any analysis of learning dynamics of the system like this. Multi-agent systems cannot be optimised with independent gradient descent in general (convergence guarantees are lost). Consequently many papers focus on methods that bring these properties back (e.g. Consensus Optimisation or Symplectic Gradient Ascent). It would be beneficial for the reader to spend some time discussing stability of the system proposed, even if only empirically and on small problems.\n\nOther remarks:\n- eq. (1) is missing \\cdot\n- it could be useful to include explicit parameters dependences in (1) and (2) so that one sees how losses really define asymmetric game between the players\n- why do we need 4 players and not just 3, with Gc and Gh being a single player/neural network? can we consider this as another ablation?\n- given small performance gaps in Table 1 can we get error estimates/confidence intervals there? Deep SVDD paper includes error estimates of the baseline methods\n- since training is performed in mini batch (it does not have to be decomposible over samples) shouldn't equations be based on expectations rather than sums?\n\n- ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}