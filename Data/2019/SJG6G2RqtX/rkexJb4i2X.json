{"title": "Missing information in the exposition", "review": "Update:\nI thank the authors for their clarifications. I have raised my rating, however I believe the exposition of the paper should be improved and some of their responses should be integrated to the main text.\n\nThe paper proposes two new modules to overcome some limitations of VIN, but the additional or alternative hypotheses used compared to VIN are not clearly stated and explained in my opinion. \n\n    Pros :\n    - experiments are numerous and advanced\n    - transition probabilities are not transition-invariant compared to VIN\n    - do not need pretraining trajectories\n\n    Cons :\n    - limitation and hypotheses are not very explicit\n\n    Questions/remarks :\n    - d_{rew} is not defined \n    - the shared weights should be explained in more details\n    - sometimes \\psi(s) is written as parametrized by \\theta, sometime not\n    - is it normal that the \\gamma never appears in your formula to update the \\theta and w? yet reading the background part I feel that you optimize the discounted sum of the rewards, is it the case?\n    - I think there is a mistake in the definition of 1_{s' \\neq \\emptyset }, it is 1 if s' is NOT terminal and 0 otherwise, am I wrong?\n    - why do you need the parameters w to represent the value function V, if you already have v^k_{i,j} available? is it just to say that your NN is updated with two distinct cost functions? \n    - I did not understand the assumptions made by VProp, do you consider that the transition function T is known? this seems to be the case when you explain that transitions are deterministic and that there is a mapping between the actions and the positions, but is never really said\n    - Compared to VIN, VProp uses an extra maximum to compute v^k_{i, j}, why? In this case, the approximation of the value function can never decrease.\n    - How is R_{a, i, j, i ', j'} broken into r^{in}_{i ', j'} - r^{out}_{i, j} in VProp? Is the reward function known to the agent at all points?\n    - In MVProp, can r_{i, j} be negative?\n    - In MVProp, how does the rewriting in p * v + r * (1-p) shows that only positive rewards are propagated? Does not it come only from the max?\n    - In the experiments, S is not fully described, \\phi(s) neither\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}