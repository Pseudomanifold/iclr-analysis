{"title": "Interesting learning algorithms for autoregressive models without MLE pretraining", "review": "Quality and Clarity:\nThe writing is good and easy to read, and the idea is clearly demonstrated.\n\nOriginality:\nThe idea of never training over the ground-truth sequence, but training on sampled prefix and an optimized suffix is very novel. The similar idea is also related to imitation learning in other domains such as self-driving car where an oracle can give optimal instruction when exploring a new state. \n\nComments:\nThe paper proposed a very interesting training algorithm for auto-regressive models especially it does not require any MLE pre-training and can directly optimize from the sampling.\n\nHere are some questions:\n(1) The idea should also apply on many \u201cincremental rewards\u201d, for instance, BLEU scores in machine translation, etc. Do you have any comparison? What if the best suffix cannot be found using dynamic programming (when the evaluation metric is not edit-distance, but a sentence-level reward)?\n(2) Can the proposed algorithm be applied in other \u201cbigger\u201d tasks such as neural machine translation?\n(3) Eq. (6) is not very clear. Do you first sample the whole sequence, and then supervise one token for each step? Or the whole suffix?\n(4) Do you have a comparison with the learning efficiency between MLE and OCD? Will it get unstable in the beginning of training as all the samples are wrong.\n\n----------------------------\nMissing Reference:\nDing, Nan, and Radu Soricut. \"Cold-Start Reinforcement Learning with Softmax Policy Gradient.\" Advances in Neural Information Processing Systems. 2017.\n\nThis paper used a very similar idea as the proposed learning method which relies on incremental rewards to find the \u201coptimal\u201d suffix (for instance, edit-distance is a special example). It would be better to have some discussion,\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}