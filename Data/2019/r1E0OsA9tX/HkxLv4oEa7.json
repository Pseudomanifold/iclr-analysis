{"title": "Empirical Bayes ideas in neural networks is a great idea. This paper is too heuristic, but the results are encouraging.", "review": "This paper first proposes an intuition that when training, neural network weights tend to be correlated. It then suggests to make this correlation more specific, and does so using a heuristic algorithm based on approximate empirical Bayes.\n\nThe paper is generally well written, in the sense that everything that is done is clearly stated. However why things are done the way they are is less clear.\n\n-\tThe correlation intuition given in the introduction isn\u2019t quite right (if the training does it anyway, why try to push it even more?). The better way to think about the benefit of correlations is to say that some weak form of weight sharing may help.\n\n-\tThe approximate notion of empirical Bayes is not well justified.  The arguments given are all about concentration and essentially unimodality. The successive MAP perspective is interesting, but again not clear why we the joint maximization of (4) won\u2019t place us far from the smoothed version of (1).\n\n-\tNot only is the AEB principle a heuristic, but the implementation via Algorithm 1 is itself a heuristic to this heuristic. This is because the optimization of the model parameters still remains approximate (due to the local search aspect), even if the optimization of the hyperparameters is shown to be exactly solvable. In the end we end up with a stationary point of the AEB objective, for which we have even less theoretical insight, even if the empirical evidence is promising.\n\n-\tA couple of comments about algorithm 1. First, the \\theta notation is flipped to refer to the model parameters, instead of the hyperparameters, and it should be fixed. Second, the role of the constant u and v is a bit unclear. On the surface, they provide extra regularization to the hyperparameters. But then, how do we choose them?\n\n-\tThe experimental results are a bit oversold. First, we have SGD methods that do thrive in smaller batch size regimes, and it\u2019s disingenuous to handicap them with larger batch sizes, when they were performing comparable at smaller ones. It *is* interesting that AED makes the learning batch-size insensitive, and I wish that was elaborated more, to see if it\u2019s a prevalent property in other data sets too. Also, the authors define better local optima is by saying that they reach a lower value of training loss. This is claimed and shown. Usually we think of them as those leading to better generalization. This is claimed, but not shown that these are indeed those that generalize better. Figure 3 (which I assume shows training cross-entropy) shows one outcome among many (the many that are averaged in the other plots), and it could very well be that this outcome did not generalize as well.\n\nDespite these shortcomings, I believe this paper is another welcome push to introducing empirical Bayes ideas into neural networks (though it\u2019s not the first), and the empirical evidence seems to indicate that there is indeed something there to investigate further, so I give it a weak accept.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}