{"title": "Lack of comparisons", "review": "This paper applies variational dropout to reduce the communication cost of distributed training of neural networks. The authors do experiments on mnist, cifar10 and svhn datasets. The technique is simple and easy to understand.\n\nHowever, I think this paper has some problems.\n1. Novelty\nApplying variational dropout for model compression is not a new idea. As this paper only combines variational dropout and distributed training, I think the novelty is a little thin.\n\nReference:\nLouizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian compression for deep learning.\" Advances in Neural Information Processing Systems. 2017.\n\n2. Comparisons\nNeed to compare with other methods that reduce the amount of communication by sparsifying gradients. \n\n3. Larger dataset & networks\nCifar10 and SVHN is not interesting for distributed training. The data / model size differs significantly with the data / models that really needs distributed training, e.g., ImageNet. The authors should at least do ImageNet and a few state-of-the-art models to make the paper convincing. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}