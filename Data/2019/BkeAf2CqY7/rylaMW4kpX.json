{"title": "A simplistic approach for improving cost of federated learning with unimpresive performance", "review": "In federated learning the data are kept privately on each device, gradients from each device are sent to the server, and the server sends back the averaged gradients to each device to update model weights. The authors propose an algorithm that reduces communication costs by sending sparse gradients from device to server and back. The sparsification relies on the variational dropout and sets partial derivatives to zero if the weights are sufficiently uncertain, where the uncertainty threshold is a hyperparameter. As a by-product of the algorithm, the size of the model is also reduced somewhat. The proposed algorithm is evaluated on several benchmark data sets using several benchmark neural network architectures and the results indicate a reduction in communication costs are compared to a non-sparsified variant of federated learning \n\nStrengths:\n\u2022\tThe algorithm jointly leads to compression of a neural network and reduction in communication cost of federated learning.\n\u2022\tThe paper is well organized and clearly written.\n\nWeakness:\n\u2022\tThe novelty is low: the paper is a straightforward application of the variational dropout paper [Molchanov et al., 2017], the federated learning setup is standard , and the architecture in Figure 1 is very similar to distributed deep learning [see Felix Sattler 2018].\n\u2022\tThere are some questions related to threshold T: if it is set too high, the model will not be compressed and communication cost will be low, but if it is set too low, the accuracy could be significantly impacted. It seems reasonable to apply an adjustable threshold, but the paper is not discussing this issue.\n\u2022\tSome details are missing: how to determine threshold T in algorithm 1. is the model compressed in each iteration, is the model compressed using the constant threshold T?\n\u2022\tThe paper does not compare with other state of the art federated learning algorithms: as listed in the related work, there are papers that compress the model, and also papers that sparsify gradients. Is the proposed variational dropout better in compression models or reducing communication cost compared to them?\n\u2022\tThe reported improvements in communication cost and reductions in model size are relatively minor\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}