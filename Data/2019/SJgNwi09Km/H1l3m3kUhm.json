{"title": "Interesting approach, but experiments could have a more in-depth analysis", "review": "The authors propose to augment the Variational AutoEncoder [1] with a latent prior modeled by a Gaussian Latent Tree Model [2], allowing to introduce a hierarchical structure of clusters in the learned representation. The LT-VAE not only learns the location of each cluster to best represent the data, but also their number and the hierarchical structure of the underlying tree. This is achieved by a three-step learning algorithm. Step 1 is a traditional training of the encoder and decoder neural networks to improve their fitting of the data. Step 2 is an EM-like optimization to better fit the parameters of latent prior to the learned posterior. And step 3 adapts the structure of the latent prior to improve its BIC score [3], which balances a good fit of the latent posterior with the number of parameter (and thus complexity) of the latent prior.\n\nExperiments on synthetic data confirms the ability of the model to discover latent multifaceted clustering, and tests on 4 datasets shows it to be competitive with other unsupervised clustering models. Qualitative interpretation of samples from the learned model shows that the model learns a clustering that is clearly relevant to the data, while maybe not obvious to interpret.\n\nThe paper is well written and easy to follow (I however found a few typos and small mistakes that I'll list at the end of this review). The idea of using a structure on the latent prior of a VAE to learn a clustering of the data is not new, but the authors propose here an interesting approach to it, with a clearly described algorithm.\n\nHowever, I would have liked to see a more in-depth analysis of the behavior of the model on the various datasets, and my reading of this paper raised several questions that found no answer:\n\n1. What gains does the hierarchical structure on the Y variables provide? The paper does not analyze whether the models they trained actually learned conditional dependencies on the Y_i variables. How would this compare to the same model, with the only difference that the Y_i are fixed to be independent of each other (but still learning the number of Y_i and how the z_j are distributed between them) ?\n\n2. This is linked to the previous one. On the tests of the dataset, how do the different facets interact with each other? How are the samples from the different clusters of facet 2 when facet 1 is fixed to a particular cluster? Assuming the learned dependency is that Y_1 is the parent of Y_2, does the interpretation of each value of Y_2 change depending on the value of Y_1?\n\n3. The VAE with diagonal gaussian latent has a natural tendency to achieve sparcity in its latent space [4], making it robust to having too many latent neurons. Does this property hold with LT-VAE? If so, are the \"unused\" neurons organized in a particular way among the different learned facets?\n\nI'd be reluctant to accept this paper without answers to points 1 and 2, which in my opinion are needed to justify the \"tree\" part of the \"latent tree model\" choice for the latent space. I'd also be very interested in an answer to point 3, which would give good insights regarding the design choices for applying this model to new problems (how important is the choice of the size of the latent space?), but I'm not considering it blocking acceptance.\n\n[1] https://arxiv.org/abs/1312.6114\n[2] http://jmlr.org/papers/volume5/zhang04a/zhang04a.pdf\n[3] https://projecteuclid.org/euclid.aos/1176344136\n[4] https://arxiv.org/abs/1706.05148\n\n--------------------------------\n\nNotes and typos:\n\n- In the introduction, \"Deep clustering network network (DCN)\", the word \"network\" is repeated \n- After equation 5, \"... where \\pi( . ) denotes the parent node ...\", the \"pi\" symbol does not appear in the equation at all, neither in the following equation, so I guess this part of the sentence should be removed\n- In section 3.3, you write that you define 5 operators, but follow by listing 7 (NI, ND, SI, SD, NR, PO and UP)\n- In section 4.1, I believe W lives in R^(10x4) not R^(10x2)\n- In section 4.5 the acronym \"MoG\" (\"Mixture of Gaussian\" I guess) is used without being introduced previously", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}