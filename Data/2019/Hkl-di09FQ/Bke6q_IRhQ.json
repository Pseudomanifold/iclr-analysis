{"title": "Unclear approach and contribution", "review": "This paper aims at comparing end-to-end learning vs separately learning a state representation and subsequently a controller.\n\nWhile this would be a relevant and important topic, the paper does not currently present consistent evidence to support this hypothesis.\n\nIn particular:\n- The approach proposed in the approach is not explained in sufficient details. After reading Sec.4 I have only a very vague and high-level idea of how the proposed approach might work. In Figure.2, what is I_t? What is the model that you are training? How are you learning this model? how do you define L_inverse?\n- The cited literature about state representation learning is absolutely incomplete. Papers like Lange et al. , Wahlstr\u00f6m et al. and Finn et al. and citations herewithin.\n- From the experimental results, it is difficult to say anything definitive about the proposed hypothesis. 1) There are multiple end-to-end approaches in the literature, with significant differences in performance. which one are you using? (it seem A2C and PPO, but to which label do they correspond in the tables?) 2) How do you tune the weights of the reward function proposed? This seems an important design choice, but it is not much discussed. 3) In the table reported (e.g., Table 1) it does not seem to me that SRL consistently outperforms other approaches. Even for the arm tasks, Random features seem to outperform the proposed approach (and indeed all the methods except the ground truth). What is going on there?\n\nOverall\u00b8 the paper would benefit from a clearer and more detailed text, and from improved experiments and comparisons.\n\nMinor comments:\n- It is unclear to me what \"goal-based robotic tasks\" means. How do you define a task without a goal?\n- An important and missing characteristic of a suitable state representation should be the generalization. In fact, a good representation would ideally allow the agent to generalize to some degree. \n- It seems very odd to me that the \"action should be implicitly encoded into the state representation\" could you elaborate of the motivation for this and the effects?\n\nReferences:\n- Autonomous reinforcement learning on raw visual input data in a real-world application\nS Lange, M Riedmiller, A Voigtlander\nNeural Networks (IJCNN), The 2012 International Joint Conference on, 1-8\n- From pixels to torques: Policy learning with deep dynamical models\nN Wahlstr\u00f6m, TB Sch\u00f6n, MP Deisenroth\narXiv preprint arXiv:1502.02251\n-  Deep Visual Foresight for Planning Robot Motion\nChelsea Finn, Sergey Levine\nInternational Conference on Robotics and Automation (ICRA), 2017 ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}