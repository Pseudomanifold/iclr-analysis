{"title": "nice contribution -- guarantees+practice", "review": "In this work the authors improve upon the work of Arora et al. mainly with respect to one aspect, i.e.,\nThey provide eps-approximation of a fully connected neural network output neuron-wise. The idea of \ncompression is very natural and has been explored by various previous works (key refs are cited). Intuitively,\nthe number of effective parameters is significantly less than the number of parameters in the neural network.\nThe authors introduce the notion of the coreset that is suitable for compressing the weight parameters \nin definition 1. Their main result is stated as Theorem 4. Finally, the authors experiment on standard benchmarks, \nperform a careful experimental analysis (i.e., they ensure fairness of comparison between methods such as \nSVD and the rest).  It would be interesting to see the histogram/distribution of the weights per layer and at an aggregate level\nfor the datasets used.  Also, in the light of the recent results of Arora et al. that show that the signal out of a layer\nis correlated with the top singular values, how would coresets\ndeveloped in the numerical linear algebraic community  (e.g., Near-optimal Coresets For Least-Squares Regression \nby Boutsidis et al.) perform, even as an experimental heuristic compared to the proposed method?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}