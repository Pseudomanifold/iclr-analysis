{"title": "Some imprecisions, but interesting new perspective", "review": "The paper casts the problems of value learning and policy optimization, which can be problematic the non-linear setting, into the bilevel optimization framework. It proposes two novel algorithms with convergence guarantees. Although other works with similar guarantees exist, these algorithms are very appealing for their simplicity. A limited empirical evaluation is provided for the value-based method in Acrobot and Mountain Car and in the Atari games Pong and Breakout for the proposed bilevel Actor Critic.\n\nThere are a few missing references to similar, recent work, including Dai et al\u2019s saddle-point algorithm (https://arxiv.org/pdf/1712.10285.pdf). Also, the claim that this is \u201cthe first attempt to study the convergence of online reinforcement learning algorithms with nonlinear function approximation\u201d can\u2019t be true (even replacing \u2018attempt\u2019 by \u2018successfully\u2019, there is e.g. Maei et al.\u2019s nonlinear GTD paper, see below).\n\nAlthough certainly interesting, the claims relating bilevel optimization and the target network are not completely right. E.g. Equation 3.6 as given is a hard constraint on omega. More explicitly, there are no guarantees that either network is the minimizer of the RHS quantity in 3.6.\n\nThe two-timescale algorithm is closer in spirit to the use of a target network, but in DQN and variants the target network is periodically reset, as opposed to what the presented theory would suggest. A different breed of \u201csoft target\u201d networks, which is more closely related to bilevel optimization has been used to stabilize training in DDPG (https://arxiv.org/abs/1509.02971).\n\nThere was some confusion for me on the first pass that you define two algorithms called \u2018online Q-learning\u2019 and \u2018actor-critic\u2019. Neither algorithm is actually that, and they should be renamed accordingly (perhaps \u2018bilevel Q-Learning\u2019 and \u2018bilevel actor-critic\u2019?). In particular, standard Q-Learning is online; and the actor-critic method does not minimize the Bellman residual (i.e. I believe the RHS of 3.8 is novel within policy-gradient methods).\n\nOnce we\u2019re operating on a bounded space with continuous operators, Theorem 4.2 is not altogether surprising \u2013 a case of Brouwer\u2019s fixed point theorem, short of the result that theta* = omega*, which is explained in the few lines below the theorem. While I do think Theorem 4.2 is important, it would be good to contrast it to existing results from the GTD family of approaches. Also, requiring that |Q_theta(s,a)| <= Qmax is a significant issue -- effectively this test fails for most commonly used value-based algorithms.\n\nThe empirical evaluation lacks any comparison to baselines and serves for little more than as a sanity check of the developed theory. This is probably the biggest weakness of the paper, and is unfortunate given the claim of relevance to e.g. deep RL.\n\n\n\nQuestions\n\nThroughout, the assumption of the data being sampled on-policy is made without a clear argument as to why. Would the relaxation of this assumption affect the convergence results?\n\nCan the authors provide an intuitive explanation if/why bilevel optimization is necessary?\n\nCan you contrast your work with Maei et al., \u201cConvergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation\u201d?\n\n\nSuggestions\n\nThe discussion surrounding the target network should be improved. In particular, claiming that the DQN target network can be viewed \u201cas the parameter of the upper level optimization subproblem\u201d is a stretch from what is actually shown.\n\nThe paper was sometimes hard to follow, in part because the claims are not crisply made. I strongly encourage the authors to more clearly relate their results to existing work, and ensure that the names they use match common usage.\n\nI would have liked to know more about bilevel optimization, what it aims to solve, and the tools used to do it. Instead all I found was very standard two time-scale methods, which was a little disappointing \u2013 I don\u2019t think these have been found to work particularly well in practice. This is particularly relevant in the context of e.g. the target network question.\n\nA proper empirical comparison to existing algorithms would significantly improve the quality and relevancy of this work. There are tons of open-source baselines out there, in particular good state of the art implementations. Modifying a standard implementation to optimize its target network along the lines of bilevel optimization should be relatively easy.\n\nRevision:\nI thank the authors for their detailed feedback, but still think the work isn't quite ready for publication. After reading the other reviews, I will decrease my score from 6 to 5. Some sticking points/suggestions:\n- Some of my concerns remain unanswered. E.g. the actor-critic method 3.8 is driven by the Bellman residual, which is not the same as e.g. the MSPBE used with linear function approximation. There is no harm in proposing variations on existing algorithms, and I'm not sure why the authors are reluctant to do. Also, Brouwer's fixed point theorem, unlike Banach's, does not require a contractive mapping.\n- The paper over-claims in a number of places. I highly recommend that the authors make their results more concrete by demonstrating the implications of their method on e.g. linear function approximation. This will also help contrast with Dai et al., etc.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}