{"title": "Interesting work but some of the claims need to be adjusted.", "review": "This paper interprets the fitted Q-learning, policy evaluation and actor-critic as a bi-level optimization problem. Then, it uses two-timescale stochastic approximation to prove their convergence under nonlinear function approximation. It provides interesting view of the these existing popular reinforcement learning algorithms that is widely used in DRL. However, there are several points to be addressed in the revision, which are mainly in some of its claims.\n\nThis paper is mainly a theoretical paper and experiments are carried out on a few simple tasks (Acrobot, MountarinCar, Pong and Breakout). Therefore, it cannot be claimed as \u201cthorough numerical experiments are conducted\u201d as in abstract. This claim should be modified.\n\nFurthermore, it cannot be claimed that this paper is a \u201cfirst attempt to study the convergence of online reinforcement learning algorithms with nonlinear function approximation in general\u201d. There is a recent work [1], which developed a provably convergent reinforcement learning algorithm with nonlinear function approximation even in the off-policy learning setting.\n[1] B. Dai, A. Shaw, L. Li, L. Xiao, N. He, Z. Liu, J. Chen, L. Song, \u201cSBEED Learning: Convergent Control with Nonlinear Function Approximation\u201d, ICML, 2018.\n\nThe actor-critic algorithm in the paper uses TD(0) as its policy evaluation algorithm. It is known that the TD(0) algorithm will diverge in nonlinear function approximation and in off-policy learning case. I think the actor-critic algorithm analyzed in the paper is for on-policy learning setting. The authors need to clarify this. Furthermore, the authors may need to comment on how to extend the results to off-policy learning setting.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}