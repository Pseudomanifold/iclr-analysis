{"title": "Attentive Task-Agnostic Meta-Learning for very-few-shot learning", "review": "The authors introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification.\nThe main idea is to learn task-independent representations, while other parameters, including the attention mechanism, are being fine-tuned for each specific task after pretraining. \nThe authors find that, for few-shot text classification tasks, their proposed approach outperforms several important baselines, e.g., random initialization and MAML, in certain settings. In particular, ATAML performs better than MAML for very few training examples, but in that setting, the gains are significant. \n\nComments:\n- I am unsure if I understand the contributions paragraph, i.e., I cannot count 3 contributions. I further believe the datasets are not a valid contribution, since they are just subsets of the original datasets.\n- Using a constant prediction threshold of 0.5 seems unnecessary. Why can't you just tune it?\n- 1-shot learning is maybe theoretically interesting, but how relevant is it in practice? ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}