{"title": "Fairly straight-forward ideas but good results and solid empirical work", "review": "Summary\n=======\nThis paper introduces a method for learning neural networks with quantized weights and activations. The main idea is to stochastically \u2013 rather than deterministically \u2013 quantize values, and to replace the resulting categorical distribution over quantized values with a continuous relaxation (the \"concrete distribution\" or \"Gumbel-Softax distribution\"; Maddison et al., 2016; Jang et al., 2016). Good empirical performance is demonstrated for LeNet-5 applied to MNIST, VGG applied to CIFAR-10, and MobileNet and ResNet-18 applied to ImageNet.\n\nReview\n======\nRelevance:\nTraining non-differentiable neural networks is a challenging and important problem for several applications and a frequent topic at ICLR.\n\nNovelty:\nConceptually, the proposed approach seems like a straight-forward application/extension of existing methods, but I'm unaware of any paper which uses the concrete distribution for the express purpose of improved efficiency as in this paper. There is a thorough discussion of related work, although I was missing Williams (1992), who used stochastic rounding before Gupta et al. (2015), and Soudry et al. (2014), who introduced a Bayesian approach to deal with discrete weights and activations.\n\nResults:\nThe empirical work is thorough, achieving state-of-the-art results in several classification benchmarks. It would be interesting to see how well these methods perform in other tasks (e.g., compression or even regression), even though the literature on quantization seems to focus on classification.\n\nClarity:\nThe paper is well written and clear.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}