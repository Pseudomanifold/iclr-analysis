{"title": "Stochastic neural networks with zero mean posterior on weights", "review": "This paper introduced a new stochastic layer termed variance layer for Bayesian deep learning, where the posterior on weight is a zero-mean symmetric distribution (e.g., Gaussian, Bernoulli, Uniform). The paper showed that under 3 different prior distributions, the Gaussian Dropout layer can converge to variance layer. Experiments verified that it can achieve similar accuracies as conventional binary dropout in image classification and reinforcement learning tasks, is more robust to adversarial attacks, and can be used to sparsify deep models.\n\nPros:\n(1)\tProposed a new type of stochastic layer (variance layer)\n(2)\tCompetitive performance on a variety of tasks: image classification, robustness to adversarial attacks, reinforcement learning, model compression\n(3)\tTheoretically grounded algorithm\n\nCons:\n(1)\tMy main concern is verification. Most of the comparisons are between variance layer (zero-mean) and conventional binary dropout, while the main argument of the paper is that it\u2019s better to set Gaussian posterior\u2019s mean to zero. So in all the experiments the paper should compare zero-mean variance layer against variational dropout (neuron-wise Eq. 14) and sparse variational dropout (additive Eq. 14), where the mean isn\u2019t zero.\n(2)\tThe paper applies variance layers to some specific layers. Are there any guidelines to select which layers should be variance layers?\n\nSome minor issues:\n(1)\tPage 4, equations of Gaussian/Bernoulli/Uniform variance layer, they should be w_ij=\u2026, instead of q(w_ij)= \u2026\n(2)\tWhat\u2019s the prior distribution used in the experiment of Table 1?\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}