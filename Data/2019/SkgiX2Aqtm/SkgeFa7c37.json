{"title": "A nice paper but needs stronger experiment results", "review": "General:\nIn general, this is a well-written paper and I feel pleasant to read the paper. The paper proposed a model named Pseudo Invertible Autoencoder(PIE) which combines invertible architecture and inference model.\n\nStrength:\n1. The explanation of the paper is very clear and consistent.\n2. The idea is interesting. A lot of papers related to the inverse problem focus on perfect invertibility, but the author(s) emphasize the importance of invertible compression and relate PIE to the inference model.\n\nPossible Improvements:\n1. The experiments could have been more convincing: 1) The only competitors are VAE and WAE. 2)The only data set has been tested was MNIST data set. There are many great works mentioned in the paper and those works should also be compared in a way.\n2. The content could be more compact so that more experiments can be shown to support the paper. It seems to me there is too much explanation to previous works in the paper. \n3. The paper has 9 pages which exceed the suggestion a little bit.\n4. I am not sure if the author(s) checked the grammar of the paper carefully. I found quite few typos in the paper. Page 3: 'Rather then' should be 'Rather than' and 'As we are interested' should be as 'As we are interested in'; Page 4: 'Can me' should be 'Can be'; Page 6: 'Better then' should be 'Better than'; Fig.6 (b): Should it be '0' or 'g(z)'?\n\nConclusion:\nThis is a good and clean paper in general. It explains the related work and presents PIE with necessary details. My biggest concern is that empirical validation(experiment) is poor. As a conclusion, I tend to vote for weak rejection.\n\nMinor Suggestion:\nRefer to the conference instead of arXiv if the paper was already published.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}