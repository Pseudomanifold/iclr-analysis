{"title": "Interesting approach, but unclear how far it is applicable", "review": "The authors propose to use importance resampling (IR) in place of importance sampling (IS) for policy evaluation tasks. The method proposed by the authors definitely seems valid, but it isn\u2019t quite clear when this is applicable.\n\nIR is often used in the case of particle filters and other SMC is often used to combat the so-called \u201cdegeneracy problem\u201d where a collection of particles (or trajectories) comes to degenerate such that all the mass is concentrated onto a single particle. This does not seem to be the case here, as the set of data (the replay buffer) does not seem to be changing over time. In particular, since the target policy and the behavior policy are fixed, the bigger issue seems to be that the distribution itself will not change over time.\n\nFinally, the results are given for somewhat simple problems. The first two settings show that the difference between IR/IS can be very stark, but it seems like this is the case when the distributions are very different and hence the ESS is very low. The IR methods seem like they can eliminate this deficiency by only sampling from this limited subset, but it is also unclear how to extend this to the policy optimization setting.\n\nOverall I have questions about where these results are applicable. And finally, as stated a moment ago, it is unclear how these results could be extended to the setting of off-policy policy optimization, where now the resulting policies are changing over time. This would necessitate updating the requisite sampling distributions as the policies change, which does seem like it would be difficult or computationally expensive (unless I am missing something). Note that this is not an issue with IS-based methods, because they can still be sampled and re-weighted upon sampling.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}