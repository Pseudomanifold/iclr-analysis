{"title": "Good idea on off-policy learning, but with limited analysis and experiments", "review": "In this work, the authors studied the technique of importance re-sampling (IR) for off-policy evaluation in RL, which tends to have low-biased (and it's unbiased in the bias-correction version) and low-variance. Different than existing methods such as importance sampling (IS) and weighted importance sampling (WIS) which correct the distribution over policy/transitions by an importance sampling ratio, in IR one stores the offline data in a buffer and re-samples the experience data (in form of state, action, & next-state) for on-policy RL updates. This approach avoids using importance sampling ratios directly, which potentially alleviate the variance issue in TD estimates. The authors further analyze the bias and consistency of IR, discuss about the variance of IR, and demonstrate the effectiveness of IR by comparing it with IS/WIS on several benchmark domains.\n\nOn the overall I think this paper presents an interesting idea of IR for off-policy learning. In particular it hinges on designing the sampling strategy in replay buffer to handle the distribution discrepancy problem in off-policy RL. Through this simple off-policy estimator, the authors are able to show improvements when compared with other state-of-the-art off-policy methods such as IS and WIS, which are both known to have high-variance issues. The authors also provided bias and consistency analysis of these estimators, which are reasonable theoretical contributions. The major theoretical question/concern that I have is in terms the variance comparisons between IR and IS/WIS. While I see some discussions in Sec 3.3, is there a concrete result showing that IR estimator has lower variance when compared to IS and WIS (even under certain technical assumptions)? This is an important missing piece for IR, as the original motivation of not using IS/WIS estimators is because of their issues on variance. \n\nIn terms of experiment, while the authors have done a reasonably good job evaluating IR on several domains based on the MSE of policy evaluation, to make it more complete can the authors also show the efficiency of IR when compared to state-of-the-art algorithms such as V-trace, ABQ or Re-Trace (which are cited in the introduction section)? ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}