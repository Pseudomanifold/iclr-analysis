{"title": "Nice work on nonconvex nonsmooth theory, needs more work on experiments and relation to loss landscape of neural networks mentioned in abstract", "review": "The paper provides a very nice analysis for the nonsmooth (l1) dictionary learning minimization in the case of orthogonal complete dictionaries and linearly sparse signals. They utilize a subgradient method and prove a non-trivial convergence result.\n\nThe theory provided is solid and expands on the earlier works of sun et al. for the nonsmooth case. Also interesting is the use a covering number argument with the d_E metric.\n\nA big plus of the method presented is that unlike previous methods the subgradient descent based scheme presented is independent of the initialization.\n\nDespite a solid theory developed, lack of numerical experiments reduces the quality of the paper. Additional experiments with random data to illustrate the theory would be beneficial and it would also be nice to find applications with real data.\n\nIn addition as mentioned in the abstract the authors suggest that the methods used in the paper may also aid in the analysis of shallow non-smooth neural networks but they need to continue and elaborate with more explicit connections.\n\nMinor typos near the end of the paper and perhaps missing few definitions and notation are also a small concern\n\nThe paper is a very nice work and still seems significant! Nonetheless, fixing the above will elevate the quality of the paper.\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}