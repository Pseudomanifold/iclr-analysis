{"title": "limited novelty", "review": "This paper provides an analysis on transfer learning by fine-tuning. It focuses on the importance of the relation between the content of the source dataset used for pre-training and the following target dataset on which the model will be applied. Several experiments show that this aspect is more important than the cardinality of the source data and this becomes particularly clear when dealing with fine-grained classification tasks in target.\n\nThe main technical contribution of this work is in the application of an importance weighting approach to select the data when training the source model.  The theory in section 3.3 is inherited from previous works and the only variant is in the way in which it is applied when the source and target class set is different: the authors propose a heuristic approximation of P_t(y) based on target self-labeling.\n\n- I find the technical novelty of this work very limited. Indeed here we just see a pre-existing sample-selection based unsupervised domain adaptation method applied on several datasets. \n- more details about the self-labeling procedure should be provided: of course the prediction of the source model on the target varies during model training, being very poor in the beginning and possibly improving in the following epochs. Is he sample selection active since the very first epoch?\n- The comparison in table 4 with state of the art should be better explained. Are the other baseline methods using the same AmoebaNet architecture? If not it is not the comparison might be unfair.\n\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}