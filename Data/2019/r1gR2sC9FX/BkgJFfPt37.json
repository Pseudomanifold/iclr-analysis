{"title": "Analysis of Spectral Bias of ReLU networks", "review": "Analysis of Spectral Bias of ReLU networks\n\nThe paper uses Fourier analysis to study ReLU network utilizing its continuous piecewise linear structure.\n\nMain finding is that these networks are biased towards learning low frequency which authors denote `spectral bias\u2019.  This provides another theoretical perspective of neural networks preferring more smooth functions while being able to fit complicated function. Also shows that in terms of parameters networks representing lower frequency modes are more robust. \n\nPro: \n- Nice introduction to Fourier analysis providing non-trivial insights of ReLU networks.\n- Intuitive toy experiments to show spectral bias and its properties \n- Thorough theoretical analysis and empirical support\n\nCon: \n- The analysis is clearly for ReLU networks although the title may provide a false impression that it corresponds to general networks with other non-linearities. It is an interesting question whether the behaviour characterized by the authors are universal. \n- At least for me, Section 4 was not as clearly presented as other section. It takes more effort to parse what experiments were conducted and why such experiments are provided.\n- Although some experiments on real dataset are provided in the appendix, I personally could not read much intuition of theoretical findings to the networks used in practice. Does the spectral bias suggest better way of training or designing neural networks for example?\n\nComments/Questions:\n- In Figure 1, two experiments show different layerwise behaviour, i.e. equal amplitude experiment (a) shows spectral norm evolution for all the layers are almost identical whereas in increasing amplitude experiment (b) shows higher layer change spectral norm more than the lower layer. Do you understand why and does Fourier spectrum provide insights into layerwise behaviour?\n- Experiment 3 seems to perform binary classification using thresholding to the logits. But how do you find these results also hold for cross-entropy loss?\n\u201cThe results confirm the behaviour observed in Experiment 2, but in the case of classification tasks with categorical cross-entropy loss.\u201d\n\n\nNit: p3 ReLu -> ReLU / p5 k \\in {50, 100, \u2026 350, 400} (close bracket) / p5 in Experiment 2 and 3 descriptions the order of Figure appears flipped. Easier to read if the figure appears as the paper reads / p7 Equation 11 [0, 1]^m\n\n\n********* updated review *************\n\nBased on the issues raised from other reviewers and rebuttal from authors, I started to share some of the concerns on applicability of Thm 1 in obtaining information on low k Fourier coefficients. Although I empathize author's choice to mainly analyze synthetic data, I think it is critical to show the decays for moderately large k in realistic datasets. It will convince other reviewers of significance of main result of the paper.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}