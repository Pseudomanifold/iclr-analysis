{"title": "Potentially impactful work, but lack clarity", "review": "The work releases a large-scale multimodal dataset recorded from the X-Plane simulation, as a benchmark dataset to compare various representation learning algorithms for reinforcement learning. The authors also proposed an evaluation framework based on some simple supervised learning tasks and disentanglement scores. The authors then implemented and compared several representation learning algorithms using this dataset and evaluation framework. \n\npros:\n1.  Releasing this dataset as a benchmark for comparing representation learning algorithms can potentially impact the community greatly;\n2. The authors combined several existing work on measuring representation learning algorithms and proposed an evaluation framework to evaluate the quality of learned representation using supervised learning tasks and disentanglement scores;\n3. The authors implemented an extended list of representation learning algorithms and compared them on the dataset;\n\ncons:\n1. the paper lacks clarification and guideline to convince the readers of the usefulness of the dataset and the evaluation framework. The authors spent almost half of the space explaining different existing representation learning algorithms. A more convincing story would be to find a few well-established representation learning algorithms to corroborate on the reliability of the dataset and the evaluation metrics;\n2. More details should be put into describing the dataset. It is not clear why this dataset is particularly suited for evaluating representation learning in the context of reinforcement learning. Do the authors have insight on the difficulty of the task? While having multi-modality is appreciated, it might worth thinking a separate dataset focusing on a single modality, e.g., image;\n3.  Given that the authors designed the dataset for evaluating representation learning for reinforcement learning, it is worth evaluating these algorithms on solving the main task using some standard RL techniques on top of the learned representations.\n4. Table 4 is difficult to parse. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}