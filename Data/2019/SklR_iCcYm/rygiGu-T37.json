{"title": "Clear and detailed description of the idea, but weak experimentation", "review": "Summary: \nThe manuscript introduces a dataset filtering technique for the purpose of speeding up training of machine learning models.\nThe technique filters the training set, yielding a subset of examples that are as diverse as possible, according to an autoencoder embedding of the input space. First, one trains a deep autoencoder, whose code layer is used as embedding of the input space. Then, for each element of the embedding the top k training samples are selected which activate that element. This reduced training set is then used for rapid training of the model in the first optimization stage, followed by slower fine-tuning on the complete data set. The experimental section presents a comparison of accuracies after training a simple CNN on CIFAR10 with and without the proposed data filtering under several constraints.\n\nStrengths: The proposed technique addresses the important problem of long training times. The description is very clear and detailed.\n\nWeakness:\nMy main criticism of this manuscript is that the experimentation is not nearly sufficient to support the central claim that dataset filtering via embeddings, as described in this manuscript, is a \u201cgeneral technique\u201d that \u201cany tasks [...] could in principle benefit from [...]\u201d.\nThe evidence from the presented experiment is rather weak, as only one architecture and one dataset is selected. Furthermore, there are quite a few confounding factors that I don\u2019t think are compensated by averaging the performances of four training runs. A few recommendations on how to improve the experimental section:\n- How are hyperparameters selected? For a fair comparison, separate hyperparameter searches should be performed for training with the full training set and with the filtered set. Simple hyperparameters can influence the performance strongly for a given dataset.\n- It requires extensive experimentation to \u201cshow the technique\u2019s merits as a generally applicable technique that is not bound to certain types of architectures\u201d, for instance trying different types of architectures and datasets. Of course the technique can be applied to most architectures and datasets. However, the question is whether it often helps, not whether it is technically possible. Does it for example improve convergence speed or performance in a state-of-the-art network trained on ImageNet? \n- The heavy use of data augmentation is a confounding factor which adds randomness that is not likely to be compensated by averaging a few training runs. Maybe you could present performances without augmentation.\n- You mention momentum is used. For reproducibility, it would be good to state the coefficient used.\n- Testing is performed on checkpoints with some form of weighted averaging of final weights. Could you describe the steps in detail for better reproducibility?\n- Is the result stable over multiple autoencoder trainings?\n- It would be interesting to see the performance before the finetuning stage!\n\nI feel the discussion section could benefit from a few thoughts on the limitations of this approach. For instance, the method might not be the best choice for highly imbalanced classification datasets. Literature on dataset resampling for such scenarios might be worth mentioning in the related work section. Also, the autoencoder\u2019s embeddings are trained to reconstruct the whole image, an objective that gives more importance to patterns that occupy a larger portion of the image. If the downstream task needs attention to detail (e.g. counting of small objects, segmentation in remote sensing or medical imaging, street-number or road-sign detection), the filtering method might also not be much better than random subsampling.\n\nThe related work section could also be improved. I see only one work on data set optimization. I\u2019ve seen work using a reducedMNIST dataset, which is probably created by random subsampling, but still more relevant than many of the aspects of embeddings cited in this section (the paragraph about arithmetic operations for instance). Katharopoulos and Fleuret (2018) seems like highly relevant recent work, which should be cited and contrasted against. The evaluation in that work seems very thorough in comparison.\n\nA general recommendation on writing: Try to limit the content to relevant details. For example, a description of hardware specifics (support for NVLink, which is not used) or stating the well-established speed-up when using GPUs for CNNs are not relevant.\n\nFigure 6 could be improved by marking on the time axis, when the fine-tuning sets in.\n\nTo summarize my feedback, I like most of the presentation and it is good to see effort towards reducing training times by selecting good training samples, but I think the manuscript requires significant effort to justify acceptance.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}