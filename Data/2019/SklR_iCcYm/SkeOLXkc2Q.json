{"title": "Training data subset selection to speed up training", "review": "This paper proposes a way to speed up initial training a model.  The key idea\nis to:\n\n1. Train an autoencoder on the full dataset and select a subset of training \nexamples.  The subset is the union of examples that maximally activate each of\nthe dimensions of the autoencoder's low-dimensional embedding.\n\n2. Then a target classifier is trained on the subset,\n\n3. followed by final fine-tuning on the full dataset.\n\nThe paper is understandably written, although some crucial experimental details\nneed a bit of guesswork.\n\nTheir proposal is evaluated on only one dataset, CIFAR10, using an autoencoder  \nand classifier of roughly similar design from the initial convolutional layers. \n\nThey mention a baseline [classifier training, I presume] classifier training \nover ~200 epochs in 736 s (~12 min) to get 83% accuracy.  This skips steps 1. \nand 2.  Since this is already fast, CIFAR10 is perhaps too small a dataset\nto spur readers to use their proposed method (which does require them to\nadditionally train an autoencoder) when tackling more ambitious problems.\n\nThey do not report the time taken to train their autoencoder for 800 epochs\n(step 1.).  For larger networks and images, it might also be important to\ninvestigate whether an autoencoder considerably simpler than the classifier\nmodel can suffice for subset selection; for example, if I want to train a\nResnet-152 classifier can I use a poorer quality autoencoder?  Since\nusing a randomly selected subset 20% of the original size works about\nas well as step 1 for CIFAR10, I cannot judge whether the time taken to\nset up and train an autoencoder makes it worthwhile to further reduce\nthe training subset from 20% to ~8% of the original size.\n\nThey do not consider alternative subset selection (1.) methods.  For example,\none might use a pretrained network to select examplar images by a clustering\nmethod (ex. [2]), possibly providing representative images per class.  Other \nselection criteria are also possible -- for example, [1] evaluates subset \nselection based on \"representativeness\" vs \"diversity\" criteria.\n\nThey do not compare with many existing approaches to training set compression.\nInstead, they dismiss (Sec. 3 \"Related Work\") most previous work on selecting a \nsmall subset of training examples.  However, googling will quickly find many\npapers on subset selection (exactly what they do) as well as related dataset\noptimization techniques (such loss-based revisiting of training examplars, or\ntraining example weighting etc.).  For example, review-type article [3] \nprovides a good introduction to existing subset selection techniques, as well\nas references to earlier papers.\n\nIt is unclear whether the autoencoder training time is included in their\nexperiments that fix the total training time to 7 minutes and compare results\nwith different numbers of fast epochs (step 2.).\n\nNo guidelines are given for how to select the dimensionality of the autoencoder\nembedding, and how the selection procedure should be done in cases with large\nnumbers of classes, although they mention the possibility of using combinations\nof activations for subset selection.  I do not understand how in problems with\nlarger numbers of classes I can guarantee that the training subset will contain\nat least one representative from each class.  Some alternative subset selection\nmethods can provide such guarantees, which might be important for training\ndatasets with class imbalance.\n\nGiven that they do not use a very large dataset, where their technique would\nreally be needed, and that they provide no comparison with other possibly\nfaster and better ways to select a subset of training examples, I cannot argue\nfor acceptance of this paper.\n\n\n[1] \"Learning From Less Data: Diversified Subset Selection and\nActive Learning in Image Classification Tasks\", Kaushal et al.\nhttps://arxiv.org/abs/1805.11191\n\n[2] Li, D., & Simske, S. (2011). Training set compression by incremental\nclustering. Journal of pattern recognition research, 1, 56-64.\n\n[3] Borovicka, T., Jirina Jr, M., Kordik, P., & Jirina, M. (2012). Selecting\nrepresentative data sets. In Advances in data mining knowledge discovery and\napplications. InTech.\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}