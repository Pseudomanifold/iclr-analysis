{"title": "The paper needs major revisions, theorems are disjoint with few explanations.", "review": "The paper aims to connect \"distributionally robust optimization\" (DRO) with stochastic gradient descent. The paper purports to explain how SGD escapes from bad local optima and purports to use (local) Rademacher averages (actually, a  generalization defined for the robust loss) to explain the generalization performance of SGD.\n\nIn fact, the paper proves a number of disjointed theorems and does very little to explain the implications of these theorems, if there are any. The theorem that purports to explain why SGD escapes bad local minima does not do this at all. Instead, it gives a very loose bound on the \"robust loss\" under some assumptions that actually rule out ReLU networks.\n\nThe Rademacher results for robust loss looked promising, but there is zero analysis suggesting why these explain anything. Instead, there is vague conjecture. The same is true for the local Rademacher statements. It is not enough to prove a theorem. One must argue that it bears some relationship to empirical performance and this is COMPLETELY missing.\n\nOther criticisms:\n\n1. One of the first issues to arise is that the definition of \"generalization error\" is not the one typically used in learning theory. Here generalization error is used for what is more generally called the risk.  Generalization error often refers to the difference R(theta) - ^R(theta) between the risk and the empirical risk (i.e., the risk evaluated against the empirical distribution). (Generally this quantity is positive, although sometimes its absolutely values is bounded instead.)  \n\nAnother issue with the framing is that one is typically not interested in small risk in absolute terms, but instead small risk relative to the best risk available in some class (generally the same one that is being used as a source of classifiers). Thus one seeks small excess risk. I'm sure the authors are aware of these distinctions, but the slightly different nomenclature/terminology may sow some confusion.\n\n2. The unbiased estimate suggested on page 2 is not strictly speaking an estimator because it depends on \\lambda_0, which is not measurable with respect to the data. The definition of K and how it relates to the estimate \\hat \\lambda is vague. Then the robust loss is introduced where the unknown quantity is replaced by a pre-specified collection of weights. If these are pre-specified (and not data-dependent), then it is really not clear how these could be a surrogate for the distribution-dependent weights appearing in the empirical distributionally robust loss.\n\nPerhaps this is all explained clearly in the literature introducing DRO, but this introduction leaves a lot to be desired.\n\n3. \"This interpretation shows a profound connection between SGD and DRO.\" This connection does not seem profound to a reader at this stage of the paper.\n\n4. Theorem 2 seems to be far too coarse to explain anything. The step size is very small and so 1/eta^2 is massive. This will never be controlled by 1/mu, and so this term alone means that there is affectively no control on the robust loss in terms of the local minimum value of the empirical risk.\n\n5. There seems to be no argument that robustness leads to any improvement over nonrobust... at least I don't see why it must be true looking at the bounds. At best, an upper bound would be shown to be tighter than another upper bound, which is meaningless.\n\n\nCorrections and typographical errors:\n\n1. There are grammatical errors throughout the document. It needs to be given to a copy editor who is an expert in technical documents in English.\n\n2. \"The overwhelming capacity ... of data...\" does not make sense. The excessive complexity of the sentence has led to grammatical errors.\n\n3. The first reference to DRO deserves citation.\n\n4. It seems strange to assume that the data distribution P is a member of the parametric model M. This goes against most of learning theory, which makes no assumption as to the data distribution, other than the examples being i.i.d.\n\n5. You cite Keskar (2016) and Dinh (2017) around sharp minima. You seem to have missed Dziugaite and Roy (2017, UAI) and Neyshabur et al (NIPS 2017), both of which formalize flatness and give actual generalization bounds that side step the issue raised by Dinh.\n\n6. \"not too hard compared\" ... hard?\n\n7. Remove \"Then\" from \"Then the empirical robust Rademacher...\". Also removed \"defined as\" after \"is\".\n\n8. \"Denote ... as an\" should be \"Let ... denote the...\" or \"Denote by ... the upper ...\"\n\n9. \" the generalization of robust loss is not too difficult\" ... difficult? \n\n10. \"some sort of \u201csolid,\u201d \" solid?\n\n11. \"Conceivably, when m and c are fixed, increasing the size of P reduces the set \u0398c\". Conceivably? So it's not necessarily true? I don't understand the role of conceivably true statements in a paper.\n\n[This review was requested late in the process due to another reviewer dropping out of the process.]\n\n[UPDATE] Authors' response to my questions did not change my opinion about the overall quality of the paper. Both theory and writing need a major revision. ", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}