{"title": "Selective Self-Training for semi-supervised Learning", "review": "This is an novel, interesting paper on an important topic: semi-supervised learning.\nEven though the proposed approach seems to have significant potential, the experimental\nis somewhat disorganized,  and it also includes some weak claims that should be removed. \n\nFor example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).  In this reviewer's opinion, it would be a lot more reasonable to have instead a learning curve showing the results for, say, 100, 500, 1K, 5K, and 10K labeled examples for all three domains.\n \nIn 4.1, you are using different epsilon policies for synthetic vs organic datasets; why?\n\nThe explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for \"stratified SSL.\" Without this extra work, your claim is just a conjecture.\n\nYou should also show the performance of regular SSL methods in the setup on Table 4.\n\nLast but not least, you have repeatedly made the claim combining SST and other SSL may further improve the performance;\nhowever, you do not provide any evidence for it, so you should avoid making such claims.   \n\nOther comments:\n- on page 2, the two terms classification & selection network appear \"out of the blue;\" it would be quite helpful to make it clear from the abstract that the proposed implementation is for neural networks.\n- figures 2 & 3 should be a lot larger in order to be readable\n- 4.1.2 top of page 7: claims such as \"SST could have obtained better performance\" have no place in such a paper; you could instead make a note about the method being \"prohibitively CPU intensive for the time being\"\n- lower on the same page you say: \"SST may get better performance\" - see above\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}