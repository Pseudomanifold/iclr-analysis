{"title": "Okay paper, but needs more substantiation", "review": "This paper tackles the classical exploration / exploitation problem in reinforcement learning.\nThe paper argues that it is necessary to propagate uncertainty correctly and argue that they can do so using the successor representation to compute the Bayesian posterior over Q-values conditioned on the data already observed.\n\nNovelty:\nThis work is similar to \u201cThe uncertainty bellman equation\u201d (UBE) (O\u2019Donoghue et al. 2018) which adds a head to a regular DQN agent to predict a function u which is an upper bound of the variance of the posterior distribution over Q-values. The difference here is that the successor features are used here to predict Q-values and the function u.\nThe successor features can be seen as a discounted state occupancy of the current policy and carry information about the future. While the relation with the UBE is highlighted in section 4.6 an empirical evaluation between the two methods would also be needed.\n\nClarity: The method is detailed comparing to contextual bandit methods, the authors then argue that applying directly these methods to reinforcement learning case does not propagate uncertainty over several timesteps. While this indeed true it is misleading and imply that propagating uncertainty is not considered by current exploration methods in RL.\n\nSoundness:\nThe method presented here is relatively reductive. The estimated posterior over the value function is correct only if the transition model P is already known, otherwise Equation 4 would also need to incorporate uncertainty over P. Similarly, as the authors point out, this doesn\u2019t include the max operation. At best, we are learning a posterior over the value function for a fixed policy, for when only the reward is unknown.\n\nAs a whole, the authors argue that their method allows a better propagation of Q-values uncertainty but provide little theoretical or experimental evidence that would back this claim.\n\nFrom a deep RL perspective, the features \\phi^l only carry local information. The authors argue that this leads to more stable features as these feature do not depend on the current policy. However it also means that in a sparse reward setting the reward observed would be zero most of the time and no useful features would be learned. In practice methods using the successor representation usually share parts of the network with other tasks to improve representation learning (see e.g. Figure 1 of Machado et al., Eigenoption discovery through the deep successor representation, 2017, & also their 2018 paper).\n\nExperiments:\nThe experiment are disappointing as they are only limited to tabular and deterministic problems. An obvious missing comparison is to UBE, at the minimum; and other \u201cdeep\u201d algorithms such as BDQN, Bootstrap DQN, etc. Some of these algorithms have been shown to perform well on the Atari benchmark, and that seems like a reasonable point of comparison also.\n\nThe method also relies on knowing the successor features. While they can be learned easily in a tabular, deterministic MDP it is not clear how the posterior would behave in larger and/or non stochastic domains when it takes more time to learn these successor features.\n\nOverall, I am not sure what I learned from reading this paper. While the idea of using the successor representation in exploration is interesting and has been considered recently, the method presented in this paper needs to be better justified and evaluated on more challenging tasks.\n\n\nMinor comments\nI would like to see a proof of Equation 4, which may be simple but is not immediate.\n\nSome papers of relevance here:\n\nAn analysis of model-based Interval Estimation for Markov Decision\nProcesses, Strehl & Littman (2008)\n(More) efficient reinforcement learning via posterior sampling. Osband et al (2013)\nCount-Based Exploration with the Successor Representation, Machado et al. (2018)\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}