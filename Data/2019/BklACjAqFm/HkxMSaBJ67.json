{"title": "Exploration using successor features representations", "review": "The paper proposes an exploration approach (either based on posterior sampling or optimism) based on successor features representation. A high probability ellipsoid confidence set (defined by the Gram matrix \\Sigma_t) is estimated based on linear regression (using some features \\phi) to the immediate reward function. Now for any policy \\pi, a confidence interval for the Q-value of any policy \\pi can be derived by application of the \\psi transformation, where \\psi = expected sum of discounted future \\phi under \\pi. The algorithm selects action by posterior sampling (or UCB) in the \\psi-space. \n\nIt would be interesting to see if this approach would converge to a good policy, maybe by doing a regret-based analysis. \nUnfortunately there is no such analysis in the paper. However my main complaint is the soundness of the approach, for two reasons:\n- First it is not clear that the uncertainty in the Q-values decreases with time. Indeed the uncertainty on Q^{\\pi}(s,a) corresponds to the width of the confidence ellipsoid in the direction of the successor features \\psi^{\\pi}(s,a). However, although we know that the uncertainty shrinks in the directions of the features \\phi_t (when action a_t is chosen in state s_t) because we do regression of the reward function, we do not have the same property for \\psi_t, which defines the Q-function. And it is not obvious that the confidence set in the direction of \\psi_t would shrink at all. Thus it could be the case that the uncertainty on the Q-values will never decrease. \n- Second, since the successor features are learnt on-policy, the uncertainty on the Q-values (assuming we can estimate them) corresponds to a mixture of the policies which have been used in the past, but not to the policy that will be used from there on, because the policy is non stationary (since the uncertainty decreases as more information is collected). I would recommend to be very careful when defining and using the successor features by emphasizing the policy under which those features are defined. \n\nSo in the end the contribution is mainly algorithmic. However I find it hard to say anything about the proposed approach, whether it improves over previous ones or not, specially because the experiments are limited to toy problems. Theoretical analysis or more complex experiments would make the paper stronger.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}