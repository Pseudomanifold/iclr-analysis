{"title": "A Bayesian method to improve probability calibration in deep neural nets, but lacks insights about why the method works well.", "review": "This paper presents an approach for calibrating the predictions of deep neural networks. The idea is quite simple and straightforward - simply use a more expressive model (Bayesian neural network with amortized inference). Surprisingly, the results show that this simple approach outperforms many of the recent approaches, such as those based on temperature scaling. A trick that they use is to control the KL term in the approximation of the ELBO using a hyperparameter (but this has been used in prior work on Bayesian neural nets).\n\nOverall, the idea as such is not that novel (just applying a Bayesian neural network with amortized inference) but the results look quite impressive. However, although the paper seems to advocate that a simple Bayesian neural network is enough to get well-calibrated probabilistic predictions, recent work has shown that even Bayesian uncertainties may be inaccurate, especially in case of model mis-specifications or due to the use of approximate inference. For example, see  \"Accurate Uncertainties for Deep Learning Using Calibrated Regression\" ( Kuleshov et al, 2018). \n\nIt is quite surprising that the proposed approach works so well but there isn't much of an insight as to why it works well. Is it the amortized inference that helps, or something else? I think a more detailed analysis needs to be done. Even some empirical analysis that, for example, shows that using MCMC gives inferior results than amortized inference would help here.\n\nBesides, I would also like to point out that there is some recent work on trainable calibration measures. See \"Trainable Calibration Measures For Neural Networks From Kernel Mean Embeddings\" (Kumar et al, 2018). It would be good to discuss this.\n\nOverall, the paper has a rather straightforward idea which seems to give good results. However, it doesn't offer any new insights as to why it works, especially since recent work, such as the one I mentioned above, has shown that taking a simple Bayesian approach that provides uncertainty estimates doesn't quite address the problem being studied here.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}