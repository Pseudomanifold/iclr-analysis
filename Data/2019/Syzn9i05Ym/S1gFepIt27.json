{"title": "appears effective, though unfamiliar with this type of model", "review": "This paper describes a method of training NRFs with auxiliary generator networks, using an error that minimizes KL(NRF || generator).  This formulation enables the use of iterative gradient-based stochastic sampling of image samples from the model distribution using SGLD/SGHMC.  Applications to both unsupervised sample generation and semi-supervised classification are evaluated.\n\nI'm not very familiar with these types of NRFs or random sampling techniques, but the approach appears sound and is evaluated rather well.  I would have liked some more background and explicit description and contrast compared to the explicit NRF.  While this is described already, I think the contrasts could potentially be spelled out even more explicitly, particularly in the descriptions of the sampling algorithms.\n\nThe toy example with mixture of gaussians is convincing showing the contrast in results between the exclusive NRF, inclusive, and sampling gradient revision steps.\n\nExperimental evaluations on MNIST, SVHN and CIFAR show that the system obtains performance similar to SOA generative systems, in both semi-supervised classification and sample generation.\n\n\nQuestions and comments:\n\n- While the paper claims the results show classification and generation performance are complementary, Table 3 appears to validate the opposite claim, that these are to a large degree a trade-off.  The fact that this system performs well at both is good, but to me it looks like it may be on the \"shoulder\" of a frontier curve if one were to plot the classification vs generation performance of the different current systems.\n\n- Table 4 and sec 4.4:  I think these could be clearer.  The first observation states that revision improves IS.  But using more iterations (increasing L) does not appear to increase IS.  There does appear to be a consistent increase from the first column (generation) to second (revision), though -- is this what this observation refers to?  In addition, I'm not entirely clear what the \"Generation IS\" vs \"Revision IS\" column refers to --- I believe \"generation\" is the initial sampling of x=g(h) (i.e. h followed by q(x | h)), and \"revision\" is the application of gradient revision.  But then how does the generation IS results change from row to row (which only modify the revision step)?\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}