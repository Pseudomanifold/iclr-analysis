{"title": "Limited setting of directional RL, but interesting approach and results.", "review": "This paper introduces policy gradient methods for RL where the policy must choose a direction (a.k.a., the navigation problem).\n\nMapping techniques from \"non-directional\" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big). The authors propose to sample directly on the sphere, using the fact that the likelyhood of an angular Gaussian r.v. has *almost* a closed form and its gradient can almost be computed, up to some normalization term (the integral which is constant in the standard Gaussian case).\n\n\nThis can be seen as a variance reduction techniques.\n\nThe proofs are not too intricate, for someone used to variance reduction (yet computations must be made quite carefully).\n\n\nThe result is coherent, interesting from a theoretical point of view and the experiment are somehow convincing. The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}