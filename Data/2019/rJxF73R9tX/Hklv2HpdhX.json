{"title": "Re: Abstention classifiers", "review": "This manuscript introduces deep abstaining classifiers (DAC) which modifies the multiclass cross-entropy loss with an abstention loss, which is then applied to perturbed image classification tasks.  The authors report improved classification performance at a number of tasks.\n\nQuality\n+ The formulation, while simple, appears justified, and the authors provide guidance on setting/auto-tuning the hyperparameter.\n+ Several different settings were used to demonstrate their modification.\n- There are no comparisons against other rejection/abstention classifiers or approaches.  Post-learning calibration and abstaining on scores that represent uncertainty are mentioned and it would strengthen the argument of the paper since this is probably the most straightforward altnerative approach, i.e., learn a NN, calibrate predictions, have it abstain where uncertain.\n- The comparison against the baseline NN should also include the performance of the baseline NN on the samples where DAC chose not to abstain, so that accuracies between NN and DAC are comparable. E.g. in Table 1, (74.81, coverage 1.000) and (80.09, coverage 0.895) have accuracies based on different test sets (partially overlapping).\n- The last set of experiments adds smudging to the out-of-set (open set) classification tasks.  It is somewhat unclear why smudging needs to be combined with this task.\n\nClarity\n- The paper could be better organized with additional signposting to guide the reader. \n\nOriginality\n+ Material is original to my knowledge.\n\nSignificance\n+ The method does appear to work reasonably and the authors provide detail in several use cases.\n- However, there are no direct comparison against other abstainers and the perturbations are somewhat artificial.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}