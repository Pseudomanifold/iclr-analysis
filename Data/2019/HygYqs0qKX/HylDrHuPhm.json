{"title": "Paper review", "review": "The paper proposes a iterative approach at inference time to improve object detections. The work relies on updating the feature activations and perform new feed forward passes to obtain improved results. \n\nPros:\n(+) The idea of iterative inference is potentially effective\n(+) The paper is well written and clear\n(+) The authors show results on compelling benchmarks\nCons:\n(-) Reported improvements are very small\n(-) Important baselines are missing\n\n\nFirst, while the authors state correctly that their updates have no memory cost and no new parameters are added, they do require more FLOPs at test time. For N-stages, the approach requires xN more operations for forward passes  and xN for backward passes. This is a serious shortcoming as it adds compute time per image for the inference stage and cannot be parallelized. \n\nThe authors show small improvements for AP on COCO. From their analysis, it seems that the biggest gains come from N=1 stages, while the improvement added for N>1 are miniscule (Table 1). Note that the authors show results on COCO minival (5k images) and from my experience there, it's expected to see a +/- 0.2% AP between different trained models of the same architecture. The authors report a +0.46% gain. \n\nIn addition, the authors do not provide results for other baseline approaches that have similar FLOPs at test time, such as iterative bounding box regression and input scale augmentation. Note that both these approaches do not add any parameters and require no additional memory, but add to the FLOPs at test time. From my personal experience, test time augmentations can add +1.5% to the final performance. Concretely, look at Mask R-CNN arXiv Table 8 last two rows. Test time augmentations add 1.5% on top of an already enhanced model. Empirically, the better the model the harder it is to get gains from inference tricks! And still test time augmentations boost performance significantly.\n\nGiven the small gains and the lack of competing baselines, it is hard to make a case for accepting the paper. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}