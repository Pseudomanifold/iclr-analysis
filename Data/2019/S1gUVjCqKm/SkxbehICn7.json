{"title": "Maybe an interesting paper but many parts are unclear", "review": "This paper develops an unsupervised classification algorithm using the idea of CycleGAN. Specifically, it constructs a piece-wise linear mapping (the Connector Network) between the discriminator network and the generator network. The learning objective is based on the cycle-consistency loss. Experiments show that it can achieve reasonable loss. This paper addresses an important problem, namely, unsupervised image classification, and may present interesting ideas. However, the paper is not in a good shape for publication in its current form.\n\nFirst, the paper is not well written and many of the key ideas are not clear. It devotes more than half of the pages to the review of the preliminary materials in Sections 2-3 while only briefly explained the main algorithm in Section 4. Many of the details are missing. For example, why L1-loss is used in (5)-(7) in Algorithm 1? What is the \u201crandom-walk Laplacian matrix L_{sym}\u201d (never defined)? More importantly, it seems that Section 3.4 is a key section to explain how to perform unsupervised classification. However, the ideas (regarding the footprints and footprint mask etc.) are totally unclear. It is assumed that all different classes have equal probabilities. In this setting, it is unclear (in its intuition) why it is possible to assign a cluster index to its true class labels. What is the key signal in the data that enables the algorithm to relate different clusters to their corresponding classes, especially when all classes have equal probability? Furthermore, it is not clear why the mapping from H to Z can be written as a sum of C_1,\u2026,C_k in Proposition 3.2. If the final mapping is piece-wise linear, how can it be written as a sum of linear mappings? Similar question arises in the first paragraph of Section 4.1: if the connector network is constructed as a piecewise linear function (as stated earlier in the paper in abstract and introduction), then how can it be written as a matrix? (Only linear mapping can be expressed as a matrix.)\n\nSecond, the experiment is insufficient. None of the experiment details are presented in the paper. Only the final accuracy of 0.874 is given without any further details. What is the model architecture and size? More experimental analysis should be presented. For example, there are many different hyperparameters in the algorithms. How are the \\lambda_D, \\lambda_G chosen when there is no labeled validation set? How sensitive is the algorithm to different model architecture and model size? Furthermore, none of the baselines results are presented and compared against.\n\nA lot of related works are missing. There have been a lot of emerging works related to unsupervised classification recently, which should be discussed and compared:\n[1] G. Lample, L. Denoyer, and M. Ranzato.  Unsupervised machine translation using monolingual corpora only. ICLR, 2018.\n[2] M. Artetxe, G. Labaka, E. Agirre, and K. Cho.  Unsupervised neural machine translation. ICLR, 2018.\n[3] Y. Liu, J. Chen, and L. Deng.  Unsupervised sequence classification using sequential output statistics. NIPS, 2017\n[4] A. Gupta, A. Vedaldi, A. Zisserman. Learning to Read by Spelling: Towards Unsupervised Text Recognition. arXiv:1809.08675, 2018.\n[5] G. Lample, M. Ott, A. Conneau, L. Denoyer, M. Ranzato. Phrase-based & neural unsupervised machine translation. EMNLP 2018.\n\nThe presentation of the paper should be significantly improved as it is currently hard to read due to many grammar and English usage issues as well as other unclear statements. Just to name a few examples below:\n-\t(1st paragraph of Introduction): \u201c\u2026imagine the learned objectconstruct\u2026\u201d\n-\tThe last paragraph in Section 1 is not in the right position and should be placed somewhere else in the introduction.\n-\tIn the first paragraph of Section 2.2, \u201cone of the clustering algorithm\u201d should be \u201cone of the clustering algorithms\u201d.\n-\tIn the first paragraph of Section 3, it is not clear what it means by \u201cwe can make the tuples (Z,X,H) for the whole dataset\u201d.\n-\tAt the end of the first paragraph of Section 3, there is a missing reference in \u201cnetwork in section()\u201d.\n-\tIn the third paragraph on page 4, there is a grammar issue in \u201cH and Z have greater than convexity than X\u2026\u201d and in \u201cit allows the linear combination on these two manifolds in the feature spaces H and Z are and\u201d.\n-\tIn the first paragraph of Section 3.2, it is not clear what it means by \u201ctwo feature spaces will be trained by the cycle consistency loss to obtain the tuples (Z,X,H) with the correct permutation, where all of elements is in the same-class manifold and shares same learned features.\u201d\n-\tIn the first paragraph of page 5, \u201ccycle-consistency loss z C(D(G(z))) and backward cycle consistency loss x G(C(D(x)))\u201d does not read well. It sounds like z is the loss C(D(G(z))) and x is the loss G(C(D(x)))?\n-\tTypo in Figure 4: \u201ca shows\u201d should be \u201c(a) shows\u201d.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}