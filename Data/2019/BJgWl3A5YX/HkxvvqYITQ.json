{"title": "More experiments are needed to show the effectiveness and novelty", "review": "This paper discuss the joint evolution of controllers and topologies of agents.\nThe PEOM algorithm in the paper incorporates Shapley value to accelerate the evolution by identifying contribution of each body part.\n\nSide note: I am an added emergency reviewer due to the missing of reviewer 2.\nSorry for the caused trouble. I am already many days late, so I try to make this review available as soon as possible.\nAlthough I would not say that I understand the paper perfectly,\nthis review should cover the contents enough for the authors\u2019 rebuttal. And I will probably update the reviews in the following days.\n\n*******    Pros    *******:\n---- I really enjoy the idea of identifying the contribution of each body parts. I believe it should be a good direction to go for research on automatic robot design or creature evolution.\n---- The topic of joint policy & morphology learning is interesting and deserves more attention.\n \n*******    Cons    *******\n---- The contribution and novelty of the paper is relatively limited other than the use of Shapley value.\n---- The experiment session really undermines the value of this paper. (see details below).\n---- It is not clear what information is delivered in the paper, in terms of what components of PEOM are beneficial and what are not. \n---- This paper could benefit from better writing. (see details below)\n\n*******    Detailed comments and suggestions    ******* \n---- Why does the author start with a multi-agent competing environment like sumo?\nAs the author also pointed out in the paper, \"Further, changes in the agent\u2019s own body also contribute to the non-stationary nature of the environment\",\nit seems to me that the algorithm should be first tested in a single agent environment.\nThe introduction of multi-agent and Elo rating, seems to me, complicates the problem.\n---- It will be great if the author could well introduce and formulate the objective function at the beginning method section.\n---- (This is not a criticism) For the training algorithm of the controllers, why do the authors choose SVG?\nThere are other more stable and well-used algorithms (both model-free and model-based) available, for example TRPO / PPO / PETS algorithm.\n---- For agent i to inherit policy network from agent j by cross-over, how does it work exactly?\nIn the paper it is simply said that \u201cand body configuration parameters are set to the target j\u2019s configuration.\u201d \nIf the agent i and agent j has different topologies, then parameters for network i and network j should not be transferable.\n---- Shapley value seems to a core idea of PEOM. For the better presentation of this paper, it will be great if a background section (with math) on Shaley value is included.\n---- Ablation study on Shapley value is needed.\nIt seems to me that the introduction of Shapley value also introduce the problem of falling into local-minima (more easily).\nMy personal understanding of the difficulty of creature evolution is that, it is easily stuck in local-minimas.\nFor example, (for a two-legged under-evolved ant) evolving only one leg might be bad, as it adds more difficulty to controlling and introduce unbalancing.\nBut adding two symmetrical leg will boost the performance.\nHowever, Shapley value might discourage evolving a leg.\nHow does POEM overcome this?\n---- More experiments are needed.\nCurrently only one environment type is tested (the ant). More experiments will greatly increasing the quality of the paper.\n---- It seems that POEM does not evolve creatures of different topologies by looking at the youtube video.\n\n\n---- General comment ---- \n\nIn general, I believe more experiments and better presentation is needed for this paper.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}