{"title": "Clearly written, but lacking comparisons", "review": "The focus on novelty (mentioned in both the abstract, and conclusion as a direct claim) in the presentation hurts the paper overall. Without stronger comparison to other closely related work, and lack of citation to several closely related models, the claim of novelty isn't defined well enough to be useful. Describing what parts of this model are novel compared to e.g. Stochastic WaveNet or the conditional dilated convolutional decoder of \"Improved VAE for Text ...\" (linked below, among many others) would help strengthen the novelty claim, if the claim of novelty is needed or useful at all. Stochastic WaveNet in particular seems very closely related to this work, as does PixelVAE. In addition, use of autoregressive models conditioned on (non-variational, in some sense) latents have been shown in both VQ-VAE and ADA among others, so a discussion would help clarify the novelty claim.\n\nEmpirical results are strong, though (related to the novelty issue) there should be greater comparison both quantitatively and qualitatively to further work. In particular, many of the papers linked below show better empirical results on the same datasets. Though the results are not always directly comparable, a discussion of *why* would be useful - similar to how Z-forcing was included.\n\nIn the qualitative analysis, it would be good to see a more zoomed out view of the text (as in VRNN), since one of the implicit claims of the improvement from dense STCN is improved global coherence by direct connection to the \"global latents\". As it stands now the text samples are a bit too local to really tell. In addition, the VRNN samples look quite a bit different than what the authors present in their work - what implementation was used for the VRNN samples (they don't appear to be clips from the original paper)? \n\nOn the MNIST setting, there are many missing numbers in the table from related references (some included below), and the >= 60.25 number seems so surprising as to be (possibly) incorrect - more in-depth analysis of this particular result is needed. Overall the MNIST result needs more description and relation to other work, for both sequential and non-sequential models.\n\nThe writing is well-done overall, and the presented method and diagrams are clear. My primary concern is in relation to related work, clarification of the novelty claim, and more comparison to existing methods in the results tables. \n\nVariational Bi-LSTM https://arxiv.org/abs/1711.05717\n\nStochastic WaveNet https://arxiv.org/abs/1806.06116\n\nPixelVAE https://arxiv.org/abs/1611.05013\n\nFiltering Variational Objectives https://github.com/tensorflow/models/tree/master/research/fivo\n\nImproved Variational Autoencoders for Text Modeling using Dilated Convolutions https://arxiv.org/abs/1702.08139\n\nTemporal Sigmoid Belief Networks for Sequential Modeling http://papers.nips.cc/paper/5655-deep-temporal-sigmoid-belief-networks-for-sequence-modeling\n\nNeural Discrete Representation Learning (VQ-VAE) https://arxiv.org/abs/1711.00937\n\nThe challenge of realistic music generation: modelling raw audio at scale (ADA) https://arxiv.org/abs/1806.10474\n\nLearning hierarchical features from Generative Models https://arxiv.org/abs/1702.08396\n\nAvoiding Latent Variable Collapse with Generative Skip Models https://arxiv.org/abs/1807.04863\n\nEDIT: Updated score after second revisions and author responses", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}