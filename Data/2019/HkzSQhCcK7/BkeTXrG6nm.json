{"title": "Ok paper with a reasonable -- though somewhat obvious -- approach to generative modeling of sequence data", "review": "This paper presents a generative sequence model based on the dilated CNN\npopularized in models such as WaveNet. Inference is done via a hierarchical\nvariational approach based on the Variational Autoencoder (VAE). While VAE\napproach has previously been applied to sequence modeling (I believe the\nearliest being the VRNN of Chung et al (2015)), the innovation where is the\nintegration of a causal, dilated CNN in place of the more typical recurrent\nneural network. \n\nThe potential advantages of the use of the CNN in place of\nRNN is (1) faster training (through exploitation of parallel computing across\ntime-steps), and (2) potentially (arguably) better model performance. This\nsecond point is argued from the empirical results shown in the\nliterature. The disadvantage of the CNN approach presented here is that\nthese models still need to generate one sample at a time and since they are\ntypically much deeper than the RNNs, sample generation can be quite a bit\nslower.\n\nNovelty / Impact: This paper takes an existing model architecture (the\ncausal, dilated CNN) and applies it in the context of a variational\napproach to sequence modeling. It's not clear to me that there are any\nsignificant challenges that the authors overcame in reaching the proposed\nmethod. That said, it certainly useful for the community to know how the\nmodel performs.\n\nWriting: Overall the writing is fairly good though I felt that the model\ndescription could be made more clear by some streamlining -- with a single\npass through the generative model, inference model and learning. \n\nExperiments: The experiments demonstrate some evidence of the superiority\nof this model structure over existing causal, RNN-based models. One point\nthat can be drawn from the results is that a dense architecture that uses multiple levels of the\nlatent variable hierarchy directly to compute the data likelihood is\nquite effective. This observation doesn't really bear on the central message\nof the paper regarding the use of causal, dilated CNNs. \n\nThe evidence lower-bound of the STCN-dense model on MNIST is so good (low)\nthat it is rather suspicious. There are many ways to get a deceptively good\nresult in this task, and I wonder if all due care what taken. In\nparticular, was the binarization of the MNIST training samples fixed in\nadvance (as is standard) or were they re-binarized throughout training? \n\nDetailed comments:\n- The authors state \"In contrast to related architectures (e.g. (Gulrajani et\nal, 2016; Sonderby et al. 2016)), the latent variables at the upper layers\ncapture information at long-range time scales\" I believe that this is\nincorrect in that the model proposed in at least Gulrajani et al also \n\n- It also seems that there is an error in Figure 1 (left). I don't think\nthere should be an arrow between z^{2}_{t,q} and z^{1}_{t,p}. The presence\nof this link implies that the prior at time t would depend -- through\nhigher layers -- on the observation at t. This would no longer be a prior\nat that point. By extension you would also have a chain of dependencies\nfrom future observations to past observations. It seems like this issue is\nisolated to this figure as the equations and the model descriptions are\nconsistent with an interpretation of the model without this arrow (and\nincluding an arrow between z^{2}_{t,p} and z^{1}_{t,p}.\n\n- The term \"kla\" appears in table 1, but it seems that it is otherwise not\ndefined. I think this is the same term and meaning that appears in Goyal et\nal. (2017), but it should obviously be defined here.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}