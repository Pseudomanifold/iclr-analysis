{"title": "Attacking an interesting but mostly solved problem with weak baselines and questionable ML techniques", "review": "The authors consider the problem of generating diverse translations from a neural machine translation model. This is a very interesting problem and indeed, even the best models lack meaningful diversity when generating with beam-search. The method proposed by the authors relies on prefixing the generation with discrete latent codes. While a good general approach, it is not new (exactly the same general approach that was used in the \"Discrete Autoencoders for Sequence Models\" [1] paper, https://arxiv.org/abs/1801.09797, for generating diverse translations, which is not cited directly but a follow-up work is cited, though without mentioning that a previous work has tackled the same problem). Also, the authors rely on additional supervised data (namely POS tags) which has no clear motivation and seems to cause a number of problems -- why not use a purely unsupervised approach when it has already been demonstrated on the same problem? Additionally, the authors compare to a weak translation baseline on small data-sets, making it impossible to judge whether the results would hold on a larger data-set. So the following ablations and comparison to baselines are missing:\n* comparing with a stronger NMT architecture and larger data-set\n* does the chosen discretization method matter? Other methods have been shown to strongly out-perform Gumbel-Softmax in this context, so a comparison would be in order.\n* comparison to fully unsupervised latents and some other system, e.g., the system from [1] above\n\nIn the absence of these comparisons and with little novelty, the paper is a clear reject.\n\n[Revision]\n\nGreatly appreciate the answers provided by the authors. The Ja-En dataset is indeed much larger than I thought, so I increased my score. When the other points are addressed (as the authors say they will do) it may be a good paper -- but the review must stick to the submitted version, not a future one.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}