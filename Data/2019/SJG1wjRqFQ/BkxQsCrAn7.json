{"title": "Poorly motivated and confusing", "review": "\nThis paper is not ready for publication in ICLR or most other venues. The model is poorly motivated, many modeling choices are confusing, and the experiments are not convincing.  I found much of the paper confusing. A (far from complete) sample:\n\n\n\u00a71 \u00b61  What is this structure an example of? What sentence structures do you mean, concretely? Syntax? The introduction is very vague\u2014I\u2019m not convinced this is meaningful.\n\n\u00a71 \u00b62-3 These paragraphs also vague.\n\n\u00a71 \u00b65 Why is this approach naive? Is this a well-known method? There are no citations.\n\nFig.1 Very confusing: it looks like the target sentence, \u201cstructural tags\u201d and \u201ccoding model\u201d form a loop! This example is also confusing because the \u201cstructural tags\u201d are non-sensical\u2026 they have no relation to this example sentence! I can\u2019t tell if this is because they were made up without relation to the input sentence, or worse, that they\u2019re an actual example from the data, in which case there is something very wrong with the tagger used in the \u201cnaive\u201d experiments.\n\nSec. 2.1 What is the motivation behind the heuristics for the \u201ctwo-step process that simplifies the POS tags\u201d?\n\nSec 2.2. The description of the model is confusing. If I understand correctly, wehave training data for these \u201ccodes\" (in the form of \u201csimplified\u201d POS tags), and a simple seq2seq model is the obvious first thing to try. Most of the choices that deviate from this (e.g. use of Gumbel-softmax, also confusingly called \u201csoftplus\u201d in Eq. 2) are never explained.\n\nSec. 3 The related work is a laundry list of papers, explained without relation to the current paper. It simply gets in the way of the rest of the paper and isn\u2019t needed.\n\nTable 1. I\u2019m not sure what the code accuracy tells us. It\u2019s also unclear to me what is means to \u201creconstruct\u201d the \u201coriginal tag sequence\u201d from the codes, esp. given the description in Sec 2.1.\n\nTable 2. Given the minor differences in these numbers and the confusing description of the model and training process, I am skeptical of these numbers, which look quite a bit like noise. Note that the use of four columns corresponding to different beam sizes is misleading\u2026 this makes it look as if there are four separate experiments for each condition, but this is not really true, we expect these scores to correlate across different beam sizes, so seeing the bold numbers at the bottom of each column does not add substantial information.\n\nTable 4. These are interesting, but it seems like a possibly natural consequence of adding a noisy sequence of characters to the beginning of the decoded sequence; I\u2019m not convinced that the sequences mean anything per se, but it\u2019s a bit like adding some random noise to the decoder state before generating the word sequence.\n\n5.1 \u201cInstead of letting the beam search decide the best \u2026 we use beam search to obtain three code sequences with highest scores.\u201d I\u2019m confused: what is the difference?\n\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}