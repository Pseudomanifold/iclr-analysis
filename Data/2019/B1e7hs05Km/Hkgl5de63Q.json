{"title": "-", "review": "This paper proposes a method for more efficient exploration in RL by maintaining uncertainty estimates over the learned Q-value function. It is comparable to Double DQN (DDQN) but uses its learned uncertainty estimates with Thompson sampling for exploration, rather than \\epsilon-greedy. Empirically the method is significantly more sample-efficient for Atari agents than DDQN and other baselines.\n\n=====================================\n\nPros:\n\nIntroduction and preliminaries section give useful background context and motivation. I found it easy to follow despite not having much hands-on background in RL.\n\nProposes a novel (to my knowledge) exploration method for RL which intuitively seems like it should work better than \\epsilon-greedy exploration. The method looks simple to implement on top of existing Q-learning based methods and has minimal computational and memory costs.\n\nStrong empirical performance as compared with appropriate baselines -- especially to DDQN(+) where the comparison is direct with the methods only differing in exploration strategy.\n\nGood discussion of practical implementation issues (architecture, hyperparameters, etc.) in Appendix A.\n\n=====================================\n\nCons/questions/suggestions/nitpicks:\n\nAlgorithm 1 line 11: \u201cUpdate W^{target} and Cov\u201d -- how? I see only a brief mention of how W^{target} is updated in the last paragraph of Sec. 3, but it\u2019s not obvious to me how the algorithm is actually implemented from this, and I don\u2019t see any mention of how Cov is updated.\n\nAlgorithm 1: I\u2019d like to know more about how sample-efficiency varies with T^{sample} given that T^{sample}>1 is doing something other than true Thompson sampling. Does the regret bound hold with T^{sample}>1? Also, based on the discussion in Appendix A, approximating the episode length seems to be the goal in choosing a setting of T^{sample} -- so why not just always resample at the beginning of each episode instead of using a constant T^{sample}?\n\nTheorem 1: I don\u2019t understand the Theorem statement (let alone the proof) or what it tells me about the proposed BDQN algorithm. First, as a \u201cnon-RL person\u201d I wasn\u2019t familiar with PSRL, but I see it\u2019s later defined as \u201cposterior-sampling RL\u201d. This should be clarified earlier for readers that aren't familiar with this line of work. But this still doesn\u2019t fully explain what \u201cthe PSRL on \\omega\u201d means. If it means you follow an existing PSRL algorithm to learn \\omega, then how does the theorem relate to the proposed algorithm? I'm sure I'm missing something but the connection was unclear to me.\n\nTheorem 1: there\u2019s a common abuse of big-O notation here that should be fixed for a formal statement -- O(f(n)) by definition is a set corresponding to an upper-bound, so this should probably be written as g(n) \\in O(f(n)) rather than g(n)<=O(f(n)). (Or alternatively, just rewritten without big-O notation.)\n\nTable 2: should be reformatted to make it clear that the rightmost 3 columns are not additional baseline methods (e.g. adding a vertical line would be good enough).\n\nAppendix A.8, \u201cA discussion on safety\u201d: this section should either be much more fleshed out or removed. I didn\u2019t understand the statement at the end at all -- \u201cone can... come up with a criterion for safe RL just by looking at high and low probability events\u201d -- huh? What is even meant by \u201csafe RL\u201d in this context? Nothing is referenced.\n\nOverall, much of the writing seems quite rushed with many typos and grammatical errors throughout. This should be cleaned up for a final version. To give a particularly common example, there are many inline references that do not fit in the sentence and distract from the flow -- these should be changed to \\citep.\n\nHow does this compare with \u201cA Distributional Perspective on Reinforcement Learning\u201d (Bellemare et al., ICML 2017) both in terms of the approach and performance? The proposed method seems to at least superficially share motivation with this work (and uses the same Atari benchmark, as far as I can tell) but it is not discussed or compared.\n\n=====================================\n\nOverall, though many parts of the paper could use significant cleanup and clarification, the paper proposes a novel yet relatively simple and intuitive approach with strong empirical performance gains over comparable baselines.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}