{"title": "Major clarity issues", "review": "Update after feedback: I would like to thank the authors for huge work done on improving the paper. I appreciate the tight time constrains given during the discussion phase and big steps towards more clear paper, but at the current stage I keep my opinion that the paper is not ready for publication. Also variability of concerns raised by other reviewers does not motivate acceptance.\n\nI would like to encourage the authors to make careful revision and I would be happy to see this work published. It looks very promising. \n\nJust an example of still unclear parts of the paper: the text between eq. (3) and (4). This describes the proposed method, together with theoretical discussions this is the main part of the paper. As a reader I would appreciate this part being written detailed, step by step.\n=========================================================\n\nThe paper proposes the Bayesian version of DQN (by replacing the last layer with Bayesian linear regression) for efficient exploration. \n\nThe paper looks very promising because of a relatively simple methodology (in the positive sense) and impressive results, but I find the paper having big issues with clarity. There are so many mistakes, typos, unclear statements and a questionable structure in the text that it is difficult to understand many parts. In the current version the paper is not ready for publication. \n\nIn details (more in the order of appearance rather than of importance):\n1. It seems that the authors use \u201csample\u201d for tuples from the experience replay buffer and draws W from its posterior distribution (at least for these two purposes), which is extremely confusing\n2. pp.1-2 \u201cWe show that the Bayesian regret is bounded by O(d \\sqrt{N}), after N time steps for a d-dimensional feature map, and this bound is shown to be tight up-to logarithmic factors.\u201d \u2013 maybe too many details for an abstract and introduction and it is unclear for a reader anyway at that point\n3. p.1 \u201cA central challenge in reinforcement learning (RL) is to design efficient exploration-exploitation tradeoff\u201d \u2013 sounds too strong. Isn\u2019t the central challenge to train an agent to get a maximum reward? It\u2019s better to change to at least \u201cOne of central challenges\u201d\n4. p.1 \u201c\u03b5-greedy which uniformly explores over all the non-greedy strategies with 1 \u2212 \u03b5 probability\u201d \u2013 it is possible, but isn\u2019t it more conventional for an epsilon-greedy policy to take a random action with the probability epsilon and acts greedy with the probability 1 \u2013 epsilon? Moreover, later in Section 2 the authors state the opposite \u201cwhere with \u03b5 probability it chooses a random action and with 1 \u2212 \u03b5 probability it chooses the greedy action based on the estimated Q function.\u201d\n5. p.1 \u201cAn action is chosen from the posterior distribution of the belief\u201d \u2013 a posterior distribution is the belief\n6. p.2 \u201cand follow the same target objective\u201d \u2013 if BDQN is truly Bayesian it should find a posterior distribution over weights, whereas in DDQN there is no such concept as a posterior distribution over weights, therefore, this statement does not sound right\n7. p.2 \u201cThis can be considered as a surrogate for sample complexity and regret. Indeed, no single measure of performance provides a complete picture of an algorithm, and we present detailed experiments in Section 4\u201d \u2013 maybe too many details for introduction (plus missing full stop at the end)\n8. p.2 \u201cThis is the cost of inverting a 512 \u00d7 512 matrix every 100,000 time steps, which is negligible.\u201d \u2013 doesn\u2019t this depend on some parameter choices? Now the claim looks like it is true unconditionally. Also too many details for introduction\n9. p.2 \u201cOn the other hand, more sophisticated Bayesian RL techniques are significantly more expensive and have not lead to large gains over DQN and DDQN.\u201d \u2013 it would be better to justify the claim with some reference\n10. Previous work presented in Introduction is a bit confusing. If the authors want to focus only on Thompson Sampling approaches, then it is unclear, why they mentioned OFU methods. If they mention OFU methods, then it is unclear why other exploration methods are not covered (in Introduction). It is better to either move OFU methods to Related Work completely, or to give a taste of other methods (for example, from Related Work) in Introduction as well\n11. p.3 \u201cConsider an MDP M as a tuple <X , A, P, P0, R, \u03b3>, with state space X , action space A, the transition kernel P, accompanied with reward function of R, and discount factor 0 \u2264 \u03b3 < 1.\u201d \u2013 P_0 is not defined\n12. p.4 \u201cA common assumption in DNN is that the feature representation is suitable for linear classification or regression (same assumption in DDQN), therefore, building a linear model on the features is a suitable choice.\u201d \u2013 the statement is more confusing than explaining. Maybe it is better to state that the last fully connected layer, representing linear relationship, in DQN is replaced with BLR in the proposed model\n13. p.5 In eq. (3) it is better to carry definition of $\\bar{w}_a$ outside the Gaussian distribution, as it is done for $\\Xi_a$\n14. p.5 The text between eq. (3) and (4) seems to be important for the model description and yet it is very unclear: how $a_{TS}$ is used? \u201cwe draw $w_a$ follow $a_{TS}$\u201d \u2013 do the authors mean \u201cfollowing\u201d (though it is still unclear with \u201cfollowing\u201d)? What does notation $[W^T \\phi^{\\theta} (x_{\\tau})]_{a_{\\tau}}$ denote? Which time steps do the authors mean?\n15. p.5 The paragraph under eq. (4) is also very confusing. \u201cto the mean of the posterior A.6.\u201d \u2013 reference to the appendix without proper verbal reference. Cov in Algorithm 1 is undefined, is it equal to $\\Xi$? Notation in step 8 in Algorithm 1 is too complicated.\n16. Algorithm 1 gives a vague idea about the proposed algorithm, but the text should be revised, the current version is very unclear and confusing\n17. pp.5-6 The text of the authors' attempts to reproduce the results of others' work (from \"We also aimed to implement...\" to \"during the course of learning and exploration\") should be formalised\n18. p. 6 \"We report the number of samples\" - which samples? W? from the buffer replay?\n19. p. 6 missing reference for DDQN+\n20. p. 6 definition of SC+ and references for baselines should be moved from the table caption to the main text of the paper\n21. p. 6 Table 3 is never discussed, appears in a random place of the text, there should be note in its reference that it is in the appendix\n22. p.6 Where is the text for footnotes 3-6?\n23. p.6 Table 2 may be transposed to fit the borders\n24. p.6 (and later) It is unclear why exploration in BDQN is called targeted\n25. p.7 Caption of Figure 3 is not very good\n26. p.7 Too small font size of axis labels and titles in plots in Figure 3 (there is still a room for 1.5 pages, moreover the paper is allowed to go beyond 10 pages due to big figures)\n27. p.7 Figure 3. Why Assault has different from the others y-axis? Why in y-axis (for the others) is \"per episode\" and x-axis is \"number of steps\" (wise versa for Assault)?\n27. Section 5 should go before Experiments\n28. p. 7 \u201cWhere \u03a8 is upper triangular matrix all ones 6.\u201d \u2013 reference 6 should be surrounded by brackets and/or preceded by \"eq.\" and it is unclear what \u201call ones\u201d means especially given than the matrix in eq. (6) does not contain only ones\n29. p. 7 \u201cSimilar to the linear bandit problems,\u201d \u2013 missing citation\n30. p. 7 PSRL appears in the theorem, but is introduced only later in Related work\n31. p. 7 \u201cProof: in Theorem. B\u201d \u2013 proof is given in Appendix B?\n32. p. 8 Theorem discussion, \u201cgrows not faster than linear in the dimension, and \\sqrt(HT)\u201d \u2013 unclear. Is it linear in the product of dimension (of what?) and \\sqrt(HT)?\n33. p.8 \u201cOn lower bound; since for H = 1\u2026\u201d \u2013 what on lower bound?\n34. p.8 \u201cour bound is order optimal in d and T\u201d \u2013 what do the authors mean by this?\n35. p.8 \"while also the state of the art performance bounds are preserved\" - what does it mean?\n36. p.8 \"To combat these shortcomings, \" - which ones?\n37. p.8 \"one is common with our set of 15 games which BDQN outperformS it...\" - what is it?\n38. p.9 \"Due to the computational limitations...\" - it is better to remove this sentence\n39. p.9 missing connection in \"where the feature representation is fixed, BDQN is given the feature representation\", or some parts of this sentence should be removed?\n40. p.9 PAC is not introduced\n41. pp.13-14 There is no need to divide Appendices A.2 and A.3. In fact, it is more confusing than helpful with the last paragraph in A.2 repeating, sometimes verbatim, the beginning of the first paragraph in A.3\n42. In the experiments, do the authors pre-train their BDQN with DQN? In this case, it is unfair to say that BDQN learns faster than DDQN if the latter is not pre-trained with DQN as well. Or is pre-training with DQN is used only for hyperparameter tuning?\n43. p.14 \u201cFig. 4 shows that the DDQN with higher learning rates learns as good as BDQN at the very beginning but it can not maintain the rate of improvement and degrade even worse than the original DDQN.\u201d \u2013 it seems that the authors tried two learning rates for DDQN, for the one it is clear that it is set to 0.0025, another one is unclear. The word \u201coriginal\u201d is also unclear in this context. From the legend of Figure 4 it seems that the second choice for the learning rate is 0.00025, but it should be stated in the text more explicitly. The legend label \u201cDDQN-10xlr\u201d is not the best choice either. It is better to specify explicitly the value of the learning rate for both DDQN\n44. p.15 \u201cAs it is mentioned in Alg. 1, to update the posterior distribution, BDQN draws B samples from the replay buffer and needs to compute the feature vector of them.\u201d \u2013 B samples never mentioned in Algorithm 1\n45. p.15 \u201cduring the duration of 100k decision making steps, for the learning procedure,\u201d \u2013 i) \u201cduring \u2026 duration\u201d, ii) what did the authors meant by \u201cdecision making steps\u201d and \u201cthe learning procedure\u201d?, and iii) too many commas\n46. p.15 \u201cwhere $\\tilde{T}^{sample}$, the period that of $\\tilde{W}$ is sampled our of posterior\u201d \u2013 this text does not make sense. Is \u201cour\u201d supposed to be \u201cout\u201d? \u201c\u2026 the number of steps, after which a new $\\tilde{W}$ is sampled from the posterior\u201d?\n47. p.15 \u201c$\\tilde{W}$ is being used just for making Thompson sampling actions\u201d \u2013 could the authors be more specific about the actions here?\n48. p.16 \u201cIn BDQN, as mentioned in Eq. 3, the prior and likelihood are conjugate of each others.\u201d \u2013 it is difficult to imagine that an equation would mention anything and eq. (3) gives just the final formula for the posterior, rather than the prior and likelihood\n49. p.16 The formula after \u201cwe have a closed form posterior distribution of the discounted return, \u201d is unclear\n50. p.17 \u201cwe use \u03c9 instead of \u03c9 to avoid any possible confusion\u201d \u2013 are there any differences between two omegas?\n51. p.17 what is $\\hat{b}_t$?\n\nThere are a lot of minor mistakes and typos also, I will add them as a comment since there is a limit of characters for the review.\n\n\n\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}