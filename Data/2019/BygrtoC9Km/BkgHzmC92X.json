{"title": "Encouraging results but contribution seems incremental", "review": "# Summary\nThis work deals with few-shot learning and classification by means of similarity learning. The authors propose a method for generating a set of convolutional kernels, i.e. a mini-CNN, for a query image given a set of support samples (with samples from the same class and some other classes). Kernels are generated for each query and are adapted to the specific visual content found in the query image, thus a new embedding space is identified. The difficulty of the task is constrained by using a common base CNN for feature extraction, making the task resolution more feasible in the few shot regime. The method is evaluated on standard benchmarks Omniglot and miniImagenet with competitive results. \n\n\n# Paper strengths\n- The paper has a good coverage of related work\n\n- The proposed method is interesting and the results are encouraging\n\n- The authors argue and study the influence of multiple elements over their contribution: number of sub-generators, distance metric, choice of architecture\n\n# Paper weaknesses\n- My main concern with this work is the incremental contribution with respect to the work by Han et al. (2018), \"Face recognition with contrastive convolution\". In that work the authors proposed a convolutional kernel generator for every pair of images to be compared/matched, while here the principle is simplified to re-use the same convolutional kernels for the a query image. The loss functions are nearly identical, both works use a classification loss and a loss ensuring kernels at different images with the same object should be similar. The visualizations of the feature maps are similar as well, though these would have been necessary any way for this type of contribution.\n\n- The architecture of the kernel generator is not clear to the reader. Is it similar with the one from Han et al.?\n\n- The related and relevant work from Gidaris and Komodakis (2018), Dynamic few-shot visual learning without forgetting, is not included as baseline in the evaluation. Their method is superior when using C4 and similar (while still keeping performance levels on previous tasks).\n\n- Given that the evaluation for few-shot classification takes random samples of query and support samples and that we're dealing with stochastic models, it's common and encouraged to include error-bars/standard deviations in the results to get a better idea on the performances. I encourage the authors to do the same.\n\n- The visualizations from Figure 3 would need some additional clarifications from the authors in the text. It's not clear what does the colormap refer to, red is for high activation and blue for low activation (as typical for jet colormap) or the other way around? If blue is highly active, it's worrying that most dogs (Q1,Q2,Q3) are active on the white dog in S3. As said, it would be useful to have some comments from the authors in the text to better explain the visualizations\n\n- Minor remarks:\n    + The evaluation protocol from Omniglot should be specified as there are 2 ways of doing it: 1) using characters from different alphabets at test time (easier); 2) using characters from the same alphabet (more difficult)\n    + There are some other works dealing with weight generation or with adaptive embedding that would be worth mentioning: \n        * Y.X. Wang et al., Learning to model the tail, NIPS 2017\n        * A. Veit and S. Belongie, Conditional similarity networks, CVPR 2017\n    \n\n# Conclusion\nThis paper advances an interesting idea for few-shot classification and gets competitive results. As mentioned in the section above, I'm worried about the incremental contribution on top of the work by Han et al.. In addition results are outperforming state of the art works, while requiring generating kernels and features for each query. My current rating is between Weak Reject and Borderline.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}