{"title": "Possibly significant result but requires more experimental analysis", "review": "This paper presents a new adversarial training defense whereby an ensemble of models is trained against both benign and adversarial examples. The authors demonstrate on the CIFAR-10 dataset that the ensemble has improved robustness against a wide variety of white-box and transfer-based black-box attacks compared to other adversarial training techniques. The results appear significant but would greatly benefit from more thorough experiments.\n\nPros:\n- Conceptually simple and intuitive.\n- Thorough baselines and attack methods.\n\nCons:\n- Limited novelty.\n- Needs more experimental validation against other datasets (e.g. ImageNet) and models (e.g. Inception-v3).\n- Table 1 shows that the clean accuracy of adversarially trained models is significantly worse, which suggests that some aspect of training was done improperly.\n\n-----------------------------\n\nI would like to clarify regarding the listed cons:\n\n- Limited novelty: Adversarial training has been well-established as a viable defense against adversarial examples, as well as training a single model against an ensemble of adversarial examples crafted on different networks (Tramer et al. https://arxiv.org/pdf/1705.07204.pdf). While this work is sufficiently different from prior methods, its novelty is insignificant.\n- More experimental validation: Due to the limited novelty, it is crucial that the authors validate their result more thoroughly to eliminate any doubt on applicability, especially against more challenging datasets such as ImageNet. While the experiments on CIFAR-10 are certainly sufficient, it is dangerous, particularly in works on defenses against adversarial examples, to restrict only to a relatively simple dataset.\n- Tramer et al. (https://arxiv.org/pdf/1705.07204.pdf) publicly released their ensemble adversarially trained Inception-v3 model that has the same top-1 and top-5 clean accuracy on ImageNet as the base model. This serves as evidence that it is certainly possible to adversarially train a model without compromise to clean accuracy, especially on a simpler dataset such as CIFAR-10.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}