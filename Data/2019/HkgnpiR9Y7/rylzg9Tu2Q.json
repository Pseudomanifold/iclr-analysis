{"title": "a novel approach to GAN inference mapping", "review": "This paper describes a novel method to provide inference mapping for GAN networks. The idea is to reuse the discriminator network's feature vector (output of layer before last) and learn a direct mapping to the GAN's latent space. This can be done very efficiently since the dimensionality of both layers are relatively small. Also, the mapping does not interfere with the learning process of the GAN itself and thus can be applied on top of any GAN method without affecting its performance. \n\nInference mapping is useful in the GAN context for several reasons that are well described in the paper. First it allows to more efficiently generate \"edited\" images as the mapping provides a good starting point in the latent space. Second it provides a sound way to evaluate GAN's performance as the reconstruction of a given image through the inference mapping and the generator provides auto-encoder-like capabilities. Comparison of GAN models have been difficult due to a lack of adequate evaluation technique. This paper proposes a novel evaluation scheme that is both fair and technically simple.\n\nIn the experimental part, the authors first compare their approach to the 'naive encoder' approach where the last layer of the discriminator is removed after training, a feature layer of the size of the encoder's latent space is added, and the rest of the discriminator's layers are frozen. The proposed approach outperforms the naive encoder approach on the CelebA dataset. The second set of experiments investigates reconstruction accuracy of various GAN models. Figure 2 shows reconstructed images for 7 GANs and 36 examples from 3 datasets. Unfortunately, no subjective comparison can be attempted since the examples are different for each GAN. In Figure 3, editing in performed on the CelebA dataset, but again, subjective comparison among the GAN's is precluded by the fact that different examples are chosen. This oversight does not affect the paper's relevance, since those comparison would be purely subjective, however it would add some visual interpretation to the quantitative comparison given in table 1. I also wish the authors would have provided the inception score for FashMNIST and CelebA and also provide the more recent FID (Frechet Inception Distance). Inception scores are trained on ImageNET and are too commonly applied to CIFAR-10 and CelebA. It would be good to compare them against the proposed method on those datasets to show that there are not good for datasets other than those on which they were trained.\n\nThe article is technically sound. The citations are adequate. The English is fine with some extraneous articles being the only issue. The article lacks a graphic for the architecture of the system and many of the figures are too small to interpret when printed out. Also there's a typo on table 1. where the inception score for WGAN-GP on CelebA should be 6.869 and not 0.6869.\n\nOverall, I find this paper provides a simple, novel significant method for evaluating GAN models and making better use of their latent space arithmetic editing capabilities. Due to the algorithm's simplicity, most of the paper is devoted to experiments and discussions.\n \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}