{"title": "Analysis and experimental comparisons are lacking", "review": "After reading the authors' response, I'm revising my score upwards from 5 to 6.\n\nThe authors propose a defense against adversarial examples, that is inspired by \"non local means filtering\". The underlying assumption seems to be that, at feature level, adversarial examples manifest as IID noise in feature maps, which can be \"filtered away\" by using features from other images. While this assumption seems plausible,  no analysis has been done to verify it in a systematic way. Some examples of verifying this are:\n\n1. How does varying the number of nearest neighbors change the network behavior?\n2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?\n3. Does just simple filtering of the feature map, say, by local averaging, perform equally well? \n4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?\n\nBased on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training. It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}