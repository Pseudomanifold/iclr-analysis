{"title": "Interesting paper, a bit problematic experimental set-up", "review": "The paper proposes using the Frank-Wolfe algorithm for fast adversarial attacks. They prove upper bounds on the Frank-Wolfe gap and show experimentally that they can attack successfully much faster than other algorithms. In general I find the paper novel (to the best of my somewhat limited knowledge), interesting and well written. However I find the white-box experiments lacking as almost every method has 100% success rate. Fixing this would significantly improve the paper.\n\nMain remarks:\n- Need more motivation for faster white-box attack. One good motivation for example is adversarial training, e.g. Kurakin et al 2017 \u2018ADVERSARIAL MACHINE LEARNING AT SCALE\u2019 that would benefit greatly from faster attacks\n\n- White-box attack experiments don\u2019t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare. Need to compare in more challenging settings where the success rate is meaningful, e.g. smaller epsilon or a more robust NN using some defence. Also stating the 100% success rate in the abstract is a bit misleading for the this reason.\n\n-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other \nattack seems odd. \n\n-The average distortion metric (that\u2019s unfavourable to your method anyway) doesn\u2019t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.\n\n- Regarding lambda>1, you write that \u201cwe argue this modification makes our algorithm more general, and gives rise to better attack results\u201d. I did not see any theoretical or empirical support for this in the paper. Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial. Some intuitive explanation on why this should help and/or empirical comparison would be a great addition.\n\n- The authors claim that this is the first zeroth-order non-convex FW convergence rate, I am not familiar enough with the field to evaluate this claim and its significance.\n\n- Alg. 1 for T>1 is very similar to I-FGM, but also \u2018pulls\u2019 x_t towards x_orig. It would be very useful to write the update more explicitly and compare and contrast this 2 very similar updates. This gives nice insight into why this should intuitively work better.\n\n- I am not sure what the authors mean by \u201cthe Frank-Wolfe gap is affine invariant\u201d. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?\n\n- I am not sure what you mean in 5.4 \u201cwe omit all grid search/ binary search steps\u2026\u201d\n\nMinor remarks:\n- In remark 4.8 in the end option I and II are inverted by mistake\n\n- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number. \n\n- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}