{"title": "Unclear if it will save wall clock time.", "review": "This paper motivates itself by observing that not all convolutional weights are required to make an accurate prediction. In the introduction the authors envision a system similar to a cascaded classifier [Viola and Jones 2001] (I draw this conclusion not the paper). However the wording of the introduction is not clear or it does not align with what is presented in the paper.\n\nThe approach in the paper does not perform early stopping dynamically during the feedforward phase. The approach removes weights which do not impact the accuracy after training has completed and the fine tunes the resulting network.\n\nThe clarity of the introduction must be addressed however the work is still interesting. I recommend the authors try to make the introduction as accessible as possible.\n\n\nAlso there are very general statements like \"The activation layer introduces non-linearity into the system for obtaining better accuracy.\" which do not contribute to the message of the paper. The paper will be fine without these statements. Shorter is better.\n\nSection 3 is good as a motivating example. However the conclusion \u201cThus, our focus is to develop an effective method of choosing a good intermediate result for saving more MAC operations with less accuracy drop\u201d is not very clear. More insights written hear would be better.\n\nOne major flaw is that no analysis with respect to time of computation was performed. GPUs offer the advantage of being optimized for convolutions so it is possible that there is no speedup. Because of this it is unclear if the method would save time. The results clearly show individual computations (MACs) are reduced but it is not clear how this correlates with wall clock time.\n\nWhy do you start with the centre layers? I understand the heuristic you\u2019re using, that the middle layers won\u2019t have high or low-level features, and that they won\u2019t break the other layers as badly if you modify them, but I feel like this is core to your method and it\u2019s not adequately justified. I\u2019d like to see some experiments on that and whether it actually matters to the outcome. Also, you don\u2019t say if you start at the middle then go up a layer, or down a layer. I think this matters since your proposed C10-Net has only 3 convolutional layers.\n\nAll the filters in the same layer share a common checkpoint. Is that a good idea? What is the cost of doing this on a per-filter level? What is the cost on a per-layer level? Discussing runtime estimates for the added computation at training would make it more clear what the cost of the method is.\nIn section 4.4.3. you mention that the majority of weight distributions in CNNs follow the Gaussian manner. Could you cite something of this? You might also want to move that in Step 1 (section 4.1.), since it seems to be your motivation for selection of checkpoint locations (5% and 32%) and I had no idea why you selected those values at that point.\n\nTypos:\nTypo on page 3: \u201cexploits redundancies inter feature maps to prune filters and feature maps\u201d\n\nStructural:\nMaybe section 4.3 should be part of the description of Section 4. Proposed Method, not its own subsection.\n\nMaybe table 2 should go at the end of section 4.4.1, because you describe it the error and columns in section 4.4.1.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}