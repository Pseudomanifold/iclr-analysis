{"title": "ok work, would like to see fairer evaluation.", "review": "\nThe authors show that using VAE framework using a generative model with a mixture-of-Gaussians latent variable structure does not work naively. They propose a wasserstein-based objective that is better amenable to optimization and showed good qualitative/quantitative results on mnist dataset. I like the presentation of the work, especially answering some of my obvious questions like what happens if the VAE is initialized with wasserstein objective. \n\nI'm on the fence about acceptance due to the following 2 questions\n\n(a) do we really need to impose a structured distribution, i.e. gaussian latent variable structure with K centers. Would'nt using K times as many parameters with a single gaussian distribution and a neural network 'g' (using notation in Kingma & Welling) not be expressive enough to capture the data distribution ? \n\n(b) one of the authors main explanation on why the collapse of the K to 1 occurs is that \"discrete KL term is negligible\". Given that the MMD term in eq(4) is weighted by \\lambda, could the authors have not chosen to weight the KL term by lambda as well ? \n\nI would like the authors to present the results of (a) and (b) to give the baseline VAE an equal footing with the author's proposal. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}