{"title": "Interesting read, but little original contribution", "review": "Paper summary:\n\nGiven a pre-trained VAE (e.g. over images), this paper is about inferring the distribution over missing variables (e.g. given half the pixels, what is a plausible completion?). The paper describes an approach based on variational inference with normalizing flows: given observed variables, the posterior over the VAE's latents is inferred (variationally) and plausible completions for missing variables are sampled from the VAE decoder.\n\nTechnical quality:\n\nThe presented method is technically correct. The evaluation carefully compares different types of normalizing flow and HMC, and seems to follow good practices.\n\nI have a suggestion for improving the GVI method. The way it's described in the paper, GVI requires computing the determinant of a DxD matrix, which costs O(D^3), and there is no guarantee that the matrix is invertible. However, this approach over-parameterizes the covariance matrix of the modelled Gaussian. Without losing any flexibility, you can use a lower triangular matrix with strictly positive diagonal elements (e.g. the diagonal elements can be parameterized as the exp of unconstrained variables). That way, the determinant costs O(D) (it's just the product of diagonal elements) and you ensure that the matrix is invertible (because the determinant is strictly positive), without hurting expressivity. You can think of this as parameterizing the Cholesky decomposition of the covariance matrix.\n\nAlso, there are more flexible normalizing flows, such as Inverse Autoregressive Flow, that can be used instead of the planar flow used in the paper.\n\nClarity:\n\nThe paper is written clearly and in full detail, and the mathematical exposition is clear and precise.\n\nSome typos and minor suggestions for improvement:\n- It'd be good to move Alg. 1 and Fig. 1 near where they are first referenced.\n- Page 2: over to \\theta --> over \\theta\n- Eq. 3: p_\\theta appears twice in the middle.\n- one can use MCMC to attempt sampling --> one can use MCMC to sample\n- Eq. 5: should be q_\\psi as subscript of E.\n- Fig. 7, caption: should be GVI vs. NF.\n- In references, should be properly capitalized: Hamiltonian, Langevin, Monte Carlo, Bayes, BFGS\n- Lemma 1: joint divergence is equivalent to --> joint divergence is equal to\n- Lemma 1: in the chain rule for KL, the second KL term should be averaged w.r.t. its free variables.\n\nOriginality:\n\nIn my opinion, there is little original contribution in this paper. The inference method presented (variational inference with normalizing flows) is well-known and already in use. The paper applies this method to VAEs, which is a straightforward application of a well-known inference method to a relatively simple graphical model (z -> {x, y}, with x, y independent given z).\n\nI don't see the need for introducing a new term (cross-coder). According to the paper, a cross-coder is precisely a normalizing flow (i.e. an invertible smooth transformation of a simple density). I think new terms for already existing ideas add cognitive load to the community, and are better avoided.\n\nSignificance:\n\nIn my opinion, constructing generative models that can handle arbitrary patterns of missing data is an important research direction. However, this is not exactly what the paper is about: the paper is about inference in a given generative model. Given that there is (in my opinion) no new methodology in the paper, I wouldn't consider this paper a significant contribution.\n\nI would also suggest that in a future version of the paper there is more motivation (e.g. in the introduction) of why the problem the paper is concerned with (i.e. missing data in generative models) is significant. Is it just for image completion / data imputation, or are there other practical problems? Is it important as part of another method / solution to another problem?\n\nReview summary:\n\nPros:\n- Technically correct, gives full detail.\n- Well and clearly written, precise with maths.\n- Evaluation section interesting to read.\n\nCons:\n- No original contribution.\n- Could do a better job motivating the importance of the problem.\n\nMinor points:\n- I don't completely agree with the way VAEs are described in sec. 2.1. As written, it follows that VAEs must have a Gaussian prior and a conditionally independent decoder. Although these are common choices in practice, they are not necessary: for example, one could take the prior to be a Masked Autoregressive Flow and the decoder a PixelCNN.\n- Same for observation 1. This is not an observation, but an assumption; that is, the paper assumes that the decoder is conditionally independent. This is of course an assumption that we can satisfy by design, but it's a design choice that restricts the decoder in a specific way.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}