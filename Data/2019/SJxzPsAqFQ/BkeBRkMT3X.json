{"title": "Overall, the proposed model seems like sound and thoughtful approach, but the lack of novelty over the existing literature is a weakness.", "review": "This paper propose a new approach to dialogue modeling by introducing two\ninnovations over an established dialogue model: the HRED (Hierarchical\nRecurrent Encoder-Decoder) network. The innovations are: (1) adding a GAN\nobjective to the standard MLE objective of the HRED model; and (2)\nmodifying the HRED model to include an attention mechanism over the local\nconditioning information (i.e. the \"call\" before the present \"response\").  \n\nWriting: The writing was mostly ok, though there were some issues early in\nSection 2. The authors rather awkwardly transition from a mathematical\nformalism that included the two halves of the dialogue as X (call) and Y\n(response), to a formalism that only considers a single sequence X. \n\nNovelty and Impact:  The proposed approach explicitly combines an established\nmodel with two components that are themselves well-established.\nIt's fair to say that the novelty is relatively weak. The model development\nis sensible, but reasonably straightforward. It isn't clear to me that a\ncareful reader of the literature in this area (particularly the GAN for\ntext literature) will learn that much from this paper. \n\nExperiments: Overall the empirical evaluation shows fairly convincingly\nthat the proposed model is effective. I do wonder why would the hredGAN\nmodel outperform the hred model on perplexity. The hred model is\ndirectly optimizing MLE which is directly related to the perplexity\nmeasure, while the hredGAN include an additional objective that should\n(perhaps) sacrifice likelihood. This puzzling result was not discussed and\nreally should be.\n\nThe generated responses, given in table 3 -- while showing some improvement\nover hred and Vhred (esp. in terms of response length and specificity) --\ndo not fit the context particularly well. This really just shows we still\nhave some way to go before this challenging task is solved. \n\nIt would be useful if the authors could run an ablation study to help\nresolve the relative contributions of the two innovations (GAN and\nattention) to the improvements in results. Perhaps the improvement in\nperplexity (discussed above) is do to the use of attention. \n\nDetailed comments / questions\n\n- In the paragraph between Eqns 2 and 3, the authors seem to suggest that\n  teacher forcing is an added heuristic -- however this is just the\n  correct evaluation of the MLE objective. \n\n- In discussing the combined MLE-GAN objective in Eqn. 8 Does the MLE\n  objective use teacher forcing? Some earlier text (discussed above) leads\n  me to suspect that it does not. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}