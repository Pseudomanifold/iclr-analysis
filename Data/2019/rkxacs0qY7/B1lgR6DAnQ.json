{"title": "This paper concerns the fitting of variational Bayesian Neural Network approximations in functional form and considering matching to a stochastic process prior implicitly via samples.", "review": "Overall Thoughts:\n\nI found this paper to be very interesting and to address a topic that I think will be of interest to the community. The gap between theoretically advantageous stochastic processes like the GP and the computational efficiency of finite BNNs is a topic not yet fully understood and I believe this paper has some useful points to make. I would be very interested and grateful to hear the authors\u2019 thoughts on the comments/questions below.\n\nSpecific Comments/Questions:\n\nI would prefix this discussion with the fact that this is quite a dense (and long) paper on a number of topics so while I hope that I have understood the essence of the approach I apologise if I have missed something and hope the authors will be able to correct me.\n\nI follow the point that it is less clear how finite variational deep BNNs relate to GPs and would agree that finding such an agreement would be a topic of interest. Is the approach taken in the paper not quite close conceptually to the variational sparse GP of Titsias? In that paper, effectively a functional bound is also being taken (i.e. an approximation of a full GP with another GP). So a stochastic process is approximating a full GP. In addition, the approximating GP is defined by a set of samples (the pseudo-input locations) that are optimised as variational parameters. The variational bound is defined in the function domain. There is also alignment with the Stein gradient estimation - the pseudo-input locations are used to define a Nystrom approximation to the full GP kernel. This would seem equivalent to the approximation being performed in (2) as long as the kernel used for the eigenfunctions is the same as that of the full GP. \n\nFollowing from the above, I would be very interested to directly contrast the differences to the Titsias approach - would it be possible to add it as a baseline to the experiments? In particular, there are potentially differences due to differences between the Stein kernel and the full GP kernel (it might be best to have both kernels the same if they are not already the same in all experiments - sorry, I couldn\u2019t tell). In the variational-GP, the pseudo-input locations are optimised directly under the bound and so whilst they are sensitive to initialisation, the optimisation is stable and guaranteed to converge to a local optimum. It is unclear to me how stable the adversarial problem in Sec3.2 is. In the proposed sampling variant, it isn\u2019t clear to me that it is a safe procedure to follow - taking a random subset from the training data and weights on c seems rather heuristic - are there any guarantees? \n\nThere would also seem to be some connections with the recent approaches on (conditional) neural processes - perhaps the authors might like to comment on this?\n\nFor a number of the GP priors in the experiments, it might be quite hard for a BNN with ReLu activations to match the posteriors? Would it be worth trying with other (more smooth) activation functions?\n\nOverall the results are interesting - would it be possible to include comparisons to the variational-GP (at least for the small-scale experiments)? It would be interesting to contrast their complexity as well if they get stuck on the large-scale ones. For some of the experiments would it be possible to include histograms rather than just error bars to check that the error distributions are similar?\n\nFor the appendix BayesOpt experiment - would it be possible to use Thompson sampling as the acquisition function? To my mind this would evaluate the predictive density more directly since it mitigates the effect of a particular choice of acquisition function on the performance. Also would a comparison with the full GP not be appropriate?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}