{"title": "Interesting problem, more experiments would be nice", "review": "This is an emergency review, so apologies for the briefness.\n\nThe paper introduces an approach to learning negotiation strategies using reinforcement learning. The authors propose a new setup in which self-interested agents must cooperatively form teams to achieve a reward. They explore two ways of proposing agreements: one involving a random agent proposing an agreement symbolically, and another in which agents form teams by moving to the same location. Results show that RL-trained models outperform simple rule-based bots, and correlate with game-theoretic predictions. I think the paper is very well clearly presented, and tackles an interesting an important problem.\n\nOne issue I have is that as I understand it, the results are only reported for training games. Could the agents just be memorizing a good outcome for that specific environment, rather than actually learning to negotiate? Why not evaluate on held out games?\n\nThe experiments are pretty interesting, and I appreciated the last one showing that limitations are due to the difficulty of RL, rather than expressive power of the network. However, I think there are some other natural questions that could be explored, including: what kind of strategies are the models learning? Could we change the environment in such a way that the proposed approach is not sufficient? Is the choice of RL approach crucial, or does anything work? I think further experiments would strengthen the paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}