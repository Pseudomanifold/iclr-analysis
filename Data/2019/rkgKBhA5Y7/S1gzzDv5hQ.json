{"title": "Very thorough analysis but limited novel contribution", "review": "OVERVIEW:\nThe paper looks at the problem of self-supervised learning using consistency-enforcing approaches. Their main contributions are two-fold:\n1. Analysis to understand current state-of-the-art methods for self-supervised learning, namely the Mean Teacher model (MT) by Tarvainen and Valpola (2017) and the \\Pi model (Laine and Aila, 2017). They show a theoretical analysis (Sec.3.1) of a simplified version of the \\Pi model and show that it reaches flatter minima leading to good generalization. They show an analysis of the SDG trajectories (Sec. 3.2) that shows how these self-supervised models achieve flatter and lower minima compared to a fully supervised approach. They also provide an intuitive explanation to explore more solutions along the SGD trajectory. Finally, in Sec.3.3, they also discuss how ensembling and weight averaging help get better solutions.\n2. Fast-SWA, which is a tweak to the SWA procedure (Izmailov et al, 2018) that averages models in the weight space along the SGD trajectory with a cyclical learning rate.\nThey show good performance on CIFAR-10 and CIFAR-100 with their proposed Fast-SWA.\n\nPROS:\n1. The paper contains a lot of empirical analysis explaining the behavior of these models and providing intuition about the optimization leading to their proposed solution. The problem and experiments are very organized and explained very well.\n2. Exhaustive experiments, plots and tables showing very good performance on the standardized benchmark.\n\nCONS:\n1. The novel contribution (as I see it) is in the theoretical analysis of Sec. 3.1 & A.5 and the Fast-SWA procedure. The Fast-SWA is a minor tweak to the regular SWA. The theoretical analysis is the main novelty and it is hidden away in the appendix ! Also, the results seems to be derived on the basis of Avron and Toledo and the authors' contribution relative to that is not clear. Also, what is the difference between the regular \\Pi model and simplified \\Pi model and how big a difference does this make in your theory ?\n2. Can the Fast SWA be used directly say while supervised training of ImageNet ? Or is it applicable only to self-supervised problems ? Comments on the generalizability of this contribution might help increase novelty.\n\nOVERALL:\nI like the thorough analysis and good results of the paper. The novelty being a little weak results in the final rating of 7.5 (rounded up to 8, subject to change depending on other reviewers).", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}