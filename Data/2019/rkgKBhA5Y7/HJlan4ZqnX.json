{"title": "Nice read + questions", "review": "The paper is nice thread, easy to follow.\n\nThe paper proposed to apply SWA (Stochastic Weight Averaging) Izmailov et al. 2018 to the semi-supervised approached based on consistency regularization. The paper first describes the related work nicely and offers a succinct explanation of two semi-supervised approaches they study. The paper then present an analysis on SGD trajectories of these 2 approaches, drawing comparisons with the supervised training and then building a case of why SWA is a valid idea to apply. The analysis section is very well described, the theoretical explanations are easy to follow and Figure 1, Figure 2 are really helpful to understand this analysis. \n\nOverall, the paper offers a useful insight into semi-supervised model trainings and offers recipe of converging to supervised results which is a valid contribution.\n\nI have following questions to the authors:\n1. Did the authors do the analysis and apply SWA on ImageNet training besides Cifar-10 and Cifar-100\n2. The accuracy number reported in abstract (5.0% error) is top-1 error or top-5 error? I think it's top-5 but explicit mention would be great.\n3. In section 3.2, authors offer an analysis by chosing epoch 170, 180. How are these epochs chosen?\n4. In section 3.1, authors consider a simple model version where only small additive perturbations to student inputs are applied. Is this a practical setup i.e. is this ever the case in actual model training?\n5. In section 3.3, pg 6, do authors have intuition into why weight averaging has better improvement (1.18) vs ensembling (0.94)?\n6. In section 5.2, page 8 , can authors provide their intuition behind the results: \"We found that the improvement on VAT is not drastic \u2013 our base implementation obtains 11.26% error where fast-SWA reduces it to 10.97%\" - why did fast-SWA not improve much?", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}