{"title": "Maybe an improvement to zero-shot MT, but methodological questions make the results unconvincing", "review": "Pros:\n- The paper proposes two loss terms to help improve zero-shot translation performance. The proposed supervised alignment loss term is simple and would be straightforward to implement. The proposed adversarial loss term is also conceptually simple, although perhaps a bit difficult to tune.\n- Both loss terms seem to improve zero-shot translation performance over the models trained without them.\n- The paper is fairly well written. It's easy to follow, although there's a few details that are missing which might make it difficult to reproduce.\n\nCons:\n- There are several methodological concerns that make it difficult to evaluate the correctness/significance of the results. For example, the authors use a non-standard valid/test set for the WMT'14 experiments (newstest2012/13 instead of newstest2013/14) which makes the results difficult to compare to other supervised baselines.\n- The results would be more convincing if the authors had evaluated on more language pairs and more standard datasets. Most of the results are based on a model trained on just two language pairs, making it difficult to evaluate how well the approach actually works.\n- There is no direct quantitative comparison to any other published results.\n- The results presented over the IWSLT dataset are heavily summarized, making it difficult to compare to other work.\n\nMethodological questions:\n- In Table 1, why do you use newstest2012 as the valid set and newstest2013 as the test set? Most other work on this dataset (e.g., Vaswani et al.) uses newstest2013 as the dev set and newstest2014 as the test set.\n- The baseline \"Direct translation\" results given in Table 1 are quite a bit lower than those reported in Vaswani et al. In particular, Vaswani uses newstest2013 as their dev set (which you use as test) and report an En-De BLEU of 25.8 in their ablation study, compared to 24.5 in your Table 3. Why is there such a large discrepancy?\n- In Section 4.3, when examining the attention context vectors, can you explain more about how you computed this? In particular, each Transformer decoder layer has an attention over the encoder outputs, so for 6 layers you'll actually have 6 attention context vectors, each depending on the previous decoder layer as well, right?\n\nMisc questions/comments:\n- Did you try combining the adversarial and pool-cosine loss terms?\n- How does your approach compare to adding identity (autoencoder) mappings, as in this WMT paper: http://www.statmt.org/wmt18/pdf/WMT009.pdf\n- You claim that prepending the <tl> token to the source has the same effect as putting it as the first token in the decoder. Did you actually test this or have some reference/citation for this? Since a lot of the problem seems to be getting the decoder to produce the correct output language, it seems possible that changing the method that you specify the desired target language could have a non-negligible effect.\n- What does a learning rate of 1.0 mean? In the original Vaswani et al. paper they use Adam and a dynamic learning rate schedule that adjusts based on the number of updates -- the effective learning rate is typically << 1.0. I believe the peak learning rate that they used is closer to 5e-4.\n- Which \"language identification tool\" do you use?\n- Table 4 seems to only give averages over the different zero-shot language pairs. Please add the full results for each language pair in the Appendix.\n- In Section 4.4, what do you mean by \"scores [...] are suspiciously close to that of bridging\"? What do you mean by bridging?\n\nThere are also a couple uncited related papers:\n- http://aclweb.org/anthology/N18-1032\n- http://www.statmt.org/wmt18/pdf/WMT009.pdf", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}