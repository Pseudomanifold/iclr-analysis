{"title": "SUPERVISED POLICY UPDATE", "review": "Overall this paper is ok. The algorithm seems novel, but is clearly very closely related to other things in the literature. The paper is also let down by poor exposition in several areas. The numerical results seem reasonably strong, at least against relatively old baselines.\n\nEquation 8 is crucial to the final algorithm, but is presented with no proof or explanation.\n\nJust above theorem 1 the sentence does not parse \"Further, for each s, let \u03bbs be the solution to \", firstly there is no 'solution' to an equation, secondly should it be \u03bbs or pi?\n\nThe discussion following theorem 1 is very messy and hard to follow and the notation is horrendous. I'm confused as to why the indicator function in the 'disaggregated' update only includes states for which the constraint is already satisfied, what about the states where it is not? I presume this is because you initialize from the previous policy, but this seems very approximate and even worse updating the parameters for one state might significantly move the policy in some other state meaning large violations are possible and not dealt with.\n\nThe connections to the papers 'MAXIMUM A POSTERIORI POLICY OPTIMISATION' and 'Relative Entropy Policy Search' should be mentioned, as another commenter said previously.\n\nI don't think TRPO/PPO is SOTA anymore, so maybe these baselines aren't particularly interesting.\n\nFigure 2 is incomprehensible.\n\nTwo of the references are repeated (Schulman et al, Wang et al).\n\nThe appendices include long lists of equalities with no explanation (e.g. appendix B), how is a reader meant to reasonably follow those steps? Each non-trivial equality needs a sentence explaining what was used to get it.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}