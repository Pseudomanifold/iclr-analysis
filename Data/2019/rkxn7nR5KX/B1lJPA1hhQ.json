{"title": "The problem of incremental few-shot learning is interesting and the presented meta-learning method seems to be effective, but the novelty is limited.  ", "review": "This work addresses incremental few-shot learning that learns novel classes without forgetting old classes, which is interesting and different from conventional few-shot learning that considers only the few-shot learning task of interest. This problem is also related closely to the important problem of life-long learning. \n\nThis work presents an interesting framework based on meta-learning by learning to learn how to attend to the old classes using an attention mechanism. Experimental results also show improvement over two related works on incremental few-shot learning. The writing is quite clear. Some concerns, especially its novelty, are listed below.  \n\n1. The novelty appears to be limited. The presented framework looks quite similar to the recent work \n\nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. CVPR'18\n\nthat addresses the same problem in a similar manner: 1) learn a base feature extractor and classifier; and then 2) attend to old classes also via meta-learning and attention mechanism.  \nAs mentioned by the authors, \"The main difference to this work is that we use an iterative optimization to compute W_b\". More discussions on the iterative optimization and why it matters may be helpful.\n\nAnother related work is \"Deep Meta-Learning: Learning to Learn in the Concept Space\", Arxiv'18, that also relies on an external base classes for few-shot learning. Similar to the proposed research, it also learns a feature extractor and a classifier from the base classes, which are used to regularize the learning of novel classes, in an end-to-end meta-learning manner. Extending it for the incremental setting seems natural. \n\n2. To learn a few novel classes, all U_k on old classes are relearned, which seems quite time-consuming with a large vocabulary of base classes.\n\n3. To learn a few novel classes, old data on base classes are still required, which seems different from how humans learn -- humans learn novel concepts solely from a few examples without forgetting old concepts, without requiring examples on old concepts.  ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}