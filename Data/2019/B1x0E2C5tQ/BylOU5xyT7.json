{"title": "Contribution weak", "review": "Summary: To carry out \"analytic\" NLP tasks focusing on morphology, syntax, semantics, and assuming we take embeddings from a neural translation model, which translation unit type (word, BPE, morpheme, character) should I use? What is the impact of character-level errors on performance on these tasks.\n\nIs the task which is undertaken in this paper really useful? I would have wished for conclusions to help one decide, based on, say, linguistic knowledge about tasks (is it based more on morphology, orthography, agreement, word order, etc) and language (is the language inflection-rich, does it have flexible word order, etc), which is the best unit size. The research questions and the conclusions are very limited. They make the entire exercise seem academic. The contribution is not really clear: how does one exploit this in practice? What should I have expected before doing the experiments, which I don't find at the end? Did I really need this to realise that (sec 6, Best of All Worlds) different aspects of the language are learnt by different unit sizes?\n\nThe paper is mostly clearly written, and well presented. The experiments are well executed. I am not convinced by the choice of tasks, which is not motivated in the paper (the paper only says \"we don't do sentiment analysis and question answering\", but why?). I could well have imagined a paper checking my linguistic intuition on tasks such as language modelling, agreement, sentence completion.\n\nOverall, despite the good presentation, I am skeptical about the contribution and impact of this paper.\n\n\nMiscellaneous criticism and praise\n* Table 3 is unclear: are the numbers word or character or sentence counts? Here and in the text (sec 5 \"Only 1863... for German\"), it is not clear why and how the de corpus wouold only have a cross validation set and no training/ test set? what does that mean?\n* sect 4: \"The classifier...trained for 10 epochs\": Log reg training is a convex problem, so the number of epochs should only affect the final result very little.\n* Figure 2: I don't understand the token frequency axis scale: absolute frequency? but then how to compare different unit sizes?\n* 5.1 \"character-based..much better..on less frequent and OOV words\": seems to apply on in Russian and German\n* based on the WMT18 systems, it seems that it would have been closer to the state of the art to study Transformer models rather than LSTM-based models.\n* footnote 4 is good. I wish the variance of results had been calculated. I regularly suspect that the papers draws conclusions from insignificant differences in performance.\n\nSuggestions and typos\n1 first bullet at end: I suggest to clarify \"when used to model a,b, and c at word level\", idem in line 2 of sec 5\n3 the connection between z (eg BPE) and x (word) is unclear\n3.1 gerund: why use teletype font?\n3.2 \"ranging from\" has no \"to\": suggest remove\nbelow table 3: I disagree that the tag S\\NP/NP indicates that \"in\" attaches to the verb\n4 The classifier is a *multi-category* logistic regression\n4 500 dim *2 layers * 2 directions (encoder) resp. 1 direction (decoder)\n5.4 performed worse\n6 present\n6 results", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}