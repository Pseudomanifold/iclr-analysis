{"title": "This paper presents the idea of learning models of the environment while interacting with it, in the form of performing the usual model-based or model-free reinforcement learning, while enforcing consistency between the real world (observations) and the model. The presented motivation is that agents, like people, can benefit through not just observing the environment and learning from it, but also by experimenting---trying actions specifically for learning", "review": "---Below is based on the original paper---\nThis paper presents a framework that allows the agent to learn from its observations, but never follows through on the motivation of experimentation---taking actions mainly for the purpose of learning an improved dynamics model. All of their experiments merely take actions that are best according to the usual model-based or model-free methods, and show that their consistency constraint allows them to learn a better dynamics model, which is not at all surprising. They do not even allow for the type of experimentation that has been done in reinforcement learning for as long as it has been around, which is to allow exploration by artificially increasing the reward for the first few times that each state is visited. That would be a good baseline against which to compare their method.\n\nOverall:\nPros:\n1. Clear writing\n2. Good motivation description.\n\nCons:\n1. Failed to connect presented work with the motivation.\n2. No comparison against known methods for exploration.\n\n\n----Below is based on the revision---\n\nThanks to the reviewers for making the paper much clearer. I have no particular issues on the items that are in the paper. However, subsections 7.2.1 and 7.2.2 are missing.", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}