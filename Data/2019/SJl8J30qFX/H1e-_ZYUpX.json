{"title": "Well written paper but method lacks novelty", "review": "Summary: This paper makes an interesting contribution of providing global explanations of black box models (such as neural nets) using a special class of models called generalized additive models.  While the paper is well written and experiments are quite detailed, I have some problems with some of the basic premises of this work. \n1. The concept of using simpler models to approximate other complex models (model distillation) is not new and has been explored quite a bit already in ML literature. The only new proposition of this work is to use generalized additive models to approximate other complex models. This seems rather incremental. \n2. The premise behind using generalized additive models (GAMs) to explain other complex models is that GAMs are interpretable. I am not convinced about this premise. While I can intuitively see that GAMs might be able to better approximate complex models compared to rules and trees, I highly doubt if they are even interpretable. \n\nPros:\n1. The paper is well written\n2. Experiments are very detailed and thorough\n\nCons:\n1. The proposed approach lacks novelty\n2. Experimentation lacks a user study which helps understand if and when GAMs are at least as interpretable as rule-based approaches.\n\nDetailed Comments: \nI actually like the way this paper is written and executed. The writing is very clear and experiments are quite thorough. But, as discussed earlier, I have some issues with the basic premises of this paper i.e., novelty of the proposed approach and justification for the claim that GAMs are interpretable. I would encourage the authors to discuss these two aspects in their rebuttal. I would strongly encourage the authors to carry out at least a simple user study which compares the interpretability aspect of GAMs with rule-based or prototype based approaches. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}