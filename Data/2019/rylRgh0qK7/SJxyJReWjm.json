{"title": "Paper is clearly sub-par, incoherent and ignores established work.", "review": "The paper addresses the problem of control in settings where the norm of the state can become arbitrarily large and diverge as time goes to infinity. In example 2.1, the authors provide an MDP where the norm of the state is multiplied by some constant >1 at every time step for certain policies. Naturally, other quantities defined in terms of the state diverge as well. Since they define a reward function that is linear in the state, the value function diverges as well, as does its gradient. \n\nThe authors are right to notice that the problem of stability of dynamical systems is important - indeed, this type of phenomenon has been crucial to the development of control theory from the beginning. The scope of the paper is clearly within the remit of ICLR.\n\nHowever, I do not think that the way they analyse and tackle the problem is correct. I have the following objections to the analysis and methods of the paper.\n\n1. The authors wrongly claim that the problem is specific to systems with deterministic transitions and that no difficulty exists in systems with stochastic ones. This is clearly false. Indeed, consider a system with stochastic transitions, which is obtained by taking the transition operator defined in Example 2.1 and adding (eta)^2 to each entry, where eta is a zero-mean normal random variable. The problem of divergence still exists. This does not contradict Silver et al. who simply assume that all the relevant quantities exist (and are bounded).\n\n2. The authors completely ignore years of research into system stability done in the control community. Systems with differentiable dynamics and differentiable actions are the cornerstone of the field and criteria for stability are well-known. This is especially troubling given Example 2.1 is linear, which is probably the most canonical class of systems that have been studied. The paper does not cite even one text on control theory!\n\n3. The whole premise of reducing the discount as a way of tackling instability (vide Lemma 1) is potentially faulty because the underlying state vector still diverges. Since machine learning assumes bounded arguments and the value function takes the state as an argument, I don't see how we can learn value functions of unbounded states in general. The authors do not address this at all.\n\n4. The authors do not address the problem of exploration. Typically, policy gradient methods are based on some notion of ergodicity / sufficient exploration assumption. I don't see at all how the authors guarantee that the discounted measure \\rho(s) covers the whole state space in the presence of entirely deterministic dynamics.\n\n5. The premise of using model-based RL to tackle the problem of diverging state is neither motivated nor sound. Indeed, if I start with an MDP where the state diverges and learn a good model, the model will simply reflect the original MDP in the sense that planning wrt. to the model will lead to divergent quantities. Therefore, the GDPG algorithm defined in section 4.2 bears no relevance at all to the divergence issues presented in earlier sections.\n\n6. The paper is very sloppily written for something which pretends to provide a result on divergence, a formal concept. The indicator function 'I' used in the line below formula (1) is never defined. The arguments used are both informal and expressed in bad English. For example, the phrase \"s (prime) is reached from the initial state with infinite steps\" makes sense neither as an English sentence nor as a mathematical statement.\n\nConsidering the above, I believe that publishing the paper as it stands would actively detract from the community by giving a platform to claims which are partly incoherent and partly downright wrong. \n\nI do not find anything valuable in this paper. I will argue to get it rejected.", "rating": "1: Trivial or wrong", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}