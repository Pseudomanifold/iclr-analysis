{"title": "Novel idea, but not without issues to be addressed", "review": "This paper presents a new quasi-Newton type method for stochastic optimization problems. The primary contributions of the paper include a new stochastic linesearch method as well as a novel way to incorporate second order information which is different from existing approaches such as BFGS or L-BFGS. \n\nIn terms of the clarity, I think this is a very well-written paper with nice organization. The paper does have some typos, though. \n\nIn terms of significance, how to incorporate second-order information in stochastic optimization has long been an important research topic. Most existing stochastic quasi-Newton methods use L-BFGS method to incorporate second order information and choose a fixed, small stepsize, with the only differences being how to compute the curvature pair (s_k, y_k). Therefore, this paper is addressing a very important question and has made respectable attempt to use mechanisms other than L-BFGS method and to incorporate a linesearch scheme. \n\nSpecifically, the paper relaxes the secant equation, which is natural for the stochastic settings because the difference in gradients y_k is computed from stochastic gradients, and the true Hessian only satisfies the secant equation in expectation. I believe replacing the secant equation is an important and promising direction.\n\nHowever, there are concerns about the new approach proposed in this paper:\n\n1.\tThe resulting Hessian inverse approximation in (14) is no longer symmetric, or guaranteed to be positive definite. While the underling true Hessian might not be positive definite because of the nonconvexity, it is always symmetric. Is it possible to impose symmetricity as a constraint in (12)?\n\n2.\tWhat is the correct way to choose regularization parameter \u03bb in (14)?\n\nThe paper also proposes a stochastic linesearch algorithm. For this part, there are several concerns as well:\n\n1.\tThe assumption that the covariance of gradient estimator is a constant multiple of identity is a strong and unrealistic assumption, which is never satisfied in machine learning. \n\n2.\tThe algorithm performs a backtracking linesearch, with the initial trial stepsize decreaing as O(1/k), which means that the stepsize used is always decreasing at least as fast as O(1/k). This is in general in stark contrast with the intuition that a O(1) stepsize should be used for a quasi-Newton method. \n\n3.\tSatisfying the Armijo condition in expectation does not lead to any useful convergence guarantee. \n\nThe paper also presented some numerical experiments. While the numerical results look promising, I would appreciate some clarification about what method they are really comparing against. For example, for LBFGS the authors cite R. Bollapragada et al. \u201cA progressive batching L-BFGS method for machine learning\u201d. Is the paper comparing against progressive batching L-BFGS? The results of LBFGS here seem to be very different from the paper cited. \n\nFinally, the paper could certainly benefit by making some mathematical statement more rigorous. For example, Lemma 1 and 2 are stated in expectation; however, since the algorithm is a stochastic algorithm, the whole sequence {x_k} generated is a stochastic process, and the expectation in the lemmas are conditional expectations. It is important to clarify w.r.t. what the conditional expectation is taken.\n\nIn summary, I believe that this paper has made a novel contribution. However, the author should address the concerns above.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}