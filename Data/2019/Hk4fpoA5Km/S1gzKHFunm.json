{"title": "Interesting paper on the challenges of GAIL", "review": "This paper investigates two issues regarding Adversarial Imitation Learning. They identify a bias in commonly used reward functions and provide a solution to this. Furthermore they suggest to improve sample efficiency by introducing a off-policy algorithm dubbed \"Discriminator-Actor-Critic\". They key point here being that they propose a replay buffer to sample transitions from. \n\nIt is well written and easy to follow. The authors are able to position their work well into the existing literature and pointing the differences out. \n\nPros:\n\t* Well written\n\t* Motivation is clear\n\t* Example on biased reward functions \n\t* Experiments are carefully designed and thorough\nCons:\n\t* The analysis of the results in section 5.1 is a bit short\n\nQuestions:\n\t* You provide a pseudo code of you method in the appendix where you give the loss function. I assume this corresponds to Eq. 2. Did you omit the entropy penalty or did you not use that termin during learning?\n\n\t* What's the point of plotting the reward of a random policy? It seems your using it as a lower bound making it zero. I think it would benefit the plots if you just mention it instead of plotting the line and having an extra legend\n\n\t* In Fig. 4 you show results for DAC, TRPO, and PPO for the HalfCheetah environment in 25M steps. Could you also provide this for the remaining environments?\n\n\t* Is it possible to show results of the effect of absorbing states on the Mujoco environments?\n\nMinor suggestions:\nIn Eq. (1) it is not clear what is meant by pi_E. From context we can assume that E stands for expert policy. Maybe add that. Figures 1 and 2 are not referenced in the text and their respective caption is very short. Please reference them accordingly and maybe add a bit of information. In section 4.1.1 you reference figure 4.1 but i think your talking about figure 3.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}