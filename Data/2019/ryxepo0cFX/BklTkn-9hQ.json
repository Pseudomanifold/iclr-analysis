{"title": "Good paper with original work, experiments could be improved", "review": "In this paper, the authors provide a new approach to analyze the behavior of\nRNNs by relating the RNNs with ODE numerical schemes. They provide analysis on\nthe stability of forward Euler scheme and proposed an RNN architecture called\nAntisymmetricRNN to solve the gradient exploding/vanishing problem. \n\nThe paper is well presented although more recent works in this direction\nshould be cited and discussed. Also, some important issues are omitted and not\nexplained. \nFor example, the analysis begins with \"RNNs with feedback\" rather than vanilla\nRNN, since vanilla RNN does not have the residual structure as eq(3). The\nauthors should note that clearly in the paper. \n\nAlthough there are previous works relating ResNets with ODEs, such as [1],\nthis paper is original as it is the first work that relates the stability of\nODE numerical scheme with the gradient vanishing/exploding issues in RNNs. \n\nIn general, this paper provides a novel approach to analyze the gradient\nvanishing/exploding issue in RNNs and provides applicable solutions, thus I\nrecommend to accept it. \n\n\nDetailed comments:\n\nThe gradient exploding/vanishing issue has been extensively studied these\nyears and more recent results should be discussed in related works.\nAuthor mentioned that existing methods \"come with significant computational\noverhead and reportedly hinder representation power of these models\". However\nthis is not true for [2] which achieves full expressive power with\nno-overhead. \nIt is true that \"orthogonal weight matrices alone does not prevent exploding\nand vanishing gradients\", thus there are architectural approaches that can\nbound the gradient norm by constants [3]. \n\nThe authors argued that the critical criterion is important in preserving the\ngradient norm. However, later on added a diffusion term to maintain the\nstability of forward Euler method. Thus the gradient will vanish\nexponentially w.r.t. time step t as: (1-\\gamma)^t. Could the authors provide\nmore detailed analysis on this issue? \n\nSince eq(3) cannot be regarded as vanilla RNN, it would be better begin the\nanalysis with advanced RNN architectures that fit in this form, such as\nResidual RNN, Statistical Recurrent Units and Fourier Recurrent Units. \n\nWhy sharing the weight matrix of gated units and recurrent units? Is there any\nother reason to do this other than reducing the number of parameters?\n\nMore experiment should be conducted on real applications of RNN, such as\nlanguage model or machine translation. \n\n\n[1] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer\nneural networks: Bridging deep architectures and numerical differential\nequations. In ICML, pp. 3276\u20133285, 2018.  \n\n[2] Zhang, Jiong, Qi Lei, and Inderjit S. Dhillon. \"Stabilizing Gradients for\nDeep Neural Networks via Efficient SVD Parameterization.\" In ICML, pp.\n5806-5814, 2018.\n\n[3] Zhang, Jiong, Yibo Lin, Zhao Song, and Inderjit S. Dhillon. \"Learning Long\nTerm Dependencies via Fourier Recurrent Units.\" In ICML, pp. 5815-5823, 2018. \n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}