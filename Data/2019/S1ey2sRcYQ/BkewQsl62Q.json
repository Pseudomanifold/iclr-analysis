{"title": "A significant contribution", "review": "The authors propose a method to apply the reparametrization trick when the random variables of interest are discrete. Their technique is based on a formulation of the objective function in terms of Gumbel-Max operators. They propose a derivation of the gradient in terms of an auxiliary variable \\epsilon, such that the resulting gradient estimate is biased but the bias is reduced as \\epsilon approaches zero, at the cost of increasing variance. Experiments are performed with VAE including discrete latent variable models. The authors show how their method converges faster than other baselines formed by estimators of the gradient given by the REBAR, RELAX and Gumbel-soft-max methods. In experiments with semi-supervised VAEs, their method outperforms the Gumbel softmax method in terms of accuracy and objective function.\n\nQuality:\n\nThe theoretical derivations seem rigorous and the experiments performed clearly indicate that the proposed method can outperform existing baselines.\n\nClarity:\n\nThe paper is clearly written and easy to read. I found that the network architecture shown in the left of Figure 1 a bit confusing and needs to be explained more clearly.\n\nSignificance:\n\nThe experimental results clearly show that the proposed method can outperform existing baselines and that the proposed contribution is significant.\n\nNovelty:\n\nThe proposed method is novel up to my knowledge. This is the first time I have seen the proposed theoretical derivations, which are significantly different from previous approaches.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}