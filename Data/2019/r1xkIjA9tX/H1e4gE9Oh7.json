{"title": "On the adoption of q-activations", "review": "The authors describe q-activation functions, stochastic relatives of common activation functions used in neural networks.  It seems like the main argument is to use them because you get a performance improvement with them. \n\nWhile the experiments appear to show better training at early epochs, none of the models appear to have been trained to convergence.  Additional justifications for why (or when) to use this should be described.\n\nWhy does the method outperform particularly when dropout is included?\n\nI also expect the lack of monotonicity in the q-activation functions to lead to the creation of (exponentially) more local minima.  Any comments?\n\nQuality: the experiments need some further work.\nClarity: aside from a few points, the paper is written clearly.\nOriginality: the work appears original to me\nSignificance: TBD, but the main argument appears to be that it leads to empirical comparative gains (but on networks not designed to be SOTA).\n\nSmall points:\n\"By prop 2, g_q(x) agrees with with original activation function\".  What does \"agrees with\" mean?\n\"Fig 2. Darker color --> lighter color?\"\n\"(Conclusion) ... can goes[sic] deeper on the error surface.\" To me, the experiments only show marginally better performance", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}