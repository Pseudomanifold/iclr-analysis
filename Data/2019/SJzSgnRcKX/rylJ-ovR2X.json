{"title": "Nice empirical paper", "review": "\nThis is a nice paper that attempts to tease apart some questions about the effectiveness of contextual word embeddings (ELMo, CoVe, and the Transformer LM). The main question is about the value of context in these representations, and in particular how their ability to encode context allows them to also (implicitly) represent linguistic properties of words. What I really like about the paper is the \u201cEdge probing\u201d method it introduces. The idea is to probe the representations using diagnostic classifiers\u2014something that\u2019s already widespread practice\u2014but to focus on the relationship between spans rather than individual words. This is really nice because it enables them to look at more than just tagging problems: the paper looks at syntactic constituency, dependencies, entity labels, and semantic role labeling. I think the combination of an interesting research question and a new method (which will probably be picked up by others working in this area) make this a strong candidate for ICLR. The paper is well-written and experimentally thorough.\n\nNitpick: It would be nice to see some examples of cases where the edge probe is correct, and where it isn\u2019t.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}