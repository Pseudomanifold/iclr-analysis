{"title": "Setting doesn't make sense for RL and experiments don't evaluate causal questions", "review": "This paper presents a method for reinforcement learning (RL) in settings where the relationship between action and reward is confounded by a latent variable (unobserved confounder). While I firmly believe that RL would benefit from taking causality more seriously, this paper has many fatal flaws that make it not ready for publication. \n\nFirst, and most importantly, the paper is unclear about the problem it is trying to solve. It talks about confounded RL as being settings in which a confounder affects both the action and reward. In typical RL settings this wouldn\u2019t make sense: in RL you get to choose the policy so it doesn\u2019t make sense to assume that the choice of action is confounded while you\u2019re doing RL. To get around this, the authors assume that they\u2019re working with observational data and doing RL on a generative model leant from the observational data. But by doing this, they have assumed away the key advantage that RL has over causal inference: the ability to experiment in the world. The authors justify this assumption by considering high-stakes settings where experimentation is either too risky or too costly, but they don\u2019t explain why you would want to do RL at all when you could just do causal inference directly. If you can\u2019t experiment, RL offers no advantages over standard causal inference methods and bring serious disadvantages (sample-efficiency, computational cost, etc.). \n\n# Method\nThe authors learn a variational approximation to a particular graphical model that they assume for their RL setting. They then treat the variational approximation as the true distribution which allows them to perform causal inference via the backdoor correction. They claim this is identified but this is false - it is only identified with respect to the variational distribution, not the true distribution and we have no a priori reason to  believe that the variational distribution well-approximated the true distribution. In principle, the authors could have tested how well this works experimentally but their experimental setup has problems which prevent this being evaluated. \n\nQuibbles:\n - Page 3: the authors claim the model is \u201cwithout loss of generality\u201d but this is false - there are many settings that would not conform to this model: e.g. the multi agent settings that economics studies; health settings with placebo effects where reward depends on observations directly; etc.\n  - Page 4 above the equations: either the equations describe the variational approximation to the generative model or the equations shouldn\u2019t all be factorized normal distributions. Real data isn\u2019t made up of factorized normals.\n\n# Experiments\n\nThe authors evaluate their method on three simulated datasets: Confounding MNIST, Confounding Cartpole and Confounding Pendulum. All three have the same methodological problems so I\u2019ll only focus on the MNIST dataset. They synthesize their MNIST dataset but corrupting a subset of MNIST digits with noise and treating actions as rotations. Rewards are given by the absolute difference in angle between the rotated digit and the original unrotated digit. \u201cConfounding\u201d is added by having a binary latent variable affect the amount that the digit is rotated - but importantly, the reward isn\u2019t affected directly by the latent variable. Because of this, there isn\u2019t actually a confounding problem - the \u201cconfounder\u201d simply changes the rotation of the digit and can be treated as additional experimentation from the perspective of causal inference. The authors evaluate their method by examining reconstructions of the MNIST digit, but this simply checks how well the variational inference is working, not whether the causal inference is working (there would be no way to evaluate the latter on this dataset because there is no confounding). Effectively all they find is a better-designed variational distribution will do a better job of reconstructing the input (without modelling the latent u, the VAE is forced to average over its two states resulting in more blurry samples). \n\nThe RL evaluations aren\u2019t described in enough detail to conclusively explain the difference observed, but it seems to be driven by the fact that the standard RL methods are working with worse variational approximation distributions.\n\n# Summary\nThis work studies a setting in which the correct baselines would be causal inference algorithms (but they aren\u2019t considered) and their experimental evaluation has serious flaws that prevent it supporting the claims made in the paper. \n", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}