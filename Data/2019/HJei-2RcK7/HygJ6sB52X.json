{"title": "Useful but straightforward idea", "review": "Summary\n========\nThe  paper  adopts  the  self-attention  mechanism  in Transformer and in message-passing graph neural networks to derive  graph-to-graph mapping. Tested on few-shot learning, medical imaging classification and graph classification problems, the proposed methods show competitive performance. \n\nComment\n========\nGraph-to-graph mapping is an interesting setting and the paper presents an useful solution and interesting applications.  The paper is easy to read.\n\nHowever, given the recent advancements in self-attention and message-passing graph modeling under various supervised settings (graph2vec, graph2set, graph2seq and graph2graph), the methodological novelty is somewhat limited. The idea of intra-graph and inter-graph message passing, for example, has been studied in:\nDo et al. \"Attentional Multilabel Learning over Graphs-A message passing approach.\" arXiv preprint arXiv:1804.00293 (2018).\n\nComputationally, the current solution is not very scalable for large input and output graphs.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}