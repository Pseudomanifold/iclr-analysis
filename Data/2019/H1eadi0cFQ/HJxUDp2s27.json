{"title": "Interesting work, though not sure how symmetry breaking happens", "review": "This is an interesting submission and I appreciate especially connecting to a body of literature which is not normally well known in our community (e.g. Fukumizu&Amari). I think the perspective is definitely new and probably quite relevant not only for practical approaches to escape saddles but also to understand learning in deep learning.  I have a few notes and suggestions:\n\n1) Name of the paper: \nI think is not descriptive of the approach and actually the words \u201cmagic\u201d makes it sound strange. I think this will reduce the amount of people reading the work. Please consider something more descriptive like: \u201cEscaping saddle points by increasing capacity\u201d. Or something else more inspired, but that also hints what the work is about. \n\n2) Notation: \nThe notation used is not ML friendly (or generically) to the average reader. I strongly suggest to use b_v for bias, W_uv for weights, and not theta_v and theta_uv which is not typical notation. \u2018u\u2019 and \u2018v\u2019 are somewhat non-typical choices either, though I understand that they come from the graph notation. Transfer functions are usual sigma. In the text you explain the process by starting with a u\u2019 and then add the clone which is u. Normally you should have started with u and add the clone that is u\u2019. x_u for the value of unit u (assuming this is in the middle of a deep net) is also quite a strange notation. I can guess the authors might be from a slightly different community, but I\u2019m worried about people from the target audience (ICLR) being turned away from the work or even worse confused because of notations.\n\n3) Related work\nThere is the Net2Net work that is related to what is going on here that is not cited (https://arxiv.org/pdf/1511.05641.pdf). I think there was some follow-up work after this.\n\n4) Symmetry breaking\nI do not understand how symmetry is broken. If I clone a unit, and have a new variant of it u\u2019 that now has the same outbound connections but multiplied by alpha (while the original unit by 1-alpha) then while the norm of the gradients differ, their direction does not. Wouldn\u2019t this mean that the units will track each other and hence no tunnel is open? In Net2Net dropout was used to break symmetry (i.e. a source of noise that would pick one path over the other). There is no source of noise here to break symmetry. \n\n5) Diagrams and analysis\nConnected to this, I feel like this could have been represented clearly with a diagram showing the net before and after. There could be some analysis, a more extended discussion of where the symmetry breaking comes from, empirical evidence that it does. I\u2019m not necessarily worried that experiments are not scaled up, I\u2019m more concerned that the hypothesis and solution is only tested by means of change in performance. What is this tunnel doing? How does it change the Hessian at the saddle? Any visualization to reinforce the intuition of what the approach is doing? \n\n\n6) Closing tunnels & re-organizing\nI don\u2019t understand the mechanism for reorganizing weights and closing tunnels. It seems first of all to confirm my intuition that there is no symmetry breaking since we can \u201cclose\u201d the tunnel by simple algebraic manipulation. So if those two units always stay in sync how do you actually change the error surface? How do you take advantage of this extra capacity to solve anything. Regardless, when it comes to re-organizing, it seems you pick two of these units that are in sync (previous tunnel I guess) and collapse them to open a new tunnel, right? How does this change anything? Which unit needs to be cloned? Any? So then why is the previous tunnel not efficient anymore? \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}