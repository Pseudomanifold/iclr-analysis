{"title": "Interesting ideas for expanding/contracting a network's size to escape saddles, but would benefit from further experiments", "review": "Summary:\n\nThis paper presents a new strategy for escaping saddle points by adding and removing hidden units during training. The method essentially finds conditions where adding a hidden unit does not change the overall input-output map, and uses these as constraints to add a hidden unit that maximally increases the gradient norm (thus potentially getting learning unstuck). Experiments show that the method can improve training speed relative to the same network with randomly added new units.\n\nMajor comments:\n\nThis paper presents interesting theoretical ideas and clearly separates the opportunities for adding/removing hidden units without changing the input-output map from the impact on the gradient due to the change in parametrization. \n\nThe experiments show that the proposed method can speed up learning when a network is genuinely stuck at a saddle point. Importantly however, the experimental evaluation intentionally seeks saddle points using Newton\u2019s method, such that learning is genuinely stuck, before adding the additional units. It is therefore less clear whether this method can offer speedups to network training in practice. Do NNs come close enough to saddle points to benefit from the method when beginning from typical initializations? Experiments on ImageNet begin from a specifically chosen random seed that happens to enter a very flat region. How many random seeds were tried before finding this one? This would speak to the importance of these findings in general. The paper would benefit greatly from applying the proposed method to networks trained under standard protocols, to identify the speed up (if any) it can confer for the average case. It would also be important to account for wall clock time, as the proposed method involves potentially expensive steps (at least in its straightforward form).\n\nThe paper notes several other strategies for expelling from saddle points. The experimental evaluation could be improved greatly by including comparisons to these alternatives. Does the proposed method escape more quickly, or have other merits relative to these alternatives?\n\nThe clarity of the paper is good overall but the title could be improved to be more informative of the content of the method.\n\nOverall the significance of the paper is not clearly established because the evaluations mostly consist of internal comparisons, in somewhat unnatural settings (where networks are initialized right next to saddle points). The theoretical observations, however, seem promising.\n\nMinor comments:\n\nI could not understand the motivation for closing the tunnel\u2014it seems as though optimization proceeds more quickly if it remains open. \n\nThe paper discusses a range of relevant work but would benefit from citing other incremental learning work in neural networks, in particular:\n\nY. Bengio et al., Convex Neural Networks, NIPS 2006\n\nF. Bach. Breaking the curse of dimensionality with convex neural networks. JMLR 2017\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}