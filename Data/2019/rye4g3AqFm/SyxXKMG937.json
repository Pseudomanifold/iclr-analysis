{"title": "A fresh study to the generalization capabilities of (deep) neural networks, with the help of the PAC-Bayesian learning theory and empirically backed intuitions.", "review": "The paper brings a fresh study to the generalization capabilities of (deep) neural networks, with the help of an original use of PAC-Bayesian learning theory and some empirically backed intuitions.\n\nExpressing the prior over the input-output function space generated by the neural network is very interesting. This provides an original analysis compared to the common PAC-Bayesian analysis of neural networks that express the prior over network parameters space. The theoretical study here appears simple (noteworthy, it is based one of the very first PAC-Bayesian theorems of McAllester that is not the most used nowadays), and the study is conducted mainly by empirical observation. Nevertheless, the experiments leading to these observations are cleverly designed, and I think it gives great insights and might open the way to other interesting studies.\n\nOverall, the paper is enjoyable to read. I also appreciate the completeness of the supplementary material. I recommend the paper acceptance, but I would like the authors to consider the concerns I rise below:\n- The paper title is a bit presumptuous. The paper presents a conjunction backed by empirical evidence on some not-so-deep neural networks. Even if I consider it as an important piece of work, it does not provide any definitive answer to the generalization puzzle. \n- Many peer-reviewed publications are cited as arXiv preprints. Please carefully complete the bibliography. Some papers are referenced by the name, title and year only (Smith and Le 2018; Zhang et al, 2017)\n- I recommend adding to the learning curves of Figures 2 and 3 the loss on the training set. \n\nOther minor comments and typos:\n- Intro: Please define \"parameter-function\" map \n- Page 4: Missing parentheses around Mand et al. (2017)\n- SGD has not had time ==> SGD did not have time\n- Please refers to the definition in the supplementary material/information the first time you mention Lempel-Ziv complexity.\n- Please mention that SI stands for Supplementary Information\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}