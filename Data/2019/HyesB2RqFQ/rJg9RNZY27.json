{"title": "review of the paper", "review": "This paper discusses the connections between HMMs and RNNs and investigates a number of architectural transformations between the two for the expressivity and interpretability.  Overall, the paper is well-written and the logic is clear.  However, I have following concerns.\n\n1.  The HMMs discussed in the paper, in my opinion, are only a subset of the HMM family.  First of all, it only covers the HMMs with a discrete emission state distribution which is commonly used in NLP, but not popular in speech recognition.  Speech recognition, which is dominantly based on HMMs, uses continuous emission state distributions such as Gaussian distributions or Gaussian mixture distributions, which is not addressed in the framework investigated in this paper.  Actually starting from the fundamentals, the authors phrase the framework as \"word prediction\",\"word distribution\" and \"word probability\", which all hints that the HMM-RNN discussion is implicitly carried out in the NLP domain.   Therefore, I would suggest the authors make it clear in the title to point it out that this discussion is about HMMs with discrete emission state distributions. \n\n2. A follow-up comment on HMMs.  I think some of the architecturally transformed HMMs, which are considered to be some special form of RNNs, are actually still within the HMM family.  The difference is that they are not homogeneous HMMs any more. Their state transitions are not fixed and state emission distributions can also be time variant. These heterogeneous HMMs are still HMMs, although they possess some characteristics of RNNs.  Again,  the assumption on HMMs in this paper is too limited to begin with as HMMs consist of a broad family of models. \n\n3. I also have concerns with the experiments.  The PTB baseline seems a bit high to me.  I will feel more comfortable if a single-layer LSTM LM can have a perplexity around 70. Note that this is not even state of the art.   Overall, I find the experimental justification is not overwhelmingly strong. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}