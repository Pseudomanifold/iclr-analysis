{"title": "Nice and simple", "review": "Summary: The paper suggests a method to reducing the space consumption of training neural nets, in exchange for additional training time. The method stores in memory only a subset of the intermediate tensors computed in the forward step, and then in the backward step it re-computes the missing tensors as they are needed by interpolating forward (again) between the stored tensors. The paper also gives a combinatorial algorithm for choosing which tensors to store on a given computation DAG annotated with vertex costs.\n\nEvaluation: I generally like the paper. The proposed method is simple and straightforward, and seems to lead to a noticeable improvement in space usage during training.\nThe part related to decomposing a DAG into \"close sets\" looks like it might overlap with existing literature in graph theory; I don't have concrete references but the authors may want to check this. The ACG solver algorithm looks somewhat wasteful in terms of the degree of the polynomial running time, but since actual computation graphs are tiny in computational terms, I guess this is not really an issue.\nOne point not addressed in the experiments is what is the overhead incurred in training time by the two space-efficient methods over usual training. I suppose one expects the training time be less than twice longer, since the re-forwards amount to one additional complete forward step per forward-backward pair.\nAnother thing that would be interesting to see is the actual stored vertices (V_R) that were chosen empirically for the rows in table 1 (or at least some rows). Since the computational graphs of the tested networks are small, and some are well-known, I imagine it should doable (though this is merely a suggestion).\n\nConclusion: The paper is simple and looks reasonable, with no major issues that I could detect.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}