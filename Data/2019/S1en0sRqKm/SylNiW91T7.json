{"title": "Limited insights in the understanding of the batch size effect", "review": "The work presented relates to the impact batch-size on the learning performances of common neural network architectures.\n\nPro: having comprehensive study of the limit of gradient-based methods is very useful in practice. This work can help practitioner to limit the number of machines used for optimization.\n\nCons: very little can be deduced from these experiments:\n- \"Increasing the batch size beyond a certain point yields no improvement in wall-clock time to convergence, even for a system with perfect parallelism.\" was a know fact (they cite Ma et al (2017) who even proved it theoretically.\n- \"Increasing the batch size leads to a significant increase in generalization error, which cannot be mitigated by existing techniques.\". It is not clear that all the regularization techniques have been tried by the authors, the increase of generalization error is very small, and there is no explanation or insight given by the authors to explain this phenomenon, making this finding of limited interest.\n- \"Dataset size is not the only factor determining the computational efficiency of large batch training.\" is something obvious to say, as there are plenty of factors that determine the computational efficiency (network connection, map-reduce implementation, etc.)\n\nEven the suggestions for future work of the authors in the conclusion does not help much: they suggest to look at \"alternative forms of parallelism\", without citing or giving any clue of what could be such alternative forms. \nAlso, there is no discussion around lock-free\n\nThe authors refer to Ma et al. (2017) for a theoretical analysis of the effect of the batch size, but they skip all the past and very relevant literature on the topic of the effect of the batch size on the convergence. For example, it is recommended to increase the size of the batch size as the iterations increase.\n\nFinally, there is no discussion on the lock-free gradient descent, that is often suggested as an alternative to batching.\n\nIn conclusion, I'm not convinced there is enough material to accept this paper at the next ICLR conference.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}