{"title": "Some interesting empirical results for a popular problem", "review": "Summary:\nThe authors present an empirical analysis of how the size of SGD batches affects neural networks' training time.   \n\nStrengths:\nAs mini-batches training is highly popular nowadays, the problem emphasized by the authors may have a high impact in the community. Together with recent analysis on the generalization properties of over-parametrized models, the paper may help understand more general open problems of neural networks' training. A nice contribution of the paper is the observation that different phases of scaling behaviour exist across a range of datasets and architectures.  \n\nWeaknesses:\nBased on empirical evaluation, the paper cannot make any claim about the generality of the obtained results. Even if the authors' analysis is based on a large set of benchmarks, it is hard to asses whether and how the results extend to cases that are not included in Section 4. In particular, it is not clear how the definition of different training phases can help the practitioner to tune the training parameters, as the size and range of the different regimes depend so strongly on the model's architecture and dataset at hand.  \n\nQuestions:\n- have the properties of mini-batches training been explored from a formal/theoretical perspective? do those results match and confirm the proposed empirical evaluation?\n- how are the empirical results obtained in the experiment section expected to depend on the specific dataset/benchmark? For example,  given a particular architecture, what are the key features that define the three training phases (shape of the nonlinearity, number of layers, underlying distribution of the dataset)?\n- what is a  batch size that does not allow one to 'fully utilize our available compute'?\n- does the amount of over-parameterization in the model have any effects on the definition of the training phases? How are the results obtained in the paper linked to the generalization gap phenomenon?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}