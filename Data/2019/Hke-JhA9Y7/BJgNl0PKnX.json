{"title": "This paper lacks technical novelty and experimental result is incomplete.", "review": "This paper introduces a genetic algorithm that maintains an archive of representations that are iteratively evolved and selected by comparing validation error. Each representation is constructed as a syntax tree consists of elements that are common in neural network architectures. The experimental results showed that their algorithm is competitive to the state-of-the-art while achieving much smaller model size.\n\nComments:\n1. I think this paper lacks technical novelty. I'm going to focus on experimental result in the following two questions.\n2. FEAT is a typical genetic algorithm that converges slowly. In the appendix, one can verify that FEAT converges at least 10x slower than XGBoost. Can FEAT achieve lower error than XGBoost when they use the same amount of time? \nCan the authors provide a convergence plot of their algorithm (i.e. real time vs test error)?\n3. From Figure 3 it seems that the proposed algorithm is competitive to XGBoost, and the model size is much smaller than XGBoost. Have the authors tried to post-processing the model generated by XGBoost? How's the performance compare?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}