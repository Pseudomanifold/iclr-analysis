{"title": "Potentially significant results, some question marks.", "review": "Disclaimer:  I am not a working expert in this specific area.  I have used spectral normalization for my own applications, and have working expertise in leveraging Lipschitz properties for various flavors of stability analyses.\n\nThis paper proposes a error convergence analysis for Lipschitz-regularized neural nets.  The analysis is framed in function space of the neural net and assumes the ability to solve the learning minimization problem.   The authors contrast their analysis with other analysis approaches in several ways.  First is that their analysis is \"more direct\" and second is that their analysis is independent of the learning approach (e.g., spectral normalization + SGD). \n \n\n***Clarity***\n\nThe paper is mostly clearly written.  Some of the statements are presented without sufficient description.  For instance:\n\n-- What does it mean when the paper states that their analysis is \"more direct\" than previous work?  There was no discussion of previous work beyond that comment.\n\n-- The statement of Lemma 2.9 is not entirely clear.  Is \\sigma_n a vector of white gaussian random variables?\n\n-- Definition 2.3 precludes the hinge loss.  Comments?\n\n-- Example 2.6, the regularized cross entropy loss doesn't satisfy L(y,z) = 0 if and only if y=z.  It might not even satisfy L >= 0. \n Comments?\n\n-- Since the function is Lipschitz, in the noisy case, can one say anything about the guarantees near the manifold for some definition of near?  E.g., can one bound how bad the tail parts of Figure 3 can be?\n\n\n***Originality***\n\nIt seems to me that the analysis aims to set up the problem such that one can leverage standard results in probability theory.    For instance, the proof of Theorem 2.7 is quite straightforward given Lemma 2.9.  Of course, setting up the problem properly is 90% of the work.  The analysis for the noisy case is much more involved.  It is unfortunately beyond my expertise to properly judge originality in this case.  Perhaps the authors can comment on how the way they set up the problem is novel?\n\n\n***Significance***\n\nMy biggest question mark w.r.t. significance are the claims of how this analysis compares with previous work.\n\n-- What does \"more direct\" analysis mean?\n\n-- What is the significance of an algorithm-agnostic analysis?  I understand the appeal from a certain perspective, but can the authors point to previous literature (perhaps not in deep learning) where an algorithm-agnostic analysis was shown to give more insight?\n\n\n***Overall Quality***\n\nConditioned on the problem setup being novel and the comparison with related work clarified, I think this is a solid contribution.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}