{"title": "Significant updates to the new version", "review": "The paper introduces a decentralized training method for multi-agent reinforcement learning, where the agents infer the policies of other agents and use the inferred models for decision making. The method is intuitively straightforward and the paper provides some justification for convergence. I think the underlying theory is okay (new but not too surprising, a lot of the connections can be made with single agent RL), but the paper would be much stronger with experiments that have more than two players, one state and one dimensional actions.\n\n(\nUpdate: the new version of the paper addresses most of my concerns. There are a lot more experiments, and I think the paper is good for ICLR. \n\nHowever, I wonder if the reasoning for PR2 is limited to \"self-play\", otherwise Theorem 1 could break because of the individual Q_i functions will not be symmetric. This could limit the applications to other scenarios.\n\nAlso, maybe explain self-play mathematically to make the paper self contained?\n)\n\n1. From the abstract, \" PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario\". Theorem 2 only shows that under relatively strong assumptions (e.g. single Nash equilibrium), the soft value iteration operator is a contraction. This seems to have little to do with the actual convergence of PR2-Q and PR2-AC, especially AC which uses gradient-based approach. Also here the \"convergence\" in the abstract seem to imply convergence to the (single) global optimal solution (as is shown in the experiments), for which I thought you cannot prove even for single agent AC -- the best you can do is to show that gradient norm converges to zero, which gives you a local optima. Maybe things are different with the presence of the (concave) entropy regularization?\n\n2. Theorem 2 also assumes that the opponent model $\\rho$ will find the global optimal solution (i.e. (11, 12) can be computed tractably). However, the paper does not discuss the case where $\\rho$ or $Q_\\theta$ in question is imperfect (similar to humans over/underestimate its opponents), which might cause the actual solution to deviate significantly from the (single) NE. This would definitely be a problem in more high-dimensional MARL scenarios. I wonder if one could extend the convergence arguments by extending Prop 2.\n\n3. The experiments mostly demonstrates almost the simplest non-trivial Markov games, where it could be possible that (11, 12) are true for PR2. However, the effectiveness of the method have not been demonstrated in other (slightly higher-dimensional) environments, such as the particle environments in the MADDPG paper. It does not seem to be very hard to implement this, and I wonder if this is related to the approximation error in (11, 12). The success in such environments would make the arguments much stronger, and provide sound empirical guidance to MARL practitioners.\n\nMinor points:\n- Are the policies in question stationary? How is PR2 different from the case of single agent RL (conditioned on perfect knowledge of a stationary opponent policy)?\n- I have a hard time understanding why PR2 would have different behavior than IGA even with full knowledge of the opponent policy, assuming each policy is updated with infinitesimally small (but same) learning rates. What is the shape of the PR2 optimization function wrt agent 1?\n- I wonder if using 2 layer neural networks with 100 units each on a 1 dimensional problem is overkill.\n- Figure 4(a): what are the blue dots?\n- Does (11) depend on the amount of data collected from the opponents? If so, how?\n- I would recommend combining Prop 1 and prop 2 to save space. Both results are straightforward to prove, but the importance sampling perspective might be useful.\n- Have you tried to compare with SGA (Balduzzi et al) or Optimistic mirror descent?\n- I am also curious about an ablation study over the components used to infer opponent policies. A much simpler case would be action-dependent baselines, which seem to implicitly use some information about the opponents.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}