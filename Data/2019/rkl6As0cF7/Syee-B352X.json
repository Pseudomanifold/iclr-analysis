{"title": "Interesting and sound ideas and algorithms, but experimental validation is weak", "review": "\n# Summary:\nThe paper proposes a new approach for fully decentralized training in multi-agent reinforcement learning, termed probabilistic recursive reasoning (PR2). The key idea is to build agent policies that take into account opponent best responses to each of the agent's potential actions, in a probabilistic sense. The authors show that such policies can be seen as recursive reasoning, prove convergence of the proposed method in self-play, a demonstrate it in a couple of iterated normal form games with non-trivial Nash equilibria where baselines fail to converge.\n\nI believe the community will find intuitions, methods, and theory developed by the authors interesting. However, I find some parts of the argument somewhat questionable as well as experimental verification insufficient (see comments below).\n\n\n# Comments and questions:\n\n## Weaknesses in the experimental evaluation:\nI find it hard to justify a fairly complex algorithm (even though inspired by cognitive science), when most of the simpler alternatives from the literature haven't been really tested on the same iterated games (the baselines in the paper are all simple gradient-based policy search methods).\n\nIn the introduction (paragraph 2), the authors point out potential limitations of previous opponent modeling algorithms, but never compare with them in experiments. If the claim is that other methods \"tend to work only under limited scenarios\" while PR2 is more general, then it would be fair to ask for a comprehensive comparison of PR2 vs alternatives in at least 1 such scenario. I would be interested to see how the classical family of \"Win or Learn Fast\" (WoLF) algorithms (Bowling and Veloso, 2002) and the recent LOLA (Foerster et al, 2018) compare with PR2 on the iterated matrix game (section 5.1).\n\nAlso, out of curiosity, it would be interesting to see how PR2 works on simple iterated matrix games, eg iterated Prisoner's dilemma.\n\n## Regarding the probabilistic formulation (section 4.3)\nEq. 8 borrows a probabilistic formulation of optimality in RL from Levine (2018). The expression given in Eq. 8 is proportional to the probability of a trajectory conditional on that each step is optimal wrt the agent's reward r^i, i.e., not for p(\\tau) but for p(\\tau | O=1).\n\nIf I understand it correctly, by optimizing the proposed KL objective, we fit both \\pi^i and \\rho^{-i} to the distribution of *optimal trajectories* with respect to r^i reward. That makes sense in a cooperative setting, but the problem arises when opponent's reward r^{-i} is different from r^i, in which case I don't understand how \\rho^{-i} happens to approximate the actual policy of the opponent(s). Am I missing something here?\n\nA minor point: shouldn't \\pi^{-i} in eq. 9 be actually \\rho^{-i}? (The derivations in appendix C suggest that.)\n\n## Regarding alternative approaches (section 4.5)\nThe authors point out intractability of trying to directly approximate \\pi^{-i}. The argument here is a little unclear. Wouldn't simple behavioral cloning work? Also, could we minimize KL(\\pi^{-i} || \\rho^{-i}) instead of KL(\\rho^{-i} || \\pi^{-i})?\n\n# Minor\n- I might be misreading it, but the last sentence of the abstract seems to suggest that this paper introduces opponent modeling to MARL, which contradicts the first sentence of paragraph 2 in the introduction.\n- It is very hard to read plots in Figure 3. Would be nice to have them in a larger format.\n\nOverall, I find the paper interesting, but it would definitely benefit from more thorough experimental evaluation.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}