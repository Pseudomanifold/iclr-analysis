{"title": "Quaternion Recurrent Neural Networks", "review": "Quality: sufficient though there are issues. Work done in automatic speech recognition on numerous variants of recurrent models, such as interleaved TDNN and LSTM (Peddinti 2017), is completely ignored [addressed in the revision]. The description of derivatives needs to mention the linear relationship between input features and derivatives (see trajectory HMMs by Zen and Tokuda) [addressed in the revision]. TIMIT is a very simple task [addressed by adding WSJ experiments]. Derivations in the appendices could be connected better [addressed in the revision]. \n \nClarity: sufficient. It would be good to see some discussion of 1) split activations and other possible options [short comment added in the revision] if any 2) expressions of derivatives and their connection to standard RNN derivatives [short comment added in the revision], 3) computational complexity [addressed in the revision]. \n\nOriginality: sufficient. This paper describes the extension of quaternion feed-forward neural networks to recurrent neural networks and a parameter initialisation method in the quaternial domain.\n\nSignificance: sufficient. \n\nPros: Audience interested in quaternial neural networks would benefit from this publication. Experimental results even if limited suggest that quaternial representation may offer a significant reduction in the number of model parameters at no loss in performance.  \n\nCons: The choice of derivatives to yield quaternions as there are other more interesting views to contemplate both in speech and other fields. A simple task makes it hard to judge how the quaternion extension would scale.  \n\nOther:\n\nThe format of references, the use of a number in parentheses, is unusual and distractive. [fixed in the revision] \nPlease at least name all the terms used in the main paper body even if they are defined later in the appendix (e.g. h_{t}^{*} in equation 10). [fixed in the revision]\nDo both W_{hh} and b_{h} contain the same \\delta_{hh}^{t} term in their update equation 11? [fixed in the revision]\nPage 7 by mistake mentions 18.2% which cannot be found in the Table 1. [fixed in the revision]\nPage 12 \"is equals to\" [remains in the revision]\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}