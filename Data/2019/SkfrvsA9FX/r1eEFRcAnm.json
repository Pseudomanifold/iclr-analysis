{"title": "Interesting paper, with some details not so clear to me", "review": "This paper proposed a general framework for policy optimization of constrained MDP. Compared with the traditional methods, such as Lagrangian multiplier methods and trust region approach, this method shows better empirical results and theoretical merits. \n\nMajor concerns: \nMy major concern is about the time-scales. RCPO algorithm, by essence, is multi-scale, which usually has a stringent requirement on the stepsizes and is difficult to tune in practice to obtain the optimal performance. The reviewer would like to see how robust the algorithm is if the multi-time-scale condition is violated, aka, is the algorithm's performance robust to the stepsizes? \n\nMy second concern is the algorithm claims to be the first one to handle mean-value constraint without reward shaping. I did not get the reason why (Dalal et al. 2018) and (Achiam et al., 2017) cannot handle this case. Can the authors explain the reason more clearly? \n\nSome minor points: \nThe experiments are somewhat weak. The author is suggested to compare with more baseline methods. Mujoco domain is not a very difficult domain in general, and the authors are suggested to compare the performance on some other benchmark domains. \n\nThis paper needs to consider the cases where the constraints are the squares of returns, such as the variance. In that case, computing the solution often involves double sampling problem. Double sampling problem is usually solved by adding an extra variable at an extra time scale (if gradient or semi-gradient methods are applied), such as in many risk-sensitive papers.\u200b", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}