{"title": "Review", "review": "Overall this is an important piece of work that deserves publication at ICLR. I recommend to the authors revise their manuscript to make it more accessible to the machine learning community and that they provide better context to allow them to assess the relative quality of the work compared to state of the art results.\n\n# Quality\n\nThe hypothesis that the authors set out to resolve is whether there is an advantage in using an energy function sampled by Langevin dynamics versus simply using a neural network to regress shape from sequence. They construct a flexible deep energy model where the sequence and structure dependent parts are separated in such a way that fast rollouts are possible. They also adapt the learning algorithm to  ensure that long rollouts can be carried out and present a clever trick for integrating internal coordinates efficiently on a GPU. \n\nThe only criticism in terms of quality of work is that it somewhat lacks putting in context with results from the larger community, for example how well does the model compare in terms of speed and accuracy with co-evolutionary approaches? I realise it will not be possible to give a completely fair like to like comparison, but it will help readers put the results in context if they understood, for example, what the average TM score for CASP12 results was, as summarized in this paper for example: https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25423. Similarly, it would be useful to compare the baseline - at least qualitatively - with the results from AlQuraishi et. al. whose model seems very similar in spirit.\n\n# Clarity\n\nI think in terms of clarity, the paper could be improved a little to take into account the audience of ICLR. In particular:\n\n* It may be useful to add a sentence of how profiles have been found to improve secondary structure prediction greatly. Currently the text makes it sound as though they constitute a sort of 'data augmentation', whereas in my opinion they add information compared to the sequence alone. In fact a brief explanation of the importance of homology might help the reader understand the relevance of the hierarchical approach taken to splitting the training set.\n\n* Fig. 2 caption. Could add some information to explain what panel B is showing. I think this would go a long way to explain why both cartesian and internal coordinates are important.\n\n* Fig. 4 second panel. The x axis should be labeled fraction or be numbered 0-100.\n\n* Fig 4. caption. The figure does not have a caption explaining what the graphs are showing. This would be a good place to explain that the colors refer to test sets that overlap with the training set in the full CATH code (black), overlap only in the CAT code (orange) etc. I admit I had found the explanation of the test/train/validation split rather confusing. It is not clear what the validation set is used for, i.e. which hyper-parameters have been tuned on it etc.\n\n* The nature of the loss. The appendix does a good job in describing each term in the loss function, but does not explain how the empirical loss function and the log-likelihood terms are mixed together. \n\n# Originality\n\nThe work is original and is references the relevant literature.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}