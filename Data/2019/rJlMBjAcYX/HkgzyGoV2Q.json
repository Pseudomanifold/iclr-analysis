{"title": "Interesting paper but with limited novelty and lacking convincing experiments", "review": "This paper proposes the so-called cross-validation gradient method (CVGM).\nThis is idea is to express the CV score as a differentiable function\nof the hyperparameters and then to update hyperparameters with gradient\ndescent. Derivations are provided with Logistic regression and Elastic-Net\nthanks to the sign splitting trick.\n\nOnce the problem is expressed as a QP, the work is mostly done\nby the qpth library that offers a differentiable layer for the QP solver.\n\nMajor points:\n\n- This idea has been around for quite some time but yes it is now certainly more\ntimely with the new DL tools such as pytorch. However the novelty is limited\nwhich means that numerical experiments should be quite extensive to\ndemonstrate a clear impact on the field. Unfortunately the experiments\nare very limited: data mostly simulated and very small. What is missing\nis a real evaluation of larger datasets and a demonstration that one can\noutperform the state of the art using CVGM. For example for the Elastic-Net\nit is unclear if CVGM is faster than glmnet that computes full grid search\nbut uses warm start so is very efficient.\n\n- Given a new dataset, how do you set step sizes? The purpose is to\nfind faster good hyperparameters than using Bayes Opt or random search\nbut if I need to fiddle with the choice of step size is it really worth it?\n\nMinor points:\n\nPlease proof read manuscript as there are a few typos.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}