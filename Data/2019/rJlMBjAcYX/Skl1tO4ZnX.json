{"title": "Clearly written but incremental with respect to related work.", "review": "The paper proposes a method to optimize for the cross-validation performance of a model by expressing said performance as a differentiable function of the model parameters and applying a gradient-based method. \n\nThe majority of the work is clear and well-written, and appears to be correct, but I find it lacking in originality. The main contribution over related work  (references cited under the \"Implicit Differentiation\" heading in section 2.2) appears to be that the hyperparameters are optimized with respect to a cross-validation loss rather than a held out validation set. That is, using K=1 in the CVGM (Algorithm 1) reduces to existing work. There is also a discussion of when the cross-validation loss will be differentiable, but no new results on this. I am not sure that these contributions justify the paper. \n\nThe experiments are also not particularly strong. Only synthetic data is considered. The logistic regression baseline for the classification application in section 5.2 is irrelevant, and the neural network baseline could be clarified. Does the baseline also use the optimal parameters in the last layer throughout training? If not, how much of the improvement of the CVGM over the baseline is due to this change? \n\nTo say that the CVGM is able to stably learn the \u201chyperparameters\u201d of the network kernel in this setting seems like an exaggeration -- the neural network baseline also learns these \u201chyperparameters\u201d. The difference is that they are optimized with respect to the CV loss rather than the training loss. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}