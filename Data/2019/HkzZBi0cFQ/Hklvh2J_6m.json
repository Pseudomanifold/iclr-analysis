{"title": "promising method for 8-bit quantization without sensitivity to outliers, limited in novelty and presentation clarity", "review": "The paper proposes channel-wise 8-bit quantization rather than layer-wise. It further takes advantage of work using moment analysis instead of just MAX values to avoid susceptibility to outliers. The main take-away seems to be that channel-wise set ups limit the need for outlier removal and the care with which you select your data subset when performing quantization.\n\nPros:\n- using channel-wise quantization (with MAX values or moment-analysis) yields improvement over layer-wise MAX approaches\n- limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)\n- shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channel-wise quantization\n\nCons:\n- unclear how much is gained over layer-wise and MAX value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channel-wise methods are the clear winner\n- unclear if the layer-wise set up with moment-analysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channel-wise set up; a few more experiments are important to determine specifically if improvement is with respect to channel-wise or moment-analysis since only layer-wise MAX results are presented\n- clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability\n\nOverall:\nThe paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channel-wise quantization (and moment-analysis), but the overall novelty is limited. With the limited novelty, the primary benefits appear to be the ease of quantization for rapid deployment and channel-wise setups. Comparisons with stronger baseline numbers when using layer-wise methods would give a more complete picture. In addition, having these stronger tuned baseline numbers on even more networks would be great to show that the channel-wise method has clear impact across the board, even with respect to well-tuned layer-wise baselines. These results could give better support for the importance of the novelty.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}