{"title": "Interesting approach to channel-wise CNN quantization with adaptive bit allocation, with evaluation on eleven modern CNNs, comparison with simple layer-wise baseline", "review": "This paper proposes a technique for channel-wise quantization of CNNs\nto 8-bit, fixed point precision. The authors propose several\ntechniques for analyzing the statistical properties of output channel\nactivations in order to select the best fractional bit length for each\nchannel. Experimental results on eleven different CNN architectures\ndemonstrate that the approaches proposed result in significantly less\naccuracy loss when compared to a layer-wise baseline.\n\nThe paper has the following strengths:\n\n 1. The experimental results on eleven different architectures (of\n    varying depth and breadth) are convincing, and are consistently\n    better than layer-wise MAX for choosing fractional bit length.\n\nThe paper has the following weak points:\n\n 1. There is not much coherence between the description of the\n    approach in section 2.1, Figure 1, Algorithm 1, and\n    Figure 2. Notation is used in Algorithm 1 which is never defined.\n 2. Related to the previous point, the proposed technique has a lot of\n    moving parts and I don't feel that it would be easy to reproduce\n    the results of the paper. There are some vague statements, like\n    \"We resolve this complication by pre-coordinating the fractional\n    lengths of the weights\", which require significantly more\n    precision. This issue -- one of the main issues with channel-wise\n    versus layer-wise quantization -- is never returned to in the\n    definition of the method.\n 3. The experimental comparison with layer-wise quantization is\n    somewhat lacking. Is layer-wise MAX the state-of-the-art in CNN\n    quantization? The results comparing channel-wise and layer-wise\n    MAX are already convincing, but are the moment-analysis approaches\n    not equally applicable to layer-wise quantization?\n    State-of-the-art results that are less sensitive to outliers\n    should be included in Table 1. A comparison with layer-wise\n    approaches would be nice to have also in Figure 4 to show\n    sensitivity to profiling set size.\n\nThe experimental results in the paper are impressive, and the analysis\nmotivating the approach is convincing. However, there are presentation\nand clarity issues in the technical development, and the comparative\nanalysis is lacking broader comparisons with the state-of-the-art (to\nbe fair, the authors recognize that layer-wise MAX as a baseline is\nparticularly susceptible to outliers). These two aspects combined,\nhowever, lead me to the opinion that this work is just not quite ready\nfor publication at ICLR.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}