{"title": "A compelling contribution that could benefit from some more quantitative detail", "review": "The authors present a deep reinforcement learning approach that uses a \u201cself-attention\u201d/\u201ctransformer\u201d-style model to incorporate a strong relational inductive bias. Experiments are performed on a synthetic \u201cBoxWorld\u201d environment, which is specifically designed (in a compelling way) to emphasize the need for relational reasoning. The experiments on the BoxWorld environment clearly demonstrate the improvement gained by incorporating a relational inductive bias, including compelling results on generalization. Further experimental results are provided on the StarCraft minigames domain. While the results on StarCraft are more equivocal regarding the importance of the relational module\u2014the authors do set a new state of the art and the results are suggestive of the potential utility of relational inductive biases in more general RL settings.\n\nOverall, this is a well-written and compelling paper. The model is well-described, the BoxWorld results are compelling, and the performance on the StarCraft domain is also quite strong. The paper clearly demonstrates the utility of relational inductive biases in reinforcement learning.\n\nIn terms of areas for potential improvement:\n\n1) With regards to framing, a naive reader would probably get the impression that this is the first-ever work to consider a relational inductive bias in deep RL, which is not the case, as the NerveNet paper (Wang et al., 2018) also considers using a graph neural network for deep RL. There are clear differences between this work and NerveNet\u2014most prominently, NerveNet only uses a relational inductive bias for the policy network by assuming that a graph-structured representation is known a priori for the agent. Nonetheless, NerveNet does also incorporate a relational inductive bias for deep RL and shows how this can lead to better generalization. Thus, this paper would be improved by properly positioning itself w.r.t. NerveNet and highlighting how it is different. \n\n2) As with other work using non-local neural networks (or fully-connected GNNs), there is the potential issue of scalability due to the need to consider all input pairs. A discussion of this issue would be very useful, as it is not clear how this approach could scale to domains with very large input spaces. \n\n3) Some details on the StarCraft experiments could be made more rigorous and quantitative. In particular, the following instances could benefit from more experimental details and/or clarifications: \n\nFigure 6: The performance of the control model and relational model seem very close. Any quantitative insight on this performance gap would improve the paper. For instance, is the gap between these two models significantly larger than the average gap between runs over two different random seeds? It would greatly strengthen the paper to clarify that quantitive aspect. \n\nPage 8: \u201dWe observed that\u2014at least for medium sized networks\u2014some interesting generalization capabilities emerge, with the best seeds of the relational agent achieving better generalization scores in the test scenario\u201d \u2014 While there is additional info in the appendix, without quantitative framing this statement is hard to appreciate. I would suggest more quantitive detail and rigorous statistical tests, e.g.,  something like \u201cWhen examining the best 10 out of ??? seeds, the relational model achieved an average performance increase of ???% compared to the control model (p=???, Wilcoxon signed-rank test). However, when examining all seeds ???? was the case.\u201d \n\nPage 8: \u201cwhile the former adopted a \"land sweep strategy\", controlling many units as a group to cover the space, the latter managed to independently control several units simultaneously, suggesting a finer grained understanding of the game dynamics.\u201d This is a great insight, and the paper would be greatly strengthened by some quantitive evidence to back it up (if possible). For instance, you could compute the average percentage of agents that are doing the same action at any point in time or within some distance from each other, etc. Adding these kinds of quantitative statistics to back up these qualitative insights would both strengthen the argument, while also making it more explicit how you are coming to these qualitative judgements. \n\nFigure 8 caption: \u201cColored bars indicate mean score of the ten best seeds\u201d \u2014 how bad is the drop to the n-10 non-best seeds? And how many seeds where used in total?\n\nPage 13: \u201cfollowing Table 4 hyperparameter settings and 3 seeds\u201d \u2014 if three seeds are used in these experiments, how are 10+?? seeds used for the generalization experiments? The main text implies that the same models for the \u201cCollect Mineral Shards\u201d were re-used, but it appears that many more models with different seeds were trained specifically for the generalization experiment. This should be clarified. Alternatively, it is possible that \u201cseeds\u201d refers to both random seeds and hyperparameter combinations, and it would improve the paper to clarify this. It is possible that I missed something here, but I think it highlights the need for further clarification. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}