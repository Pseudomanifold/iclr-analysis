{"title": "A variety of methods combine to give L2-robustness", "review": "This paper presents a combination of methods that, together, yield neural networks that are robust to small changes in L2 distance. The main idea is to ensure that changing the input by a bounded L2 distance never changes the output by more than the same L2 distance. Then, the difference between the highest-scoring class and the second-highest scoring class provides a bound on how much the input must change. The trivial way to do this is to rescale the final output layer so that all of the magnitudes are very small; however, this would give no additional robustness at all. To counteract this, the paper introduces several additional heuristics for increasing the gap between the highest-scoring class and the second-highest scoring one. Adversarial training can be used to make the models even more robust. \n\nExperimental results on MNIST and CIFAR look impressive, although most are in terms of L2 distance, while most previous work optimizes L_infinity distance.\n\nThe methods described by this paper are similar to max-margin training, which is already known to be optimally robust to L2 perturbations for linear models (e.g., Xu et al. (2009)). This paper would be stronger with more discussion and analysis of this connection, although that might be work for a future paper.\n\nAlthough the method relies heavily on heuristics, the empirical results are promising. The analysis of the contribution of the heuristics is fairly thorough as well. The MNIST results are strong. The CIFAR results show improved robustness, though at reduced accuracy on natural images. A combination of robust and non-robust classifiers improves the accuracy somewhat.\n\nOverall, this is interesting work with promising empirical results. The biggest weaknesses are:\n\n- Limited theory. The loss function is particularly strange. \n\n- The majority of the comparisons focus on L2-robustness, but are comparing to a model optimized for L_infinity-robustness. (Thankfully, the authors also do some comparisons on L_infinity-robustness.)\n\n- Robustness comes at a cost in accuracy, though this is not uncommon for adversarial training.\n\nThe biggest strengths are:\n\n- Strong empirical robustness\n\n- Analysis of combinations of methods and their interactions: different loss function, different architecture, different weight constraints, and adversarial training are all evaluated together and separately.\n\n- Wide variety of experiments, including generalization on training data with noisy labels and analysis of the confidence gaps.\n\n\n\nQuestions for the authors:\n\n- For equation (4) in the loss function, why would rescaling the layers in the middle of the network be equivalent to a linear transformation (u1, u2, ..., u_K) of the output?\n\n- In equation (6), what is the average averaging over?\n\n- The connection between confidence gap and robustness is discussed empirically, as a correlation, rather than theoretically, as a bound.  Doesn't the confidence gap give a lower bound on the minimum perturbation to change the predicted class?\n\n---------\n\nEDIT: After the author response, I remain positive about this paper. In addition to addressing my concerns, I admire the authors' patience in answering the concerns of other reviewers and commenters. I think that this is a solid paper that makes a good contribution to the literature on adversarial machine learning.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}