{"title": "Nice idea, a few cool results, and a couple missing steps ", "review": "I read this paper with some excitement. The authors propose a very sensible idea: simultaneously maximizing the confidence gap and constraining the Lipschitz constant of the network, thus achieving a guarantee that no L2-bounded perturbation can alter the prediction so long as the perturbation is bounded by some function of the confidence gap. \n\nThe main idea consists of three parts:\n 1) smooth networks (fixed, low Lipschitz constant)\n (2) loss function that explicitly maximizes the confidence gap (distance between largest and second-largest logits). \n (3) \u201cthe network architecture restricts confidence gaps as little as possible. We will elaborate.\u201d   \n\nThe first two conditions make plain sense. The third condition and subsequent elaborations are far too vague. What precisely is the property of restricting confidence gaps? At first glance this seems akin to the smoothness sought in property one. Even in the bulleted list, the authors owe the reader a clearer explanation.\n\nThe proposed model, denoted L2-nonexpansive neural networks (L2NNNs) and consists of a sensible form of Lipschitz-constant-enforcing weight regularization, a loss function that penalizes the confidence gap.\n\nTo address the third condition, the authors say only \u201cwe adapt various layers in new ways for the third condition, for example norm-pooling and two-sided ReLU, which will be presented later\u201d which is far too vacuous. At this point the reader is exposed to the third condition for the second time and yet it remains shrouded in mystery. The authors should elaborate here and describe what precisely, if anything, this third condition consists of. If it is not rigorously defined but only a heuristic notion, that would be fine, but this should be communicated clearly to the reader. \n\nA following paragraph introduces the notion of \u201cpreserving distance\u201d. However, what follows is too informal a discussion, and the rigorous definition never materializes. The authors say in one place \u201ca network that maximizes confidence gaps well must be one that preserves distance well\u201d. In this case, why do we need the third condition at all if the second condition appears to be sufficient?\n\nIn the next sections the authors describe the methods in greater detail and summarize their results. I have placed some more specific nittier comments in the ***small issues*** section below. But comment hear on the empirical findings.\n\nOne undersold finding here is that the existing methods (including the widely-believed-to-be-robust method due to MAdry 2017) that appear robust under FGSM attacks break badly under iterated attacks, and that the attacks go stronger up to 1M iterations, bringing accuracy below 10%. \n\nIn contrast the proposed method reaches 24% accuracy, which isn\u2019t magnificent, but does appear to outperform the model due to Madry. A comparison against the method due to Kolter & Wong seems in order. The authors do not implement methods based on the adversarial polytope due to their present un-scalability, but that argument would be better supported if the authors were addressing larger models on harder datasets (vs MNIST and CIFAR10).\n\nIn short, I like the main ideas in this paper although some more empirical elbow grease is in order, the third condition needs to be discussed more rigorously or discarded. Additionally the choice of loss function should be better justified. Why do we need the original cross-entropy objective at all. Why not directly optimize the confidence gap? Did the authors try this? Did it work? Apologies if I missed this detail. Overall, I am interested in the development of this paper and would like to give it a higher vote but believe the authors have a bit more work to do to make this an easier decision. Looking forward to reading the rebuttal.\n\n\n***Small issues***\nPage 1 \u201cnonexpansive neural networks (L2NNN)\u201d for agreement on pluralization, should be \u201cL2NNNs\u201d\n\n\u201cThey generalize better from noisy training labels than ordinary networks: for example, when 75% of MNIST training labels are randomized, an L2NNN still achieves 93.1% accuracy on the test set\u201d\nWhen you make a claim about accuracy of a proposed model, it must be made in reference to a standard model, even in the intro. It\u2019s well-known in general that DNNs perform well even under large amounts of label noise. Hard to say without reference if 93.1% represents a significant improvement.\n\nRepeated phrase on page 2:\n\u201cHow to adapt subtleties like recursion and splitting-reconvergence is included in the appendix.\u201d\n\u201cDiscussions on splitting-reconvergence, recursion and normalization are in the appendix.\u201d\n\nInputs to softmax cross-entropy should be both a set of logits and the label -- here the way the function is used in the notation does not match the proper function signature\n\nFigure --- do not put \u201cModel1, Model2, Model3, Model4\u201d. This is unreadable. Put some shortname and then define it in the caption. Once one knows the abbreviations, they should be able to look at the figure and understand it without constantly referencing the caption. \n\nTable 1-4 should be at the top of the page and arranged in a grid.  This wrapfigure floating in the middle of the page, while purely a cosmetic issue that should not bear on our deliberations, tortures the template unnecessarily, turning the middle 80% of page 5 into a one-column page unnecessarily.\n\nTable 4 should show comparison to Madry model. Also this is why you need a shortname in the legend. In order to understand table 4, the reader has to consult the caption for tables 1 and 2. \n\n\u201cIt is an important property that an L2NNN has an easily accessible measurement on how robust its decisions are\u201d\nI AGREE!", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}