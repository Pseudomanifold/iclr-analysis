{"title": "Issues in Theory and Experiments", "review": "This paper suggests that one way to prevent VAE from ignoring the encoder when trained on text is to make the prior distribution multi-modal. The author further suggests that injecting the encoder's output with BoW features can make the prior distribution multi-modal, and in turn that can help VAE to learn better distribution on text without ignoring the encoder, tested on a self-attention encoder and an LSTM encoder.\n\nThere are several issues with the theory and the experiments that make the paper incorrect on several fronts.\n\nIssue #1 (theoretical): multi-modality of prior distribution can be achieved by injecting the BoW feature.\nThis is incorrect in the sense that the prior distribution Pr(z) is a given distribution for the model. At best, it is an additive shift of the feature vector from the encoder, which parameterizes the mean and standard deviation of Pr(z|x) by assuming it is a parameterized Gaussian, which is still uni-modal. Besides, the experimental setting in section 3.1.1 suggests the same re-parameterization trick from Kingma & Welling (2013) is used, which assumes the prior distribution Pr(z) is N(0,1) -- and it is uni-modal!\n\nIssue #2 (theoretical): prior multi-modality implies that posterior distribution cannot ignore the encoder.\nThis issue is a two-fold problem. 1) In Appendix A, the paper suggests that (assuming p(z) is multi-modal) \"if q(z|x) is a uni-modal distribution, there is no way to satisfy p(z) = q(z|x)\". This is true, however this does not imply that posterior distribution will not ignore the encoder. In the case that Pr(z) is multi-modal, it is entirely possible that the encoder simply performs a clustering on the input x for each mode of z, ignoring the detailed textual information. The posterior collapse is therefore could not be entirely prevented by simply making Pr(z) multi-modal. 2) In Appendix A, the paper suggests \"a hypothesis that the modifiction of the decoer is not necessary if multi-modal prior distribution is used\". This is not only a logical fallacy of circular reasoning between theory and experiments, but also a faulty result because the actual experiments use a uni-modal prior (see issue #1).\n\nIssue #3 (experimental): benchmark methods only reflect the quality of auto-encoding reconstruction\nThe paper's benchmark methods do not reflect performance in text generation. According to section 4.3, the 3 benchmark methods used (reconstruction loss, KL-divergence, and BLEU) are all performed against a given text, which can only benchmark the auto-encoding reconstruction Pr[x|z]Pr[z|x] but not the quality of generated text Pr[x|z]. Here I am assuming the KL-divergence is performed on the output word probabilities given a sample, not on the entire generated sentence (which is not feasible to compute). This defies the purpose of VAE, which is to learn a generative model.\n\nIssue #4 (experimental): the title suggests no weakening of the decoder is needed, but experiments are not performed with decoders of varying sizes\nDespite all the issues above with incorrect theory and experiments, it could still be possible that feature injection with BoW is helpful to prevent posterior collapsing of text VAEs. The paper failed to show this in a convincing manner either, because there is no experiment on different capacities of the decoder. One would expect that increasing the size (capacity) of the decoder will gradually make the posterior collapsing problem worse, and BoW feature injection could help because it can tolerate a larger decoder than a baseline model.\n\nDue to these issues, I recommend rejection for the paper. If the authors did observe that BoW feature injection is helpful for the posterior collapsing phenomenon, I encourage the authors to re-consider the theory, the benchmark methods, and perform ablation study as suggested in issue #4.\n", "rating": "1: Trivial or wrong", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}