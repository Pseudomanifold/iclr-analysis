{"title": "Interesting and inspiring observations, but need some further enhancement", "review": "This paper discusses the effect of increasing the widths in deep neural networks on the convergence of optimization. To this end, the paper focuses on RNNs and applications to NLP and speech recognition, and designs several groups of experiments/measurements to show that wider RNNs improve the convergence speed in three different aspects: 1) the number of steps taken to converge to the minimum validation loss is smaller; 2) the distance from initialization to final weights is shorter; 3) the step sizes (gradient norms) are larger. This in some sense complements the theoretical result in Arora et al. (2018) for linear neural networks (LNN), which states that deeper LNNs accelerates convergence of optimization, but the hidden layers widths are irrelevant. This also shows some essential difference between LNNs and (practical) nonlinear neural networks. \n\n### comments about writing ###\nThe findings are in general interesting and inspiring, but the explanations need some further improvement. In particular, the writing lacks some consistency and clarity in the wordings. For example, it is unclear to me what \"weight space traversal\" means, \"training size\" is mixed with \"dataset size\", and \"we will show that convergence ... to final weights\" seems to be a trivial comment (unless there is some special meaning of \"convergence rate\"), etc. It also lacks some clarity and organization in the results -- some more summarizing comments and sections (and in particular, a separate and clearer conclusion section), as well as less repetitions of the qualitative comments, should largely improve the readability of the paper.\n\n### comments about results ###\nThe observations included in the work may kick off some interesting follow-up work, but it is still a bit preliminary in the following sense:\n1. It lacks some discussions with its connection to some relevant literature about \"wider\" networks (e.g., Wide residual networks, Wider or deeper: revisiting the ResNet model for visual recognition, etc.).\n2. It lacks some discussions about the practical implication of the improvement in optimization convergence with respect to the widening of the hidden layers. In particular, what is the trade-off between the validation loss increase and the optimization convergence speed-up resulted from widening hidden layers? A heuristic discussion/approach should largely improve the impact of this work.\n3. The simplified theory about LNNs in the appendix seems a bit too far from the explanation of the difference between the observations in this paper and Arora et al. (2018).\n\n### typos and small suggestions ###\n1. It is suggested that the full name of LNN is provided at the beginning, and the font size should be larger in Figure 1.\n2. There are some mis-spellings that the authors should check (e.g., gradeint -> gradient).\n3. In formula (4), the authors should mention that the third line holds for all $t$ is a sufficient condition for the previous two equivalent lines.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}