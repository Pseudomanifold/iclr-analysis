{"title": "Mostly descriptive experimental analysis", "review": "This paper presents an empirical analysis of the convergence of deep NN training (in particular in language models and speech).\n\nStudying the effect of various hyperparameters on the convergence is certainly of great interest. However, the issue with this paper is that its analyses are mostly *descriptive*, rather than conclusive or even suggestive. For example, in Figure 2, it is shown that the convergence slope of Adam is steeper than that of SGD, when the x-axis is the model size. Very naturally I would be interested in a hypothesis like \u201cAdam converges quicker than SGD as we increase the model size\u201d, but there is no discussion like that. Throughout the paper there are many experimental results, but results are presented one after another, without many conclusions or suggestions made for practice. I don\u2019t have a good take-away after reading it.\n\nThe writing of this paper also needs to be improved significantly. In particular, lots of statements are made casually without justification. For example,\n\n\u201cIf hidden dimension is wide enough to absorb all the information within the input data, increasing width obviously would not affect convergence\u201d -- Not so obvious to me, any reference? \n\n\u201cFigure 4 shows a sketch of a model\u2019s convergence curve ...\u201d -- it\u2019s not a fact but only a hypothesis. For example, what if for super large models the convergence gets slow and the curve gets back up again?\n\nIn general, I think the paper is asking an interesting, important question, but more developments are needed from these initial experimental results.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}