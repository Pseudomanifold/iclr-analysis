{"title": "Cakewalk is too similar to the cross-entropy method to warrant acceptance", "review": "## Summary ##\n\nThe authors apply policy gradients to combinatorial optimization problems. They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size. They demonstrate performance on a clique-finding problem.\n\n\n## Assessment ##\n\nI don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR. \n\n I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method.\n\nThey both approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.\n\n\n## Specific Comments and Questions ##\n\n1. Cakewalk is *very* closely related to the cross-entropy method. The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it. Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.\n2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems. Consider $x$ a binary vector and reward equal to the parity $S(x) = \\sum{x_j} % 2$.\n3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4). Is there any explanation for this?\n4. How were the hyperparameters (learning rate, AdaGrad $\\delta$, Adam $\\beta_1, \\beta_2$) chosen? It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function. I would suggest tuning these values for each method independently.\n5. It would be nice to see experimental results on more than one problem. The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet. \n6. In Table 3, the figure in bold is not the lowest (best) in the table. The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing. I would replace these values with N/A or something similar.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}