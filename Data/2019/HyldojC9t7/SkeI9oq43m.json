{"title": "This is an incremental work with insufficient depth and novelty.", "review": "The paper proposes a kernel function based on a dissimilarity measure between a pair of instances. A general dissimilarity measure does not necessarily have properties of a metric and the standard transformations from dissimilarities to kernel or similarity functions typically result in indefinite kernel matrices. For example, the two most frequently used transformations are negative double-centering characteristic to multidimensional scaling and the exponentiation of the negative squared dissimilarities between a pair of instances (e.g., exponentiated negative squared geodesic distance defines an indefinite kernel).\n\nThe main idea of the paper is to mimic the random Fourier features approximation of the stationary kernels and use dissimilarities as basis function. In particular, the proposed kernel function based on dissimilarities can be written as\n\nk(x,x')=\\int \\phi(w, x) \\phi(w, x') p(w) dw,\n\nwhere \\phi is a dissimilarity function and p(w) is a probability density function. This is the same format of the kernel function as the one considered in [1-3], with \\phi chosen to be different from the cosine basis function. While the focus in [1-3] was on stationary kernels, their theoretical derivations and presentation was designed for general basis functions. Having this in mind, all the theoretical derivations by the authors are readily obtained from [1-3] and I fail to see any novelty here. For example, the result in Proposition 2 follows directly from [1, Claim 1] or [2].\n\nFor the proposed kernel, I also fail to see a significant difference compared to [4] where dissimilarities are used as features. In fact, for a large number of dissimilarity functions/features and l1 penalty on the linear model the approach by [4] retrieves the proposed approximate kernel with the `optimal' density function. An experiment along these lines was, for instance, reported in [5].\n\nIn the related work part, the authors also mention previous approaches for learning with indefinite kernels and that they suffer from the non-convexity of the optimization problem for risk minimization. A recent approach builds on [6] and alleviates this shortcoming with a non-convex problem for which a globally optimal solution can be found in polynomial time (e.g., see [7]).\n\nIn the experiments, the most appropriate baseline would be the approach from [5] and yet the approach is not even listed in the related work section.\n\n[1] A. Rahimi and B. Recht (NIPS 2008). Random features for large-scale kernel machines.\n[2] A. Rahimi and B. Recht (IEEE 2008). Uniform approximation of functions with random bases.\n[3] A. Rahimi and B. Recht (NIPS 2009). Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning.\n[4] T. Graepel, R. Herbrich, P. Bollmann-Sdorra, and K. Obermayer (NIPS 1999). Classification on pairwise proximity data.\n[5] I. Alabdulmohsin, X. Gao, and X.Z. Zhang (PMLR 2015). Support vector machines with indefinite kernels.\n[6] C.S. Ong, X. Mary, S. Canu, and A. Smola (ICML 2004). Learning with non-positive kernels.\n[7] D. Oglic and T. Gaertner (ICML 2018). Learning in Reproducing Kernel Krein Spaces.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}