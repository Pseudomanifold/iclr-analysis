{"title": "An interesting idea but experiments and writeup need improvement. ", "review": "A genetic algorithm is used to do an evolutionary architecture search to find better tree-like architectures with multiple memory cells and recurrent paths. To speed up search, an LSTM based seq2seq framework is also developed that can predict the final performance of the child model based on partial training results.\n\nThe algorithms and intuitions based on novelty search are interesting and there are improvements over baseline NAS model with the same architecture search space. \n\nAlthough, the experiments are not compared against latest architectures and best results. For example on PTB, there are new architectures such as those created by ENAS that result in much lower perplexity than best reported in Table 1, for the same parameter size. While you have mentioned ENAS in the related work, the lack of a comparison makes it hard to evaluate the true benefit if this work compared with existing literature. \n\nThere is no clear abolition study for the Meta-LSTM idea. Figure 4 provides some insights but it'd be good if some experiments were done to show clear wins over baseline methods that do not employ performance prediction.\n\nThere are many typos and missing reference in the paper that needs to be fixed.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}