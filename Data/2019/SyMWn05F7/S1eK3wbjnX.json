{"title": "Good use of mapping for exploration", "review": "This is a well explained and well executed paper on using classical SLAM-like 2D maps for helping a standard Deep RL navigation agent (convnet + LSTM) explore efficiently an environment and without the need for extrinsinc rewards. The agent relies on 3 convnets, one processing RGB images, one the image of a coarse map in egocentric referential, and one of the image of a fine-grained map in egocentric referential (using pre-trained ResNet-18 convnets). Features produced by the convnets are fed into a recurrent policy trained using PPO. Two rewards are used: the increase in the map's coverage and an obstacle avoidance penalty. The agent is further bootstrapped through imitation learning in a goal-driven task executed by a human controlling the agent. The authors analyze the behavior of the navigation algorithm by various ablations, a baseline consisting of Pathak's (2017) Intrinsic Curiosity Module-based navigation and, commendably, a classical SLAM baseline with path planning to empty, unexplored spaces.\n\nUsing an explicit map is a great idea but the authors need to acknowledge how hand-engineered all this is, when comparing it to actual end-to-end methods. First, the map reconstruction is done by back-projections of a depth image (using known projective geometry parameters) onto a 3D point cloud, then by slicing it to get a 2D map, accumulated over time using nearly perfect odometry. SLAM was an extremely hard problem to start with, and it took decades and particle filters to get to the quality of the images shown in this paper as obvious. Normally, there is drift and catastrophic map errors, whereas the videos show a nearly perfect map reconstruction. Is the motion model of the agent unrealistic? Would this ever work out of the box on a robot in a real world? The authors brush off the need for bundle adjustment, saying that the convnet can handle noisy local maps. Second, how do you get and maintain such nice ego-centric maps? Compared to other end-to-end work on learning how to map (see Wayne et al. or Zhang et al. or Parisotto et al., referred to later in the paper), it looks like the authors took a giant shortcut. All this SLAM apparatus should be learned!\n\nOne crucial baseline that is missing is that of explicit extrinsic rewards encouraging exploration. These rewards merely scatter reward-yielding objects throughout the environment; over the course of an episode, an object reward that is picked does not re-appear until the next exploration episode, meaning that the agent needs to cover the whole space to forage for rewards. Examples of such rewards have been published in Mnih et al. (2016) \"Asynchronous methods for deep reinforcement learning\" and are implemented in DeepMind Lab (Beatie et al., 2016). Such an extrinsic reward would be directly related to the increase of coverage.\n\nA second point of discussion that is missing is that of the collision avoidance penalty: roboticists working on SLAM know well that they need to keep their robot away from plain-texture walls, otherwise the image processing cannot pick useful features for visual odometry, image matching or ICP. What happens if that penalty is dropped in this navigation agent?\n\nFinally, the authors mention the Neural Map paper but do not discuss Zhang et al. (2017) \"Neural SLAM\" or Wayne et al. (2018) \"Unsupervised Predictive Memory in a Goal-Directed Agent\", where a differentiable memory is used to store map information over the course of an episode and can store information relative to the agent's position and objects' / obstacles' positions as well.\n\nMinor remark: the word \"finally\" is repeated twice at the end of the introduction.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}