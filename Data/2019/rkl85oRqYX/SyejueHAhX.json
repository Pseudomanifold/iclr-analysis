{"title": "The submission suggests a methodology compressing the parameters of neural network (NN) architectures but the novelty of the method is limited.", "review": "The submission suggests a methodology compressing the parameters of neural network (NN) architectures, by first converting them to higher-order tensors and then factorizing them with various tensor decompositions. The main assumption of this work is that there may be structure in lower-order tensors (e.g., matrices or low-order tensors) contained as parameters of a NN such as: periodicity (e.g., the vector [1,2,3,1,2,3,1,2,3]) or modularity (e.g., the vector [1,1,1,2,2,2,3,3,3]). The main argument of this work is that such invariant structures in lower-order objects often go unexploited in previous low-rank approximation techniques tackling the problem of compressing the parameters of a deep neural network. The authors argue that such structures may be revealed by reshaping lower-order tensors into higher-order ones and factorizing the resulting tensors.\nDespite the fact that the results seem promising, the paper suffers from the following issues:\n1. Tensorizing to a higher-order tensor with lower mode sizes and factorizing through tensor network approaches is not a new approach, as suggested in the main contributions. The authors should more clearly position this part of the work with respect to existing literature. I provide more details on this point below.\n2. There is no experiment or discussion to back up the assumption that: 1) the invariant structures suggested by the authors do appear within the parameters of a neural network architecture; 2) such invariant structures are captured by the higher-order tensor decomposition proposed. As such, it seems that there is no solid intuition presented to justify why the approach works. Even if the authors stress out in the experiments that: \"These results confirm the existence of extra invariant structure in the parameter space of deep neural networks\", this is really not clear/well-justified.\n3. The experiment setup is missing crucial discussions on the hyper-parameter search. For example, as the tensor order increases, there are many more potential choices for setting the low rank parameters of a tensor train decomposition, as well as the corresponding mode sizes of the input tensor to be decomposed. How does this increased space of hyper-parameters to search for affects performance seems a crucial performance factor, but it is not discussed at all.\n4. The description of the sequential fine-tuning procedure seems really hand-waving. Despite the lengthy appendix, there is not even a pointer from the sequential fine-tuning section so that the reader can go over this process in more detail with precise notation. The lack of any detailed description of this step raises doubts regarding this step's novelty/methodological significance.\nAdditional comments w.r.t. first issue: The idea of reshaping a lower-order object into a higher-order tensor with low mode sizes and then factorize it does exist in the literature. Especially, in combination with tensor train decomposition, the framework has been called as the Quantized-Tensor Train (QTT). For example, the authors could check Section 2.9 of the survey: Grasedyck, Lars, Daniel Kressner, and Christine Tobler. \"A literature survey of low\u2010rank tensor approximation techniques.\" GAMM\u2010Mitteilungen 36.1 (2013): 53-78. and references therein. \nI think the authors should mention the connections existing with this line of work and argue about how their approach differs. Admittedly, it seems that this is the first effort of applying those ideas in the context of compressing neural network (NN) parameters, as well as deriving the update rules w.r.t. a NN; still, appropriate credit should be given to the QTT line of work.\nAnother issue w.r.t. comparing with related work is that the authors should definitely try to include the comparison and discussions with other low-rank approximation-based NN compression in the main text, rather than the appendix. \nMinor comments: Tensor partial outer product should be more clearly defined in the main text. Currently, it is really not clear to the reader what this operation signifies.\nAlso, it is not clear on how to interpret CP tensor network diagrams as they are presented by the authors. It seems that the current presentation does not take into account the diagonal core tensor that is present in CP.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}