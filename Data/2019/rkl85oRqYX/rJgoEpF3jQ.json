{"title": "Three critical problems", "review": "In this paper, a compressive representation of convolutional neural networks is proposed. The main idea is reshaping the kernel of a convolutional layer into a higher order tensor (e.g. 9x9x9 => 3x3x3x3x3x3) and applying tensor decomposition to reduce the number of parameters. Experiments with standard data sets (CIFAR10, MNIST, ImageNet) are conducted. \n\nThe upside of this paper is that the technical details such as the gradient formulations are sufficiently covered in Appendix. This is nice in terms of self-contentedness. \n\nHowever, I found this paper contains three critical problems. \n\n1. Prior work is not explicitly referred. The core idea of reshaping & tensor decomposition was originally proposed by Novikov et al. (2015). However, this fact is only mentioned in Appendix and in the main paper, there is no reference. This is unfair because some reader can misunderstand that the main idea is purely invented in this paper. \n\n2. Contributions are not enough. In Appendix A, it is claimed that Novikov et al. (2015) proposed the core idea for fully connected layers and the authors of this paper (and Wang et al. (2018)) extends for convolutional layers. However, it seems Garipov et al. (2016) already addressed this direction. What is the difference from their approach? Furthermore, even if the extension for convolutional layers is new, I feel the technical contribution is incremental (just changing the decomposing tensor from a fully-connected weight to convolution kernel) and not enough as an ICLR publication.\n\n3. The experiments are not exhaustive & results are not particularly good. Only tensor decomposition methods are used as baselines and no other compression method is compared. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}