{"title": "An interesting and relatively unexplored variant of RL.", "review": "\nThis paper investigates reinforcement learning with a perturbed reward signal. In particular, the paper proposes a particular model for adding noise to the reward function via a confusion matrix, which offers a nuanced notion of reward-noise that is not too complicated so-as to make learning impossible. I take this learning setting to be both novel and interesting for opening up areas for future work. The central contributions of the work are to 1) leverage a simple estimator to prove the convergence of Q-Learning under the reward-perturbed setting along with the sample-complexity of a variant of (Phased) Q-Learning which they call \"Phrased\" Q-Learning, and 2) An algorithmic scheme for learning in the reward-perturbed setting (Algorithm 1), and 3) An expansive set of experiments that explore the impact of various reward models on learning across different environment-algorithm combinations. The sample complexity term extends Phased Q-Learning to incorporate aspects of the reward confusion matrix, and to my knowledge is novel. Further, even though Theorem 1 is unsurprising (as the paper suggests), I take the collection of Theorem 1, 2, and 3 to be collectively novel.\n\nIndeed, the paper focuses on an interesting and relatively unexplored direction for RL. Apart from the work cited by the paper (and perhaps work like Krueger et al. (2016), in which agents must pay some cost to observe true rewards), there is little work on learning settings of this kind. This paper represents a first step in gaining clarity on how to formalize and study this problem. I did, however, find the analysis and the experiments to be relatively disjointed -- the main sample complexity result presented by the paper (Theorem 2) was given for Phased Q-Learning, yet no experiments actually evaluate the performance of Phased Q-Learning. I think the paper could benefit from experiments focused on simple domains that showcase how traditional algorithms do in cases where it is easier to understand (and visualize) the impact of the reward perturbations (simple chain MDPs, grid worlds, etc.) -- and specifically experiments including Phased Q-Learning. \n\nPros:\n\t- General, interesting new learning setting to study.\n\t- Initial convergence and sample complexity results for this new setting.\n\t- Depth and breadth of experimentation (in terms of diversity of algorithms and environments), includes lots of detail about the experimental setup.\n\nCons:\n\t- Clarity of writing: lots of typos and bits of math that could be more clear (see detailed comments below) [Fixed]\n\t- The plots in Section 4 are all extremely jagged. More trials seem to be required. Moreover, I do think simpler domains might help offer insights into the reward perturbed setting. [Fixed]\n\t- The reward perturbation model is relatively simple.\n\nSome high level questions/comments:\n\t- Why was Phrased Q-Learning not experimented with?\n\t- Why use majority voting as the rule? When this was introduced it sounded like any rule might be used. Have you tried/thought about others?\n\t- Your citation to Kakade's thesis needs fixing; it should read:\n\t\t\"Kakade, Sham Machandranath. On the sample complexity of reinforcement learning. Ph.D Thesis. University of London, 2003.\"\n\n\t\t(right now it is cited as \"(Gatsby 2003)\" throughout the paper)\n\t- You might consider picking a new name for Phrased Q-Learning -- right now the name is too similar to Phased Q-Learning from [Kearns and Singh NIPS 1999].\n        - As mentioned in the \"cons\" section, the confusion matrix is still a somewhat simple model of reward noise. I was left wondering: what might be the next most complicated form of adding reward noise? How might the proposed algorithm(s) respond to this slightly more complex model? That is, it's unclear how general the results are, or if they are honed too tightly to the specific proposed reward noise model. I was hoping the authors could respond to this point.\n\n\t\nSection 0) Abstract:\n\t- Not immediately clear what is meant by \"vulnerability\" or \"noisy settings\". Might be better to pick a more clear initial sentence (same can be said of the \"sources of noise...\"\")\n\nSection 1) Introduction:\n\t- \"adversaries in real-world\" --> \"adversaries in the real-world\"\n\t- You might consider citing Loftin et al. (2014) regarding the bulleted point about \"Application-Specific Noise\".\n\t- \"unbiased reward estimator aided reward robust reinforcement learning framework\" --> this was a bit hard to parse. Consider making more concise, like: \"unbiased reward estimator for use in reinforcement learning with perturbed rewards\".\n\t- \"Our solution framework builds on existing reinforcement learning algorithms, including the recently developed DRL ones\" --> cite these up front So, cite: Q-Learning, CEM, SARSA, DQN, Dueling DQN, DDPG, NAF, and PPO, and spell out the acronym for each the first time you introduce them.\n\t- \"layer of explorations\" --> \"layer of exploration\"\n\nSection 2) Problem Formulation\n\t- \"as each shot of our\" --> what is 'shot' in this context?\n\t- \"In what follow,\" --> \"In what follows,\"\n\t- \"where 0 < \\gamma \\leq 1\" --> Usually, $\\gamma \\in [0,1)$, or $[0,1]$. Why can't $\\gamma = 0$?\n\t- The transition notation changes between $\\mathbb{P}_a(s_{t+1} | s_t)$ and $\\mathbb{P}(s_{t+1} | s_t, a_t)$. I'd suggest picking one and sticking with it to improve clarity.\n\t- \"to learn a state-action value function, for example the Q-function\" --> Why is the Q-function just an example? Isn't is *the* state-action value function? That is, I'd suggest replacing \"to learn a state-action value function, for example the Q-function\" with \"to learn a state-action value function, also called the Q-function\"\n\t- \"Q-function calculates\" --> \"The Q-function denotes\"\n\t- \"the reward feedbacks perfectly\" --> \"the reward feedback perfectly\"\n\t- I prefer that the exposition of the perturbed reward MDP be done with C in the tuple. So: $\\tilde{M} = \\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{R}, C, \\mathcal{P}, \\gamma \\rangle$. This seems the most appropriate definition, since the observed rewards will be generated by $C$.\n\t- The setup of the confusion matrix for reward noise over is very clean. It might be worth pointing out that $C$ need not be Markovian. There are cases where C is not just a function of $\\mathcal{S}$ and $\\mathcal{R}$, like the adversarial case you describe early on.\n\n\nSection 3) Learning w/ Perturbed Rewards\n\t- Theorem 1 builds straightforwardly on Q-Learning convergence guarantee (it might be worth phrasing the result in those terms? That is: the addition of the perturbed reward does not destroy the convergence guarantees of Q-Learning.)\n\t- \"we firstly\" --> \"we first\"\n\t- \"value iteration (using Q function)\" --> \"value iteration\"\n\t- \"Definition 2. Phased Q-Learning\" --> \"Definition 2. Phrased Q-Learning\". I think? Unless you're talking about Phased Q from the Kearns and Singh '99 work.\n\t- \"It uses collected m samples\" --> \"It uses the collected m samples\"\n\t- Theorem 2: it would be helpful to define $T$ since it appears in the sample complexity term. Also, I would suggest specifying the domain of $\\epsilon$, as you do with $\\delta$.\n\t- \"convergence to optimal policy\" --> \"convergence to the optimal policy\"\n\t- \"The idea of constructing MDP is similar to\" --> this seems out of place. The idea of constructing which MDP? Similar to Kakade (2003) in what sense?\n\t- \"the unbiasedness\" --> \"the use of unbiased estimators\"\n\t- \"number of state-action pair, which satisfies\" --> \"number of state-action pairs that satisfy\"\n\t- \"The above procedure continues with more observations arriving.\" --> \"The above procedure continues indefinitely as more observation arrives.\" Also, which procedure? Updating $\\tilde{c}_{i,j}$? If so, I would specify.\n\t- \"is nothing different from Eqn. (2) but with replacing a known reward confusion\" --> \"replaces a known reward confusion\"\n\n\n4) Experiments:\n\t- Diverse experiments! That's great. Lots of algorithms, lots of environment types.\n\t- I expected to see Phrased Q-Learning in the experiments. Why was it not included?\n\t- The plots are pretty jagged, so I'm left feeling a bit skeptical about some of the results. The results would be strengthened if the experiments were repeated for more trials.\n\n5) Conclusion:\n\t- \"despite of the fact\" --> \"despite the fact\"\n\t- \"finite sample complexity of Q-Learning with estimated surrogate rewards are given\" --> It's not really Q-Learning, though. It's a variant of Q-Learning. I'd suggest being explicit about that.\n\nAppendix:\n\n\t- \"It is easy to validate the unbiasedness of proposed estimator directly.\" --> \"It is easy to verify that the proposed estimator is unbiased directly.\"\n\t- \"For the simplicity of notations\" --> \"For simplicity\"\n\t- \"the Phrased Q-Learning could converge to near optimal policy\" --> \"\"the algorithm Phrased Q-Learning can converge to the near optimal policy\"\"\n\t- \"Using union bound\" --> \"Using a union bound\"\n\t- Same comment regarding $\\gamma$: it's typically $0 \\leq \\gamma < 1$.\n\t- Bottom of page 16, the second equation from the bottom, far right term: $c.j$ --> $c,j$.\n\t- \"Using CauchySchwarz Inequality\" --> \"Using the Cauchy-Schwarz Inequality\"\n\n\nReferences:\n\tLoftin, Robert, et al. \"Learning something from nothing: Leveraging implicit human feedback strategies.\" Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on. IEEE, 2014.\n\n\tKrueger, D., Leike, J., Evans, O., & Salvatier, J. (2016). Active reinforcement learning: Observing rewards at a cost. In Future of Interactive Learning Machines, NIPS Workshop.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}