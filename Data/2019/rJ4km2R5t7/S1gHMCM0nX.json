{"title": "Interesting new benchmark", "review": "Summary:\nGLUE is a benchmark consisting of multiple natural language understanding tasks\nthat functions via uploading to a website and receiving a score based on\nprivately held test set labels.\nTasks include acceptability judgement, sentiment prediction, semantic equivalence\ndetection, judgement of premise hypothesis entailment, question paragraph pair\nmatching, etc..\nThe benchmark also includes a diagnostic dataset with logical tasks such as\nlexical entailment and understanding quantifiers.\n\nIn addition to presenting the benchmark itself, the paper also presents models\nfor performance baselines.\nThere is some brief analysis of the ability of Sentence2Vector vs. more complex\nmodels with e.g. attention mechanisms and of single-task vs. multi-task training.\n\nEvaluation:\nThe GLUE benchmark seems like a well designed benchmark that could potentially\nignite new progress in the area of NLU.\nBut since I'm not an expert in the area of language modeling and know almost\nnothing about existing benchmarks I cannot validate the added benefit over\nexisting benchmarks and the novelty of the suggested benchmarking approach.\n\nDetails:\nThe paper is well written, clear and easy to follow.\n\nThe proposed benchmarks seem reasonable and illustrate the difficulty of\nbenchmark tasks that involve logical structure.\n\nPage 5: showing showing (Typo)\n", "rating": "7: Good paper, accept", "confidence": "1: The reviewer's evaluation is an educated guess"}