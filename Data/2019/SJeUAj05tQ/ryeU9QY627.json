{"title": "This paper proposes a consensus-based distributed method, namely DADAM, for online optimization. The technical details are well presented and the empirical results are convincing. ", "review": "The proposed DADAM is a sophisticated combination of decentralized optimization and the adaptive moment estimation. DADAM enables data parallelization as well as decentralized computation, hence suitable for large scale machine learning problems. \n\nCorollary 10 shows better performance of DADAM. Besides the detailed derivations, can the authors intuitively explain the key setup which leads to this better performance?\n\nThe experimental results are mainly based on sigmoid loss with simple constraints. The results will be more convincing if the authors can provide studies on more complex objective, for example, regularized loss with both L2 and L1 bounded constraints.  \n\nTh experimental results in Section 5.1 is based on \\beta_1 = \\beta_2 = \\beta_3 = 0.9. From  the expression of \\hat v_{i,t} in Section 2, this setting implies the most recent v_{i,t} plays a more important role than the historical maximum, hence ADAM is better than AMSGrad. I am curious what the results will look like if we set \\beta_3 as a value smaller than 0.5. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}