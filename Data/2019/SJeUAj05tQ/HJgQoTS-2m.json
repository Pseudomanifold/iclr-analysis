{"title": "A consensus-based distributed adaptive gradient method for online optimization", "review": "Title: DADAM: A consensus-based distributed adaptive gradient method for online optimization\n\nSummary: \n\nThe paper presented DADAM, a new consensus-based distributed adaptive moment estimation method, for online optimization. The author(s) also provide the convergence analysis and dynamic regret bound. The experiments show good performance of DADAM comparing to other methods. \n\nComments: \n\n1) The theoretical results are nice and indeed non-trivial. However, could you please explain the implication to equation (7a)? Does it have absolute value on the LHS? \n\n2) Can you explain more clearly about the section 3.2.1? It is not clear to me why DADAM outperform ADAM here. \n\n3) Did you perform algorithms on many runs and take the average? Also, did you tune the learning rate for all other algorithms to be the best performance? I am not sure how you choose the parameter \\alpha here. What if \\alpha changes and do not base on that in Yuan et al. 2016? \n\n4) The deep learning experiments are quite simple. In order to validate the performance of the algorithm, it needs to be run on more datasets and networks architectures. MNIST and CIFAR-10 and these simple network architectures are quite standard. I would suggest to provide more if the author(s) have time. \n\nIn general, I like this paper. I would love to have discussions with the author(s) during the rebuttal period. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}