{"title": "Great start for the novel task of modeling source code edits", "review": "The paper presents a very interesting new task of modeling edits to a piece of source code. The task also has immediate practical applications in the field of Software Engineering to help with code reviews, code refactoring etc. \n\nPros:\n1. The paper is well-written and easy to follow.\n2. The task is novel (to my knowledge) and has various practical applications.\n3. Many obvious questions that would arise have been answered in the paper for eg., the contrast between explicit and implicit data representations.\n4. The use of synthetic data to learn about the types of edits that the models can learn is a very good idea and its inclusion is much appreciated.\n5. Evaluation on a very large real dataset demonstrates the usefulness of the model for real world tasks.  \n\nCons:\n\nIn general, I really like the task and a lot of the models and experiments, but the description of the real world experiments is severely lacking in information and results. Also, there are many unanswered questions about the synthetic experiments.\n\n1. Firstly, where is the data obtained from? Who are the programmers? What was the setting under which the data was collected in ?\n2. The paper doesn't provide examples from this real world dataset nor are there examples of model predictions on this dataset.\n3. What are the kinds of edits on the real world dataset? What happens if someone adds a 100 lines to a file and saves it? How is this edit added to the dataset?\n4. Some Error analysis on the real world data? It's hard to understand how the model is doing by just reporting the accuracy as 61.1%. Lots of the accuracy points maybe obtained for obvious commonplace edits like keywords, punctuation etc.. ? \n5. Some more dataset stats related to the real world data. For eg., how many tokens are in each snapshot?\n6. \"We evaluate models on their ability to predict a sequence of subtokens up to the next token boundary\" <-- Require more details about this. This section needs more clarity, its hard to interpret the results here.\n7. Are you going to release this real world dataset?\n8. If the real world dataset is large, why don't you report running time stats on that? Why use the synthetic dataset for testing scalability? If you have 8 million edits, that seems like a big enough dataset. How long did your models take on these?\n\nRe: the synthetic dataset\n\n1. It's nice that you've tested out different single synthetic edit patterns, but what happens in a dataset with multiple edit patterns? Because this will be the scenario in a real world dataset.\n2. What happens when the order of edits is different in the prediction but the outcome is the same? Suppose we have edit A followed by B and the model predicts B followed by A, but the end result is the same? The accuracy metric will fail here? How can this be addressed?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}