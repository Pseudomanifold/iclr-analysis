{"title": "Good baselines for a new task", "review": "The paper provides good baselines for predicting edits in a text (evaluated on source code) learned from a history of changes. This is an interesting problem that has not beed systematically studied in literature, with the exception of several sporadic works from the software engineering community. Fully predicting the code text writing process as opposed to the code itself is an interesting task with possible big impact, IF the accuracy of this edit model manages to significantly outperform simple left-to-right code text prediction techniques.\n\nOne of closest related works to this A somewhat similar system with language models for predicting APIs based on sequences is [1], it would help to compare to it at least on a conceptual level. is [2] that predicts if a \"change\" is complete i.e if it misses to complete a change. However it does not predict the entire edit process, but only the last missing piece of a change (usually a bug if it is missed).\n\nPro:\n - First fine grained text (code) evolution evaluation and formulation of the challenge to provide labels for predicting the process of code writing.\n - Discussion about effectiveness of the introduced explicit vs implicit tasks and good trade-offs discussion.\n - Techniques are applicable and interesting beyond code.\n - Well-written and easy to follow text\n\nAgainst:\n - The introduced models are based on simple sequence representations of code tokens. Once more semantic representation (let's say ASTs) are taken, the implicit attention techniques may need to also be updated.\n - It would help to elaborate more on the process defined in 5 for the real dataset that given a pair of code snapshots, assumes that one is obtained by simply inserting/removing the tokens in sequence. In particular, it could be that a candidate model predicts a different sequence with the same results in the snapshots. Then the training loss function should not penalize such solutions, but it will if the sequence is not strictly left-to-right. Did you need to tweak something here especially for larger changes?\n\n[1] Anh Nguyen, Michael Hilton, Mihai Codoban, Hoan Nguyen, Lily Mast, Eli Rademacher, Tien Nguyen, Danny Dig. API Code Recommendation using Statistical Learning from Fine-Grained Changes\n[2] Thomas Zimmermann, Andreas Zeller, Peter Weissgerber, Stephan Diehl. Mining version\nhistories to guide software changes.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}