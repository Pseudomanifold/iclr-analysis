{"title": "Overall idea is interesting, but novelty is limited and evaluation is poor", "review": "The paper proposes a modular approach to the problem of mapping instructions to robot actions. The first of two modules is responsible for learning a goal embedding of a given instruction using a learned distance function. The second module is responsible for mapping goals from this embedding space to control policies. Such a modular approach has the advantage that the instruction-to-goal and goal-to-policy mappings can be trained separately and, in principle, allow for swapping in different modules. The paper evaluates the method in various simulated domains and compares against RL and IL baselines.\n\nSTRENGTHS\n\n+ Decoupling instruction-to-action mapping by introducing goals as a learned intermediate representation has advantages, particularly for goal-directed instructions. Notably, these together with the ability to train the components separately will generally increase the efficiency of learning.\n\n\nWEAKNESSES\n\n- The algorithmic contribution is relatively minor, while the technical merits of the approach are questionable.\n\n- The goal-policy mapping approach would presumably restrict the robot to goals experienced during training, preventing generalization to new goals. This is in contrast to semantic parsing and symbol grounding models, which exploit the compositionality of language to generalize to new instructions.\n\n- The trajectory encoder operates differently for goal-oriented vs. trajectory-oriented instructions, however it is not clear how a given instruction is identified as being goal- vs. trajectory-oriented.\n\n- While there are advantages to training the modules separately, there is a risk that they are reasoning over different portions of the goal space.\n\n- A contrastive loss would seemingly be more appropriate for learning the instruction-goal distance function.\n\n- The goal search process relies on a number of user-defined parameters\n\n- The nature of the instructions used for experimental evaluations is unclear. Are they free-form instructions? How many are there? Where do they come from? How different are the familiar and unfamiliar instructions?\n\n- Similarly, what is the nature of the different action spaces?\n\n- The domains considered for experimental evaluation are particularly simple. It would be better to evaluate on one of the few common benchmarks for robot language understanding, e.g., the SAIL corpus, which considers trajectory-oriented instructions.\n\n- The paper provides insufficient details regarding the RL and IL baselines, making it impossible to judge their merits.\n\n- The paper initially states that this distance function is computed from learned embeddings of human demonstrations, however these are presumably instructions rather than demonstrations.\n\n- I wouldn't consider the results reported in Section 4.5 to be ablative studies.\n\n- The paper incorrectly references Mei et al. 2016 when stating that methods require a large amount of human supervision (data annotation) and/or linguistic knowledge. In fact Mei et al. 2016 requires no human annotation or linguistic knowledge.\n\n- Relevant to the discussion of learning from demonstration for language understanding is the following paper by Duvallet et al.\n\nDuvalet, Kollar, and Stentz, \"Imitation learning for natural language direction following through unknown environments,\" ICRA 2014\n\n- The paper is overly verbose and redundant in places.\n\n- There are several grammatical errors\n\n- The captions for Figures 3 and 4 are copied from Figure 1.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}