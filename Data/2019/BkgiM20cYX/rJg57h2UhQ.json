{"title": "Review", "review": "This paper presents an instruction-following model consisting of two modules: a\ngoal-prediction model that maps commands to goal representations, and an\nexecution model that maps goal representations to policies. The second module is\ntrained without command supervision via a goal exploration process, while the\nfirst module is trained supervisedly in a metric learning framework.\n\nThis paper contains an important core insight---much of what's hard about\ninstruction following is generic planning behavior that doesn't depend on the\nsemantics of instructions, and pre-learning this behavior makes it possible to\nuse natural language supervision more effectively. However, the paper also\ncontains a number of serious evaluation and presentation issues. It is obviously\nnot ready to publish (uncaptioned figures, paragraphs interrupted mid-sentence,\netc.) and should not have been submitted to ICLR in its present form.\n\nSUPERVISION AND COMPARISONS\n\nI found comparisons between supervision conditions in this paper difficult to\nunderstand. It is claimed that the natural language instruction following\napproaches described in the first paragraph \"require a large amount of human\nsupervision\" in the form of action sequences. This is not exactly true, as some\napproaches (e.g. Artzi 2013), can be trained with only task completion signals.\nMore problematically, all these approaches are contrasted with reinforcement and\nimitation learning approaches, which are claimed to use \"little human\nsupervision\". In fact, most of the approaches listed in this section use exactly\nthe same supervision---either action sequences (imitation learning) or task\ncompletion signals (reinforcement learning). Indeed, the primary distinction is\nthat the \"NLP-style\" approaches are typically evaluated on their ability to\ngeneralize to new instructions, while the \"RL-style\" approaches are evaluated on\nthe (easier) problem of fitting the complete instruction distribution as quickly\nas possible.\n\nThis confusion carries into the evaluation of the approach proposed in this\npaper, which is compared to RL and IL baselines. It's hard to tell from the\ntext, but it appears that this is an \"RL-style\" evaluation setting, where we\nonly care about rapid convergence rather than generalization. But the baselines\nare inadequately described, and it's not clear to me that they condition on the\ncommands at all. More significantly, it's not clear what an evaluation based on\n\"timesteps\" means for a behavior-cloning approach---is this the number of\ndistinct trajectories observed? The number of gradient steps taken? Without\nthese explanations it is impossible to interpret the experimental results.\n\nGENERALITY OF PROPOSED APPROACH\n\nDespite the advantages of the high-level two-phase model proposed, the specific\nimplementation in this paper has two significant shortcomings:\n\n- No evidence that it works with real language: despite numerous claims\n  throughout the paper that the model is designed to interpret \"human\n  instructions\", it is revealed on p7 that these instructions consist of one or two\n  5-way indicator features. This is an extremely impoverished instruction space,\n  especially compared to the numerous papers cited in the introduction that make\n  use of large datasets of complex natural-language strings generated by human\n  annotators. The present experiments do not support the use of the word \"human\"\n  anywhere in the paper.\n\n- No support for combinatorial action spaces. Even if we set aside the\n  distinctions between human-generated instructions and synthetic command\n  languages like used in Hermann Hill & al., the goal -> policy module is\n  defined by a buffer of cached trajectories and goal representations. While\n  this works for the simple environments considered in this paper, it cannot\n  generalize to real-world instruction-following scenarios where the number of\n  distinct goal configurations is too large to tractably enumerate. Again, this\n  is a shortcoming that existing approaches do not suffer from (given\n  appropriate assumptions about the structure of goal space), so the lack of\n  comparisons is problematic.\n\nCLARITY\n\nThe whole paper would benefit from copy-editing by an experienced English\nspeaker, but a few sections are particularly problematic:\n\n- The first paragraph of 4.1.1 is extremely difficult to understand What does\n  the fingertip do? What exactly is the action space?\n\n- The end of the second paragraph is also difficult to understand; after reading\n  it I still don't know what the extra \"position\" targets do.\n\n- 4.1.4 is cut off mid-way through a sentence.\n\n- last sentence of 4.2\n\nThe figures are also impossible to interpret: three of the four are captioned\n\"overview of the proposed framework\", and none are titled.", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}