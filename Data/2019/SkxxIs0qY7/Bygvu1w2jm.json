{"title": "nice idea", "review": "Pros:\nThis paper is easy to follow. The idea is nice in three folds. \n1. By changing the auxiliary model's role from a discriminator to a mediator, it directly optimizes the JSD measure, which is a symmetrized and smoothed version of KL divergence.  \n2. Moreover, the mediator and the generator follow similar predictive goals, rather than the opposite  goals of G and D in GANs. \n3. For discrete sequential data, it avoids approximating expected rewards using Markov rollouts.  \n \nCons:\nSome details are missing in the experiments. \n1. In Table 2 of [A], LeakGAN, SeqGAN and RankGAN all show significantly better performances in terms of BLEU on EMNLP2017 WMT, compared to results reported in Table 3 of the submission. Any difference?\n2. The Word Mover Distance is computed by training a discriminator, which could be unstable. Could you provide other metrics to evaluate diveristy like self-bleu?\n\n[A] Guo, Jiaxian, et al. \"Long text generation via adversarial training with leaked information.\" arXiv preprint arXiv:1709.08624 (2017).\n\nMisc:\n1. How will the number of samples (i.e. batch size) affect CoT ?\n2. How is the applicability of CoT for continuous data? It seems to me there is no theoretical difficulties to apply CoT on continuous data.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}