{"title": "Interesting and promising method for generative modelling of sequence data without policy gradient", "review": "The paper proposes an interesting method, where the discriminator is replaced by a component that estimates the density that is the mixture of the data and the generator's distributions. In a sense, that component is only a device that allows estimating a Jensen-Shannon divergence for the generator to then be optimized against. Other GAN papers have replaced their discriminator by a similar device (e.g., WGANs, ..), but the present formulation seems novel. The numerical experiments presented on a synthetic Turing test and text generation from EMNLP's 2017 news dataset appear promising. \n\nOverall, the mediator seems to allow to achieve lower Jensen-Shannon (JS) divergence values in the experiments (and is kind of designed for that). Although this may be an improvement with respect to existing methods for discrete sequential data, it may also be limited in that it may not easily extend to other types of divergences that have proved superior to JS in some continuous settings.\n\nThe paper is rather clear, although there are lots of small grammatical errors as well as odd formulations which end up being distracting or confusing. The language should be proof-read carefully. \n\nPros:\n- Generative modeling of sequence data still in its infancy\n- Potentially lower variance than policy gradient approaches\n- Experiments are promising\n\nCons:\n- Lots of grammatical errors and odd formulations\n\nQuestions:\n- Equation 14: what does it mean to find the \"maximum entropy solution\" for the given optimization problem?\n- Figure 2: how do (b) and (c) relate to each other?\n\nRemarks, small typos and odd formulations:\n- \"for measuring M_\\/phi\": what does measuring mean in this context?\n- What does small m refer to? Algorithm 1 says the total number of steps  but it is also used in the main text as an index for J and \\pi (for mediator?)\n- Equation block 8: J_m has not been defined yet\n- \"the supports of distributions G and P\"... -> G without subscript has now been defined in this context\n- \"if the training being perfect\"\n- \"tend to get stuck in some sub-optimals\"\n- the learned distribution \"collapseS\"\n- \"since  the data distribution is, thus ...\"\n- \"that measures a\" -> \"that estimates a ...\"?\n- \"a predictive module\": a bit unclear - generative v. discriminative is more usual terminology\n- \"is well ensured\"\n- \"with the cost of diversity\" -> \"at the cost of diversity\"?\n- \"has theoretical guarantee\"\n- in the references: \"ALIAS PARTH GOYAL\" (all caps)\n- \"let p denote the intermediate states\": I don't understand what this is. Where is \"p\" used? (proof of Theorem 3)\n- \"CoT theoretically guarantees the training effectiveness\": what does that mean?\n- Figure 3: \"epochs\" -> \"Epochs\"\n- Algorithm 1: what does \"mixed balanced samples\" mean? Make this more precise\n- \"wide-ranged\"\n- Equation 10 is too long and equation number is not properly formatted\n- Figures hard to read in black & white\n- Figure 2 doesn't use the same limits for the Y axis of the two NLL plots, making comparisons difficult. The two NLL plots are also not side-by-side", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}