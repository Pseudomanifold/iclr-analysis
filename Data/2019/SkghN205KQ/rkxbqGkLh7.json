{"title": "Review", "review": "The paper proposes to use a reward function to guide the learning of energy-based models for structured prediction. The idea is to update the energy function based on a random search algorithm guided by a reward function. At each iteration, the SPEN proposes a solution, then a better one is found by the search algorithm, and the energy function is updated accordingly.  Experiments are made on three use-cases and show that this method is able to outperform other training algorithms for SPENs. \n\nIn term of model, the proposed algorithm is interesting since it can allow us to learn from weakly supervised datasets (i.e a reward function is enough). Note that in Section 3, the reward function R is never properly defined which would be nice. The algorithm is quite simple and well presented in the paper. The fact that it is based on a margin could be discussed a little bit more since the effect of the margin is not clear in the paper (the value of alpha). Moreover, the structured prediction problem has already been handled as the maximization of a reward function using RL techniques (see works by H. Daume, and works by F. Maes) and the interest of this approach w.r.t these papers is not clear to me. A clear discussion on that point (and experimental comparison) would be nice. \n\nThe experimental section could be improved. First, the experiments on multi-label classification do not provide any comparison with SoTA methods while the two other use-cases provide some comparisons. Moreover, as far as I understand, the different use-cases could be fully supervised, and different reward functions could be defined. So investigating more deeply the consequences of the nature of the supervision/reward on these use-cases could be interesting and strengthen the paper.  Moreover, training sets are very small and it is difficult to know if this method can work on large-scale problems. \n\nPro:\n* interesting algorithm for structured prediction (base on reward)\n* interesting results on some (toy) use-cases\n\nCons:\n* Lack of discussion on the positive/negative point of the approach w.r.t SoTA, and on the influence of the reward function\n* Lack of experimental comparisons \n* Only toy (but complicated) problems with limited training sets\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}