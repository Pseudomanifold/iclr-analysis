{"title": "The present work interestingly clarifies several counter-intuitive behaviors of the information bottleneck (IB) method for the learning of a deterministic rule. We note, however, that the necessity of noise for its application to supervised learning was already known.", "review": "This work analyses the information bottleneck (IB) method applied to the supervised learning of a deterministic rule Y=f(X).\n\nThe idea as I understood it is as follows:\n1) In a first section the authors discuss the relationship between supervised learning through minimization of the empirical cross entropy and the maximization of the empirical mutual information with an intermediate latent variable T. \n2) They show that in the case of a deterministic rule, the information bottleneck curve has a simple shape, piecewise linear, and is not strictly concave. \n3) They show that the optimization of the IB Lagrangian for different \\beta does not lead to a point by point exploration of the IB curve.\n4) They propose a cure to the previous issue by introducing the squared IB Lagrangian. \n5) They exhibit uninteresting representations (noisy versions of the output Y) that are on the IB curve.\n6) They show that multiple successive representations (like in DNNs), have identical predicting power (mutual information with output Y) when they allow for perfect prediction. \n7) They use the IB method to train a neural net on MNIST, using the Kolchinsky estimate of the mutual informations. \n\t- they show that the optimization of the squared IB reaches more different points on the IB curve,\n\t- but that these representations are possibly uninteresting (hard clustering of uneven numbers of grouped classes) \n\t- they show that for large enough value of beta, zero error is reached. \n\nThe necessity of noise in the IB theory has been already pointed out by (Gilad-Bachrach et al., 2003; Shwartz-Ziv et al.  2017), although the more thorough analysis proposed here is novel. In practice, besides a few recent propositions (Kolchinsky et al., 2017; Alemi et al., 2016; Chalk et al., 2016) the IB Lagrangian is not a usual objective function for supervised learning. The motivation and impact of this work studying deterministic rules is therefore not completely convincing. \n\nFurther pros and cons:\n\nPros:\n- The discussion is generally well written. \n- This work provides in depth clarification of the counter-intuitive behaviors of the IB method in the case to the learning of a deterministic rule. \n- These are demonstrated with experiments conducted on the MNIST dataset for concreteness.\n\nCons:\n- The fact that multiple successive representations have identical predicting power when the prediction error is zero, was already observed for example in Shwartz-Ziv et al.  2017. It is not clear why this should be considered as an issue. It also seems to be a straightforward observation when restricting to the empirical measure on the training set. \n- The fact that the entire IB curve is not explored point by point by the IB Lagrangian is not necessarily an issue for learning. In the experiments of the present paper, the results seem to suggest that the interesting intermediate representations (separation in 10 compact clusters of the MNIST classes) is actually easier to obtain (large range of \\beta) optimizing the IB Lagrangian rather than the proposed squared IB Lagrangian. \n\nQuestions:\n- Do the authors know of an application where the full probing of the IB curve would be necessary?\n- In Section 2, when injecting the decomposition of the prediction density q(y|x) over the intermediate variable t in eq (3) was a Jensen inequality replaced by an equality?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}