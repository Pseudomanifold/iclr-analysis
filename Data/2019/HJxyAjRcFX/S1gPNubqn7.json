{"title": "An interesting paper in analyzing and improving model collapse problems in conditional GANs", "review": "This paper analyzes the model collapse problems on training conditional GANs and attribute it to the mismatch between GAN loss and reconstruction loss. This paper also proposes new types of reconstruction loss by measuring higher statistics for better multimodal conditional generation.\n\nPros:\n1.\tThe analysis in Sec 4.4 is insightful, which partially explains the success of MLMM and MCMLE over previous method in generating diverse conditional outputs.\n2.\tThe paper is well written and easy to follow.\n\nCons:\nAnalysis on the experiments is a little insufficient, as shown below.\n\nI have some questions (and suggestions) about experiments. \n1.\tHow does the training process affected by changing the reconstruction loss (e.g., how the training curve changes?)? Do MLMM and MCMLE converge slower or faster than the original ones? What about training stability? \n2.\tWhy only MLMM_1 is not compared with other methods on SRGAN-celebA and GLCIC-A? From pix2pix cases it seems that Gaussian MLMM_1 performs much better than MLMM_{1/2}.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}