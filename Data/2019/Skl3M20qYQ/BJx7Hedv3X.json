{"title": "Important topic, but lack of experiments", "review": "The authors aim at training a VAE that has disentangled latent representations in a \"synergistically\" maximal way.\nFor this they  use one (of several possible) versions of synergy defintions and create a straight forward penalization term for a VAE objective (roughly the whole mutual information minus the maximum mutual information of its parts).\nThey train this VAE on one dataset, namely dsprites, and compare it to a VAE with total correlation penalization. \n\nThe paper is well written and readable. The idea of using synergy is an important step forward in understanding complex models. The concept of synergy has great potential in machine learning and is highly relevant.\n\nThe main concepts of synergy are not developed in this paper and the used penalization term is straight forward.\nThe number of experiments conducted and comparisons done is quite limited. Also the potential of synergy is not really demonstrated, e.g. for representation learning, causality, etc., and appears here ad hoc. \nAlso why one should use the authors' suggested penalization term instead of total correlation is not discussed, nor demonstrated as they perform similarly on both disentanglement and synergy loss.\n\nI hope the authors find more relevant applications or data sets in the future to demonstrate the importance of synergy.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}