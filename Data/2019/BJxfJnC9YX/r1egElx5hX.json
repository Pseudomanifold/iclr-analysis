{"title": "Gradient based optimization of spiking networks via differentiable approximations during backpropagation", "review": "In this work, the authors investigate the training of spiking (leaky integrate and fire) neural networks, for which the normally used threshold function is replaced with the sigmoid function for backpropagation. They demonstrate the proposed method for an autoencoder network on MNIST and Fashion-MNIST, as well as an audio to image synthesis task. The reconstruction loss of the ANN and SNN models is compared, typically with an advantage for the ANN model, but on Fashion-MNIST the SNN results look better.\n\nThe paper addresses learning of deep SNNs, which is a difficult topic with relevance to both the machine learning and computational neuroscience communities. Since the paper does not aim for biological plausibility, the target audience is ML and neuromorphic engineering. For ICLR this is a paper of medium relevance.\n\nThe originality of the work is low, because approaches that propose to replace the non-differentiable threshold function with a differentiable proxy, in this case membrane potentials, have been known for several years, e.g. \"Spiking Deep Networks with LIF Neurons\", Eric Hunsberger and Chris Eliasmith, 2015, and \"Training Deep Spiking Neural Networks Using Backpropagation\", Jun-Haeng Lee et al. 2016. The main novelty is therefore the application to autoencoders, but overall this reduces originality and hence relevance for ICLR. A thorough discussion and differentiation to previous work is missing.\n\nAnother criticism is that the relevance of the multi-modal experiments for the idea of replacing the threshold function with a sigmoid function is not clear. Instead, the authors could provide additional experiments on classification tasks or investigate the scalability of the idea for deep (more layers) networks (e.g., on CIFAR10 classification). In general, the experimental section could be more tailored towards the central idea of the paper.\n\nWhile some of the parameters are explored in detail, the choice of others is not motivated. For example, the network sizes are just given without further justification, similarly the parameters for ANN training, which might have required more fine-tuning (e.g. learning rates). Figure 3(b) remains strange, because at some batch number the unmasked version simply ceases to work, but this could have been explained without a graph. Especially the comparison to the ANN without ANN would have required more fine tuning of learning rates, thus I do not see much value in the AE-SNN outperforming this method. The resulting reconstructions in Figures 4c look like they have some completely dark and some very strongly firing pixels, which indicates a far-from-perfect reconstruction. Actually the final reconstruction MSE of 0.2 on the binary MNIST images looks rather high.\nI find it surprising that the SNN outperforms the ANN on Fashion-MNIST, and I don't really see a reason why this should happen, and the argument in the article is not convincing, neither are the differences explored in detail. I therefore assume this is a bad parameter setting for the ANNs, and would encourage the authors to test and evaluate this in more detail.\n\n\nPros:\n+ Overall the paper is well written and has a transparent and meaningful structure. The central ideas are laid out in a comprehensive manner.\n+ In general, the training of spiking neural networks remains a challenging task and investigation of potential solutions is relevant to ICLR.\n\nCons:\n- Low originality and missing comparison to related work\n- Unconvincing experimental section\n\n\nMinor remarks:\n* The title is misleading in the sense, that the spatial part of the spatio-temporal representation is not related to actual space, and further, that the backpropagation is not spike-based\n* The figures appear blurry, and in the case of Fig. 2 is very hard to read.\n* In Figure 3c the unit of the spike train duration is not given in figure or caption.\n* There are a number of grammatical errors.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}