{"title": "Neuromorphic learning rule that needs comparisons and a temporally more interesting task", "review": "\"Learning spatio-temporal representations using spike-based backpropagation\" submitted to ICLR 2019 describes a gradient based learning algorithm for discrete-time feed-forward leaky-integrate-and-fire network. Based on (1) the mean square error of the membrane potential for misclassified time points, and (2) smooth approximation of the firing threshold with a sigmoid funciton, the authors obtain a chain rule through the network. The resulting algorithm is very far from being biologically plasible. They train an autoencoder for images and audio-to-image translator. Although this reviewer is happy to see spiking neural network in ICLR, there are several major flaws.\n\n1. Comparison with previous methods: the authors cite (Bohte 2002) and (Lee 2016) as alternate backpropagation schemes, however their performance is not compared. In addition, a large portion of the literature on training spiking neural networks is missing: Spikeprop, ReSuMe, Tempotron (and its extensions), SuperSpike, Huh & Sejnowski 2017, to name a few. I suggest comparing with these existing methods.\n\n2. Effect of leak coefficient (Fig 3a) indicates that alpha = 0.1 performs significantly better than less leak. Why didn't you investigate even larger alpha (shorter time constant)? As indicated by Fig 2b, your time constants seems to be very long relative to the time steps.\n\n3. (Comment) Bernoulli encoding of intensity (Poisson process discretized over 15 or 60 time steps) in the image is similar to dropout regularization, but not equivalent because (1) you only apply it to the encoding layer, and (2) probability depends on the strength of the image. Despite this difference, this encoding scheme seems to be providing some robustness to the encoding.\n\n4. (Minor) Spike-MSE is just number of spikes that were precisely matched in discrete time, and normalized MSE is proportional to the correlation coefficient, right? The names of the metric you use are somewhat misleading.\n\n5. (Wishlist) I'd love to see the features learned by the auto/trans-encoder. Does it extract receptive fields that are gabor-like? For MNIST, does it obtain common line segments?\n\n6. The sigmoid activation function assumption is unprincipled. How is the width of the sigmoid chosen? Have you tried to optimize the sigmoid for training performances? Is this related to the escape rate approximation?\n\n7. In general, training SNN with gradient descent is difficult because of the non-smooth threshold. Small changes can bring very large changes to the entire future outputs. The proposed method is no exception. The reason it doesn't seem destructive due to the static nature of the task. In a realistic setting where spike times are sparse and relative precisions carries information, I do not believe this algorithm can solve the temporal credit assignment problem. I suggest including a temporal task where the input is changing over time (not just the noise).\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}