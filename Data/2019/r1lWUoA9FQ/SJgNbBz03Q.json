{"title": "interesting maths; implications less clear", "review": "The paper considers the problem of adversarial examples in (mostly high-dimensional) multi-class classification problems. Although the results are not specific necessarily to very high dimensional data or two images, the paper mostly uses images as a running example, and so will I in the review. \n\nAssume that the data all lies in the unit box in R^n ([0, 1]^n). A multiclass classifier with K classes partitions the unit cube into K parts, each part corresponding to a given class. There are distributions \\rho_c associated with each class and there is a bound on their density given by U_c and the fraction of examples of class c is f_c. And (eps, p) adversarial point y for some point x is such that |x - y|_p <= \\eps and the classifier classifies x & y differently. \n\nThe paper shows that under this modeling assumption adversarial examples are inevitable. The results mostly use standard (but deep) results from probability theory. The technical proofs themselves are not particular difficult (provided one has the right background). I think the overall implications are interesting, and I will recommend the paper be accepted. \n\nHowever, I also feel that this is a missed opportunity. To some extent the authors do try to have some high-level discussion about adversarial examples, but I think this could be expanded on more. For instance, why should it be assumed that an example that is \\eps far should automatically have the same class label? Surely, being \"eps\"-far away is an equivalence relation, thus this would mean that all the hypercube would have to be labeled by the same class. This is clearly not the case. One plausible explanation is that if you take two points that are in two different classes, then any sequence of points that take one to the other with the property that each adjacent pair is at most \\eps far away, must have the property that some intermediate mass have negligible chance of being a \"natural\" image. \n\nOn the other hand, doesn't the fact that humans are not susceptible to most adversarial examples, imply that adversarial-example resistant classifiers exist? My own feeling is the assumption that U_c is bounded is the strongest assumption that may not hold true with real data. In any case, the paper has enough technical content to merit acceptance and I hope the open review forum will lead to a fruitful discussion about some of these questions.\n\n--\n\nMinor comments:\nPage 6 (just after Thm 2). Isn't the bound in Eqn. (5) true for all \\ell_p norms for p \\geq 2? (not just \\ell_2 as the sentence says)\nParas on Page 6 (just below Thm 2). It would be more pleasant if equation x could be replaced by Eq. (x) or Equation (x). \nPara in Sec 7 on Unbounded density: Clarify what norm you mean when you talk about \\eps/2 perturbations.\nThm 5: Seems odd to have a theorem about MNIST. Surely the result is a lot more general!!!", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}