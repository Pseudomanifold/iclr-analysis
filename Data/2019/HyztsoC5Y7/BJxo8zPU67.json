{"title": "Shows superior sample complexity of model-based Meta RL, but not much further insight", "review": "The paper proposes using meta-learning and fast, online adaptation of models to overcome the mismatch between simulation and the real world, as well as unexpected changes and dynamics. This paper proposes two model-based meta-learning reinforcement algorithms, one based on MAML and the other based on recurrence, and experimentally shows how they are more sample efficient and faster at adapting to test scenarios than prior approaches, including prior model-free meta-learning approaches.\n\nI do have an issue with the way this paper labels prior work as model-free meta-learning algorithms, since for example, MAML is a general algorithm that can be applied to model-free and model-based algorithms alike. It would be more accurate in my opinion to label the contributions of this paper as model-based instantiations of prior existing algorithms, rather than new algorithms outright.\n\nI\u2019m a bit confused with equation 3, as the expectation is over a single environment, and the trajectory of data is also sampled from a single environment. But in the writing, the paper describes the setting as a potentially different environment at every timestep. Equation 3 seems to assume that the  subsequence of data comes from a single environment, which contradicts what you say in the text. As described, equation 3 is then not really much different from previous episodic or task based formulations.\n\nThe results themselves are not unexpected, as there has already been prior work that this paper also mentions showing that model-based RL algorithms are more sample efficient than model-free.\n\nSection 6.1, I like this comparison and showing how the errors are getting better.\n\nFor section 6.2, judging from the plots, it doesn\u2019t seem you are doing any meta-learning in this experiment, so then are you just basically running a model-based RL algorithm? I\u2019m very confused what you are trying to show. Are you trying to show the benefit of model-based vs model-free? Prior work has already done that. Are you trying to show that even just using a meta-learning algorithm in an online setting results in good online performance? Then you should be comparing your algorithm to just a model-based online RL algorithm. You also mention that the asymptotic performance falls behind, is this because your model capacity is low, or maybe your MPC method is insufficient? If so, then wouldn\u2019t it be more compelling to, like prior work, combine this with a model-free algorithm and get the best of both worlds?\n\nSection 6.3 results look good.\n\nSection 6.4, I really like the fact you have results on a real robot.\n\nOverall I think the paper does successfully show the sample complexity benefits and fast adaptation of model-based meta-RL methods. The inclusion of a real world robot experiment is a plus. However the result is not particularly surprising or insightful, as prior work has already shown the massive sample complexity improvement of model-based RL methods.\n\nUPDATE (Dec 4, 2018):\n\nI have read the author response and they have addressed the specific concerns I have brought up. I am overall positive about this paper and the new changes and additions so I will slightly increase my score, though I am still concerned about the significance of the results themselves.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}