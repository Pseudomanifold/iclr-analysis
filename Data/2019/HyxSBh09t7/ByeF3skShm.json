{"title": "Interesting paper, may lack novelty and clarity, seems to have problems in experimental evaluation.", "review": "Summary:\nThe paper presents a generative model for graphs which is a VAE-like architecture where the encoder is a scattering transform with fixed parameters (rather than a trainable neural net)\n\nPros:\n+ The problem of graph generative models is very important, and a lot of the existing methods are not very scalable.\n+ Using spectral methods in place of standard \"neural net operations\" makes a lot of sense. \n+ Numerical results for the \"link prediction\" task seem to be significantly better than those of baselines.\n\nCons: \n- The paper contains various imprecisions (see the non-exhaustive list below), and significant amount of statements which are hard to understand.\n- I am not sure if the work can be considered particularly novel: in particular, it is not really emphasised what is the difference with [Angles & Mallat '2018].\n- The motivation for the work is not entirely clear: it is true that GANS and VAEs have their issues, but in my view it is not really explained / argued why the proposed method would solve them.\n- I find the argument about the efficiency not very convincing, especially after looking at the members (bottom of p. 7): the scattering transform alone takes several orders of magnitude longer than the baseline. Authors also mention that their method does not require training \fof the encoder, but I do not see any comparisons with respect to number of parameters.\n- The experimental evaluation for \"signal generation\" and \"graph generation\" is not very convincing. For the former there is no real comparison to existing models. And for the latter, the experimental setup seems a bit strange: it appears that the models were trained on different subsets of the dataset, making the comparison not very meaningful. Also, I would expect to see the same methods to be compared to a cross all the tasks (unless it is impossible for some reason).\n\nVarious typos / imprecisions / unclear statements:\np.1, \"are complex as well as difficult to train and fine-tune.\": not at all clear what this means.\np.1, \"Their development is based on fruitful methods of deep learning in the Euclidean domain, such as convolutional and recurrent neural networks.\": Recurrent and convolution neural network are not necessarily restricted to Euclidean domains. \np.1, \"Using a prescribed graph representation, it is possible to avoid training\nthe two components at the same time, but the quality of the prescribed representation is important\nin order to generate promising results.\": not clear what this sentence means.\np.2, \"Unlike GAN or VAE, the model in this paper does not require training two components either iteratively or at the same time.\": I do not see why that would necessarily be a bad thing, especially in the case of VAE where traditional training in practice corresponds to training a single neural net.\np.3, \"GAN-type graph networks use a discriminator in order to compete with the generator and make its training more powerful.\": I am not sure this statement is strictly correct.\np.9: \"We remark that the number of molecules in the training sets are not identical to that in ...\": does this mean that the models are effectively trained on different data? In that case, the comparison is not very meaningful.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}