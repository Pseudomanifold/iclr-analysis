{"title": "Interesting, but no convincing results and analysis", "review": "This paper proposes a scalar modulator adding to hidden nodes before an activation function. The authors claim that it controls the sensitivity of the hidden nodes by changing the slope of activation function. The modulator is combined with a simple CNN, DesneNet, and a LSTM model, and they provided the performance improvement over the classic models. \n\nThe paper is clear and easy to understand. The idea is interesting. However, the experimental results are not enough and convincing to justify it. \n\n1) The authors cited the relevant literature, but there is no comparison with any of these related works. \n\n2) Does this modulator actually help for CNN and LSTM architectures? and How? Recently, there are many advanced CNN and LSTM architectures. The experiments the authors showed were with only 2 layer CNNs and 1 layer LSTM. There should be at least some comparison with an architecture that contains more layers/units and performs well. There is a DenseNet comparison, but it seems to have an error. See 4) for more details.\n\n3) The authors mentioned that the modulator can be used as a complement to the attention and gate mechanisms. Indeed, they are very similar. However, the benefit is unclear. More experiments need to be demonstrated among the models with the proposed modulator, attention, and gates, especially learning behavior and performance differences. \n\n4) The comparison in Table 2 is not convincing. \n- The baseline is too simple. For instance on CIFAR10, a simple CNN architecture introduced much earlier (like LeNet5 or AlexNet) performs better than Vanilla CNNs or modulated CNNs.\n- DenseNet accuracy reported in Table 2 is different from to the original paper: DenseNet (Huang et al. 2017) CIFAR10 # parameters 1.0M, accuracy 93%, but in this paper 88.9%. Even the accuracy of modulated DenseNet is 90.2% which is still far from the original DenseNet.\nFurthermore, there are many variations of DenseNet recently e.g., SparsenNet: sparsified DenseNet with attention layer (Liu et al. 2018), # parameters 0.86M, accuracy 95.75%. Authors should check their experiments and related papers more carefully.\n\nSide note: page 4, Section 3.1 \"The vanilla DenseNet used the structure (40 in depth and 12 in growth-rate) reported in the original DenseNet paper (Iandola et al., 2014)\". This DenseNet structure is from Huang et al. 2017 not from Iandola et al. 2014.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}