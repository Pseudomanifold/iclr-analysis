{"title": "Interesting idea but the overall state of the paper needs improvements", "review": "Summary: \nThis paper introduces an architectural change for basic neurons in neural network. Assuming a \"neuron\" consists of a linear combination of the input, followed by a non-linear activation function, the idea is to multiply the output of the linear combination by a \"modulator\", prior to feeding it into the activation function. The modulator is itself a non-linear function of the input. Furthermore, in the paper's implementation, the modulators share weights across the same layer. The idea is demonstrated on basic vision and NLP tasks, showing improvements over the baselines. \n\nI - On the substance:\n1. Related concepts and biological inspirations\nThe idea is analogous to attention and gating mechanisms, as the authors point out, with the clear distinction that the modulation happens _before_ the activation function. It would have been interesting to experiment a combination of modulation and attention since they do not act on the same levels. \nAlso, the authors claim inspiration from the biological neurons, however, they do not elaborate in depth on the connections to the neuronal concepts mentioned in the introduction. \n\n2. The performance of the proposed approach\nIn the first experiment, the modulated CNN at 150 epochs seems to have comparable performance with the vanilla CNN at 60 (the latter CNN starts overfitting afterwards). Why not extending the learning curve to more epochs since the modulated CNN seems on a positive slope? \nThe other experiments show some improvements over the baselines, however more experiments are necessary for claiming generality. Especially, the baselines remain too simple and there are some well-known well-performing architectures, for both image and text processing, that the authors could compare to (cf winning architectures for imagenet for instance). They could also take these same architectures and augment them with the modulation proposed in the paper. \nFurthermore, an ablation study is clearly missing, what about different activation functions, combination with other optimization techniques etc.?\n\nII - On the form:\n1. the paper is sometimes unclear, even though the overall narrative is sound,\n2. wiggly red lines are still present in the caption of Figure 1 right.\n3. Figure 6 could be greatly simplified by putting its content in the form of a table, I don't find that the rectangles and forms bring much benefit here.\n4. Table 5 (should it not be Figure?): it is not fully clear what the lines represent and based on which input. \n5. some typos: \n - abstract: a biological neuron change[s]\n - abstract: accordingly to -> according to \n - introduction > paragraph 2 > line 11: Each target node multipl[i]es \n\nIII - Conclusion: \nThe idea is interesting and some of the experiments show nice results (eg. modulated densenet-lite outperforming densenet) but the overall paper needs further improvements. In particular, the writing needs to be reworked, the experiments to be consolidated, and the link to neuronal modulation to be further investigated. ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}