{"title": "Review for \"Noise-Tempered Generative Adversarial Networks\"", "review": "The authors make two observations. 1) Matching real and fake with postprocessing by an additive noise model is attained when the real and fake distributions are the same (thus there is a priori no need to anneal the noise like in previous work), however while a necessary condition for Pr = Pg, it is not sufficient. 2) The condition becomes sufficient if P_{r+epsilon} = P_{g + epsilon} for all noise distributions in a certain *class*. Thus, they propose to match multiple noisy versions of probability distributions. This has the known advantageous property that doesn't suffer from the disjoint / negligible intersection of supports problem, but it benefits as well from the fact that if the class of noise models is large enough, the solution to the proposed loss is the real distribution, and that by not anealling the noise term, it doesn't degenerate back to the original ill-conditioned problem.\n\nThen, however, the authors propose two particular kinds of family of noise distributions. In particular, the additive noise is a (random) mixture between a delta at 0 and gaussians with varying sigmas (let's call this family A) or a (random mixture) between a delta at 0 and N(w) with w ~ Gaussian(0, I) with N a learned neural net. Furthermore, for some reason pick the minimum cost over noise distributions (with some regularizer), which is unexplained.\n\nHere are my concerns about this:\n- For these particular kinds of distributions, and this optimization process (minimizing the cost as a function of sigma / N, with this regularizers), why would the minimum be only at Pr=Pg? Namely, the authors point out that P_{r+e} = P_{g+e} for all e dists implies that P_r = P_g, why is that the case when considering only the distribution that minimizes the inner loop and in this limited family of distributions? This seems to be a crucial point. \n- By doing a mixture with the unmodified (probably colapsed to a low dimensional structure) data and fake distributions, the resulting distributions are likely not absolutely continuous. Thus, a priori I don't see why the JSD(P_{r+epsilon}, P_{g+epsilon}) is a continuous quantity as a function of the generator's parameters, or why it provides a usable gradient.\n\nThe authors only provide results in reasonably mid-dimensional benchmarks, (CIFAR, celeb-A, LSUN), and results are not particularly impressive. I would urge the authors to do more controlled experiments: check that for the proposed distributions, the gradients of the cost with respect to the generator don't vanish / explode when the discriminator gets more confident (as in Arjovsky & Bottou). Check that the discriminator doesn't get 'perfect' too quickly. In a 1D or 2D experiment, actually plot the gradients of the cost with respect to the discriminator with respect to the input (i.e. what's the gradient field), or plot the discriminator itself (e.g. figure 1 of wgan) to see if it provides a usable gradient. Make sure to plot these for a very well trained discriminator, since this is what would ensure that no careful balance between gen and disc needs to be maintained. The only controlled experiments (figure 5) are not particularly encouraging, since it seems that the generator has still a quite significant amount of samples away from the modes.\n\nThe paper is in general well written, and the ideas and observations are simple, to the best of my knowledge novel, and have clear consequences. The actual instantiation of these ideas seem to have several caveats, or aspects that need elaboration, as mentioned before. Furthermore, the experiments aren't too convincing. Some parts also require better writing. For example, in points 1 and 2 (noise descriptions) in page 4, delta(epsilon) is not a notation commonly employed to refer to no noise (which is what the authors mention in text). If the noise is additive then this is just delta_0. As well, int delta(eps - N(w)) p(w) dw is hard to understand / nonstandard, the authors should just say N(w) with w ~ N(0, I) or N^# N(0, I) [and clarify that # means push-forward of a distribution].\n\nPet peeve: In the third line of the abstract 'probability density function' -> 'probability distribution'. A low dimensional distribution doesn't have a density :).", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}