{"title": "good progress; but simulation requires some work", "review": "This paper shows that a class of deep neural networks have no spurious local valleys \u2013--implying no strict local-minima. The family of neural networks studied includes a wide variety of network structure such as (a variant of) DenseNet. Overall, this paper makes some progress, improving previous results on over-parametrized networks. \n\nPros: The flexibility of the network structure is an interesting point.\nCons: CNN was covered in previous related works (so weight sharing is not a new contribution); DenseNet is not explicitly covered in this work (I mean current DenseNet does not have N skip-connections to output; correct me if wrong). \n  The simulation part is not that clear, and I have a few questions that I hope the authors can answer. \n\nSome comments/suggestions:\n1) Training error needs to be discussed.\n   Page 8 says \u201cThis effect can be directly related to our result of Theorem 3.3 that the loss landscape of skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training error\u201d. This relation is not justified. The implication of Thm 3.3 is that getting zero training error is easier, but the tables are only for test error. Showing training error is the only way to connect to Thm 3.3. I expect to see a high training error for C-10, original VGG and sigmoid activation functions, and zero training error for both skip-SGD (rand) and skip-SGD (SGD). \n    This paper has no theory on generalization, thus if a whole section is just about \u201cinvestigating generalization error\u201d, then the connection to theoretical parts is weak --btw, one connection is the comparison of two algorithms, which fits the context well, and thus interesting (though comparison result itself probably not surprising).   \n\n2) Data augmentation.\n  \u201cNote that the rand algorithm cannot be used with data augmentation in a straightforward way and thus we skip it for this part.\u201d Why? \n   With data augmentation, is M still larger than N? If yes, then the number of added skip connection is different for C-10 and C-10-plus, which is not mentioned in the instruction of Table 2. \n\n3)It may be better to mention explicitly that \"it is possible to have bad local min\" \u2013perhaps in abstract and/or introduction. \n  --Although \u201cno sub-optimal strict local minima\u201d is mentioned, readers, especially non-optimizers, might not notice \"strict\".\n  --In fact, in the 1st round read, I do not have a strong impression of \"strict\". Later I realized it. Mentioning this can be helpful. \n\n4) Some references I suggest to include:\n   [R1] Yu, X. and Chen, G. On the local minima free condition of backpropagation learning. 1995.  --related work. \n   [R2] Lu, H., Kawaguchi, K. Depth creates no bad local minima. 2017. --also deep nets.\n   [R3] Liang, S., Sun, R., Li, Y., & Srikant, R. \"Understanding the loss surface of neural networks for binary classification.\" 2018. --Also study SoftPlus neurons.\n   [R4] Nouiehed, M., & Razaviyayn, M. Learning Deep Models: Critical Points and Local Openness. 2018. --also deep nets. \n\nMinor questions:\n  --Exact 10% test accuracy for a few cases. Why exact 10%?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}