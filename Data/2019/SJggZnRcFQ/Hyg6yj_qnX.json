{"title": "I tried very hard but I think ultimately failed to understand this paper.", "review": "The fundamental idea proposed in this paper is a sensible one:  design the functional form of a policy so that there is an initial parameterized stage that operates on perceptual input and outputs some \"symbolic\" (I'd be happier if we could just call them \"discrete\") characterization of the input, and then an arbitrary program that operates on the symbolic output of the first stage.\n\nMy fundamental problem is with equation 3.  If you want to talk about the factoring of the probability distribution p(a | s) that's fine, but, to do it in fine detail, it should be:\nP(a | s) = \\sum_sigma P(a, sigma | s) = \\sum_sigma P(a | sigma, s) * P(sigma | s)\nAnd then by conditional independence of a from s given sigma\n = \\sum_sigma P(a | sigma) * P(sigma | s)\nBut, critically, there needs to be a sum over sigma!  Now, it could be that I am misunderstanding your notation and you mean for p(a | sigma) to stand for a whole factor and for the operation in (3) to be factor multiplication, but I don't think that's what is going on.\n\nThen, I think, you go on to assume, that p(a | sigma) is a delta distribution.  That's fine.\n\nBut then equation 5 in Theorem 1 again seems to mention delta without summing over it, which still seems incorrect to me.\n\nAnd, ultimately, I think the theorem doesn't make sense because the transformation that the program performs on its input is not included in the gradient computation.  Consider the case where the program always outputs action 0 no matter what its symbolic input is.   Then the gradient of the log prob of a trajectory with respect to theta should be 0, but instead you end up with the gradient of the log prob of the symbol trajectory with respect to theta.\n\nI got so hung up here that I didn't feel I could evaluate the rest of the paper.  \n\nOne other point is that there is a lot of work that is closely related to this at the high level, including papers about Value Iteration Networks, QMDP Networks, Particle Filter Networks, etc.  They all combine a fixed program with a parametric part and differentiate the whole transformation to do gradient updates.  It would be important in any revision of this paper to connect with that literature.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}