{"title": "interesting task and dataset", "review": "This paper collects a new annotated dataset for knowledge grounded dialog task. The proposed models combine two recent neural networks, Memory Net and Transformer, for the purpose of the task. I highly appreciate the efforts to collect such a precious dialog dataset for the community. Also, the setup in data collection actually narrows down the scope of chitchat dialog into a specific topic by grounding it to a set of knowledge. \n\nHere are summaries of my concerns and questions about the paper. \n\n# applicability of the knowledgeable bot\nWhat is the basic motivation of this work? Once you develop a chatbot that can produce a response grounded by knowledge, how could it be applied to real-world applications? Are you trying to teach a student who is looking for more knowledge about a topic? If so, you should be more careful about what knowledge the student (or apprentice in the paper) knows or don\u2019t know about the topic and how their knowledge models dynamically change over the chat. Otherwise, the proposed model seems a simple knowledge retrieval model given the dialog context. Would you please provide motivations of the work?\n\n# No explicit goal of a dialog makes the chat divergent and open-ended\nWithout a specific goal given to the annotators or a restriction in the instruction, a dialog in the current setting might diverge beyond the context. For example, if an apprentice says about her/his personal opinion about the topic (e.g., I hate the Gouda cheese) or past experience (e.g., I went to a music festival by Michael Jackson 23 years ago), then how do you control the chat between two annotators or how do you train a model not to pay much attention on out-of-topic utterances?  \n\n# Lack of further analysis of the dataset\nData collection part itself seems to be the biggest contribution to this work. Why don\u2019t you bring one of real dialog example in Figure 3 to the main paper and say more about it? For example, what other interesting applications can you develop on this dataset? \n\nCompared to the Wizard, the role of apprentice seems unclear to me. I found from the examples in Figure 3 that most of the apprentices\u2019 responses are a follow-up question about the knowledge, a personal agreement or feeling or their preference. Do you have any post analysis on the types of responses from the apprentices so highlighting utilities of the dataset in a real application? \n\n# Some questions on data collection\nDo you have any incentive mechanism to make annotators more engage in the dialog?\nDid you filter out some bad dialogs? Then, how did you measure the quality of a dialog? \nHow do you penalize bad annotators that often make aggressive words or don\u2019t follow the instruction you set up? \n\n# A question on the model\nCompared to previous works such as (Zhang at al., ACL18), the proposed model seems to have the only replacement with Transformer encoder and a loss term for knowledge selection. Have you tried another way of dealing with the knowledge part? For example, a ranking loss might be better than the attention. \n\n# Questions on the Experiment section\nAny experiment to show the effect of different \\lambda value in the loss of the generative model? \n\nWhen you evaluate the generative model, have you also tried other automatic metrics such as BLEU instead of only PPL and Unigram-F1? For this task, the possible response grounded by the topic+knowledge might be too diverse to measure though. Could you possibly add some constraints to the annotators to do some clear tasks over the dialog so you can systematically evaluate the dialog w.r.t the constraint? Otherwise, evaluation of this task seems to be mostly the same as chitchat systems.\n\nIn Table 5, human evaluators only measure the likeness of the dialog which seems very naive. Why don\u2019t you measure whether the apprentice gets new knowledge of which s/he didn\u2019t know before, whether the knowledge provided from the model was informative, whether the dialog was fun and engaging or more? The current human evaluation seems very weak though. \n\nThis might be an auxiliary question: have you tried to train the model for apprentice and make two models chat with each other? How does the chat look like then?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}