{"title": "Review", "review": "\n\nThis paper tackles the question generation problem from a logical form and proposes an addition called Scratchpad Encoder to the standard seq2seq framework. The new model has been tested on the WebQuestionsSP and the WikiSQL datasets, with both automatic and human evaluation, compared to the baselines with copy and coverage mechanisms.\n\nMajor points:\n\nOverall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty. I don\u2019t recommend to accept this paper, at least in the current format.\n\nThe paper states two major contributions (the last paragraph of Introduction), one is the new model Scratchpad Encoder, and the other is \u201cpossible to generate a large high quality (SPARQL query, local form) dataset\u201d. For the second contribution, there isn\u2019t any evaluation/justification about the quality of the generated questions and how useful this dataset would be in any KB-QA applications. I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.\n\nFor the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn\u2019t explain well the intuition of this \u201cwrite\u201d operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?\n\nIn general I find Section 3 pretty difficult to follow. What does \u201ckeeping notes\u201d mean? It seems that the goal of this model is to keep updating the encoder hidden vectors (h_0, .., h_T) instead of fixing them at the decoder stage. I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode. \\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.\n\nMinor points:\n- tau Yih et al, 2016 --> Yih et al, 2016\n- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.\n- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}