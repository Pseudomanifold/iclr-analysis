{"title": "Nice extensions of continual learning techniques, but not sure about GANs as a benchmark", "review": "Summary:\nThis paper proposes the use of GANs as a realistic benchmark for continual learning, and shows how continual learning techniques applied to the discriminator can alleviate mode collapse. Existing continual learning approaches for discrete task structure (EWC and IS) are adapted to the continually shifting domain of GANs, and evaluated on a toy mixture of Gaussians, CelebA and CIFAR-10 image generation as well as textGANs.\n\nThis is a clearly written paper that nicely addresses some of the challenges of bridging the toy problems of continual learning with a real world problem of GAN training. The experiments and ablations are thorough, but the empirical gains in terms of improving GAN metrics are relatively minor. It's also not obvious that GAN training is really a continual learning problem, as every time the generator distribution shifts, the discriminator has to shift as well. Thus progress on stabilizing and improving GANs might not transfer back to domains that truly represent continual learning where the goal is to build a single network that perform well at all points in time. In terms of more realistic benchmarks for continual learning, I believe a controlled synthetic dataset would be more practical than the sequence of GAN checkpoints proposed here.\n\nStrengths:\n+ Clearly written, with good background discussion of continual learning approaches and challenges.\n+ Interesting adaptation of EWC and IS to the continual setting with task rates, online memory (sum of quadratics is quadratic), and controlled forgetting with a time decay.\n+ Thorough experiments and ablations on toy tasks, CelebA and CIFAR-10, and textGANs. Nicely includes error bars and compares computation time for each approach.\n\nWeaknesses:\n- The paper could benefit from more discussion on the goals of continual learning, and what is wrong with existing toy benchmarks. Why not come up with a tractable toy problem that addresses these difficulties directly?\n- I remain unconvinced that GANs as a good benchmark for continual learning. For example, it has been argued that many of the problems with GANs arise from dynamics of minimax optimization difficulties, and there are many recent approaches that were not compared to that focus on this optimization aspect of GAN training (Metz et al., Roth et al., Mescheder et al.). How would you relate these theoretical ideas to continual learning?\n- Most the experimental improvements are incremental. How did you choose or tune hyperparameters of your approach? ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}