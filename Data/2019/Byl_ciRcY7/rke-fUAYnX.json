{"title": "This paper started from Breiman's dilemma and showed that it relies on dynamics of normalized margin distribution.", "review": "The authors found that general generalization bounds fail to capture the ramp loss. However, once the network scaled by its Lipschitz constant, it becomes efficient to get an upper bound of generalization error, while also needs to trade-off the constant in the margin error. Due to the limitation of fixing the constant in margin error, the authors  tried to use the quantile margin to change the bound, which is easy to tune the hyper-parameter. They also conducted the experiments that the quantile margin generalization bound could be used to predict the tendency of loss curve both in training and test in some sense.\n\nIt's really an interesting work to provide a way for early stopping and to show the quantile margin maybe a substitution of tendency in training error as well as test error.\n\nQuestions: \n\nIt could be difficult to judge from the phase transition, if exists, in the evolution of normalized margin distributions curve. Maybe  some quantitative descriptions are needed. \n\nBesides, the authors' quantile margin bound (Theorem 2) shows the upper bound of margin (or say margin error). But the bound is not direct to support the powerful experiments results, the relationship between the tendency of quantile margin, training and test error.\n\nTypos:\n In Eqn. (10), the first $f_t$ should be $\\widetilde{f}_t$ .\nIn Eqn. (9) and (11), there is  $1$.\nIn Proof in Lemma A.1, the convolution operator is $x(v)$ not $x(u)$, since Lemma is also true.\nIn Proof in Lemma D.4, though the proof is same in the book `Foundations of machine learning' by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, please check the typo.\nIn Proof of Proposition 1, lack $\\frac{1}{n}$ in Rademacher complexity.\nIn Proof of Theorem 1, maybe you should take $\\mathbb{E}$ not $\\mathbb{P}$ before $\\ell_{\\gamma_1,\\gamma_2}(\\xi(\\widetilde{f}(x,y))))$.\nIn Proof of Theorem 2, page 18 the last line in the equation, why can the second term after divided by $L_f$ bounded by $L$, maybe need some conditions or I missed something.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}