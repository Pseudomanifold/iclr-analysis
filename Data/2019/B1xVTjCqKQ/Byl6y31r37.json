{"title": "Interesting theory, shakey experiments", "review": "Quality & Clarity:\nThis is a nice paper with clear explanations and justifications. The experiments seem a little shakey.\n\nOriginality & Significance:\nI'm personally not familiar enough to say the theoretical work is original, but it is presented as so. However it seems significant. The numerical results do not seem extremely significant, but to be fair I'm not familiar with state of the art nearest neighbor results ie Fig 3.\n\nPros:\nI like that you don't take much for granted. E.g. you justify using convolutional net in 2.1, and answered multiple of my questions before I could type them (e.g. why didn't you include nonlinearities between convolutions, why bother with cascaded convolutions, and what you mean by near-optimal).\n\nCons:\nThe visual comparisons in Figure 4 are difficult to see. DLAMP appears to be over-smoothing but in general it's hard to compare to low-ish resolution noisy-looking textures. I strongly recommend using a test image with a clear texture to illustrate your point (eg the famous natural test image that has on the side a tablecloth with zig-zag lines)\n\nThe horizontal error bars are obfuscated by the lines between markers in Fig 3a.\n\nI don't understand Fig 3a. You are varying M, which is on the Y-axis, and observing epsilon, on the X-axis?\n\nQuestions:\nCan you state what is novel about the discussion in the \"Theoretical Insights\" subsection of 2.1? I guess this is described in your abstract as \"we cast the problem ... by using a maximum likelihood protocol...\" but your contribution could be made more explicit. For example \"We show that by jointly optimizing phi and lambda (sensing and recovery), we are maximizing the lower bound of mutual information between reconstructions (X) and samples (Y)\" (that is my understanding of the section)\n\nWhy don't you use the same M for all methods in the Figure 3 experiments? ie why did you use a different M for numax/random versus deepSSRR/DCN?\n\nWhy do you choose 20-layers for the denoiser? Seems deep...\n\nThe last part of the last sentence of the 2nd paragraph of section 3.1 should be a complete sentence \"though, with more number of parameters\". Does that mean that the DCN has more parameters than the DeepSSRR?\n\nI am willing to change score based on the response\n\n******************\nUpdate after author response:\nThanks for the clear response and Figure 3, and nice paper. My score is updated.\nPS: I still think that the (tiny) error bars are obfuscated because the line connecting them is the same thickness and color.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}