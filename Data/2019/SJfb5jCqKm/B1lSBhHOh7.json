{"title": "Good paper but contribution feels isolated from related work", "review": "-- Paper Summary --\n\nThe proposed methodology draws on the connection between boosting in ensemble learning and SGD for training DNNs, whereby misclassified instances are implicitly targeted in later training iterations once easier examples have been classified correctly. The authors observe that this incurs a trade-off in which easily-classified examples become susceptible to overfitting at later stages in the training procedure when the network parameters adapt to fit more complex examples. Two early stopping algorithms are proposed in order to mitigate this issue. The first approach, PES, is more robust, but too computationally expensive to be applied in practice; on the other hand, AES approximates the former procedure by directly assuming that easier training examples will be learnt earlier on in the training procedure. The proposed technique is shown to calibrate the confidence scores obtained from state-of-the-art approaches for training deep nets, resulting in substantial performance improvements with respect to the proposed E-AURC metric. \n\n-- General Commentary --\n\n- The paper isolates itself from other post-calibration methods by stating that \u2018our focus here is only on the core task of ranking uncertainties\u2019. In doing so, there is no comparison to other calibration methods, which makes it difficult to properly assess the impact of this work in comparison to other papers addressing the poor calibration of uncertainty typically associated with deep nets. The authors immediately dismiss PES as being too computationally expensive, so I\u2019d be interested in at least seeing AES be compared to more lightweight calibration methods.\n\n - This paper champions the use of an alternative metric (E-AURC) for assessing model quality, which is the sole quantity of interest in the experimental evaluation. While the E-AURC metric is indeed well-motivated in Section 3, I could see there being some scepticism as to why more traditional metrics such as log likelihood aren\u2019t used here. This would also facilitate comparison to other post-calibration methods. In this regard, the authors should consider supplementing their experiments with more widely-used metrics not limited to uncertainty ranking.\n\n- I would be interested in seeing the analysis shown in Figure 2 extended to each of the baseline models discussed in the paper. Such examples would give a clearer perspective of which methods are particularly susceptible to the overfitting problem targeted by the methodology proposed in this work.\n\n- Some of the notation in the problem statement is a bit confusing, with i being simultaneously  used as the training iteration number as well as an index for Y. This needs to be updated.\u2028\n\n- There\u2019s a lot of whitespace in Figure 1 which could be avoided by giving additional examples of how the metric works.\n\n- \u2018Early Stopping without a Validation Set (Mahsereci et al, 2017)\u2019 warrants a citation here.\n\n- The paper is otherwise generally well-written and a pleasure to read. Some spotted typos:\n\nP1: for highly confident instance(s)\nP3: which borrows element(s)\nP3: \u2018unit-less\u2019 : this is unhyphenated in another part of the text\nP5: Final reference to Figure 2(b) should refer to Figure 2(c) instead   \nP7: which (is) initialized\n\n-- Recommendation --\n\nI admit to feeling fairly ambivalent about this paper - on one hand, the paper is well-written and its contributions are effectively communicated. While myopic, the experiments also convincingly showcase the performance improvements obtained by applying AES over the baseline methods. On the downside, this paper limits itself to comparing the proposed approaches to baseline methods where no other calibration is carried out. Lack of direct comparison against other post-calibration methods results in the paper adding little to the overall literature on DNNs other than asserting that calibration through early stopping is better than not doing anything else.\n\nPros/Cons: \n\n+ Properly-motivated contributions and well-written paper.\n+ The two early stopping algorithms are explained well, even if the appealing connection to boosting gets lost somewhere along the way.\n+ Results show that AES improves the results of several DNN training approaches.\n\n- Use of E-AURC as the sole metric for assessing quality in the Experiments section exposes this paper to instant criticism.\n- The notion of preserving model snapshots can be problematic when training requires thousands of epochs.\n- No comparison to other post-calibration techniques. \n\n\n** Post-rebuttal\n\nScore increased to a 7 following rebuttal and paper revision.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}