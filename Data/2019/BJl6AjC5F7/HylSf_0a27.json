{"title": "This paper looks at learning to represent edits for text revisions and code changes. The main contribution is in defining a new task, providing a new dataset, and building simple neural network models that show good performance.", "review": "This paper looks at learning to represent edits for text revisions and code changes. The main contributions are as follows:\n* They define a new task of representing and predicting textual and code changes \n* They make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change\n* They try simple neural network models that show good performance in representing and predicting the changes\n\nThe NLP community has recently defined the problem of predicting atomic edits for text data (Faraqui, et al. EMNLP 2018, cited in the paper), and that is the source of their Wikipedia revision dataset. Although it is an interesting problem, it is not immediately clear from the Introduction of this paper what would be enabled by accurate prediction of atomic edits (i.e. simple insertions and deletions), and I hope the next version would elaborate on the motivation and significance for this new task. \n\nThe \"Fixer\" dataset that they created is interesting. Those edits supposedly make the code better, so modeling those edits could lead to \"better\" code. Having that as labeled data enables a clean and convincing evaluation task of predicting similar edits.\n\nThe paper focuses on the novelty of the task and the dataset, so the models are simple variations of the existing bidirectional LSTM and the gated graph neural network. Because much of the input text (or code) does not change, the decoder gets to directly copy parts of the input. For code data, the AST is used instead of flat text of the code. These small changes seem reasonable and work well for this problem.\n\nEvaluation is not easy for this task. For the task of representing the edits, they show visualizations of the clusters of similar edits and conduct a human evaluation to see how similar these edits actually are. This human evaluation is not described in detail, as they do not say how many people rated the similarity, who they were (how they were recruited), how they were instructed, and what the inter-rater agreement was. The edit prediction evaluation is done well, but it is not clear what it means when they say better prediction performance does not necessarily mean it generalizes better. That may be true, but then without another metric for better generalization, one cannot say that better performance means worse generalization. \n\nDespite these minor issues, the paper contributes significantly novel task, dataset, and results. I believe it will lead to interesting future research in representing text and code changes.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}