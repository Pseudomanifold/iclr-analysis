{"title": "Expect more experiments", "review": "This paper puts forward a new global+local memory pointer network to tackle task-oriented dialogue problem.\n\nThe idea of introducing global memory is novel and experimental results show its effectiveness to encode external knowledge in most cases.\n\nHere're some comments:\n1. In global memory pointer, the users employ non-normalized probability (non-softmax). What is the difference in performance if one uses softmax?\n\n2. In (11), there's no linear weights. Will higher weights in global/local help?\n\n3. As pointed out in ablation study, it's weird that in task5 global memory pointer does not help.\n\n4. The main competitor of this algorithm is mem2seq. While mem2seq includes DSTC2 and In-car Assistant, and especially in-car assistant provides the first example dialogue, why does the paper not include expeirments on these two datasets?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}