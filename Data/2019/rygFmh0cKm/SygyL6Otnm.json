{"title": "Interesting idea & fair results", "review": "This paper analyzes the limitation of probability density distillation with reverse KL divergence, and proposes two practical methods for probability distillation.\n\nDetailed comments:\n\n1) Typo: should be WaveNet, not Wavenet.\n\n2) In Proposition 1. $c_i$ should be $\\rho_i$.\n\n3) One may explain \u201cpath derivative\u201d with more details. Also, I am really confused by Proposition 1 and its underlying implication. Given p_s and p_t are centered at the origin, isn\u2019t p_s(x) already the optimal if it\u2019s just a unit Gaussian. Why do we need a derivative pointing away from the origin? At least, one need parameterize p_s as N(0, \\phi)?\n\n4) In section 3.2, \u201cset $\\mu = [2, 2]^T$\u201d? Isn\u2019t $\\mu$ a T dimensional vector?\n\n5) A lot of important details are missing in neural vocoder experiment. For x-reconstruction, do you use L1 or L2 loss?  For student model, do you use Gaussian IAF with WaveNet architecture as in ClariNet, or Logistic IAF as in Parallel WaveNet? Following this question, do you compute KLD in closed-form? Do you use the regularization term introduced in ClariNet? Student with KL loss and power loss outperforms x-reconstruction. Did you try x-reconstruction along with power loss?\n\nPros:\nCertainly, there are some interesting ideas in this paper. \n\nCons:\nThe experiment results are not good enough. The paper is poorly written. A lot of important details are missing.  \n\nHowever, I would like to raise my rating to 6, if these comments can be properly addressed.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}