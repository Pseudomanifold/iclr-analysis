{"title": "Not very novel and rather confusing", "review": "Being familiar but not an expert in reinforcement learning, my review will focus on the overall soundness of the proposed method\n\nSummary:\n\nThe authors are interested in the problem of sample efficiency in reinforcement learning, i.e. how to learn a policy achieving good performance (discounted reward) in a RL setting using as little interaction with the environment as possible.\nTo do this the authors propose to learn a policy in a new environment where the reward has changed: an exploration bonus is added to the reward that should bias the agent towards the least frequently visited states.\n\nThe algorithms proposed throughout the manuscript are extensions of a two-part algorithm of the following flavour: 1) An estimate of visitation count is done in an online fashion using a modified version of the successor representation (SR). 2) This estimates parametrizes the exploration bonus of the environment . Both learning algorithms are optimized together.\n\nThis initial algorithm is fairly simple in its description and builds on well established ideas in RL. The authors then \u2018evaluate the effectiveness of the proposed exploration bonus in a standard model-based algorithm\u2019 against other baselines. They do explain how the model is learned, but not how the policy is optimized.\n\nThe remainder of the manuscript applies the same idea to different settings.\nFor large state spaces, The SR expected visits are learned using TD along with state action value functions. The counts of visitations are replaced by features that are also learned.\n\nOverall, the manuscript is rather confusing.\nThe SSR theorem is stated (with no real intuition and the actual bounds on n(s) left for the reader to derive). It is not well motivated. Why would we want expected counts and not the discounted version?\nThen the remainder of the paper actually makes no use of this theorem, but only use it as a distant inspiration. Tentative connections are made such as TD underestimating SR thus leading to a result more akin to SSR, which is highly speculative. It is also irrelevant since features are learned anyway.\n\nThe final proposed architecture has many additions to a simple DQN (the reconstruction + the exploration bonus + the MMC). This makes it difficult to understand what the contribution of the exploration bonus is.\nIt does not help that results are manually extracted from histograms found in  papers.\n\nOverall, although the intuition is interesting (though not so new).\nThe overall motivation and structure of this manuscript makes think it does not match the standards of ICLR for publication\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}