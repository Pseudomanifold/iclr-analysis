{"title": "Excellent method and results but need more comparisons and better writing", "review": "I'll start with a disclaimer: I have reviewed the NIPS 2019 submission of this paper which was eventually rejected. Compared to the NIPS version, this manuscript had significantly improved in its completeness. However, the writing still can be improved for rigor, consistency, typos, completeness, and readability.\n\nAuthors propose a novel variational inference method for a locally linear latent dynamical system. The key innovation is in using a structured \"parent distribution\" that can share the nonlinear dynamics operator in the generative model making it more powerful compared. However, this parent distribution is not usable, since it's an intractable variational posterior. Normally, this will prevent variational inference, but the authors take another step by using Laplace approximation to build a \"child distribution\" with a multivariate gaussian form. During the inference, the child distribution is used, but the parameters of the parent distribution can still be updated through the entropy term in the stochastic ELBO and the Laplace approximation. They use a clever trick to formulate the usual optimization in the Laplace approximation as a fixed point update rule and take one fixed point update per ADAM gradient step on the ELBO. This allows the gradient to flow through the Laplace approximation.\n\nSome of the results are very impressive, and some are harder to evaluate due to lack of proper comparison. For all examples, the forward interpolate (really forecasting with smoothed initial condition) provides a lot of information. However, it would be nice to see actual simulations from the learned LLDS for a longer period of time. For example, is the shape of the action potential accurate in the single cell example? (it should be since the 2 ms predictive r^2 shows around 80%).\n\nExcept in Fig 2, the 3 other examples are only compared against GfLDS. Since GfLDS involves nonconvex optimization, it would be reasonable to also request a simple LDS as a baseline to make sure it's not an issue of GfLDS fitting.\n\nFor the r^2=0.49 claim on the left to right brain prediction, how does a baseline FA or CCA model perform?\n\nWas input current ignored in the single cell voltage data? Or you somehow included the input current as observation model?\n\nAs for the comment on Gaussian VIND performing better on explaining variance of the data even though it was actually count data, I think this maybe because you are measuring squared error. If you measured point process likelihood or pseudo-r^2 instead, Poisson VIND may outperform. Both your forecasting and the supplementary results figure show that Poisson VIND is definitely doing much better! (What was the sampling rate of the Guo et al data?)\n\nThe supplementary material is essential for this paper. The main text is not sufficient to understand the method.\n\nThis method relies on the fixed point update rule operating in a contractive regime. Authors mention in the appendix that this can be *guaranteed* throughout training by appropriate choices of hyperparameters and network architecture. This seems to be a crucial detail but is not described!!! Please add this information.\n\nThere's a trial index suddenly appearing in Algorithm 1 that is not mentioned anywhere else.\n\nIs the ADAM gradient descent in Algorithm 1 just one step or multiple?\n\nMSE -> MSE_k in eq 13\n\nLFADS transition function is not deterministic. (page 4)\n\nlog Q_{phi,varphi} is quadratic in Z for the LLDS case. Text shouldn't be 'includes terms quadratic in Z' (misleading).\n\nregular gradient ascent update --> need reference (page 4)\n\nDue to the laplace approximation step, you don't need to infer the normalization term of the parent distribution. This is not described in the methods (page 3).\n\nEq 4 and 5 are inconsistent in notation.\n\nEq (1-6) are not novel but text suggests that it is.\n\nPredict*ive* mean square error (page 2)\n\nIntroduction can use some rewriting.\n\narXiv papers need better citation formatting.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}