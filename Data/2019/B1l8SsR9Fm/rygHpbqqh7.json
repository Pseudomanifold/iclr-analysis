{"title": "Unclear if the approach will give practical benefits", "review": "\nThis paper proposes an algorithm for data compression which learns\nthe most representative samples of a dataset by solving a mixed\ninteger program. The paper then finds a stationary point of the\nprogram by a greedy method. Empirical results show that compressing\na large fraction of samples using the method does not cause a\nnoticeable drop in learning performance.\n\nThe algorithm and theory in this paper are clearly presented. The\nidea however is not quite novel. The impact of reducing samples on\nlearning problems have been studied in literature, e.g. [1][2][3].\nAlgorithms like sketching [2] and importance sampling [3] have been\nshown both in theory and practice that can reduce dataset sample\nsize without a big drop in performance. Besides, both [2][3]\nrequires less computation time than solving the original problem.\nHowever, the proposed algorithm takes more computation than\nminimizing the loss.\n\nSome applications of the proposed algorithm are presented in\nappendix B. However, it would be better to also show the learning\nor optimization problems in these applications since they can help\nus understand how to apply the proposed algorithm in practice. Some\nexperiments on these applications are also preferred.\n\nFor experiments, both synthetic data and large-scale real data are\ntested. However, it would be better comparing with other algorithms\nlike [1][2][3] to show the novelty of the proposed algorithms. In\nsection 4.2, the synthetic data experiments are conducted using\nonly one constructed dataset and multiple random noise. Generating\na random dataset each time will show that the performance of the\nalgorithm is not a special case for some specific datasets and will\nalso give insights on the robustness of the algorithm. The proposed\nmethod can be applied to both regression and classification\nproblems. Some experiments on classification would help better\nexploring this algorithm. The experiments on reduced sample size\nand learning performance are comprehensive. However, I would like\nto see experiments on running time, since running time represents\nthe key performance metric in real world applications.\n\nSome questions: In Section 2.4: Please explain what robust learning\nin your case is. In your experiments, how you choose your parameter\nepsilon? What is the average loss in your experiments? Average loss\nis more commonly used in practice. In your experiments, what if you\npick these selected samples and train them? Data compression is\nalso well studied in learning theory [4] (Part IV, Section 30). It\nis also interesting to know if the proposed algorithm can reach the\ntheoretical bound.\n\n[1] Understanding Black-box Predictions via Influence Functions\n[2] sketching as a tool for numerical linear algebra\n[3] Randomized algorithms for matrices and data\n[4] understanding machine learning from theory to algorithms", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}