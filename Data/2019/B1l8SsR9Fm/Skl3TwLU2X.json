{"title": "Dataset selection and compression: a work done without much motivation, in isolation, without an interesting theoretical conclusion.", "review": "The paper addresses the problem of selecting a subset of a labeled dataset that retains the performance of training on the full dataset. The techniques presented in the paper are able to select just 5% of the dataset with only a small loss in error.\n\nThe paper has several severe issues with it: motivation, related work, and theoretical novelty.\n\nFirst of all, the problem setting is somewhat strange. In active learning, it is natural to select a subset of the dataset to label, for when the lableling cost is high. When there is too much labeled data, it is natural to select a subset for computational reasons (e.g. core-set selection). However, this work doesn't save on labeling cost or on computational costs. Instead, the paper claims the application is distributed learning where communication costs are expensive (and presumably where computation is not limited on the agents, which is unrealistic in some of the applications the paper mentions). However, the paper doesn't really address this problem in the experiments, algorithm, or theory they just mention it off-hand as an application. Why restrict the communication protocol to datasets rather than say, model parameters? How would \"sufficient datasets\" be combined and how would this change things?\n\nPerhaps an even larger issue is the lack of comparison to other methods. This paper doesn't have a related work section or compare to any baselines (not even random sampling). As examples, core-set selection and machine teaching are very similar setups, but these aren't mentioned anywhere. This glaringly shows up in the experiments section when only results for the algorithm presented are shown.\n\nFinally, from a theoretical perspective, the conceptual/theoretical novelty that datasets can be compressed sub-linearly for a given model is not really that interesting. Intuitively, once a model class has seen enough data, it won't be able to learn much more from further data. From theory, we expect that for a given model trained on a given dataset, there will be approximation error and estimation error. The approximation error is fixed (for the model family) while the estimation error typically goes down as 1/n or 1/sqrt{n}. For such large datasets and low-dimensional data, I do not find it very surprising that there is only a small decrease in estimation error. In fact, I wouldn't be suprised to see the same phenomenon with randomly sampled data on some of the datasets (probably not as good as their method, but qualitatively similar). Unfortunately, the paper does not compare to a random sampling baseline. \n\nAs another point, for their \"novel problem\" (2a)-(2c), I think (2c) doesn't make much sense. Why require the dataset to be of a certain size if the generalization error is already required to be low? In fact, this constraint is in the opposite direction of what we want, I could see trying to minimize the size of the dataset. I suspect that this constraint is only to make the optimization technique presented in this paper work.\n\nIn summary, this paper solves an unmotivated problem, in isolation, with a theoretically uninteresting conclusion.\n\nSmall points:\nf: X |-> Y should be f: X -> Y. \nThe natural numbers (\\mathbb{N}) are already positive so the + is unnecessary. Did you mean \\mathbb{Z}^+?\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}