{"title": "Forward search planning", "review": "The authors introduce two techniques:\n\nOne is (old school) forward search planning https://en.wikipedia.org/wiki/State_space_planning#Forward_Search\nfor input/output-provided sequential neural program synthesis on imperative Domain Specific Languages with an available partial program interpreter (aka transition function)(from which intermediate internal states can be extracted, e.g. assembly, Python). \nPrevious work did:\n  which_instruction, next_neural_state = neural_network(encoding(input_output_pairs), neural_state)\nThis technique:\n  which_instruction = neural_network(encoding(current_execution_state_output_pairs))\n  next_execution_state = vectorized_transition_function(current_execution_state, which_instruction)\n\nThe second one is ensembles of program synthesizers (only ensembled at test-time). \n\n\nGuiding program synthesis by intermediate execution states is novel, gets good results and can be applied to popular human programming languages like Python.\n\nPros\n+ Using intermediate execution states\nCons\n- State space planning could be done in a learnt tree search fashion, like e.g. Monte Carlo Tree Search\n- Ensembling synthesizers at test time only\n- why not have stochastic program synthesizers, see them as a generative model, and evaluate top-k generalization?\n\nPage 7\nTable 3 line 3: \"exeuction\" -> \"execution\"", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}