{"title": "Unclear formulation, and Benefit of approach is not Demonstrated", "review": " Summary: \n%%%%%%%%%%%%%%%\nThe paper explores ways to adapt the learning rate rule through a new minimax formulation.\nThe authors provide regret bounds for their method in the online convex optimization setting.\n\nComments:\n%%%%%%%%%%%%%%%\n-I found the motivation of the approach to be very lacking.\nConcretely, it is not clear at all why the minimax formulation even makes sense, and the authors do not explain this issue.\n\n-While the authors provide regret guarantees for their method, the theoretical analysis does not reflect when is their approach  beneficial compared to standard adaptive methods. Concretely, their bounds compare with the well known bounds of AdaGrad. \nIt is nice that their approach enables to extract AdaGrad as a private case. But again, it is not clear what is the benefit of their extension.\n\n-Finally, the experiments do not illustrate almost any benefit of the new approach compared to standard adaptive methods.\n\n\nSummary\n%%%%%%%%%%%%%%%\nThe paper suggests a different approach to adapt the learning rate.\nUnfortunately, the reasoning behind the new approach is not very clear.\nAlso, nor theory neither experiments illustrate the benefit of this new approach over standard methods.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}