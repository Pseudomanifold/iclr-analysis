{"title": "The proposed idea is not properly supported and is not convincing.", "review": "This paper presents a new method for learning diverse policies that can potentially transfer better to new environments. The proposed method aims to find simulation configurations that lead to diverse behaviors using \u201csubmodular optimizations\u201d technique; an idea stemmed from past data summarization methods.  \n\nPros:\n\n-The paper deals with an important problem in RL which is learning robust policies that can transfer better\n\n- Simple idea based on prior information theory literature is proposed. \n\n- Good incorporation of past methods for improving robustness in RL.\n\nCons:\n\n>>The paper has provided weak evidence and to support the effectiveness and significance of the proposed approach. The current analysis and experimental evaluations are not convincing.\n\nRelevant baselines are missed and limited tasks are explored:\n- A relevant prior work of Pinto et al. (2017) is not considered in the baselines for comparison.  \n- Only two tasks are considered in experiments which are not representative of the effectiveness of the proposed approach and does not provide convincing evidence that the proposed method performs better than prior works.\n-In the HalfCheetah task only the performance of random baseline is shown and comparison with EpOpt is not reported\n-Results of the base DDPG policy is reported in the experiments (Fig 3 and Table 1). For a thorough comparison these missing results are necessary. \n\n>>Correct citation to the prior work of \u201cdomain randomization\u201d is necessary:\n- The second paragraph of related work section (first paragraph of page 2) explains that the idea of domain randomization has been first used in computer vision by Tobin et al. 2017. The idea of domain randomization was first proposed and deployed on a real robot platform in Sadeghi and Levine 2016 (Sadeghi, Fereshteh, and Sergey Levine. \"CAD2RL: Real single-image flight without a single real image.\"\u00a0arXiv preprint arXiv:1611.04201\u00a0(2016).) and was later used in Tobin et al. 2017 (this is also explained in the Tobin et al. 2017). Adding proper citation to Sadeghi and Levine 2016 is necessary. \n\n>> Relevant prior works are missed:\n- The proposed idea in the paper is relevant to multi-task RL (e.g. Teh Y, Bapst V, Czarnecki WM, Quan J, Kirkpatrick J, Hadsell R, Heess N, Pascanu R. Distral: Robust multitask reinforcement learning. InAdvances in Neural Information Processing Systems 2017 (pp. 4496-4506). Adding related discussion about prior multi-task RL methods is highly recommended for improving the paper.\n- Citating several relevant prior RL methods that deal with transfer learning is missed. A few examples are:\n\nRusu AA, Vecerik M, Roth\u00f6rl T, Heess N, Pascanu R, Hadsell R. Sim-to-real robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286. 2016 Oct 13.\n\nRusu AA, Rabinowitz NC, Desjardins G, Soyer H, Kirkpatrick J, Kavukcuoglu K, Pascanu R, Hadsell R. Progressive neural networks. arXiv preprint arXiv:1606.04671. 2016 Jun 15.\n\nRusu AA, Colmenarejo SG, Gulcehre C, Desjardins G, Kirkpatrick J, Pascanu R, Mnih V, Kavukcuoglu K, Hadsell R. Policy distillation. arXiv preprint arXiv:1511.06295. 2015 Nov 19.\n\n\n>> The format of the paper needs to be improved. \n- The introduction section is rather incomplete and does not properly motivate the problem and the proposed solution.\n-The organization of the experimental section should be improved. It is hard to follow the experiments and find the key experimental results in the current version of the paper.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}