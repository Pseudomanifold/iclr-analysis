{"title": "Interesting idea, but both writing and experiments need further improvement", "review": "In this submission, the authors propose the Y-learner to estimate conditional average treatment effect(CATE), which simultaneously updates the parameters of the outcome functions and the CATE estimator. The co-learning strategy idea is interesting, but the details of the method are not well explained and the experiment results seem not convincing. The detailed comments are as follows:\n\nFirst, the motivation of Y-learner is not well presented. As mentioned in the paper, Y-learner is an improvement of X-learner, so the authors need to clearly illustrate the deficiency of X-learner. Also, whether the Y-learner can keep the advantages of X-learners and overcome its disadvantages?\n\nSecond, in section 3, the authors mention the importance of the learning rate in the training of GAN as an analogy to explain why co-learning can achieve better performance. However, it doesn\u2019t convince me well, and the reasons are as follows: in GAN, the generator and discriminator have the adversarial relationship, but this relationship is no longer exist in Y-learner as the outcome prediction networks (f_{\\theta_0}, f_{\\theta_1}) and CATE estimation network (f_{\\tau}) are complementary. Whether the phenomenon observed in the GAN training can be an analogy to explain the superiority of co-learning is still doubtful. The authors should give more detailed explanations.\n\nThird, I have some questions about the results (figure 2) of the experiment which aims to test the importance of co-learning in the Y-learner. (1) Which category (method with co-learning or without co-learning) the method Y-learner with no backpropagation belongs to? Is the method Y-learner with no backpropagation the same as Algorithm 1 excluding the line 3 and 7? (2) In figure 2, the method Y-learner with no backpropagation achieves similar performance with the full Y-learner. Why does this happen? Whether the method Y-learner with no backpropagation can replace the Y-learner? Why the authors design the full backpropagation through f_{\\theta_0}, f_{\\theta_1} when training f_{\\tau} in the Y-learner?\n\nForth, whether the machine learning methods adopted in R, S, T, X-learners are all neural networks when conducting the experiment on the simulated data as well as the GET-OUT-THE-VOTE dataset? \n\nFifth, the experiment results are not convincing. In the introduction, the authors claim that Y-learner can achieve state of the art performance with only a fraction of the data on several CATE estimation tasks. However, the results on six simulated datasets and the MNIST task don\u2019t support this claim: (1) In the simulated datasets, only in dataset 2 (complex linear case), the Y-learner has better performance with fewer training samples, and in the other five datasets, there always exits some baselines that perform better than Y-learner. (2) In the MNIST task, the baseline method S-learner performs much better than the proposed method, and also S-learner requires much less training sample to learn good CATE estimator compared with Y-learner. \n\nSixth, the analysis of the experiment results are missing. For example, the authors should explain why the proposed method doesn\u2019t perform as well as S-learners in MNIST task. \n\nMinor question: the notations \\pi_{\\theta_0}, \\pi_{\\theta_1}, and \\pi_{\\tau} in Figure 1 are not explained. Are they typos? Or they should be f_{\\theta_0}, f_{\\theta_1} and f_{\\tau} instead of \\pi?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}