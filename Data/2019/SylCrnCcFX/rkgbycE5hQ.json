{"title": "Very nice work with clear intuition and impressive results", "review": "1. This is a very relevant and timely work related to robustness of deep learning models under adversarial attacks. \n\n2. In recent literature of verifiable/certifiable networks, (linear) ReLU network has emerged as a tractable model architecture where analytically sound algorithms/understanding can be achieved. This paper adopts the same setting, but very clearly articulates the differences between this work and the other recent works (Weng et al 2018, Wong et al. 2018).  \n\n3. The primary innovation here is that the authors not only identify the locally linear regions in the loss surface but expand that region by learning essentially leading to gradient stability. \n\n4. A very interesting observation is that the robustifying process does not really reduce the overall accuracy which is the case of many other methods. \n\n5. The visualizations show the stability properties nicely, but a bit more explanations of those figures would help the readers quite a bit.\n\n6. While I understand some of the feasibility issues associated with other existing methods, it would be interesting to try to compare performance (if not exact performance, the at least loss/gradient surfaces etc.) with some of them.\n\n7. The adversarial scenarios need to be explained better. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}