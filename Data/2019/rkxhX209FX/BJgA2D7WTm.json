{"title": "Introducing active learning to robust policy search for efficient sampling.", "review": "This paper introduced an active learning mechanism on top of robust policy search in RL for better sampling efficiency. The authors proposed EffAcTS active learning framework and combined it with policy search method EPOpt. Theoretical analysis of active learning efficiency was not investigated. Simulation experiments were done on Hopper and Half Cheetah, 5 runs for each parameter setting.\n\nThe paper is well written and easy to follow. The authors quickly went through several key topics (active learning, linear bandits, multi-task, etc.) without too many details. However, there is a huge lack of key references in these topics. It would be better to notice that they were not introduced together with DRL.\n\nOverall, it is a nice paper with incremental contributions on every dimension the authors claimed (e.g. comparing to Sharma et al., 2018).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}