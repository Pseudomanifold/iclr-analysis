{"title": "Combination of several existing methods + none quite convincing experiments", "review": "Summary:\n\nIn this paper, the authors propose a policy gradient algorithm for solving a Bayes-Adaptive MDP (BAMDP). At each iteration, the algorithm samples several MDPs from the prior distribution and simulates a trajectory for each sampled MDP. During the simulation, the algorithm uses a Bayes filter to update the posterior belief distribution at each time step. Finally, the algorithm uses the sampled trajectories and update the policy using the TRPO algorithm. \n\nThe authors propose to pass the state and belief through separate encoders, to reduce their dimensions, and then put them together and give them to the policy network. Although the experiments show that the encoding did not improve the performance significantly, except in the Lightdark problem. \n\nThe authors show that their algorithm can also be used to solve POMDPs by replacing the state-belief pair with just belief. Basically turning a POMDP to a belief state MDP and then applying the algorithm. They evaluate their algorithm in two POMDP problems, one discrete and one continuous, in both their algorithm achieves a reasonable performance. \n\nA tricky part of the algorithm is how to define a Bayes filter for continuous latent states. This is crucial in updating the posterior after each observation. The way the authors handle this is by discretization, and how the discretization should be done (high or low resolution) is a hyper parameter. Although the experiments indicate that the proposed algorithm, especially with encoders, is quite robust w.r.t. the discretization. \n\n\nComments:\n\n- The idea behind the algorithm proposed in the paper is quite simple. It is a combination of Bayesian optimization (sampling several MDPs from the prior), using a Bayes filter to update the belief, and a policy gradient algorithm (TRPO) to estimate the gradient and update the policy parameter. The only challenges are 1) the design of the Bayes filter, in particular when the latent state is continuous, in which the idea used in the paper is very simple, discretization, and 2) dealing with potentially high dimensional state-belief pair, which was handled by the encoders. \n\n- The structure of the paper could be improved significantly. Four pages have been dedicated to the preliminaries and related work, and another four pages to the experiments. This leaves only less than two pages for the algorithm. While I think a comprehensive discussion of the experiments is quite helpful, I found the preliminaries and related work too long. I even think that the experiments could have been written better. There are parts that have been explained too much and parts that are not clear or left for the appendix. With a better structure, the algorithm could have been explained better. I personally would like to see more discussion on how the distribution over MDPs is updated. \n\n-  I did not find the experiments very convincing. In BAMDP problems (Chain and MuJoCos), the proposed algorithm performs similarly to the adaptive policy gradient method. We only see improvement in the POMDP tasks (Tiger, Lightdark), which I think the main reason is that the algorithms selected for comparison are not the right algorithms for POMDPs. For example, many different algorithms have been used to solve Tiger (or other discrete POMDPs) in the POMDP literature, and I do not see any of them in the paper. \n ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}