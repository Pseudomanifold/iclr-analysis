{"title": "Proposes enhancing reinforcement learning by also solving the same MDP but with the negative of the original rewards.", "review": "This paper proposes that, in addition to learning the normal action values, an RL agent should also learn the action values for an alternate \u201cinverse\u201d problem consisting of the same transitions as the original MDP and the negative of the original rewards. The intuitive argument is that the values for the inverse problem clearly identify what actions should not be taken. Results are presented on OpenAI Gym problems in which the new method performed better than conventional methods. \n\nThe paper is not yet ready for publication for many reasons. First, the idea is not presented clearly, and it is not clear why it ever could be sensible. The inverse problem has a different solution than the base problem. Its solution would appear to have an arbitrary relationship to the base problem\u2019s solution. The two optimal policies may choose different actions, as suggested in the text, but this is not necessarily true; in some states the two policies may choose the same action. I don\u2019t see how anything can be said in general, and no significant theoretical results are presented. (They do prove a form of convergence on the inverse problem, but this is not a new result; the inverse problem is just another problem and needs no new result.)\n\nThe new hybrid method is never fully explained (e.g., the reader has to guess at what Q^H is). But by combining the solutions to the base and inverse problems in some way, it seems inevitable that the final optimal policy would be changed. Suppose the function approximation is completely successful and the correct values are exactly found. Then those for the base problem would give the optimal policy. Any alteration of them by the correct action-value function for the inverse problem could only make them worse and could only cause them to produce worse (or the same) behavior. There is no room for improvement and so this technique could only make things worse asymptotically if the function approximation is completely successful. This is the only thing that I see that can be clearly said about the new technique, and of course it is not a good thing.\n\nThere are results presented, but they are not well done; they provide no significant evidence for any conclusion. The methods are not completely presented, and the results seem to be for a single run, in which case any relative ordering of the methods could be obtained.\n\nGenerally, the paper is unfortunately poorly written. The grammar is not good. The notation is unnecessarily complex and confusing. The citations are made in an unusual, poor way.", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}