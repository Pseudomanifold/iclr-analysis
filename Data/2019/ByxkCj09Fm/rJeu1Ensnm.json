{"title": "Missing key references", "review": "SUMMARY\nThe paper presents a method for classification which takes into account the semantic hierarchy of output labels, rather than treating them as independent categories. In a typical classification setup, the loss penalizes the KL-divergence between the model\u2019s predicted label distribution and a one-hot distribution placing all probability mass on the single ground-truth label for each example. The proposed method instead constructs a target distribution which places probability mass not only on leaf category nodes but also on their neighbors in a known semantic hierarchy of labels, then penalizes the KL-divergence between a model\u2019s predicted distribution and this target distribution. This model is used for classification on ImageNet-1k, and for zero-shot classification on ImageNet-21k where a model must predict superclasses seen during training for images of leaf categories not seen during training.\n\nPros:\n- Method is fairly straightforward\n- Modeling relationships between labels is an important problem\n\nCons:\n- Missing references to key prior work in this space\n- Minimal comparison to prior work\n- Confusing experimental setup\n- Paper is difficult to read\n\nMISSING REFERENCES\nThis paper is far from the first to consider the use of a semantic hierarchy to improve classification systems; see for example:\n\nDeng et al, \u201cHedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition\u201d, CVPR 2012\n\nDeng et al, \u201cLarge-scale object classification using label relation graphs\u201d, ECCV 2014 (Best Paper)\n\nJiang et al, \u201cExploiting feature and class relationships in video categorization with regularized deep neural networks\u201d, TPAMI 2017\n\nNone of these are cited in the submission. [Deng et al, 2014] is particularly relevant, as it considers not just \u201cis-a\u201d relationships as in this submission, but also mutual exclusion relationships between categories. Without citation, discussion, and comparison with some of these key pieces of prior work, the current submission is incomplete.\n\nCOMPARISON TO PRIOR WORK\nThe only direct comparison to prior work in the paper is the comparison to DeViSE on ILSVRC12 classification performance in Table 3. However since DeViSE was intended to be used for zero-shot learning and not traditional supervised classification, this comparison seems unfair.\n\nInstead the authors should compare their method against DeViSE and ConSE for zero-shot learning. Indeed, in Section 4.3 the authors construct a test set \u201cin a [sic] same manner defined in Frome et al\u201d but do not actually compare against this prior work.\n\nI suspect that the authors chose not to perform this comparison since unlike DeViSE and ConSE their method cannot predict category labels not seen during training; instead it is constrained to predicting a known supercategory when presented with an image of a novel leaf category. As such, the proposed method is not really \u201czero-shot\u201d in the sense of DeViSE and ConSE.\n\nEXPERIMENTAL SETUP\nFrom Section 3.1, \u201cwe adopt a subset of ImageNet the ILSVRC12 dataset which gather [sic] 1K classes [...]\u201d. The 1000 category labels in ILSVRC12 are mutually exclusive leaf nodes; when placed in the context of the WordNet hierarchy there are 820 internal nodes between these leaves and the WordNet root. As a result, for the method to make sense I assume that all models must be trained to output classification scores for all 1820 categories rather than the 1K leaf categories. This should be made more explicit in the paper, as it means that none of the performance metrics reported in the paper are comparable to other results on ILSVRC12 which only measure performance on the 1K leaf categories.\n\nThe experiments on zero-shot learning are also confusing. Rather than following the existing experimental protocol for evaluating zero-shot learning from [Frome et al, 2013] and [Norouzi et al, 2013] the authors evaluate zero-shot learning by plotting SG-hit vs SG-specificity; while these are reasonable metrics, they make it difficult to compare with prior work.\n\nPOOR WRITING\nThe paper is difficult to follow, with confusing notation and many spelling and grammatical errors.\n\nOVERALL\nOn the whole, the paper addresses an important problem and presents a reasonable method. However due to the omission of key references and incomplete comparison to prior work, the paper is not suitable for publication in its current form.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}