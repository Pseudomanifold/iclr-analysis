{"title": "Ok, but not good enough", "review": "This paper is more like a network pruning paper, where we first get an initial network structure by some method (ENAS in this case), prune unimportant connections and add some neurons back to remedy the performance degradation. The novelty lies on the third step, using evolutionary algorithm to insert the add-on neurons across layers. In this way, the paper got the state-of-the-art result on Cifar-10 with less parameters and less searching time. However, the paper has following problems.\n1. As a general method, the proposed neuron search not only can work with structures found by ENAS, but also other network structures, for example ResNet. What are the effects on other common network structures? It would be better to also try other network architectures and evaluate on several datasets or tasks to show its generality, instead of just 12-layer CNN found by ENAS and on CIFAR 10. Without further experiments, the authors can only claim that their method is effective (to some extent) when coupled with ENAS.\n2. The neuron adding procedure increases the accuracy from 96.20% to 96.52%, which is not significant.\n3. The authors did not give enough details for others to reproduce their work. For example, the subroutines \"Mutate\", \"GenerateLoc\" and \"GenerateDir\" are not described in details. It is also not clear how the number of segments and the number genes in each segment are determined, and how they affect the performance. \n4. The paper says \u201cExperiments show that the NHN outperforms the original ENAS architecture\u201d (in Conclusion). But from Section 3 \u201cLayer hierarchy search\u201d, it says the ENAS achieves test accuracy of 96.55% on Cifar-10 with Cutout, while the final test accuracy of NHN is 96.52%. Apparently, NHN does not outperform the unpruned ENAS structure.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}