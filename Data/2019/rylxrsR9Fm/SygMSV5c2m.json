{"title": "Official Review.", "review": "Summary:\nThis paper proposes a method to design architectures for neural networks. Their method has 3 steps.\n\n1. Search for an architecture using an existing automatic model designing algorithm. ENAS (Pham et al 2018) is used in this work.\n\n2. Prune some existing neurons in the model found in step (1), using a pruning algorithm. The algorithm (dynamic network pruning, DNP) comes from an Anonymous submission, so I don\u2019t know what it is. However, I suspect that other algorithms can also be used here.\n\n3. Add extra neurons and computations to the pruned network using evolutionary search.\n\nThe ultimate architecture resulting from the search indeed improves compared to the architecture found in step (1): having fewer parameters and achieving a smaller error rate on CIFAR-10.\n\nStrengths:\nThe NHN method is well-motivated. Pruning of networks is an important technique, but there does not seem to be methods to undo over-prunning. NHN can be applied there.\n\nThe empirical results of the paper also demonstrated that (ENAS + pruning + NHN) is better than both ENAS and (ENAS + pruning). I think this is a strong point.\n\nWeaknesses:\n1. The paper is not very well-written. Followings are some points that confused me:\n- Section 2.3: \u201cthe standard deviation thresholds are used to determine the optimal add-on directions\u201d. Which stddev is it? Are you running the existing structure through a batch of data and measure the stddev of each neuron?\n\n- Figure 3 is not very illustrative. My understanding of Figure 3 is as follows. Each foll. Please correct me if I\u2019m wrong, but if I am correct, then I strongly suggest that you explain each circle in Figure 3 corresponds to a channel in the input and output layer.\n\n- When you perform add-on search, how are the add-on neuron used? Are there outputs added to the existing output layers, or are they concatenated, followed by 1x1 convs etc.?\n\n- In Section 2.2, the dynamic network pruning (DNP) algorithm is from a paper in submission. In this case, I suspect that the authors should take the responsibility to briefly explain how the algorithm works. At least, some intuitions should be given.\n\n2. Experimental results of this paper are relatively weak.\n\nFirst, the accuracy achieved by NHN is almost always behind that of block-based search methods, as reported in Table 3 (Zoph et al 2018; Pham et al 2018; Liu et al 2018). It has already been established that block-based search methods are stronger, since they have a smaller search space. Perhaps NHN should be applied on a cell found by a search-based method, rather than on a whole convnet found by ENAS.\n\nSecond, I am not a fan of the Score metric in Table 1. \\sqrt{(e/2)^2 + (n/25)^2}, where e is the error rate and n is the number of parameters, is a strange metric. I wonder where do the numbers 2 and 25 come from, and had they been chosen differently (really, one can SGD-search for these numbers!), some other entries in Table 1 would have been bolded.\n\n3. It\u2019s unclear how much SmoothInit affects (i.e. improves) the performance of NHN. Can you comment on this?\n\n4. Missing citations: Net2Net [1] also expands existing network architectures, albeit with methods to ensure the stability of the \u201cadd-on layers\u201d, which can perfectly be applied to the search for add-on neurons in your method.\n\n5. Nit-picking:\n- Section 2.2: grouped convolutions where the number of groups is equal to the number of input (and output) channels is called \u201cdepthwise convolutions\u201d. I think this is a less confusing name.\n\nReferences.\n[1] Net2Net: Accelerating Learning via Knowledge Transfer. https://arxiv.org/pdf/1511.05641.pdf\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}