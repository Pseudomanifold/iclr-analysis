{"title": "Variational Auto-Encoder", "review": "\n\nThe authors made a method that consider latent binary representation instead of continuous representations in Seq2seq models, and showed that it could help regularization in multiple sequence prediction tasks. \n\nTo be effective, the authors considered a simple approximation of the average embedding that prevent them from using REINFORCE. \n\nThe intuition is that hard latent binary units might be better at representing vocabulary choices and leads to a better inductive bias of the model. However, no inspection of the model to understand in which sentences the averaging of binary variables makes a real difference in prediction.\n \nThere is no analysis of the condition under which this approximation is correct. What is the impact of this approximation? Are the results the same if reinforce is used?\n\nOverall, the article is clearly written and conclusions sufficiently relevant for this article to be published in ICLR.\n\nMinor comments:\n- the term Amortized Variational Inference is not the most intuitive and Variational Auto-Encoder seems to be the de-facto standard term for these techniques.\n- For the MT results, why the authors did not consider more common language pairs so that the results are readily comparable?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}