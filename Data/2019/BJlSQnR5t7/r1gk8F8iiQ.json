{"title": "review of Deepstrom networks", "review": "Paper 1358 considers the problem of reducing the cost of fully-connected layers in deep architectures. For this purpose, the deep fried architecture was recently introduced. It involves replacing a fully connected layer by a structured projection layer. Followed by a cos/sin non-linearity, this actually leads to an approximation of the RBF kernel. \n\nPaper 1358 aims at extending this work beyond the RBF kernel. For this purpose, paper 1358 uses the Nystr\u00f6m approximation. It involves computing an explicit embedding based on a subset of training samples {x_i, i=1\u2026m}. Given a sample x, the embedding is the multiplication of the sample kernel vector [k(x,x_1}, \u2026, k(x, x_m)] with a projection matrix. Two variants are proposed: \n1)\tthe projection matrix is the inverse of the m x m sample kernel matrix;\n2)\tthe projection matrix is learned through backpropagation.\n\nThe approach is validated on four public benchmarks.\n\nOn the positive side: the paper is rather clear and easy to follow.\n\nOn the negative side:\n-\tThe novelty of the proposed approach is modest. All in all, it feels very much like applying the Nystr\u00f6m approximation on top of convnet features. Indeed, to the best of my understanding, the gradients were not back-propagated through the convolutional layers. This point is actually unclear and should be clarified. \n-\tIn this sense, the term \u201cdeepstr\u00f6m\u201d is somewhat misleading. This seems to indicate that the proposed approach is deep. However, if one compounds the matrices W and lc into a single matrix, then the propose Nystr\u00f6m module has a single hidden layer.\n-\tThe claim is made that the deep-fried module is restricted to approximating the RBF kernel. This is incorrect. Other kernels such as the arc-cosine kernel can be approximated by replacing the sin/cos non-linearity by a reLu.\n-\tThe experiments are lacking in some respects.\no\tStraightforward non-deep baselines are missing. For instance, the authors could have applied the \u201cefficient additive kernels\u201d of Vedaldi and Zisserman (CVPR\u201910) on top of convnet features. \no\tIt is unclear why a single hidden layer was used for the dense architecture baseline. Is this because the results did not improve with more? Or because the proposed deepstr\u00f6m architecture has a single hidden layer?\no\tIn the result figures, it is unclear whether the # parameters on the x-axis is the total number of stored parameters, including the samples {x_i, i=1\u2026m}, or just the estimated parameters, i.e. W and lc. \no\tThere is no result at Imagenet scale, contrary to the original deep-fried convnet paper.\no\tAt the end of section 4.1, the authors spend a lot of time explaining that the purpose is not to compete with the state-of-the-art. Yet, in the third paragraph of section 4.2, they claim that their approximation reaches the state-of-the-art. So which is it?\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}