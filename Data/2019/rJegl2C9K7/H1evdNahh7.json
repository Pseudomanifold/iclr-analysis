{"title": "Important direction of research, unfortunately vague and lacks content", "review": "Reliably transferring knowledge between networks is a crucial task in many aspects of training. It helps reduce the size of networks without sacrificing performance, it helps the student network to achieve good results on expert tasks trained by the outputs of various specialized teacher models, or it can speed up training to name a few benefits... The work \"Feature Matters\" is interested in the idea of expending the knowledge transfer to the hidden layers of a given network. It's promise of being a method that is task and dataset independent is quite big and punchy. However, the paper falls short in many aspects that, unfortunately, renders it as a work in progress.\n\nTo start on a positive note, knowledge transfer at the level of hidden layers is an important area that should be explored deeper, and the experiments that are presented in the paper are strong indicating the paper is indeed in the right direction. And therefore I strongly encourage the authors to revise the presentation of their work and solidify their methodology further. \n\nA major flaw in the present paper is in its omission of several lines of research that are focusing on representations of hidden layers, see for instance http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf There is a deep literature on representations of hidden layers that is relevant when one is interested in transferring the knowledge of the hidden layers. Are they generic representation, do they depend on the task or dataset? All are relevant questions that fit into the framework of the present paper. In section 3, at Observation 1, the paper mentions that networks are separated into two parts: feature extraction and feature application. However, the feature extractor is almost all of the network in ResNets except for the last layer. What exactly is meant by this separation? How is it related to the hidden layer representations? All similar questions stay up in the air.\n\nA second major flaw is seen in the way the method is proposed: it is entirely mysterious how the method is supposed to be implemented from the point of view of the reader who is interested in re-implementing the proposed method. The figure is not descriptive enough, and the text is only bloated with vague and unclear descriptions. The reader is only provided with some queues here and there (e.g. the first paragraph of section 4.3). Clearly, it is not enough details to replicate the setup. \n\nOverall, the direction in which the paper is taking us is very interesting, and I can imagine a re-written version of the present paper in the previous years' ICLR Workshop format to be a very good candidate but for a paper that has no theory, that is very light on its experimental descriptions and details, that is very light on relevant research review, and that is very loaded with vague and imprecise descriptions, I can not recommend the submission for ICLR as it stands. \n\nTo improve the paper further, I would recommend:\n- Shorten and sharpen the exposition as opposed to the current casual style\n- Include a thorough literature review with more precise (but short) explanation of their contributions\n- Include a sketch of the proposed algorithm (make sure a reader can replicate the core algorithm)\n- An attempt at formalization of the method would be also helpful for the theory-minded readers\n- Systematic reports of the experiments, resources used, time and memory analysis etc would be useful additions (since it is also part of the claim)\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}