{"title": "Interesting ideas, but clarity must be improved", "review": "Authors consider a problem of off-policy reinforcement learning in a setting explicitly constrained to a fixed batch of transitions. \nThe argument is that popular RL methods underperform significantly in this setting because they fail to account for extrapolation error caused by inevitably sparse sampling of the possible action-state space.\nTo address this problem, authors introduce the notion of batch-constrained RL which studies policies and associated value functions only on the state-space covered by the available training data.\nFor practical applications a deep RL method is introduced which enables generalisation to the unseen states and actions by the means of function approximation.\n\nI find the problem studied in the paper very important. It is indeed strongly connected to the idea of imitation learning which has been studied previously, but I like the explicit point from which authors see the problem.\nThe experimental results seem quite appealing to justify use of the proposed approach.\n\nHowever, on the clarity side the paper should be improved before publication.\n\nThe interplay between action generating VAE G_w(s) and \\pi is unclear to me.\nFirst, what does it mean that G(s) is trained to minimise the distance D_A?\n\nIf G(s) is a VAE, then it is trained to minimise the corresponding variational lower bound, how is minimisation of the distance over actions is incorporated here? And what exactly is this distance?\nSimilarly, what does \u201cD_S will be defined by the implicit distance induced by the function approximation\u201d exactly mean?\n\nOther comments / questions:\n\nPage 6: \u201cTheorem 1 implies with access to only\na subset of state-action pairs in the MDP, the value function\u2026 This suggests batch-constrained policies\nare a necessary tool for combating extrapolation bias.\u201d\nThis might be true, but it does not follow from the Theorem 1 as it only applies to the batch Bellman operator and not the standard one used in most methods.\n\nCorollary 1 and 2: What is Q^* here?\n\nPage 7, first sentence: should there be if A_s, e != \\emptyset? \n\nEpsion-Batch-constrained policy iteration: would the beam search actually maximize Q function? This needs to be proven or at least discussed.\n\nI don\u2019t see how is epsilon used in the iteration scheme.  This needs to be clarified.\n\nEquation 11: the subscript of the max operator looks weird, should there be just a_i?\n\nFigure 4: where is \u201cTrue value\u201d curve on the plots?\n\nThe notation \\pi(s, a; \\Phi) used throughout the paper is confusing and can be interpreted as a joint distribution over states and actions.\n\nAs I said, currently the paper does not appear to be easy to follow to me and even if it does contain important ideas, I believe they must be communicated in a clearer way.\nI am eager to revise my evaluation if authors make substantial effort to improve the paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}