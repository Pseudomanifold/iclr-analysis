{"title": "Review", "review": "In this paper, the authors consider CNN models from the lens of kernel methods. They build upon past work that showed that such models can be seen to lie in appropriate RKHS, and derive upper and lower bounds for the kernel norm. These bounds can be used as regularizers that help train more robust neural networks, especially in the context of euclidean perturbations of the inputs, and training GANs. They show that the bounds can also be used to recover existing special cases such as spectral norm penalizations and gradient regularization. They derive generalization bounds from the point of view of adversarial learning, and report experiments to buttress their claims.\n\nOverall, the paper is a little confusing. A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper. It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models). It might be nice to carefully delineate the authors' work from the former, and present their contributions. \n\nPage 4: Other Connections with Lower bounds: The first line \" \"we may also consider ... \". This line is vague. How will you ensure the amount of deformation is such that the set \\bar{U} is contained in U ?\n\nPage 4 last paragraph: \"One advantage ... complex architectures in practice\" : True, but the tightness of the bounds *do* depend on \"f\" (specifically the RKHS norm). It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?\n\neqn (8): use something else to denote the function 'U'. You used 'U' before to denote the set. \n\neqn (12): does \\tilde{O} hide polylog factors? please clarify. \n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}