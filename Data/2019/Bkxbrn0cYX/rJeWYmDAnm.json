{"title": "Interesting work addressing an interesting class of problems, novel regularizer and good experiments, but writing and paper organization need work", "review": "REVISION AFTER REBUTTAL\nWhile the revision does not address all of my concerns about clarity, it is much better. I still think that the introduction is overly long and the subsequent sections repeat information; if this were shortened there could be room for some of the figures that are currently in appendix. I appreciate the new figures; I think that it would be great if especially figure 10 were included in the main paper. \nI agree with the other two reviewers that the work is somewhat incremental, but the differences are well explained, the experimental results are interesting (particularly the differences of parameter vs representation-based sparsity, and the plots in appendix showing neuron importance over tasks), and the progression from SNI to SLNID is well-presented.  I think overall that this paper is a good contribution and I recommend acceptance. I have updated my review to a 7. \n===============\n\"Activations\" \"Representation\" and \"Outputs\" are used somewhat interchangably throughout the work; for anyone not familiar it might be worth mentioning something about this in the intro.\n \nProblem setting is similar to open set learning (classification); could be worth mentioning algorithms for this in the related work which attempt to set aside capacity for later tasks.\n\nResults are presented and discussed in the introduction, and overall the intro is a bit long, resulting in parts later being repetitive.\n\nWorth discussing sparsity vs. distributed representations in the intro, and how/where we want sparsity while still having a distributed representation.\n\nShould be made clear that this is inspired by one kind of inhibition, and there are many others (i.e. inhibition in the brain is not always about penalizing neurons which are active at the same time, as far as I know)\n\nChanges in verb tense throughout the paper make it hard to follow sometimes. Be consistent about explaining equations before or after presenting them, and make sure all terms in the equation are defined (e.g. SNI with a hat is used before definition). Improper or useless \"However\" or \"On the other hand\" to start a lot of sentences.\n\nFigure captions could use a lot more experimental insight and explanation - e.g. what am I supposed to take away from Figure 10 (in appendix B4), other than that the importance seems pretty sparse? It looks to me like there is a lot of overlap in which neurons are important or which tasks, which seems like the opposite of what the regularizer was trying to achieve. This is a somewhat important point to me; I think this interesting and I'm glad you show it, but it seems to contradict the aim of the regularizer.\n\nHow does multi-task joint training differ from \"normal\" classification? The accuracies especially for CIFAR seem very low.\n\nQuality: 7/10 interesting and thoughtful proposed regularizer and experiments; I would be happy to increase this rating if the insights from experiments, especially in the appendix, are a bit better explained\nClarity:  6/10 things are mostly clearly explained although frequently repetitive, making them seem more confusing than they are. If the paper is reorganized and the writing cleaned up I would be happy to increase my rating because I think the work is good. \nOriginality: 8/10 to my knowledge the proposed regularizer is novel, and I think think identifying the approach of \"selfless\" sequential learning is valuable (although I don't like the name)\nSignificance: 7/10 I am biased because I'm interested in LLL, but I think these problems should receive more attention.\n\nPros:\n - proposed regularizer is well-explained and seems to work well, ablation study is helpful\n\nCons:\n - the intro section is almost completely repetitive of section 3 and could be significantly shortened, and make more room for some of the experimental results to be moved from the appendix to main text\n - some wording choices and wordiness make some sentences unclear, and overall the organization and writing could use some work\n\nSpecific comments / nits: (in reading order)\n1. I think the name \"selfless sequential learning\" is a bit misleading and sounds like something to do with multiagent cooperative RL; I think \"forethinking\" or something like that that is an actual word would be better, but I can't think of a good word... maybe frugal? \n2.  Mention continual/lifelong learning in the abstract\n3. \"penalize changes\" maybe \"reduce changes\" would be better?\n4. \"in analogy to parameter importance\" cite and explain parameter importance\n5. \"advocate to focus on selfless SL\" focus what? For everyone doing continual learning to focus on methods which achieve that through leaving capacity for later tasks? This seems like one potentially good approach, but I can imagine other good ones (e.g. having a task model)\n6. LLL for lifelong learning is defined near the end of the intro, should be at the beginning when first mentioned\n7. \"lies at the heart of lifelong learning\" I would say it is an \"approach to lifelong learning\"\n8. \"fixed model capacity\" worth being specific that you mean (I assume) fixed architecture and number of parameters\n9. \"those parameters by new tasks\" cite this at the end of the sentence, otherwise it is unclear what explanation goes with which citation\n10.  \"hard attention masks, and stored in an embedding\" unclear what is stored in the embedding. It would be more helpful to explain how this method relates to yours rather than just describing what they do.\n11. I find the hat notation unclear; I think it would be better just to have acronyms for each setting and write out the acronyms in the caption\n12.\"richer representation is needed and few active neurons can be tolerated\" should this be \"more active neurons\"?\n13. Comparison with state of the art section is repetitive of the results sections", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}