{"title": "Thorough continual learning work but limited to task-based case", "review": "[REVISION]\nThe work is thorough and some of my minor concerns have been addressed, so I am increasing my score to 6. I cannot go beyond because of the incremental nature of the work, and the very limited applicability of the used continual learning setup from this paper.\n\n[OLD REVIEW]\nThe paper proposes a novel, regularization based, approach to the sequential learning problem using a fixed size model. The main idea is to add extra terms to the loss encouraging representation sparsity and combating catastrophic forgetting. The approach fairs well compared to other regularization based approaches on MNIST and CIFAR-100 sequential learning variants.\n\nPros:\nThorough experiments, competitive baselines and informative ablation study.\nGood performance on par or superior to baselines.\nClear paper, well written.\n\nCons:\nThe approach, while competitive in performance, does not seem to fix any significant issues with baseline methods. For example, task boundaries are still used, which limits applicability; in many scenarios which do have a continual learning problem there are no clear task boundaries, such as data distribution drift in both supervised and reinforcement learning.\nSince models used in the work are very different from SOTA models on those particular tasks, it is hard to determine from the paper how the proposed method influences these models. In particular, it is not clear whether these changes to the loss would still allow top performance on regular classification tasks, e.g. CIFAR-10 or MNIST even without sequential learning, or in multitask learning settings. \n\nSummary:\nAlthough the work is substantial and experiments are thorough, I have reservations about extrapolating from the results to settings which do have a continual learning problem. Although I am convinced results are slightly superior to baselines, and I appreciate the lengthy amount of work which went into proving that, the paper does not go sufficiently beyond previous work.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}