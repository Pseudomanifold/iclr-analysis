{"title": "lacking convincing comparison", "review": "This paper studies an interesting and meaningful topic that what is the potential of curriculum learning (CL) in training dnn.  The authors decompose CL into two main parts: scoring function and pacing function. Towards both parts, several candidate functions are proposed and verified.  The paper is presented quite clear and gives contribution to better understand CL in the literature of DNN.\n\nHowever, I have several concerns towards the status of this paper.\n\nFirst, quite a few important related works are missing by the authors. Just name a few, [1] studies designing data curriculum by predictive uncertainty. [2,3] studies how to derive data driven curriculum along NN training. In particular, the objective of [2] is exactly \u201clearning the right examples at the right time\u201d. All these three papers focus on, or at least talk about, neural network training. Unfortunately, none of them are compared with, or even referenced. \n\nSecond, although comprehensive study towards different curriculum strategy are given, I found it largely unconvincing. I tried hard to discover a *detailed accuracy number on a benchmark dataset with unchanged setting* but found only case 4. By \u2018unchanged\u2019 I mean it is not a subpart of the whole dataset, or using a rarely seen nn architecture.  If it is such `changed\u2019 settings, the results are largely unconvincing since we do not know what the exact baseline is. For the only \u2018unchanged\u2019 setting 4 including VGG on CIFAR100, unfortunately the results seem not good (Fig 4a). I understand that some previous work such as the cited [Weinshall et.all 2018] also used the same setting: however it does not mean such settings give *clear and convincing* results of whether CL plays significant role in training DNN. Furthermore, I also expect the results of comparing in terms of wall clock time (including all your bootstrapping training time) but not merely batch numbers. \n\n[1] Chang, Haw-Shiuan, Erik Learned-Miller, and Andrew McCallum. \"Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples.\" NIPS. 2017.\n\n[2]  Fan, Y., Tian, F., Qin, T., Li, X. Y., & Liu, T. Y. Learning to Teach. ICLR 2018\n\n[3] Jiang, Lu, et al. \"MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels.\" ICML. 2018.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}