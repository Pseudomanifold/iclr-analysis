{"title": "Review", "review": "This paper described a spatial value decomposition for RL. The idea is: instead of temporally sum reward for Q function, multiply future states will be visited at the same time. Each future state is weighted by visiting frequency. The new Q function will be weighted average of future states reward.  In order to make the problem tractable, a function approximation method by using Gamma function of state, action and future state is introduced for visiting frequency. In appendix, the authors show equivalence between temporal and spatial formulation of Q function.\n\nA few experiments with different environments are performed, showing spatial value decomposition is able to converge faster.\n\nComment:\n\nTo my knowledge, it is a novel decomposition. However the major concern is: this method is introducing extra bias, because visiting frequency may or may not have strong correlation to final reward. The experiments also indicate that, with larger $m$, the bias effect significantly reduces return. A more detailed discussion of the bias effect will be helpful. Detailed explanation on bias effect will be helpful to understand whether this method will be helpful for sparse reward as well.\n\nI don't have much hands-on experiences on RL experiments. As one of the anonymous pointed, it seems less significant. \n\nIn summary, given the novelty of the paper, I tend to accept it.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}