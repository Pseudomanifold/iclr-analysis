{"title": "Not enough novelty or significance, unsatisfactory experimental evaluations.", "review": "Summary: Authors proposed a model for input method for mobile or desktop devices. The goal is to convert the input sequence (from one language to another) or predict the next word. Their model is based on an LSTM with modified softmax activation function that is adjustable for large vocabulary sizes. They showed experimental results on Japanese BCCWJ data set.\n\nClarity: Paper is well-written and well-organized. Notions and methods are clearly expressed. \n\nOriginality: This paper builds on an LSTM model without enough work or idea to show novelty. \n\nSignificance: It is below average. Using LSTM is a well-known method for these types of tasks in the literature. Incremental selective softmax is potentially a good approach, however, this work lacks showing significant improvement. The experiments are limited and are done only on one data set.\n\nMore detailed comments:\n\n- My concerns about this work are both on modeling aspects and experiments. Authors mainly focus on highlighting the benefits comparing to n-gram models, and briefly discuss the ongoing developments in neural based models. For example sequential modelings using RNN's have shown promising results in capturing long-term dependencies [1]. Unfortunately authors did not include any discussion on how their approach would compare to that framework nor did they present any experimental comparisons to them.\n\n- Although mentioned briefly in the introduction and related work sections, no analytical or experimental comparisons are made to machine translation approaches when their work is closely related to it. I strongly suggest that authors compare their experimental results to some of benchmarks in neural based machine translation discussed in the related works.\n\n- In the incremental selection softmax, they use \"match\" to return all lexicon items matching the partial sequence. How is this done and what are the effects of it on the computational time of the algorithm? Also, It is not clear how authors correct old probabilities in IS softmax step. As mentioned, they add logits of missing vocabulary to the denominators, how do they keep the properties of softmax so that it sums up to 1? And later in the discussion authors mentioned that in practice they compute union of all missing vocabularies, it is not clear how this is done since the advantage of using IS softmax is expressed to be incremental increasing. \n\n[1] A.B. Dieng, C. Wang, J. Gao and J. Paisley. TopicRNN: A recurrent neural network with long-range semantic dependency, International Conference on Learning Representations (ICLR), 2017.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}