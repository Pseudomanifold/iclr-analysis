{"title": "Batching for back propagation", "review": "Batching of similar and independent operations in a neural network computation graph is a common way to improve efficiency through computational parallelism. Optimization is often applied to the computation graph by grouping independent operations into batches that can be computed in parallel. \nExisting techniques typically optimizes the feed forward computation and the backward computation follows the same grouping as the feed forward computation, which may not be optimal. In particular, the authors argue that a separate batching strategy should be applied to the back propagation computation to further improve the efficiency and showed that a recurrent network can benefit from such an optimization. The proposed solution is an automatic batching strategy that work for dynamic computation graphs.\n\nPros:\n\n- The results (for both CPU and GPU) show that the proposed method improves on top of two existing batching strategies (by depth and by agenda) across three different tasks. \n\nCons:\n\n- The feed forward and backward (gradient) computation can be viewed as a single computation graph. As such, would applying optimization to this graph achieve the same thing? Some clarifications/discussions will be helpful.\n\n- It is unclear why the proposed method is better for dynamic computation graph.  The benchmark results on CPU show that the proposed method is worse than the baseline for Tree-LSTM.  Will be useful to also do a similar profiling for the cases where the proposed method did not help.\n\n   \n  ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}