{"title": "Straightforward method, need more analysis.", "review": "This paper proposed a just-in-time optimization method of neural network calculation on dynamic computation graphs. The method focused on batching summation of gradients on the backward calculation which was performed independently in conventional toolkits, and experiments on 3 LSTM tasks showed that in several settings the proposed method improved the speed of backward computation.\n\nThe proposed method is straightforward and reasonable in terms of improving the speed of the backward computation. Authors discussed the proposed method on only the neural network toolkits with a dynamic computation strategy, but this kind of optimization can be applied to any existing toolkits even which has a non-dynamic strategy. This point looks a kind of misleading of the discussion on the paper.\n\nThe paper provided a detailed analysis of time consumption on only a success-case (Table 4). Unfortunately, Table 2 and 3 showed that the proposed method does not have a global effectiveness and suggest a necessity for a further discussion about when to use the proposed method. Since this discussion can surely be conducted by comparing analyses of success and failure-cases, authors should provide analyses of all experiments.\n\nA conceivable weakness of the method may be the increase of memory consumption. If the toolkit plan to perform batch operations for summations of gradients, it needs to store all available gradients about each use of the corresponding variables. If the variable has a large shape and is used very frequently (e.g., variables in the softmax layer), the amount of total memory consumed by its gradient tends to be a serious problem. The non-batching strategy can mitigate this problem by discarding gradient information as soon as it is propagated back to all preceding nodes. The paper does not provide any information about memory consumption but it is important to discuss this kind of perspective.\n\nOthers:\nIn Table 2 and 3, please provide the ratio of speeds which are more reasonable to judge the real improvement rather than the one-zero decision (showed as up/down arrows).\nIn Table 4, why the time of the forward propagation slightly increased?\nYou should write a full list of authors of the DyNet paper that the official README provided:\nhttps://github.com/clab/dynet/blob/master/README.md", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}