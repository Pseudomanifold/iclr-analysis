{"title": "Nice idea, but a limited study presented with lack of clarity", "review": "# Summary\n\nThis work describes a shortcoming in existing dynamic batching strategies, namely that they operate only on the forward pass while some operations can be batched only in the backward pass. For example, the gradient of the transition matrix in a RNN consists of the sum of partial derivatives over each time step; the terms of this sum and the summation can be batched into a single matrix multiplication. The authors implement this batching strategy in DyNet and show empirically that it can lead to decent (0-25%) speedups.\n\n# Quality\n\nThe proposed technique comes with a trade-off which is not discussed in the paper: Delaying computations until several can be batched together can increase peak memory usage. In particular, the memory requirements of a RNN would increase from O(T) to O(2T) since each forward and backward state must now be stored. (In fact, the authors use a separately allocated contiguous block of memory that they copy the states and gradients into, which would bring this to O(3T) or O(4T) memory complexity.)\n\nA second observation that should have been made is that the potential for speedups depends on the batch size, hidden state size, and number of time steps (or tree depth). Given a small batch size and large hidden state, the batching method effectively replaces a series of outer products with a single matrix multiplication. One would expect good speedups in this scenario. On the other hand, for a large batch size with a small hidden state, the dynamic batching strategy effectively replaces a series of inner products with a single larger inner product, which would be far less beneficial. The experiments in this work use relatively small batch sizes (64), which gives little insight about whether the proposed method would lead to speedups in a wide range of models (for example, batches of 512 are common in some NLP applications).\n\nSome smaller comments:\n\n* Multi-threading on a multicore architecture does not necessarily imply that operations are performed sequentially.\n* Input sequences in NLP are not always sentences given as sequences of words.\n* The argument that padding always leads to unnecessary computation is overly simplistic; the added control flow and branching required to perform irregular computation can often make it slower than doing regular computation plus masking (additionally, sparse kernels are often memory bandwidth bound, leading to different performance properties).\n* The authors say that operations of the same \"type\" can be batched together, but don't specify what \"type\" means. I assume the type is defined by both the operation as well as the shapes of its inputs and outputs?\n* No distinction is made between different ways of batching and their performance characteristics. Two matrix-vector multiplications gemv(X, y1) and gemv(X, y2) can be efficiently batched as gemm(X, [y1 y2]') which reduces the number of times X needs to be loaded into working memory. This is not the case when batching distinct inputs such as gemv(X1, y1) and gemv(X2, y2). On the other hand, gemv(X1, y1) + gemv(X2, y2) can be efficiently batched as gemv([X1 X2], [y1', y2']'), reducing the number of memory accesses in the output buffer.\n* Why perform 3 runs and report the fastest speed? Why not report the range, or better yet, perform more runs and report confidence intervals.\n\n# Clarity\n\nThe writing in this paper needs significant improvement. In terms of structure, the introduction (section 1) and background (section 2) are very repetitive. The third, fourth, fifth and sixth paragraph of the introduction are effectively repeated in full in sections 2.1, 2.2, 2.3 and 3.1 respectively. On the other hand, the inclusion of table 1 at the beginning puts the reader on the wrong foot thinking that this paper will consider NMT models, whereas the paper only deals with POS tagging and sentiment analysis.\n\nThe text contains grammatical errors (\"days even weeks\", \"The parallel computing helps\"), tautological definitions (\"batching [...] means organizing the same operations of computation graphs into batches\", \"padding, which is to pad the input sequences\"), unclear use of language (\"cooperating with the existing strategies\"), and typographical mistakes (multiple citations are separately parenthesized). Overall, the lack of clarity inhibits the understanding of the paper.\n\n# Originality and significance\n\nThe central contribution of this paper is relatively straightforward in retrospect, but can certainly be beneficial for the training of some particular models. I am no expert in the literature, but the authors' claim that they are the first ones to consider this technique seems justified. The paper has no reference to code, so it is hard to judge how easy it would be for practitioners to use the suggested technique.\n\n# Summary\n\nPros:\n\n  * Useful dynamic batching trick that can lead to speedups\n  * Empirical evaluation compares to two existing techniques and breaks down individual components of runtime\n\nCons:\n\n  * No critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage\n  * Some questionable statements and assumptions\n  * Lack of formalization and clear definitions\n  * Paper reads long-drawn-out, subpar writing hurts readability", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}