{"title": "Potential overfitting criteria remain vague and were not properly validated", "review": "Overview:\nThe authors aim at finding and investigating criteria that allow to determine whether a deep (convolutional) model overfits the training data without using a hold-out data set.  \nInstead of using a hold-out set they propose to randomly flip the labels of certain amounts of training data and inspect the corresponding 'accuracy vs. randomization\u2018 curves. They propose three potential criteria based on the curves for determining when a model overfits and use those to determine the smallest l1-regularization parameter value that does not overfit. \nI have several issues with this work. Foremost, the presented criteria are actually not real criteria (expect maybe C1) but rather general guidelines to visually inspect 'accuracy over randomization\u2018 curves. The criteria remain very vague and seem be to applicable mainly to the evaluated data set (e.g. what defines a \u2019steep decrease\u2019?). Because of that, the experimental evaluation remains vague as well, as the criteria are tested on one data set by visual inspection. Additionally, only one type of regularization was assumed, namely l1-regularization, though other types are arguably more common in the deep (convolutional) learning literature.  \nOverall, I think this paper is not fit for publication, because the contributions of the paper seem very vague and are neither thoroughly defined nor tested.\n\n\nDetailed remarks:\n\nGeneral:\nA proper definition or at least a somewhat better notion of overfitting would have benefitted the paper. In the current version, you seem to define overfitting on-the-fly while defining your criteria. \n\nYou mention complexity of data and model several times in the paper but never define what you mean by that.\n\n\nDetailed:\nPage 3, last paragraph: Why did you not use bias terms in your model?\n\nPage 4, Assumption. \n- What do you mean by the data being independent? Independent and identically distributed?  \n- \"As in that case correlation in the data can be destroyed by the introduction of randomness making the data easier to learn.\u201c What do you mean by \"easier to learn\"? Better generalization? Better training error? \n- I don\u2019t understand the assumptions. You state that the regularization parameter should decrease complexity of the model. Is that an assumption? And how do you use that later?\n- What does \"similar scale\u201c mean? \n\nPage 4, Monotony. \n- You state two assumptions or claims, 'the accuracy curve is strictly monotonically decreasing for increasing randomness\u2018 and 'we also expect that accuracy drops if the regularization of the model is increased\u2019, and then state that 'This shows that the accuracy is strictly monotonically decreasing as a function of randomness and regularization.\u2018 Although you didn\u2019t show anything but only state assumptions or claims (which may be reasonable but are not backed up here). \nI actually don\u2019t understand the purpose of this paragraph.\n\n- Section 3.3 is confusing to me. What you actually do here is you present 3 different general criteria that could potentially detect overfitting on label-randomized  training sets. But you state it as if those measures are actually correct, which you didn\u2019t show yet.\n\nMy main concern here, besides the motivations that I did not fully understand (s.b.), is the lack of measurable criteria. While for criterion 1 you define overfitting as 'above the diagonal line\u2018 and underfitting as \u201abelow the line\u2018, which is at least measurable depending on sample density of the randomization, such criteria are missing for C2 and C3.       Instead, you present vague of \u2019sharp drops\u2019 and two modes but do not present rigorous definitions. You present a number for C2 in Section 5, but that is only applicable to the present data set (i.e. assuming that training accuracy is 1). \n\nCriterion 2 (b) is not clear.  \n- I neither understand \"As the accuracy curve is also monotone decreasing with increasing regularization we will also detect the convexity by a steep drop in accuracy as depicted by the marked point in the Figure 1(b)\" \nnor do I understand \"accuracy over regularization curve (plotted in log-log space) is constant\"?\nDoes that mean that you assume that whenever the training accuracy drops lower than that of the model without regularization, it starts to underfit?\n\nDue to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter \\lambda=0.00011 on the cifar data set.  In my view, this evaluation of the (vague) criteria is not fit for showing their possible merit.\n\n\n\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}