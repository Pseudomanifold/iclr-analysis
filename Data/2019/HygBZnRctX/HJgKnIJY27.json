{"title": "Interesting idea, sufficient empirical evidence, with certain questionable tricks", "review": "This paper proposes Leap, a meta-learning procedure that finds better initialization for new tasks. Leap is based on past training/optimization trajectories and updates the initialization to minimize the total trajectory lengths. Experiments show that Leap outperforms popular alternatives like MAML and Reptile.\n\nPros\n- Novel idea\n- Relatively well-written\n- Sufficient experiment evidence\n\nCons\n- There exist several gaps between the theory and the algorithm\n\nI have several concerns.\n1. The idea is clearly delivered, but there are several practical treatments that are questionable. The first special treatment is that on page 5, when the objective is increased instead of decreased, the sign of the f part is flipped, which is not theoretically sound. It is basically saying that when we move from psi^i to psi^{i+1} with increased objective, we lie to the meta-learner that it is decreasing. The optimization trajectory is what it is. It would be beneficial to see the effect of removing this trick, at least in the experiments. Second, replacing the Jacobian with the identity matrix is also questionable. Suppose we use a very small but constant learning rate alpha for a convex problem. Then J^i=(I-G)^i goes to the zero matrix as i increases (G is small positive). However, instead, the paper uses J^i=I for all i. This means that the contributions for all i are the same, which is unsubstantiated.\n\n2. The proof of Thm1 in Appendix A is not complete. For example, \"By assumption, beta is sufficiently small to satisfy F\", which I do not understand the inequality. Is there a missing i superscript? Isn't this the exact inequality we are trying to prove for i=0? As another example, \"if the right-most term is positive in expectation, we are done\", how so? BTW, the right-most term is a vector so there must be something missing. It would be more understandable if the proof includes a high-level proof roadmap, and frequently reminds the reader where we are in the overall proof now.\n\n3. The set \\Theta is not very well-defined, and sometimes misleading. Above Eq.(6), \\Theta is mathematically defined as the intersection of points whose final solutions are within a tolerance of the *global* optimum, which is in fact unknown. As a result, finding a good initialization in \\Theta for all the tasks as in Eq.(5) is not well-defined.\n\n4. About the experiments. What is the \"Finetuning\" in Table 1? Presumably it is multi-headed but it should be made explicit. What is the standard deviation for Fig.4? The claim that \"Leap learns faster than a random initialization\" for Breakout is not convincing at all.\n\nMinors\n- In Eq.(4), f is a scalar so abs should suffice. This also applies to subsequent formulations.\n- \\mu is introduced above Eq.(8) but never used in the gradient formula.\n- On p6, there is a missing norm notation when introducing the Reptile algorithm.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}