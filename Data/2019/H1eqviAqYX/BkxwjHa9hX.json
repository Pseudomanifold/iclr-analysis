{"title": "main contribution: improving neural response by de-emphasizing universal responses by modifying loss/training", "review": "The paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase. The authors show that the approach yields better results in the dataset considered using various measures and human evaluation.\n\nImprovement Points\n- the explanation for low ROGUE measure due to the method favoring non-repetitive words sounds like it can be supported using numerical statistics, than hand-waiving argument\n- for the timing, how much time was taken to tune the additional parameters (how the # of negative responses sampled for each positive response was chosen as four via uniform sampling)\n- some description about\na) how many users are there, what type of conversation/active users/topics etc.\nb) what time frame was used during data collection (this may have implications for lemma asserting zipf)\n- it would be interesting to know for the trivial questions if the performance was impacted by the deemphasizing (one that do result in universal replies)", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}