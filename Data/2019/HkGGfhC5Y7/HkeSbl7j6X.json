{"title": "interesting parts, but needs more rigour", "review": "This paper discusses VQ-VAE for learning discrete latent variables, and its application to NMT with a non-autoregressive decoder to reduce latency (obtained by producing a number of latent variables that is much smaller than the number of target words, and then producing all target words in parallel conditioned on the latent variables and the source text). The authors show the connection between the existing EMA technique for learning the discrete latent states and hard EM, and introduce a Monte-Carlo EM algorithm as a new learning technique. They show strong empirical results on EN-DE NMT with a latent Transformer (Kaiser et al. (2018)).\n\nThe paper is clearly written (excepting the overloaded appendix), and the individual parts of the paper are interesting, including the link between VQ-VAE training and hard EM, the Monte-Carlo EM, and strong empirical results. I'm less convinced that the paper as a whole delivers on what it promises/claims.\n\nThe first contribution of the paper is that it shows a simple VQ-VAE to work well on the EN-DE NMT task, in contrast to the results by Kaiser et al. (2018). The paper attributes this to tuning of the code-book, but the results (table 3) seem to contradict this, with a code-book size of 2^16 even slightly better than the 2^12 that is used subsequently. The reason for the performance difference to Kaiser et al. (2018) remains opaque. While interesting, the empirical effectiveness of Monte-Carlo EM is a bit disappointing, achieving +0.3 BLEU over the best configuration for EN-DE (after extensive hyperparameter tuning, seen in table 4), and -0.1 BLEU on EN-FR. Monte-Carlo EM also seems very sensitive to hyperparameters, namely the sample size (tables 4,5), contradicting the later claim that EM is robust to hyperparameters. The last claimed contribution (using denoising techniques) is hidden in the appendix, an application of an existing technique, and not compared to knowledge distillation (another existing technique).\n\nI'd like to see some of the results in the paper published eventually. However, the claims need to better match the empirical evidence, and for a paper that has \"better understanding\" in the title, I'd like to gain a better understanding of the differences to Kaiser et al. (2018) that make VQ-VAE fail for them, but not in the present case.\n\n+ clearly written paper\n+ interesting, novel EM algorithm for VQ-VAE\n+ strong empirical results on non-autoregressive NMT\n\n- the strong performance of the VQ-VAE baseline remains unexplained, and the claimed explanation contradicts empirical results.\n- the new EM algorithm gives relatively small improvements, with hyperparameters that were likely selected based on test set scores .\n- most of the empirical gain is attributable to knowledge distillation, which is not a novel contribution", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}