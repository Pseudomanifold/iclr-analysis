{"title": "New platfrom for research", "review": "Summary:\nThis paper presents a research platform with a simulated human (a.k.a bot) in the loop for learning to execute language instructions in which language has compositional structures. The language introduced in this paper can be used to instruct an agent to go to objects, pick up objects, open doors, and put objects next to other objects. MiniGrid is used to build the environments used for this platform. In addition to introducing the platform, they evaluate the difficulty of each level by training an imitation learning baseline using one million demonstration episodes for each level and report results. Moreover, the reported results contain data efficiencies for imitation learning and reinforcement learning based approaches to solving BabyAI levels. \n\nA platform like this can be very useful to expedite research in language learning, machine learning, etc. In my view, work like this should be highly encouraged by this conference and alike.  \n\nComments:\n1.  There are following papers should be cited as they are very related to this paper:\n    a) Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\n        https://arxiv.org/abs/1711.07280 \n    b) AI2-THOR: An Interactive 3D Environment for Visual AI\n         https://arxiv.org/abs/1712.05474 \n2. Paper is well-written and easy to follow. The only part which needs an improvement is section 3.4 as the text is a bit confusing.\n3. Heuristic expert, simulated human, human, and bot are exchangeably used in the text. It is better to pick one of these to avoid any confusion in the text. In general, it is not clear to why 'a human in the loop' is chosen, isn't just a program/bot that has is engineered using knowledge of the tasks?\n4. In Table 5, in GoToObjMaze row, data efficiency for \"With Pretraining\" is greater than \"Without Pretraining\", is this a typo? if not, why this is the case?\n5. One useful baseline which can be added to this paper is task-oriented language grounding. This task will be a better measure than current baselines, especially for RL case. Authors can check out the following paper:\nGated-Attention Architectures for Task-Oriented Language Grounding\nhttps://arxiv.org/abs/1706.07230\nThe code is available for this paper. \n\nQuestion:\nWhen this platform will be available for public? ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}