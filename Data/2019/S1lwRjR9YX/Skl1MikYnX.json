{"title": "Review", "review": "This paper studies the algorithmic stability of SGD with momentum and provides an upper-bound on true risk through convergence analysis.\nThis bound clarifies dependencies of convergence speed on the size of dataset and the momentum parameter.\n\nThe presentation is easy to follow and technically sounds good.\nSGD with momentum is heavily used for learning linear models and deep neural networks, hence to analyze its convergence behavior is quite important.\nThis paper achieves this goal well by extending a previous result on vanilla SGD in a straightforward manner, although it is not technically difficult.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}