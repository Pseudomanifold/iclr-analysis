{"title": "Mostly trivial claims", "review": "The paper claims that multi-hop reasoning (that is required in bAbI and WikiHop) is (1) not easy to learn directly and (2) direct supervision of the hop is often necessary. The paper also claims that (3) doing well on WikiHop doesn't necessarily mean the model is actually learning to hop.\n\nThe paper is easy to understand. I also agree with the claims. However, I think the claims are mostly trivial.\n\n(1) and (2) seem to be well-known to the community. In fact, the original Memory networks paper by Weston et al. (2015) uses strong supervision to solve bAbI, and the follow-up paper, End-to-end memory networks, attempts to solve bAbI without strong supervision, in which the authors were clearly aware of the fact that strong supervision helps the model to learn multi-hop much more easily. Furthermore, there exist numerous models, e.g. Kumar et al. (2016) that the authors cite, that do very good multi-hop reasoning on bAbI. Since many of these models can be considered as variants of end-to-end memory networks, the authors' claim that strong supervision is critical is not well-supported. Also, I feel that the paper is not comprehensively reviewing these related works.\n\nLastly, (3) could be a helpful and interesting observation of WikiHop dataset / Memory network, but pointing out the flaw of a dataset or the model alone does not seem to have enough contribution for ICLR.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}