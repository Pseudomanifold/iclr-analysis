{"title": "Review", "review": "This paper shows that training of a 3 layer neural network with 2 hidden nodes in the first layer and one output node\nis NP-complete. This is an extension of the result of Blum and Rivest'88. The original theorem was proved for \nthreshold activation units and the current paper proves the same result for ReLU activations. The authors do this\nby reducing the 2-affine separability problem to that of fitting a neural network to data. The reduction is well \nwritten and is clever. This is a reasonable contribution although it does not add significantly to the current state of the art. \n  ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}