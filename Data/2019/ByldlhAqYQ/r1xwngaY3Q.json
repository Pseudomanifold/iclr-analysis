{"title": "The paper proposed to use RNN/LSTM with collocation alignment as a representation learning method for transfer learning/domain adaptation in NLP.", "review": "The proposed method is suitable for many NLP tasks, since it can handle the sequence data.\n\nI find it difficult to follow through the model descriptions.  Perhaps a more descriptive figures would make this easier to follow, I feel that the ART model is a very strait forward and it can be easily described in much simpler and less exhausting (sorry for the strong word) way, while there is nothing wrong with being as elaborating as you are, I feel that all those details belong in an appendix. \nCan you please explain the exact learning process?\nI didn\u2019t fully understand the exact way of collocations, you first train on the source domain and then use the trained source network when training in the target domain with all the collocated words for each training example? I deeply encourage you to improve the model section for future readers. \nIn contrast to the model section, the related work and the experimental settings sections are very thin.\nThe experimental setup for the sentiment analysis experiments is quite rare in the transfer learning/domain adaptation landscape, having equal amount of labeled data from both source and target domains is not very realistic in my humble opinion.\nMore realistic setup is unsupervised domain adaptation (like in DANN and MSDA-DAN papers) or minimally supervised domain adaptation (like you did in your POS and NER experiments).\n\nIn addition to the LSTM baseline (which is trained with target data only), I think that LSTM which is trained on both source and target domains data is required for truly understand ART gains \u2013 this goes for the POS and NER tasks as well.\nThe POS and NER experiments can use some additional baselines for further comparison, for example:\nhttp://www.aclweb.org/anthology/Q14-1002\nhttps://hornhehhf.github.io/hangfenghe/papers/14484-66685-1-PB.pdf\n\nI am not sure I understand the \u201ccell level transfer\u201d claim, did you mean that you are the first to apply inner LSTM/RNN cell transfer or that you are the first ones to apply word-level fine grained transfer, the latter has already been done:\nhttps://arxiv.org/pdf/1802.05365.pdf\nhttps://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=4531&context=sis_research\nhttp://www.aclweb.org/anthology/N18-1112\nhttps://openreview.net/pdf?id=rk9eAFcxg\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}