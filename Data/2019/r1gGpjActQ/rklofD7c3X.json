{"title": "Good results, although knowledge distillation and its use in non-autoregressive NMT should be discussed better.", "review": "This paper proposes to distill knowledge from intermediary hidden states and\nattention weights to improve non-autoregressive neural machine translation.\n\nStrengths:\n\nResults are sufficiently strong. Inference is much faster than for\nauto-regressive models, while BLEU scores are reasonably close.\n\nThe approach is simple, only necessitating two auxiliary loss functions during\ntraining, and rescoring for inference.\n\nWeaknesses:\n\nThe discussion of related work is deficient. Learning from hints is a variant\nof knowledge distillation (KD). Another form of KD, using the auto-regressive\nmodel output instead of the reference, was shown to be useful for non-autoregressive\nneural machine translation (Gu et al., 2017, already cited). The authors mention using\nthat technique in section 4.1, but don't discuss how it relates to their work. [1] should\nalso probably be cited.\n\nHu et al. [2] apply a slightly different form of attention weight distillation.\nHowever, the preprint of that paper was available just over one month before the\nICLR submission deadline.\n\nQuestions and other remarks:\n\nDo the baselines use greedy or beam search?\n\nWhy batch size 1 for decoding? With larger batch sizes, the speed-up may be\nlimited by how many candidates fit in memory for rescoring.\n\nPlease fix \"are not commonly appeared\" on page 4, section 3.1.\n\n[1] Kim, Yoon and Alexander M. Rush. \"Sequence-Level Knowledge Distillation\" EMNLP. 2016.\n[2] Hu, Minghao et al. \"Attention-Guided Answer Distillation for Machine Reading Comprehension\" EMNLP. 2018", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}