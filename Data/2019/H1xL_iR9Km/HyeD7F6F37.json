{"title": "Good Idea but major issues with formulation and experimental setup.", "review": "The paper is proposing a method which is aimed for multi-source domain adaptation with weak supervision. In addition to the classical setup, paper also consider the case in which some of the source domains have a possibly non-overlapping label space with the target task. Proposed idea is rather simple and effective. The paper consider a model in which shared parameters are updated with convex combination of gradients, computed for available sources. The mixture weights for the convex combination is chosen such that the resulting update decreases the loss function of the few labelled examples from the target domain. This defines a bilevel optimization problem and paper uses a simple proxy which is minimising the cosine distance between the mixture and the gradient for the few labelled examples.\n\nThe biggest strength of the paper is its effectiveness in large set of experiments. Considering that it is also very easy to implement, I would expect the idea to be useful for the deep learning community. Idea of using adaptive weights for each batch and using different mixture weights for different layers are interesting and novel. Having said that, the paper is not ready to be published yet in my honest opinion as I summarize the issues below.\n\nMAJOR ISSUES:\n- Experimental Setup: Although the method is largely compared with domain adaptation methods, the setup is almost identical to few shot learning. In few shot learning, the test(target) and train(source) distributions typically have different label distribution. Hence, all few-shot learning methods are valid baselines for this paper. I think recent ones should be included in the experimental study. I would also suggest to use few-shot datasets like OmniGlot and CARS for the experiments since the performance of few-shot learning algorithms on them is already widely presented in the literature.\n\n- Related Work: Although the paper does a good job comparing with related work in domain adaptation, it misses two recent work which poses a bilevel optimization problem for meta-learning in order to solve a similar problem. These references are: [Bilevel Programming for Hyperparameter Optimization and Meta-Learning, ICML 2018] and [Deep Bilevel Learning, ECCV 2018]. Although these papers addresses slightly different problems with different optimization methods, fundamental idea is closely related. Authors should discuss these works.\n\n- Correctness: I think the way the optimization problem (defined in Equation 5) is solved is NOT correct. First of all, the inner product is a linear operator; hence the problem is a linear program in terms of w_{s_i}^l. Hence, the statement that it is a non-linear optimization problem is not correct. Second of all, as long as any of the inner-products is positive, the optimal value is infinity as the w is not bounded. Even if it was bounded, it would be a one hot vector such that the weight corresponding to largest inner product is 1 and others are zero. I do not think the paper solves (eq 5) correctly. Please correct me if I misunderstood some part.\n\nMINOR ISSUES\n-Implementation Details: Paper is missing majority of implementation details. For example, how the parameters are shared between different domains and tasks? This is rather crucial and should be discussed in details.\n\nSUMMARY: I think the fundamental idea in the paper is interesting although a similar ideas have been used for similar problems already. I think the paper would still be useful to the community even after considering the limited novelty. However, I do not think the paper is correct. Moreover, I think it needs a stronger empirical study comparing with few-shot learning methods. Hence, I suggest authors to fix the correctness issue. extend the experiments with few-shot learning setup, and re-submit.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}