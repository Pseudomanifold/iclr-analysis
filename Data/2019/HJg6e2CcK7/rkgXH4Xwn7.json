{"title": "I think this paper adds an original and valuable angle to the existing literature on data poisoning attacks", "review": "Overall I am positive about this manuscript:\n- I find the motivation is clear and valid. As far as I know, this is a novel contribution (my confidence is not very high on that one though - I might be unaware of related work).\n- The paper is well-written and organized.\n- Experiments are conducted systematically, although certain parts could be better explained (see my questions below).\n\nI think this paper adds an original and valuable angle to the existing literature on data poisoning attacks. I don't see any major flaws, therefore I think it should be accepted.\n\nA few points which might need clarification:\n- How exactly is \"attack success\" being measured?\n- Which model is used to generate the adversarial samples? Is this an (adversarially) pretrained model? (If that's the case, then what is the model architecture?) Or are adversarial samples generated on the fly using the currently trained/poisoned model?\n- At the end of Section 4.4: if the images with larger noise rely more on the backdoor, why does this have an adverse effect? Shouldn't it increase the effectiveness of the attack?\n- Was the data augmentation (flips, crops etc) performed before or after the poisoning pattern was applied?\n\nMinor comments:\n- definition of the encoding at the bottom of page 4: this should be argmax instead of max\n- typo in Sec. 5.1: \"to evaluate the uat a wide variety\"\n- repetitive sentence in Sec. 5.2: \"we find that images generated with $\\tau \\leq 0.2$ remain [fairly] plausible\"\n\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}