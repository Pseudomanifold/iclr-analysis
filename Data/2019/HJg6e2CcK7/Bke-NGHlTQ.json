{"title": "Clean-Label Backdoor Attacks", "review": "\nThis work explores backdoor attacks -- attacks that alter a fraction of training examples which can alter inference -- while ensuring that the poisoned inputs are consisten\nt with their labels. These attacks are attained through either a GAN mechanism or using adversarial perturbations.\n\nThe ideas proposed (i.e. GAN mechanism and adversarial mechanism) are interesting additions to this literaature. I found the observation of greater effectiveness of adversa\nrial mechanism particularly interesting.\n\nThe paper also does a good job of investigating effectiveness of the attack under data augmentation and propooses a limited solution.\n\nMain criticism: there are a number of typos that need fixing.\n~                                            ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}