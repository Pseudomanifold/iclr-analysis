{"title": " ICLR 2019 Conference Paper576 AnonReviewer3", "review": "This paper studied an extension of the Information Bottleneck Principle called Information Bottleneck Hierarchy (IBH).  The goal of IBH is to extract meaningful information from a Markov Chain. Then the authors studied case of the Gaussian linear dynamic and proposed an algorithm for computing the IBH. Then an experiment was conducted to show the usage of IBH to practical problems.\n\nGenerally I like the idea of extending Information Bottleneck to dynamic systems and I think the experiment is interesting. But I have some major questions to the paper and these questions are important about the principle you are proposing.\n\n1. About Figure 1, there is a link between X_{k-1} and B_k, but there are no link between X_k and B_{k+1}. I understand what you said --- B_k needs to compress X_{k-1} and delivers information to B_{k+1}. My question is ---- Figure 1 can not be generated to a longer Markov Chain. It seems that the principle you proposed only works for 3 random variables X_{k-1}-X_k-X_{k+1}, which weaken the principle a lot. Please draw a longer Markov Chain like Figure 1 to illustrate your principle.\n\n2.  About the \\epsilon_{1,2,3} in formula (3). \\epsilon_1 is claimed to bound the accuracy of the prediction of X_k by B_{k-1}, but where not B_{k-1} appear in the formula (actually B_{k-1} is not even in Figure 1)? \\epsilon_3 is claimed to define the closeness of prediction of X_{k+1} by B_{k+1}, but why does I(X_{k-1},X_{k+1}) need to be small? In the \"Markov chains are considered\" before formula (3), there are some typos, for example, X_{k+1}-B_k-B_{k+1} seems not a Markov Chain. Also why you are bounding the difference of two mutual informations, but not take the absolute value (I think the difference you are considered are not guaranteed to be non-negative)? I think formula (3) is the key to understand the IBH principle, but it is not well illustrated for the readers to understand.\n\n3. I understand that you can only derive an algorithm for Gaussian linear dynamic, since non-Gaussian case might be difficult and Gaussian linear dynamic might be good enough for modeling real random processes. But I wonder what is the physical or practical meaning for the matrices \\Psi and \\Delta? Why \\Delta can be used to predict sentiment intensity in your experiment? It seems that \\Delta carries the information from B_k to B_{k+1}, so it is only one-hop information and the sentiment intensity involves multi-hop information. How do you combine the different \\Delta for different hops to predict sentiment intensity? These questions are not well illustrated in the paper.\n\nSo I think the paper can be accepted if the author can provide some more insightful illustrations, especially for Figure 1, formula (3) and the experiment. But overall I think the idea in this paper is interesting, if well illustrated.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}