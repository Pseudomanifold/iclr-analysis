{"title": "Excellent contribution to solving an important problem", "review": "The authors propose a modification of existing adaptive variants of SGD to avoid problems with generalization. It is known that adaptive gradient algorithms such as Adam tend to find good parameter values more quickly initially, but in the later phases of training they stop making good progress due to necessarily low learning rates so SGD often outperforms them past a certain point. The suggested algorithm Padam achieves the best of both worlds, quick initial improvements and good performance in the later stages.\n\nThis is potentially a very significant contribution which could become the next state-of-the-art optimization method for deep learning. The paper is very clear and well-written, providing a good overview of existing approaches and explaining the specific issue it addresses. The authors have included the right amount of equations so that they provide the required details but do not obfuscate the explanations. The experiments consist of a comprehensive evaluation of Padam against the popular alternatives and show clear improvements over them.\n\nI have not evaluated the convergence theorem or its proof since this is not my area of expertise. One thing that stood out to me is that I don't see why theta* should be unique.\n\nSome minor suggestions for improving the paper:\n\nTowards the end of section 2 you mention a non-convergence issue of Adam. It would be useful to add a few sentences to explain exactly what the issue is.\n\nI would suggest moving the details of the grid search for p to the main text since many readers would be interested to know what's typically a good value for this parameter.\n\nWould it make sense to try to adapt the value of p, increasing it as the training progresses? Since that's an obvious extension some comment about it would be useful.\n\nOn the bottom of page 6: \"Figures 1 plots\" -> \"Figure 1 plots\".\n\nMake sure to protect the proper names in the bibliography so that they are typeset starting with uppercase letters.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}