{"title": "The contribution is relatively minor and an important baseline is missing in comparison.", "review": "This paper proposes a small modification to current adaptive gradient methods by introducing a partial adaptive parameter, showing improved generalization performance in several image classification benchmarks.\n\nPros:\n- The modification is simple and easy to implement.\n- The proposed method shows improved performance across different datasets, including ImageNet.\n\nCons:\n- Missing an important baseline - AdamW (https://arxiv.org/pdf/1711.05101.pdf) which shows that Adam can generalize as well as SGD and retain faster training. Basically, the poor generalization performance of Adam is due to the incorrect implementation of weight decay.\n- The experimental results for Adam are not convincing. It's well-known that Adam is good at training but might perform badly on the test data. However, Adam performs much worse than SGD in terms of training loss in all plots, which is contrary to my expectation. I doubt that Adam is not tuned well. One possible explanation is that the training budget is not enough, first-order methods typically require 200 epochs to converge. So I suggest the authors training the networks longer (make sure the training loss levels off before the first drop of learning rate.).\n- Mixing the concept of generalization and test performance. Note that generalization performance typically measures the gap between training and test error. To make the comparison fair, please make sure the training error is zero (I expect both training error and training loss should be close to 0 on CIFAR).\n- In terms of optimization (convergence) performance, I cannot think of any reason that the proposed method would outperform Adam (or Amsgrad). The convergence analysis doesn't say anything meaningful.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}