{"title": "A very interesting idea for combining MCMC and VI.", "review": "This paper proposes a clever and sensible approach to using the structure learned by the auxiliary variational method to accelerate random-walk MCMC. The idea is to learn a low-dimensional latent space that explains much of the variation in the original parameter space, then do random-walk sampling in that space (while also updating a state variable in the original state, which is necessary to ensure correctness).\n\nI like this idea and think the paper merits acceptance, although there are some important unanswered questions. For example:\n- How does the method work on higher-dimensional target distributions? I would think it would be hard for a low-dimensional auxiliary space to have high mutual information with a much higher-dimensional space. In principle neural networks can do all sorts of crazy things, but phenomena like VAEs with low-dimensional latent spaces generating blurry samples make me suspect that auxiliary dimension should be important.\n- How does the method work with hierarchical models, heavy-tailed models, etc.? Rings, MoGs, and flat logistic regressions are already pretty easy targets.\n- Is it really so valuable to not need gradients? High-quality automatic differentiation systems are widely available, and variational inference on discrete parameters with neural nets remains a pretty hard problem in general.\n\nSome other comments:\n\n* It\u2019s probably worth citing Ranganath et al. (2015; \u201cHierarchical Variational Models\u201d), who combine the auxiliary variational method with modern stochastic VI. Also, I wonder if there are connections to approximate Bayesian computation (ABC).\n\n* I think you could prove the validity of the procedure in section 2.1 more succinctly by interpreting it as alternating a Gibbs sampling update for \u201ca\u201d with a Metropolis-Hastings update for \u201cx\u201d. If we treat \u201ca\u201d as an auxiliary variable such that\np(a | x) = \\tilde q(a | x)\np(x | a) \\propto p(x) \\tilde q(a | x)\nthen the equation (2) is the correct M-H acceptance probability for the proposal\n\\tilde q(a\u2019, x\u2019) = \u03b4(a\u2019-a) \\tilde q(x\u2019 | a).\nAlternating between this proposal and a Gibbs update for \u201ca\u201d yields the mixture proposal in section 2.1.\n\n* It\u2019s also possibly worth noting that this procedure will have a strictly lower acceptance rate than the ideal procedure of using the marginal\n\\tilde q(x\u2019|x)\nas a M-H proposal directly. Unfortunately that marginal density usually can\u2019t be computed, which makes this ideal procedure impractical. It might be interesting to try to say something about how large this gap is for the proposed method.\n\n* \"We choose not to investigate burn-in since AVS is initialized by the variational distribution and therefore has negligible if any burn-in time.\u201d This claim seems unjustified to me. It\u2019s only true insofar as the variational distribution is an excellent approximation to the posterior (in which case why use MCMC at all?). It\u2019s easy to find examples where an MCMC chain initialized with a sample from a variational distribution takes quite a while to burn in.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}