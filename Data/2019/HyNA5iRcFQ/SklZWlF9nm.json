{"title": "An interesting paper", "review": "Main contribution: devising and evaluating an algorithm to find inputs that trigger arbitrary \"egregious\" outputs (\"I will kill you\") in vanilla sequence-to-sequence models, as a white-box attack on NLG models.\n\nClarity:\nThe paper is overall clear. I found some of the appendices (esp. B and C) to be important for understanding the paper and believe these should be in the main paper. Moving parts of Appendix A in the main text would also add to the clarity.\n\nOriginality:\nThe work looks original. It is an extension of previous attacks on seq2seq models, such as the targeted-keyword-attack from (Cheng et al., 2018) in which the model is made to produce a keyword chosen by the attacker.\n\nSignificance of contribution:\nThe lack of control over the outputs of seq2seq is a major roadblock towards their broader adoption. The authors propose two algorithms for trying to find inputs creating given outputs, a simple one relying on continuous optimization this is shown not to work (breaking when projecting back into words), and another based relying on discrete optimization. The authors found that the task is hard when using greedy decoding, but often doable using sampled decoding (note that in this case, the model will generate a different output every time). My take-aways are that the task is hard and the results highlight that vanilla seq2seq models are pretty hard to manipulate; however it is interesting to see that with sampling, models may sometimes be tricked into producing really bad outputs.\nThis white-box attack applicable to any chatbot. As the authors noted, an egregious output for one application (\"go to hell\" for customer service) may not be egregious for another one (\"go to hell\" in MT).\n\nOverall, the authors ask an interesting question: how easy is it to craft an input for a seq2seq model that will make it produce a \"very bad\" output. The work is novel, several algorithms are introduced to try to solve the problem and a comprehensive analysis of the results is presented. The attack is still of limited practicality, but this paper feels like a nice step towards more natural adversarial attacks in NLG.\n\nOne last thing: the title seems a bit misleading, the work is not about \"detecting\" egregious outputs.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}