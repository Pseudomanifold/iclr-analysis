{"title": "Interesting work. Can benefit from better experiments.", "review": "This work provides theoretical insights on recent learning rate proposals such as Cyclical Learning Rates (Smith et al.). The authors focus on stochastic approximation i.e. how large is the SGD loss as a function of condition number and horizon. The critical contribution is the theoretical benefit of oscillating learning rates over more traditional learning rate schemes. Authors provide novel upper/lower bounds to establish benefit of oscillating LR, support their theory with experiments and provide insights on finite horizon learning rate selection. An important drawback is that results only apply to linear regression which is a fairly simple setup.\n\nI have two important comments regarding this work:\n1) I believe proof of Theorem 3 has a bug. In the proof, authors use the inequality\n(1-gamma_t lambda^k)^2 < exp(-2lambda^k gamma_t).\nObviously this can only be correct for gamma_t lambda^k<1. However, checking the setup of the problem, it can be seen that for largest eigenvalue and gamma_0, ignoring log factors:\n\ngamma_0L = L/(mu T_e)=kappa / T_e=kappa/T.\n\nSince, no restriction is imposed on T, gamma_0L can be as large as O(kappa) and invalidates the above inequality. So T should be T>O(kappa). I am not sure if this affects the overall statement or the remaining argument.\n\n2) The paper can benefit from more detailed experiments (e.g. Figs 1 and 2). Arguably the most obvious baseline is \"constant learning rate\". However, authors compare to 1/T or 1/sqrt(T) learning rates. It is not at all clear from current experiments, if the proposed approach beats a good constant LR choice.\n\nI am happy to increase my score if the comments above are addressed.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}