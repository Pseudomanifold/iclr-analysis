{"title": "Well written, some parts require clarification", "review": "This paper presents a theoretical study of different learning rate schedules. Its main result are statistical minimax lower bounds for both polynomial and constant-and-cut schemes.\n\nI enjoyed reading the paper and I think the contributions in it shed some light in step size schedules that have shown to be useful in practice. I do have however some concerns that I hope the authors can address in their rebuttal. My initial rating is marginally below acceptance but I will gladly increase this rating if my concerns are addressed.\n\n\n# Pros\n\n* The paper is written in a way that's both clear and accessible.\n\n* The Theoretical contributions are important, as they address the choice of step size in one of the most used optimization methods machine learning and are novel to the best of my knowledge.\n\n* Due to time constraints, I only skimmed through the proofs, but results seem correct.\n\n\n# Concerns\n\n\nMy biggest concern is that its unclear how realistic is their noise model. The authors assume that the noise in the stochastic gradients e verifies E[e e^T ] = \\sigma H. While they claim that this is verified for problems like least squares, it is not clear to me that this is indeed the case. Related work like (Moulines and Bach, 2013) and (Flammarion and Bach, 2015) take the same setting but can only assume that the covariance of the noise is _bounded_ by a matrix of sigma times H. How do the authors obtain a much stronger condition on the noise covariance with the same assumption? I would be much more convinced with a proof in appendix clearly showing that the assumptions in footnote 8 imply the aforementioned covariance of the noise and a paragraph comparing their noise model with that of related literature like the aforementioned references (I'm not affiliated with any of that work). \n\nAlso, the authors claim that their results hold for an arbitrary noise covariance matrix but the proofs are all done with the specific \\sigma H matrix. I don't think its OK to say \"our results hold for a more general setting\" without proof. If they do hold for a more general setting then the proofs should be done in the general setting. If not, it should only be mentioned as future work. Please edit that remark accordingly.\n\n* The paper does not compare or discuss against constant step size with averaging, which has been shown to be theoretically optimal in some scenarios (see aforementioned papers). This should at least be mentioned, and ideally also included in experiments.\n\n\n# Presentation issues\n\nClarity of the proofs can be improved. For example, in Theorem 1, the formula for v_T(1) and v_T(d) follow from a recurrence that is stated _below_ the formula, needing several passes to understand. The proofs could benefit from a pass on them to improve the flow.\n\n\nIt is never clear whether expectations are taken with respect to the full randomness of the algorithm or conditioned on previous randomness. The E in Eq. just-before-section-4 (please add equation numbers) is a full expectation while the E[e] should be conditioned on previous randomness. The expectation in footnote 8 is also unclear if its wrt to the stochasticity of the algorithm or the randomness in the data generating process.\n\n\nNo equation numbers  makes it difficult to reference equations. Please add equation numbers so that reviewing is not more difficult than it should (and others can reference your work more precisely).\n\nOther minor presentation issues include:\n\n  * Page 1: Why l-BFGS and not L-BFGS? the lowercase l makes it look like a 1.\n  * Page 2: There important -> There *are* important.\n  * Page 2: In fact, at least ... (missing parenthesis around Omega tilde).\n  * Page 4: \"a stochastic gradient oracle which gives us\" the second w should also be boldface.\n  * Page 4: I would have appreciated\n\n  * Page 11: \"variance in the i-th\" direction. It would be more correct to say in the i-th coordinate as otherwise it can be mistaken with the i-th update direction.\n\nUpdate:\n  I am satisfied with the answers and have upgraded my rating.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}