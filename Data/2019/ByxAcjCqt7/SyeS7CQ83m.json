{"title": "Generative model for point clouds, based on local-global latent variables.", "review": "Summary:\nThis paper proposes a generative point cloud model based on adversarial learning and definitti\u2019s representation theorem of exchangeable variables.\nThe main focus in experiments and the exposition is on 3D point clouds representing object shapes (seems the surface, but could also be the interior of objects, please clarify). \nThe main idea is to represent a point cloud using a global latent variable that captures the overall shape, and a collection of local latent variables that code for the position of a point on the shape.\nThe model consists of thee components:\n(i) an \u201cencoder\u201d that takes a point cloud as input and maps it to a (point estimate of) the global latent variable of the shape represented by the input cloud, a point-net architecture is used here\n(ii) a \u201cdecoder\u201d that takes the estimated global latent variable, and a local latent variable, and maps it to an \u201coutput\u201d point in the cloud to be produced by the model. \n(iii) a \u201cdiscriminator\u201d network that aims to distinguish points from a *given* shape, and the points produced by pipe-lining the encoder and decoder. Critically different from conventional GANs, the discriminator is optimized *per shape*, ie each point cloud is considered as a *distribution* over R^3 specific to that shape. \n(iv) a \u201cshape prior\u201d that, once the encoder-decoder model from above is trained, is used to model the distribution over the global latent variables. This model is trained, presumably, in a conventional GAN style using the global latent variable representations inferred across the different training point clouds.\n\nAs compared to prior work by Achiloptas et al (2017), the proposed approach has the advantage to allow for sampling an arbitrary number of points from the target shape, rather than a fixed pre-defined number. \n\nIn addition, the authors propose to minimize a weighted average of a lower bound and upper bound on the Wasserstein distance between the distributions of points corresponding to given shapes. \nThis approach translates to improved quantitative evaluation measures, \n\nExperiments are conducted on a simple toy data set, as  a proof of concept, and on data from ModelNet10 and ModelNet40. \nTwo performance metrics are introduced to assess the auto-encoding ability of the model: to what extent does the encoder-decoder pipeline result in point clouds similar to the shape from which the input point-cloud is generated. \n\nOverall I find the idea of the paper interesting and worth publishing, but the exposition of the paper is less than ideal and needs further work. \nThe experimental validation of the proposed approach can also be further improved, see more specific comments below. \n\nSpecific comments:\n\n- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.\n\n- The notation in section 3 (before 3.1) is rather sloppy. \nFor example, \n- please define P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3.\n- it is not defined in which space theta lives, it is not clear what the authors intend with the notation G_theta(u) \\sim p(theta). \n- what prior distributions p(z) and p(u) are used? What is the choice based on?\n\n- abbreviation IPM is referred several times in the paper, but remains undefined in the paper until end of page 4, please define earlier. \n\n- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?\n\n- Lack of clarity in the following passage: \u201cIn our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images\u201d\n\n- The notion of divergence D(P|G) is not made concrete in section 3 and 3.1, which makes the notation of rather little use.\n\n- The following paper merits a discussion in the related work section: \n\u201cTOWARDS A NEURAL STATISTICIAN\u201d, ICLR\u201917, https://openreview.net/pdf?id=HJDBUF5le\n\n- The manuscript contains many typos. For example\n \u201cvedio\u201d op page 4, \u201ccircile\u201d on page 5, \u201ccondct\u201d on page 8, etc.\nPlease proof read your paper and fix these.\nThe refenence to  Bengio 2018 is incomplete: what do you refer to precisely?\n\n- There seems to be no mention of the dimension of the \u201clocal\u201d latent variables z_i. \nPlease comment on the choice, and its impact on the behavior of the model.\n\n- The quantitative evaluation in table 1 is interesting and useful. \nIt is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape. \nQuantitative evaluation of generative modeling performance is unfortunately missing from this paper, as it is in much of the GAN literature. \nCould you please comment on how this can/will be fixed?\n\n- The toy data set experiments could be dropped  to make room for experiments suggested below.\n\n- An experimental study of the effect of the mixing parameter \u201cs\u201d would be useful to include. \nFor example, by taking s on a grid from 0 to 1, one could plot the coverage and distance-to-face measures.\n\n- Experimental evaluation of auto-encoding using a variable number of input points is interesting to add: ie how do the two evaluation measures evolve as a function of the number of points in the input point cloud?\n\n- Similar, it is interesting to evaluate how auto encoding performs when non-uniform decimation of the input cloud is performed, eg what happens if we \u201cchop off\u201d part of the input point cloud (eg the legs of the chair), does the model recover and add the removed parts? This is potentially useful to practitioners which have to deal with incomplete point clouds acquired by range scanners. \n\n- Analysis of shapes with different genus and dimensions would be interesting. \nDoes the model manage to capture that some shapes have holes, or consists of a closed 2D surface (ball) vs an open surface (disk),  despite a simple prior on the local latent variables z?\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}