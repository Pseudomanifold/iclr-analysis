{"title": "An interesting new nugget of a problem", "review": "The authors introduce the problem of Model Completion (MC) to the machine learning community.  They provide a thorough review or related works, and convincingly argue that existing solutions to this sort of task (i.e., homomorphic encryption and multi-party computation) are not fully satisfactory in the domain of neural network learning.\n\nThe authors also provide extensive numerical experiments attempting to quantify their proposed measure of hardness-of-model-completion, MC-hardness_T(\\alpha) on a diverse set of Supervised and RL-related tasks, and they provide extensive analysis of those results.\n\nI find the paper to raise more questions than it answers (in a good way!).  The authors note that their measure depends strongly on the peculiarities of the particular (re)training scheme used.  Do the authors worry that such a measure could end up being too loose--essentially always a function of whatever the fastest optimization scheme happens to be for any particular architecture?  \n\nMore broadly, there's an additional axis to the optimization problem which is \"How much does the training scheme know about the particulars of the problem?\", ranging from \"Literally has oracle access to the weights of the trained model (i.e., trivial, MC-hardness = 0 always)\" to \"knows what the architecture of the held-out-layer is and has been designed to optimize that particular network (see, e.g., learned optimizers)\" to \"knows a little bit about the problem structure, and uses hyperparameter tuned ADAM\" to \"knows nothing about the problem and picks a random* architecture to use for the held out weights, training it with SGD\".\n\nModel completion seems, morally (or at least from a security stand-point) slightly under-specified without being more careful about what information each player in this game has access to.  As it stands, it's an excellent *empirical* measure, and captures a very interesting problem, but I'd like to know how to make it even more theoretically grounded.\n\nAn excellent contribution, and I'm excited to see follow-up work.\n\n\n\n* We of course have tremendous inductive bias in how we go about designing architectures for neural networks, but hopefully you understand my point.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}