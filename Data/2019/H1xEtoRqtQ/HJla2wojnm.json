{"title": "Review: Problem motivation and analysis", "review": "This paper proposes and studies the \u201cmodel completion\u201d problem: given a trained network (and the data on which is was trained), if a subset of the network is reinitialized from scratch, how many retraining iterations are needed to achieve the original network accuracy (or some percentage of it)? For a variety of networks and problems in both supervised and reinforcement learning, model-completion (MC) hardness is quantified for individual network layers/sections. The experiments are the core of the paper and are generally well documented and seem reproducible.\n\nHowever, there are two issues that cloud the paper:\n\t1. The problem motivation (bounding the security of model splitting) is a bit odd. Has model splitting been proposed in the literature as a potential solution to shared model governance? Otherwise it feels like the problem setting was invented to justify the analysis in this paper: \u201cthe tail wagging the dog\u201d as the saying goes\u2026\n\t2. Model completion yet still be an interesting analytical tool for deep networks, but this requires a different evaluation. For instance, model completion provides a way to study how complicated different network layers are to learn or maybe to quantify how much of the inference task may be contained in each. (Though these concepts would need precise language and experimental evidence.) But how do these observations compare to other ways of obtaining similar observations? For instance, from the pruning literature, (Molchanov, 2017, ICLR, https://openreview.net/pdf?id=SJGCiw5gl) includes several figures detailing the statistics of individual network layers and how \u201cprunable\" are the filters in each.\n\nThis is largely an analytical paper, and I\u2019ll readily acknowledge that it is difficult to pull a clear and insightful study out of a jumble of experimental observations (and hard to review such a paper too). But the limitations of the problem motivation (point #1) and (in my opinion) the misaligned focus of the analysis (point #2), hurt the clarity and significance of this paper. For it to really be a useful tool in understanding deep learning, some additional work seems to be needed.\n\nOther notes:\n\t3. Pruning literature would be a reasonable comparison in the related work. For instance, (Han, ICLR, 2017, https://arxiv.org/abs/1607.04381) describes a dense-sparse-dense method where a (dense) model is pruned (sparse), after which the pruned connections are reinitialized and retrained (dense) leading to improved accuracy relative to the original dense model.\n\t4. Consider replacing the uncommonly used \u201cca.\u201d with \u201c~\u201d, e.g. \u201c~1000x\u201d instead of \u201cca. 1000x\u201d.\n\t5. The specifics about ImageNet in the intro to Section 3 should be moved to Section 4.\n\t6. In Section 3.2 paragraph 2, clarify if \u201closs\u201d refers to test loss as stated in the intro to Section 3.\n\t7. In Figure 2 (alpha=0.9) and Figure 3 (alpha=1.0, bottom), why are the values constant?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}