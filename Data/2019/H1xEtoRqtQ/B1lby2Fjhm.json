{"title": "Interesting idea but needs better positioning, metrics, and analysis", "review": "This paper proposes the interesting idea of analyzing how difficult\nit is to re-initialize and re-train layers in neural networks.\nThey study these techniques in the context of ImageNet classification\nand reinforcement learning in the Atari and DeepMind lab domains.\nWhile these are interesting ideas and domains to study, I have\nconcerns with the positioning and execution of the paper.\n\n[Positioning, execution and motivation]\nOn the positioning of the paper, a significant part of the introduction\nand related work section is spent arguing that this approach can be used\nfor shared model governance in contexts where homomorphic encryption\nor secure multi-party computation would instead be used.\nComparing the approaches studied in this paper to these\nsophisticated cryptographically-motivated techniques seems\nlike too much of a stretch, as the methods serve very different\npurposes and in most cases cannot even be directly compared.\n\nThe first and second paragraph discuss the vision of distributing\nthe training of models between multiple parties.\nI agree that this is a useful area to study and direction\nfor the community to go, but as the introduction of this paper\nstates, this is the most interesting when the parties have\ncontrol over logically separate components of the modeling pipeline\nand also when joint training of the components is being done,\npotentially on disjoint and private datasets.\nThe empirical results of this paper do none of this,\nas they only look at the case when a single layer is being\nreplaced.\n\nFurthermore the motivation and positioning of the paper is\nnot carried through in the empirical setup, where they\ninvestigate approaches that do training over all\nof the parameters of the model, breaking the assumption\nthat the parties should be independent and should\nnot share information.\n\n[Metrics for measuring model completeness]\nSection 3.1 defines the metric of completion hardness that is\nused throughout the rest of the paper. The metric looks at the\nnumber of iterations that re-training the model takes to\nreach the same performance as the original model.\nIt's not clear why this is an important metric and I am\nnot convinced it is the right one to use as it:\n1) does not give a notion of how nicely the missing portion\nwas recovered, just that the accuracy reached the\nsame accuracy as the original network, and\n2) methods with a very long per-iteration runtime such as\nsecond-order and sampling-based methods could be used to\nreach a good performance in a small number of iterations,\nmaking these methods appear to be very \"good\" at\ncompleting models. I don't think it is nice that this\nmetric relies on the same optimizer being used for the\noriginal model and the completed model.\n\nI think it's more interesting to study *how much* data is\nrequired to recover missing portions of the model instead\nof how many iterations are needed to recover the same performance.\nThe supervised learning experiments appear to be done\nusing the entire dataset while the RL experiments do\npresent a setting where the data is not the same.\n\n[Empirical results]\nI am also surprised by the empirical finding in Section 5.1\nthat T1 outperforms T2, since it seems like only optimizing\nthe parameters of the missing layer would be the best\napproach. I think that if a similarity metric was used\ninstead, T2 would be significantly better at finding the\nlayer that is the most similar to the layer that was removed.\n\nSome smaller comments:\n\n1. In Section 3.1, the definition of C_T does not use T explicitly\n   inside of it.\n2. In the last paragraph of Section 3.1 and first paragraph of\n   Section 3.2, N should be defined as an iteration that\n   reaches the best loss.\n3. The description of T3 does not say what method is used to\n   optimize the over-parameterized layer, is it T1 or T2?\n4. Why does T4 use T1 instead of T2?\n5. In the experimental setup, why is T2 applied with a different\n   learning rate schedule than the original training procedure?\n6. Why is T2 not shown in the AlexNet results for Figure 2?\n7. The dissimilar axes between the plots in Figure 2 and\n   Figure 3 make them difficult to compare and interpret.\n8. It's surprising that in Figure 3, the hardness of \\alpha=1.0\n   for T2 is 1.0 for everything.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}