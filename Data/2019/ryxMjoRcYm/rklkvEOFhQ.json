{"title": "Neat idea but somewhat underwhelming proposition", "review": "    The authors of the article propose to use logical propositions to simplify the expression of high-level objectives in reinforcement learning problems. For that, from logical formulas, an automaton is generated and, from this automaton, is deduced a reward function. The reward function is defined in such a way that the agent reaches an acceptable absorbing state. Thus, it is easier to define sub-objectives that should have been encoded by hand in the reward function. The authors formally define the combined framework of MDP with LDBA, then they describe an adaptation of neural fitted iteration, Voronoi quantizer and fitted value iteration in this context. They compared their new methods in 2 different environments with different constraints.\n\n\n    The paper is well written and easy to read. However, these works do not really overcome the problem of safety or expressing \"complex high-level control objectives\" because logic rules are only used to define a reward function. Once this function is defined, there is no other applied constraints, so during the learning phase there is no more safety than using conventional RL. Moreover, on the example studied, adding a dimension to the state and defining an adequate reward function with this additional dimension would allow obtaining the same behaviors. The proposed framework would be more convincing if it were illustrated with an example where the reward function is really easier to define with the proposed approach, but difficult to define without it.\n\n    Pros :\n    - beautiful clean formalization\n    - interesting experimentation\n\n    Cons :\n    - policies/algorithms are not directly logically constrained but more guided by constraints\n    - the obtained reward function is sparse, e.g., subgoals don't receive any additional rewards\n    - experiments are difficult to reproduce (no link to source, missing a full description of neural networks used, etc.)\n    - some details are not necessary for the overall understanding, while some essential elements are missing (what happens if a non-accepting absorbing state is reached during the training phase, what is the concrete inputs of your neural networks ? how your discrete actions are handled in your neural networks, what happens if y = 0 for LCNFQ?)\n    - satisfiability of the generated policies is only little studied : what their proportions during the learning phase are, ...\n\n\n    Others questions/remarks :\n    - it seems that you do not use any exploration strategy, is it because your environment is already stochastic on the action ?\n    In this case, \"LCNFQ stops when a satisfying policy is generated.\" seems to be a bad idea as the algorithm might just have been lucky.\n    - it would be better if the definition of the reward function of the general framework does not contain a LCNFQ specific hack\n    - In (4), the reward function should also depend on the next state if it is deterministic. Besides, the definition of rand is slightly ambiguous, does it return a new random value every time the reward is evaluated?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}