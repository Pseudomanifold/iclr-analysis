{"title": "Interesting direction but not easy to understand and weak empirical evaluation and weak discussion of related work ", "review": "The paper extends neural fitted q-iteration in such a way that \nthe traces of the resulting policies satisfy a Linear Temporal \nLogic (LTL) property. \n\nThe main idea is to define a product MDP of the original \nMDP and a Limit Deterministic Buechi Automaton (LDBA).\nUnfortunately, the paper is not well written and presented.  \n\n\nThe part on LTL is hard to grasp for an informed outsider such as the reviewer. \nThe meanings of the operators are not introduced in an easy to grasp way. For instance, \nwhat exactly is L in the Definition of 3.1? What are words in general? Likewise, the product MDP defined is hard to grasp since no intuitive examples are provided. The intuition is that the LDBA is constraining the MDP according to the LTL. However, how \nthis is actually done algorithmically is not explained. The paper, as presented, just boils down to \"we build the product MDP\". In this way, one can also use say some first order logic, since properties of the product MDP are not discussed. For instance, \ncan the product MDP explode in size? Likewise the resulting LCNFQ is not really providing more insights. It essentially just says, learn n feed-forward neural networks. Does\nthe order of the Rprop upates really make no difference? Should be follow the temporal order? Is it really the best to train the networks independently? Why not training a single neural network that take the LDBA state as input (using some embedding)? Remark 4.2\nonly shows that one particular encoding does not work. This is even more important given that there are relational MDPs in the literature that are unfortunately not discussed, \nsee e.g. \n\nScott Sanner, Craig Boutilier:\nPractical solution techniques for first-order MDPs. \nArtif. Intell. 173(5-6): 748-788 (2009)\n\nand the overview in \n\nLuc De Raedt, Kristian Kersting, Sriraam Natarajan, David Poole:\nStatistical Relational Artificial Intelligence: Logic, Probability, and Computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, Morgan & Claypool Publishers 2016 \n\nThe authors should discuss the difference to these approaches. Likewise, \nthe authors should connect to the large body of work on safe reinforcement learning, \nwhere general constraints on the traces of a RL agent are imposed, too. Finally, the experimental evaluation should be improved. Right now, only one domain is considered: \nautonomous Mars rover. To show that this is of general interest, other domains should be considered. \n\nTo summarize, the general direction is really important. However, the paper is hard to follow, the experimental evaluation is not convincing, and there is missing related work.  ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}