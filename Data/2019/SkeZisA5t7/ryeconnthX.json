{"title": "This paper uses information to show compression of DNN with unbounded activation function, but it gives no questions to what form the compression.", "review": "This paper proposes a method for the estimation of mutual information for networks with unbounded activation functions and the use of L2 regularization to induce more compression.  The use of information planes to study the training behavior of networks is not new.  This paper addresses the issue of unbounded  hidden state activities.  As the differential mutual information in DNN is ill-defined, the authors proposed to add noise to the hidden activity by using the binning process.   It is not clear in the paper that if the binning is applied just for visualizing the information plane or for computing the activities of hidden units in upper layers.   If it is the latter one, it creates unnecessary distortions to the DNN.  As the authors pointed out, different initializations can lead to different behaviour on the information plane.  It would be difficult to draw conclusions based on the experimental results, even they come from the average of 50 individual networks.  Also, the experiences are performed using a particular task, it is not sure if similar behavior is observed in other tasks.   It is, however, more important to understand what makes the compression.   For the L2 regularization, the compression is expected as the regularization tends to limit the values of the  weights. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}