{"title": "Nice Application of Variational Bayes semi-supervised learning", "review": "This paper jointly trains a sentiment classifier with a sentiment and domain-aware embedding\nmodel, using both labeled and unlabeled data. When sentiment label is observed, this model is trained\nwith the usual cross entropy and maximum likelihood objectives; for unlabeled data, it uses pseudo\nlabels produced by the sentiment classifier with variational Bayes objective. This idea is not novel but the authors report that there is no previous work that jointly trains sentiment aware embeddings with a sentiment classifier specifically, and makes use of an unlabeled corpus to improve both. However, there are general and broader methods such as 'Toward Controlled Generation of Text' by Hu et al that apply semi-supervised techniques for generation (not classification) with specific constraints (sentiment, domain, etc). There are other recent methods such as 'Improving Language Understanding by Generative Pre-Training' by Redford et al that use the idea of generative pre-training with discriminative fine-tuning that are task-agnostic and achieve very good performance - how does the paper compare to this approach? \nThe experiments and analysis is very well written in the paper. Table 4 also shows very interesting, somewhat surprising results in the paper. The authors say that they will release the code and data for this technique which will be useful for the sentiment analysis research community.  \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}