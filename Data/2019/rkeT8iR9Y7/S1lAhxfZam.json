{"title": "The theory looks good but how can it be used?", "review": "\nGradient stochasticity is used to analyse the learning dynamics of SGD. It consists of two aspects: norm stochasticity and directional stochasticity. Although the norm stochasticity is easy to compute, it vanishes when the batch size increases. Therefore, it can be hard to measure the learning dynamics of SGD. The paper is motivated by measuring the learning dynamics by the directional stochasticity. Directly measuring the directional stochasticity with the ange distribution is hard, so the paper uses vMF distribution to approximate the uniformity measurement. The paper theoretically studies the proposed directional uniformity measurement. In addition, the experiments empirically show the directional uniformity measurement is more coherent with the gradient stochasticity.\n\n1. As I\u2019m not a theory person, I\u2019m not very familiar with the related work on this line. But the analysis on the directional uniformity is interesting and original. So is the vMF approximation.\n2. The theoretical analysis looks comprehensive and intuitive. And the authors did a reasonably good job on the experiments.\n3. This paper provides some insights that warn people to pay attention to the directions of SGD. But the paper didn\u2019t provide an answer on how this study can inform people to improve SGD. It\u2019s true that the directional uniformity increases over training and it is correlated to the gradient. But what could this bring us remains unstudied.\n4. Can the authors provide any theoretical or empirical analysis on why the directional uniformity didn\u2019t increase in deep models like CNN and why it increases when BN and Res are applied?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}