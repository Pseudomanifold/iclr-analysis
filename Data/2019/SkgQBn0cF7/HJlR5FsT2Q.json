{"title": "Review of \"Modeling the Long Term Future in Model-Based Reinforcement Learning", "review": "The authors claim that long-term prediction as a key issue in model-based reinforcement learning. Based on that, they propose a fairly specific model to which is then improved with Z-forcing to achieve better performance.\n\n## Major\n\nThe main issue with the paper is that the premise is not convincing to me. It is based on four works which (to me) appear to focus on auto-regressive models. In this submission, latent variable models are considered. The basis for sequential LVMs suffering from these problems is therefore not given by the literature. \n\nThat alone would not be much of an issue, since the problem could also be shown to exist in this context in the paper. But the way I understand the experimental section, the approach without the auxiliary cost is not even evaluated. Therefore, we cannot assess if it is that alone which improves the method. The central hypothesis of the paper is not properly tested.\n\nApart from that, the paper appears to have been written in haste. There are numerous typos in text and in equations (e.g. $dz$ missing from integrals).\n\nTo reconsider my assessment, I think it should be shown that the problem of long-term future prediction exists in the context of sequential LVMs. Maybe this is obvious for ppl more knowledgeable in the field, but this paper fails to make that point by either pointing out relevant references or containing the necessary experiments. Especially since other works have made model-based control work in challenging environments:\n\n- Buesing, Lars, et al. \"Learning and Querying Fast Generative Models for Reinforcement Learning.\" *arXiv preprint arXiv:1802.03006* (2018).\n- Karl, M., Soelch, M., Becker-Ehmck, P., Benbouzid, D., van der Smagt, \n  P., & Bayer, J. (2017). Unsupervised Real-Time Control through \n  Variational Empowerment. *arXiv preprint arXiv:1710.05101*.\n\n## Minor\n\n- The authors chose to use the latent states for planning. This turns the optimisation into a POMDP problem. How is the latent state inferred at run time? How do we assure that the policy is still optimal?\n- Application of learning models to RL is not novel, see references above. But maybe this is a misunderstanding on my side, as the Buesing paper is cited in the related work.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}