{"title": "Good work, but bad presentation", "review": "In this paper, the authors proposed a unified framework which computes spectral decompositions by stochastic gradient descent. This allows learning eigenfunctions over high-dimensional spaces and generating to new data without Nystrom approximation. From technical perspective, the paper is good. Nevertheless, I feel the paper is quite weak from the perspective of presentation. There are a couple of aspects the presentation can be improved from. \n\n(1) I feel the authors should formally define what a Spectral inference network is, especially what the network is composed of, what are the nodes, what are the edges, and the semantics of the network and what's motivation of this type of network.\n\n(2) In Section 3, the paper derives a sequence of formulas, and many of the relevant results were given without being proven or a reference. Although I know the results are most likely to be correct, it does not hurt to make them rigorous. There are also places in the paper, the claim or statement is inclusive. For example, in the end of Section 2.3, \"if the distribution p(x) is unknown, then constructing an explicitly orthonormal function basis may not be possible\". I feel the authors should avoid this type of handwaving claims.  \n\n(3) The authors may consider summarize all the technical contribution in the paper. \n\nOne specific question:\n\nWhat's Omega above formula (6)? Is it the support of x? Is it continuous or discrete? Above formula (8), the authors said \"If omega is a graph\". It is a little bit confusing there. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}