{"title": "interesting use of depth information from simulators as priviledged information for unsupervised domain adaptive segmentation", "review": "The paper focuses on the problem of semantic segmentation across domains. The most standard setting for this task involves real world street images as target and synthetic domains as sources with images produced by simulators of photo-realistic hurban scenes.  This work proposes to leverage further depth information which is actually produced by the simulator together with the source images but which is in general not taken into consideration.\nThe used deep architecture is a GAN where the generator learning is guided by three components: (1) the standard discriminator loss (2) the cross entropy loss for image segmentation that evaluates the correct label assignment to each image pixel (3) an  l1-based loss which evaluates the correct prediction of the depth values in the original and generated image. A further perceptual regularizer is introduced to support the learning.\n\n+ overall the paper is well organized and easy to read\n+ the proposed idea is smart: when starting from a synthetic domain there may be several hidden extra information that are generally neglected but that can instead support the learning task\n+ the experimental results seem promising \n\nStill, I have some concerns\n\n- if the main advantage of the proposed approach is in the introduction of the priviledged information, I would expect that disactivating the related PI loss we should get back to results analogous of those obtained by other competing methods. However from Table 2 it seems that SPIGAN-no-PI is already much better than the  FCN Source baseline in the Cityscape case and much worse in the Vistas case. This should be better clarified -- are the basic structure of SPIGAN and FCN analogous?  \n\n- the ablation does not cover an analysis on the role of the perceptual regularizer. This is also related to the point above: the use of a perceptual loss may introduce a basic difference with respect to competing methods. It should be better discussed.\n\n- section 4.1 mentions the use of a validation set. More details should be provided about it and on how the hyperparameters were chosen.\nA possible analysis on the robustness of the method to those parameters could provide some further intuition about the network stability.\nIt might be also interesting to check if the  the loss weights provide some intuition  about the relative importance of the losses in the learning process.\n\n- the negative transfer rate is another way to measure the advantage of using the PI with respect to not using it. However, since it is not evaluated for the competing methods its value does not add much information and indeed it is only quickly mentioned in the text. It should be better discussed.\n\n- some recent papers have shown better results than those considered here as baseline:\n[Learning to Adapt Structured Output Space for Semantic Segmentation, CVPR 2018]\n[Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training, ECCV 2018]\nthey should be included as related work and considered as reference for the experimental results.\n\nOverall I think that the proposed idea is valuable but the paper should better clarify the points mentioned above.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}