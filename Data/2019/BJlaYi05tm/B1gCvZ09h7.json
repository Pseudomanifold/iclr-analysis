{"title": "interesting work, but not significant enough", "review": "This paper studies the geometry of convolutional networks with ReLU activation functions. It provides some insights into the way neural networks process data. However, the overall quality of this paper is relatively low. \n\nPros:\n\nThis paper introduces a method to compute the preimages of a neural network layer using dual bases defined by the affine mappings of the network. The authors also relate the number of layers in convolutional neural network to the contraction of data. Some interesting phenomena are illustrated using examples. Although not very rigorous, the conclusion of this paper gives some insights on the geometry of convolutional neural networks.\n\nCons:\n\nThe paper is not written very clearly. First of all, the analysis of the paper is based on the assumption that each layer of the neural network has exactly d (the dimension of the input) nodes. However it seems that this assumption is not mentioned anywhere in this paper. Some of the notations are misleading. For example, in Equation (20) d is redefined as a parameter of an affine transform. There are also grammar errors and missing punctuation which I suggest the authors should carefully fix. \n\nThe contribution of this paper is not very significant. No results in this paper can be formulated into rigorous theorem. As previously mentioned, the proposed method to compute preimages only works when all the layers have same amount of nodes, which is not the case in practice. Most of the conclusions the authors make are vague and lack impact. The authors do not convincingly demonstrate how the proposed preimage calculation method can be applied to practically useful network structures. The claim that nested cones efficiently contract input data is not very convincing either, and looks not very relevant to the preimage calculation part of the paper. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}