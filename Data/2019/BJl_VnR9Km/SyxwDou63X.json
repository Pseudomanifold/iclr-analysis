{"title": "SOTA results in video prediction and interesting analysis but the presentation is severely lacking clarity", "review": "Summary:\nThe paper presents a novel architecture for video prediction consisting of a feed-forward path with sparse convolutions and an LSTM generating predictions of chunks of video based on the sequence of input chunks. A feedback path links the LSTMs of the different sparse prediction modules. Experiments in video prediction are performed on moving-MNIST and the KTH action recognition dataset and the model achieves state-of-the-art performance on both. Interestingly, the model is exhibits prediction suppressions effects as have been observed during neurophysiological experiments in the inferotemporal cortex of macaque monkeys. The proposed method exhibits prediction suppression effects also in the lower layers, motivating a neurophysiological experiment in the earlier V1/V2 regions, which yielded an observation similar to the model\u2019s prediction.\n\nStrengths:\nThe performance improvements over competing methods on Moving-MNIST and KTH presented in the experimental section are significant. The analysis seems fairly thorough.\n\nWeaknesses and requests for clarification:\n- The description of the sparse predictive module is difficult to follow, and I am not sure I understood it completely. I find it a bit unintuitive to start the description with the errors, instead of explaining what is computed from beginning to end. The section reads more like a loose description of isolated parts instead of an integrated whole. Maybe walking the reader step-by-step through one complete iteration of the computation helps to clarify this. Also, not every character in equations 1-5 and the algorithm has been defined. For example, what is L? \n- The text makes it sound like the idea of using 3d convolutions in a convLSTM is novel. 3D convLSTMs have been previously used in 3d vision, see \nChoy, C. B., Xu, D., Gwak, J., Chen, K., & Savarese, S. (2016, October). 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In European conference on computer vision (pp. 628-644). Springer, Cham.\nThe application of 3d convLSTMs to video might be new, but the mentioned paper by Choy et al. (2016) should be cited.\n- You mention that padding is used for rows and columns. Are you using padding on the temporal axis as well?\n- The paper seems to be written in a rush, as it contains way too many typos and grammar mistakes, e.g. \u201ca hierarchical of\u201d (should be \u201ca hierarchy of\u201c or just \u201chierarchical\u201d), \u201cfeedforwad\u201d, \u201cExpriment\u201d (section 4 heading), \u201cachievedbetter\u201d, \u201ctrained monkeys to image pairs\u201d, \u201cpervious\u201d, \u201cperserves\u201d, \u201cprocessure\u201d, \u201csequnence\u201d \u201cviusal\u201d. Many typos could have been caught by a spellcheck! This would improve readability a lot!\n- The citations are not properly formatted: (1) If the author names are used as part of the sentence, use e.g. Lotter et al. (2016), else (2) If the author names are not part of the sentence, use (Lotter et al., 2016). These two styles are mixed randomly in the current draft. This makes the manuscript, which already contains a lot of language mistakes, difficult to read.\n- Abbreviations that are used but not introduced: CNN, IT, PSTH, DCNN, LSTM.\n- The related work section could benefit from referring to some of the related work in neuroscience.\n- Adding a sentence explaining the intuition behind using SatLU in equation (1) might be helpful\n\nTo summarize my feedback: I think experimental results and analysis are strong, but the presentation is strongly lacking! The description of the approach definitely needs to be improved to make replication of the results easier. It might help to have someone who doesn\u2019t know the model already read the description and explain it back to you while revising the draft. I hope I could provide some helpful suggestions. I would recommend the manuscript for acceptance, if the presentation is significantly improved!", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}