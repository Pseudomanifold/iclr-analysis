{"title": "explanation could use more work, but a solid idea that seems to work in practice", "review": "Author proposes general framework to use gradient descent to learn a preconditioner related to inverse of the Hessian, or the inverse of Fisher Information matrix, where the inverse may take a particular form, ie, Kronecker-factored form like in KFAC. I have tracked down the implementation of this method by author from earlier paper Li 2018 and verified that it works and speeds up convergence of convolutional networks in terms of number of iterations needed. In particular, Kronecker Factored preconditioner using approach in the paper worked better in terms of wall-clock time on MNIST LeNet5, comparing against an existing PyTorch implementation of KFAC from C\u00e9sar Laurent.\n\n\nSome comments on the paper:\n\nSection 2\nThe key seems to be equation 8. The author provides loss function, the minimum is what is achieved by inverse of the Hessian. Given the importance of the formula, it feels like proof should be included (perhaps in Appendix).\n\nJustification of the criterion is relegated to earlier work in Li (https://arxiv.org/pdf/1512.04202.pdf), but I failed to fully grasp the motivation. There are simpler criteria being introduced, such as criterion 1, equation 17, which simply minimizes the difference between predicted gradient delta and observed, why not use that criterion?\n\nThe justification is given that using inverse Hessian may \"amplify noise\", which I don't buy. When using SGD to solve least-square regression, dividing by Hessian does not have a problem of amplifying noise, so why is this a concern here?\n\n\nSection 3\n\nThe paper should make it clear that empirical Fisher matrix is used, unlike \"unbiased estimate of true Fisher\" which used in many natural gradient papers.\n\nSection 4\nIs \"Lie group\" used anywhere in the derivations? It seems the same algebra holds even without that assumption. The motivation for using \"natural gradient for learning Q\" seems to come from Amari. I have not read that paper, how important it is to use the \"natural\" gradient for learning Q? What if we use regular gradient descent for Q?\n\nSection 7\nFigure 1 showed that Fisher-type criterion didn't work for toy problem, it would be more informative if it used square root of Fisher-type criterion. The square root comes out of regret-analysis (ie, AdaGrad uses square root of gradient covariance)\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}