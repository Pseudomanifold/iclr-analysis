{"title": "well written, interesting approach, well evaluated", "review": "This works proposes a scalable way of approximating the eigenvectors of the Laplacian in RL by optimizing the graph drawing objective on limited sampled states and pairs of states. The authors empirically show the benefits of their method in two different types of goal achieving task. \n\nPros:\n- Well written, well structured, an overall enjoyable read.\n- The related work section appears to be comprehensive and supports the motivations for the presented work.\n- Clear and rigorous derivations. \n- The method is evaluated both in terms of how well it is able to approximate the optimal Laplacian-based representations with limited samples compared to baseline models and how well it solves reward shaping in RL.\n\nCons:\n- In the experimental section, the methods used to learn the policies, DQN and DDPG, should be briefly explained or at least referenced.\n- A further discussion on why the authors chose a half-half mix of the L2 distance and sparse reward could be beneficial. The provided explanation (L2 distance doesn't provide enough gradient) is not very convincing nor justified.\n ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}