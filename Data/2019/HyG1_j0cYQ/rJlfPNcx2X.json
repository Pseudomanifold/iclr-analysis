{"title": "Original idea with promising experimental results, but a limited contribution", "review": "A new method for defending against label noise during training of deep neural networks is presented. The main idea is to \u201cforget\u201d about wrongly labeled examples during training by an ascending step in the gradient direction. This is a meta-approach that can be combined with other methods for noise robustness; the detection of the noisy examples is specific of the base method utilised. Experimental results are promising.\n\nIn general, I find the idea original, and of potential practical use, but I believe the contribution of paper to be limited and not well supported by experiments. Moreover, I think that some claims made in this work are poorly justified.\n\n== Method\n\nThe paper would greatly benefit from some theoretical backing of the proposed optimization scheme, even on a simplified scenario. An idea would be to prove that, given a dataset with noisy labels, PumpOut converges close to the best model (= the one learned without noise), for certain hyperparameters. I think this would be new and interesting. A result of similar fashion was proven in [A].\n\nI found the following arguments not well or only heuristically supported:\n* section 2: the scaling factor \\gamma. Why using \\gamma=1 is suboptimal? One could claim that as much as you want to memorize the true image-label patterns, you also want to forget the image-noise ones.\n* Why MentorNet + PumpOut does not suffer from the selection bias effect of CoTraining? This is unclear to me\n* The non-negative version of the BackwardCorrection, and its appeal to Kiryo et al 17 is interesting, but it sidesteps its justification. A loss that can be negative does not necessarily means that it not lower-bounded. In fact, for a minimization problem to be well-defined, all you need is a lower bounded objective. Then, adding the lower bound makes your loss non-negative. Notice that Patrini et al 17 did not state that BC is unbounded, but only that it can be negative. Can you show more that that -- maybe, at least experimentally?\n\nThe statement of Theorem 2 is trivial. In fact, no proof is given as it would be self-evident. Moreover, the Theorem is not used by PumpOut. Algorithm 3 uses a if-else conditional on the scale of the backward correction, without the max(0, .). I suggest to remove this part. I have notice later that the non-negative version of BC is used as a baseline in the experiment, but I think that is the only use.\n\nRegarding the presentation, in section 3, I suggest to move the explanation of MentorNet and BackwardCorrection before their upgrade by PumpOut.\n\n== Experiments\n\nTable 1 can be removed as these are extremely common datasets.\n\nThe experimental results look very promising for applications. As a side effect of this analysis, I can also notice an improvement over BC given to the nnBC, which is nice per se. Although, I would have strengthen the empirics as follow.\n* SET2 is only run on MNIST. Why not even on CIFAR10 which is used in SET1? Any future reader will wonder \u201cdid it work on CIFAR10?\"\n* A much harder instance of noise, for instance open set [B] or from a real dataset [Xiao et al 15] would have more clearly supported the use of PumpOut for real applications.\n* Can the authors elaborate on \u201cthe choices of \\beta and \\gamma follows Kirkyo et al 2017\u201d ? And how assuming their knowledge gives a fair comparison to BC which does not require them? I believe this is a critical point for the validity of the experiments.\n\nMinor:\n* \u201cLRELU active function\u2019 -> activation function. What is a LRELU? LeakyReLU?\n\n[A] Malach, Eran, and Shai Shalev-Shwartz. \"Decoupling\" when to update\" from\" how to update\".\" Advances in Neural Information Processing Systems. 2017.\n[B] Veit, Andreas, et al. \"Learning From Noisy Large-Scale Datasets With Minimal Supervision.\" CVPR. 2017.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}