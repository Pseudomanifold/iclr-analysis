{"title": "unclear motivation", "review": "In summary, this paper does the following:\n- The initial problem is to analyze the trajectory of SGD in training ANNs in the space of  P of probability measures on Y \\times Y. This problem is interesting, but difficult. \n- the paper constructs a Markov chain that follows a shortest path in TV metric on P\n(the \\alpha SMLC)\n- through experiments, the paper shows that the trajectories of SGD and \\alpha-SMLC have  similar conditional entropy. \n\nMy issues with this paper are:\na/ The main result is a simulation. How general is this? Could it depend on the dataset? Could you provide some intuition or prove that for certain dataset, these two trajectories are the same (or very close)? \nb/ Meaning of this trajectory. This is not the trajectory in P, it is the trajectory of the entropies. In general, is there an intuitive explanation on why these trajectories are similar? And what does it mean -- for example, what would be a possible implication for training SGD? Could it be that all learning methods will have this characteristic parabolic trajectory for entropies? \nc/ The theoretical contribution is minor: both the techniques and results quoted are known. \n\nOverall, I think the paper lacks a take-away. It is an interesting observation that the trajectory of \\alpha-SMLC  is similar to that of SGD in these plots, but the authors have not made a sufficient effort to interpret this. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}