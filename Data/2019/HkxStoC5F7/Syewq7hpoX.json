{"title": "Few-shot learning, based on amortized inference network for parameters of logistic regression head models. Uses learning criterion based on predictive distributions on train/test splits. Extensive comparison, achieves state-of-the-art despite simpler setup than many competitors", "review": "Summary:\nThis work tackles few-shot (or meta) learning from a probabilistic inference viewpoint. Compared to previous work, it uses a simpler setup, performing task-specific inference only for single-layer head models, and employs an objective based on predictive distributions on train/test splits for each task (rather than an approximation to log marginal likelihood). Inference is done amortized by a network, whose input is the task training split. The same network is used for parameters of each class (only feeding training points of that class), which allows an arbitrary number of classes per task. At test time, inference just requires forward passes through this network, attractive compared to non-amortized approaches which need optimization or gradients here.\n\nIt provides a clean, decision-theoretic derivation, and clarifies relationships to previous work. The experimental results are encouraging: the method achieves a new best on 5-way, 5-shot miniImageNet, despite the simple setup. In general, explanations in the main text could be more complete (see questions). I'd recommend shortening Section 4, which is pretty obvious.\n\n- Quality: Several interesting differences to prior work. Well-done experiments\n- Clarity: Clean derivation, easy to understand. Some details could be spelled out better\n- Originality: Several important novelties (predictive criterion, simple model setup, amortized inference network). Closely related to \"neural processes\" work, but this happened roughly at the same time\n- Significance: The few-shot learning results are competitive, in particular given they use a simpler model setup than most previous work. I am not an expert on these kind of experiments, but I found the comparisons fair and rather extensive\n\nInteresting about this work:\n- Clean Bayesian decision-theoretic viewpoint. Key question is of course whether\n   an inference network of this simple structure (no correlations, sum combination\n   of datapoints, same network for each class) can deliver a good approximation to\n   the true posterior.\n- Different to previous work, task-specific inference is done only on the weights of\n   single-layer head models (logistic regression models, with shared features).\n   Highly encouraging that this is sufficient for state-of-the-art few-shot classification\n   performance. The authors could be more clear about this point.\n- Simple and efficient amortized inference model, which along with the neural\n   network features, is learned on all data jointly\n- Optimization criterion is based on predictive distributions on train/test splits, not\n   on the log marginal likelihood. Has some odd consequences (question below),\n   but clearly works better for few-shot classification\n\nExperiments:\n- 5.1: Convincing results, in particular given the simplicity of the model setup and\n   the inference network. But some important points are not explained:\n   - Which of the competitors (if any) use the same restricted model setup (inference\n      only on the top-layer weights)? Clearly, MAML does not, right? Please state this\n      explicitly.\n   - For Versa, you use k_c training and 15 test points per task update during\n      training. Do competitors without train/test split also get k_c + 15 points, or\n      only k_c points? The former would be fair, the latter not so much.\n- 5.2: This seems a challenging problem, and both your numbers and reconstructions\n   look better than the competitor. I cannot say more, based on the very brief\n   explanations provided here.\n   The main paper does not really state what the model or the likelihood is. From\n   F.4 in the Appendix, this model does not have the form of your classification\n   models, but psi is input at the bottom of the network. Also, the final layer has\n   sigmoid activation. What likelihood do you use?\n   One observation: If you used the same \"inference on final layer weights\" setup\n   here, and Gaussian likelihood, you could compute the posterior over psi in closed\n   form, no amortization needed. Would this setup apply to your problem?\n\nFurther questions:\n- Confused about the input to the inference network. Real Bayesian inference would\n   just see features h_theta(x) as inputs, not the x's. Why not simply feed features in\n   then?\n   Please do improve the description of the inference network, this is a major\n   novelty of this paper, and even the appendix is only understandable by reading\n   other work as well. Be clear how it depends on theta (I think nothing is lost by\n   feeding in the h_theta(x)).\n- The learning criterion based on predictive distributions on train/test splits seem\n   to work better than ELBO-like criteria, for few-shot classification.\n   But there are some worrying aspects. The marginal likelihood has an Occam's\n   razor argument to prevent overfitting. Why would your criterion prevent overfitting?\n   And it is quite worrying that the prior p(psi | theta) drops out of the method\n   entirely. Can you comment more on that?\n\nSmall:\n- p(psi_t | tilde{x}_t, D_t, theta) should be p(psi_t | D_t, theta). Please avoid a more\n   general notation early on, if you do not do it later on. This is confusing\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}