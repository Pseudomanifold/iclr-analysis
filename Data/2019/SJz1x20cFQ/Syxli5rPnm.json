{"title": "Good results, major assumption", "review": "Brief summary: \nHRL method which uses a 2 level hierarchy for sparse reward tasks. The low level policies are only provided access to proprioceptive parts of the observation, and are trained to maximize change in the non-proprioceptive part of the state as reward. The higher level policy is trained as usual by commanding lower level policies. \n\nOverall impression:\nI think the paper has a major assumption about the separation of internal and external state, thereby setting the form of the low level primitives. This may not be fully general, but is particularly useful for the classes of tasks shown here as seen from the strong results. I would like to see the method applied more generally to other robotic tasks, and a comparison to Florensa et al. And perhaps the addition of a video which shows the learned behaviors. \n\nIntroduction: \nthe difficulty of learning a high-level controller when the low-level policies shifts -> look at \u201cdata efficient hierarchical reinforcement learning\u201d (Nachum et al)\n\nThe basic assumption that we can separate out observations into proprioceptive and not proprioceptive can often be difficult. For example with visual inputs or entangled state representations, this might be very challenging to extract. This idea seems to be very heavily based on what is \u201cinternal\u201d and what is \u201cexternal\u201d to the agent, which may be quite challenging to separate. \n\nThe introduction of phase functions seems to be very specific to locomotion?\n\u2028Related work: \nThe connection of learning diverse policies should be discussed with Florensa et al, since they also perform something similar with their mutual information term. DeepMimic, DeepLoco (Peng et al) also use phase information in the state, worthwhile to cite. \n\nSection 3.1:\nThe pros and cons of making the assumption that representation is disentangled enough to make this separation, should be discussed. \n\nAlso, the internal and external state should be discussed with a concrete example, for the ant for example. \n\nSection 3.2:\nThe objective for learning diverse policies is in some sense more general than Florensa et al, but in the same vein of thinking. What are the pros and cons of this approach over that?\u2028\u2028The objective is greedy in the change of external state. We\u2019d instead like something that over the whole trajectory maximizes change?\n\nSection 3.3: \u2028How well would these cyclic objectives work in a non-locomotion setting? For example manipulation\n\nSection 3.4:\u2028This formulation is really quite standard in many HRL methods such as options framework. The details can be significantly cut down, and not presented as a novel contribution. \n\nExperiments:\nIt is quite cool that Figure 2 shows very significant movement, but in some sense this is already supervised to say \u201cmove the CoM a lot\u201d. This should be compared with explicitly optimizing for such an objective, as in Florensa et al. I\u2019m not sure that this would qualify as \u201cunsupervised\u201d per se. As in it too is using a particular set of pre-training tasks, just decided by the form of choosing internal and external state.\n\nall of the baselines fail to get close to the goal locations.-> this is a bit surprising? Why are all the methods performing this poorly even when rewarded for moving the agent as much as possible.\n\nOverall, the results are pretty impressive. A video would be a great addition to the paper. \n\nComparison to Eysenbach et al isn\u2019t quite fair since that method receives less information. If given the extra information, the HRL method performs much better (as indicated by the ant waypoint plot in that paper).", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}