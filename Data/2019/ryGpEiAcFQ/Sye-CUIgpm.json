{"title": "Not clear if this is -- or will be -- a practically useful approach", "review": "The authors propose a hybrid neural network, composed of a synapse graph that can be embedded into and a standard neural network, such that the entire architecture can be trained in a way that is compatible with the gradient descent and backpropagation of. As a proof of concept, the hybrid architecture is trained to classify MNIST.\n\nI am not convinced by the way this work is motivated. What problem are the authors actually addressing? Just because biological neurons use synapses does not mean we should try hard to put a certain instance of them into deep neural networks. Clearly this is not an attempt to add to neuroscience, as beyond the inspiration of neurons having synapses, there is little attempt to biologically plausible. As an attempt to add to machine learning research, the neuroscience motivation is unconvincing. Provided that the math works out (and I admit that I did not attempt to follow the detailed derivations), this looks like an interesting intellectual exercise, but it also seems a bit like a discovery of a hammer that is in need for nails to be applied to. And it\u2019s not even clear to me how practical the hammer would actually be, even if we had a convincing problem setting at hand. How scalable is it beyond toy-settings? The final sentence makes a tantalizing claim, but at this stage the work has to resort to promising potential, rather than being able to demonstrate that it is practically useful.\n\nMoreover, this work is not presented right for the venue and audience, and would need substantial rewriting and restructuring to make the central claims and contributions sufficiently clear. ", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}