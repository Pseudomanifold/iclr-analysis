{"title": "Possibly important paper.", "review": "Revision 2: The new comparisons with CPC are very helpful.  Most of my other comments are addressed in the response and paper revision.  I am still uncomfortable with the sentence \"Our method ... compares favorably with fully-supervised learning on several classification tasks in the settings studied.\"  This strongly suggests to me that you are claiming to be competitive with SOTA supervised methods.  The paper does not contain supervised results for the resnet-50 architecture.  I would recommend that this sentence should either be dropped from the abstract or have the phrase \"in the settings studied\" replaced by \"for an alexnet architecture\".  If you have supervised results for resnet-50 they should be added to table 3 and the abstract could be adjusted to that.  I apologize that this is coming after the update deadline (I have been traveling).  The authors should simply consider the reaction of the community to over-claiming.  Because of the new comparisons with CPC on resnet-50 I am upping my score.  My confidence is low only because the real significance can only be judged over time.\n\nRevision 1: This is a revision of my earlier review.  My overly-excited earlier rating was based on tables 1 and 2 and the claim to have unsupervised features that are competitive with fully-supervised features. (I also am subject to an a-priori bias in favor of mutual information methods.)  I took the authors word for their claim and submitted the review without investigating existing results on CIFAR10.  It seems that tables 1 and 2 are presenting extremely weak fully supervised baselines.  If DIM(L) can indeed produce features that are competitive with state of the art fully supervised features, the result is extremely important.  But this claim seems misrepresented in the paper.\n\nOriginal review:\n\nThere is a lot of material in this paper and I respect this groups\nhigh research-to-publication ratio. However, it might be nice to have\nthe paper more focused on the subset of ideas that seem to matter.\n\nMy biggest comment is that the top level spin seems wrong.\nSpecifically, the paper focuses on the two bullets on page 3 ---\nmutual information and statistical constraints.  Here mutual\ninformation is interpreted as the information between the input and\noutput of a feature encoder.  Clearly this has a trivial solution\nwhere the input equals the output so the second bullet --- statistical\nconstraints --- are required.  But the empirical content of the paper\nstrongly undermines these top level bullets.  Setting the training\nobjective to be the a balance of MI between input and output under a\nstatistical consrtraint leads to DIM(G) which, according the results in\nthe paper, is an empirical disaster.  DIM(L) is the main result and\nsomething else seems to be going on there (more later).  Furthermore,\nthe empirical results suggest that the second bullet --- statistical\nconstraints --- is of very little value for DIM(L). The key ablation\nstudy here seems to be missing from the paper.  Appendix A.4 states\nthat \"a small amount of the [statistical constraint] helps improve\nclassification results when used with the [local information\nobjective].  No quantitative ablation number is given.  Other measures\nof the statistical constraint seem to simply measure to what extent\nthe constraint has been successfully enforced.  But the results\nsuggest that even successfully enforcing the constraint is of little,\nif any, value for the ability of the features to be effective in\nprediction.  So, it seems to me, the paper to really just about the\nlocal information objective.\n\nThe real power house of the paper --- the local information objective\n--- seems related to mutual information predictive coding as\nformalized in the recent paper from deep mind by van den Oord et al\nand also an earlier arxiv paper by McAllester on information-theoretic\nco-training.  In these other papers one assumes a signal x_1, ... x_T\nand tries to extract low dimensional features F(x_t) such that F(x_1),\n..., F(x_t) carries large mutual information with F(x_{t+1}).  The\nlocal objective of this paper takes a signal x1, ..., x_k (nXn\nsubimages) and extracts local features F(x_1), ... F(x_k) and a global\nfeature Y(F(x_1), ..., F(x_k)) such that Y carries large mutual\ninformation with each of the features F(x_i).  These seem different\nbut related.  The first seems more \"on line\" while the second seems\nmore \"batch\" but both seem to be getting at the same thing, especially\nwhen Y is low dimensional.\n\nAnother comment about top level spin involves the Donsker-Varadhan\nrepresentation of KL divergence (equation (2) in the paper).  The\npaper states that this is not used in the experiments.  This suggests\nthat it was tried and failed.  If so, it would be good to report this.\nAnother contribution of the paper seems to be that the mutual\ninformation estimators (4) and (5) dominate (2) in practice.  This\nseems important.\n\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}