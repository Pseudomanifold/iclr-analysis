{"title": "Minor novelty", "review": "In this paper a method for pose estimation is proposed, which is based on the well known neural model \u201cstacked hourglass networks\u201d. The novelty in the proposed paper is a multi-scale formulation, which creates multiple scales from the input image and feeds them into different hourglass modules. The different scales are weighted differently, where the weights for a given scale depend on the error obtained on previous scales.\n\nImportant issues:\n\nThe novelty seems to be not sufficient to me, as multi-scale solutions are not new in computer vision and have been applied a lot in pose estimation as well, be it deep neural models or older techniques. In particular in deep models, multi-scale techniques have been proposed extensively for resolution preserving image to image mappings (which is done here), beginning with quite \u201cold\u201d techniques (in deep learning time scales) going back to 2012 (Farabet, C., Couprie, C., Najman, L., LeCun, Y., 2012. Scene parsing with multiscale feature learning, purity trees, and optimal covers. In: ICML.), or formulations which integrate scales and layers, starting with hyper-columns in 2015 (Hypercolumns for object segmentation and fine-grained localization, CVPR 2015) and many more recent variants.\n\nIn 2018, multi-scale formulations are now standard techniques in computer vision with deep networks. I am not sure how the proposed method makes a difference. Also, I am wondering whether there shouldn\u2019t be some parameter sharing between the models of the different scales, as is often done in the literature now to reduce model capacity.\n\nThe way the multi-scale Since different resolutions are created is particular. Why not just simply subsample the input images to different resolutions? Why are these trained layers needed as a preprocessing?\n\nWeighting different scales is not fundamentally new. We also don\u2019t know whether it improves performance, it is not part of the ablation study.\n\nThe method has been compared to several methods, but which are not state of the art anymore. Most papers are from 2016, the field advanced quickly. The performance gains of the multi-scale formulation are pretty low, and overally, the method is not state of the art on the targeted benchmark. Obviously I do not want to say that a paper needs to be state of the art on a benchmark to get published, even at a top-level conference, far from it. However, if the methodological and theoretical contribution of the paper is rather minor, than the performance and evaluation should be flawless.\n\nMinor remarks:\n\nThe writing of this paper is somewhat fuzzy, using non-standard technical language, which I could not decipher and which seems to me somewhat misleading. Examples are:\n\n- \u201cthe stacked hourglass network theoretically increases the stacked depth\u201d: how does theory tell us anything about depth in this context?\n\n- \u201cdifficult to form differentiated and determined collaboration mechanisms for each stacked hourglass\u201d: I don\u2019t understand what this means, simply. This phrase is also repeated several times in the paper in different places\n\n- \u201cinformation loss caused by the functional consistency of hourglass networks\u201d: I don\u2019t understand what functional consistency means here, and why it leads to information loss.\n\nThe description of Algorithm 1 does not seem to be necessary to me. It basically follows from the equations 4-6, which are executed in order and per layer. \n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}