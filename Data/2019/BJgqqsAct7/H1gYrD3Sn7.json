{"title": "a nice bound for the ImageNet  ", "review": "The paper presents an application of PAC-Bayesian bounds to the problem of \nImangeNet classification (a deep neural network model). The authors provide \ninteresting empirical bounds for the risk of the ImageNet classifier. More specifically, \nthe authors introduce some clever choices for the prior distribution (on the \nhypothesis space) that allow one to incoperate a compression scheme and obtain \na (non-vacuous) bound for the predictor. \nOverall, This is an original work with clear presentation.\n\nMajor comments:\n1). In Theorem 2.1, why do you need \\lambda > 1 ?\nTo my knowledge, \\lambda only needs to be positive.\nWhy do you have to introduce the parameter \\alpha here? \nand consequently the additional log term?\n2) It is unclear for me, why are your bounds non-vacuous?\nProbably, a more clear explanation of Theorem 4.3 is to be required.\nAlso, some comparisions with the bounds in [Neyshabur et al 2018] and [Barlett et al 2017]\nwould make the paper more significant and interesting.\n\nMinor comments:\n1) in Theorem 2.1, after the formula (3), the \\Phi^{-1} should be  \\Phi^{-1}_{\\gamma}.\n2) in the sentence, page 4,: \"To strengthen a na\u00efve Occam bound, we use the idea that that deep networks are insensitive to mild... \"   an extra \"that\" should be removed.\n3) in Section 5, the first paragraph, in sentence:  \"The lone exception is Dziugaite & Roy (2017), which succeeds by ....\"\nshould be \"The one exception....\"\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}