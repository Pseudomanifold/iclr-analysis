{"title": "Want to see the generated videos", "review": "This paper discusses how to synthesize videos from texts using a text-conditioning scheme. The results are shown on a synthetic moving shape dataset, the Kinetics human action dataset, and CUB bird dataset.\n\nThe proposed algorithm is quite reasonable to me; however, I hope the experimental results could be more convincing.  More specifically,\n\n- There is no comparison against previous works (e.g. T2V by Li et al 2018) in the moving shape experiment. \n\n- The Kinetics experiment is most exciting. However, the submission should provide generated videos for the readers and reviewers. It is a paper of generating videos, so the audience would want to watch these videos.\n\n- The CUB bird dataset is irrelevant since it is all images, not videos.\n\nAt last, it will be nice to provide the optimization details and conduct other ablation experiments using different hyperparameters.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}