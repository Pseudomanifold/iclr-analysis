{"title": "Review", "review": "This paper proposes a new technique for detecting adversarial examples by introducing \"fingerprints\" into the landscape while training, and exploiting the fingerprints at test time to detect adversarial examples. The idea is novel and the paper is well-written, but concerns about gradient masking prevent me from recommending acceptance just yet.\n\nPositives: \nThe paper is extremely well-written, and the approach is clear and presented well. The authors also clearly put significant effort into the evaluation, and accurately/consistently describe threat models that they consider. The approach is also clearly novel, and is interesting. \n\nConcerns:\nMy biggest concern is that this detection mechanism masks gradients in its loss function. The two reasons I strongly believe this is the case are (a) Figure 5 and (b) the authors themselves state that their loss is highly non-convex and that no gradient-based method may be able to find a solution. This, however, does not guarantee robustness (see [1] for why such \u201cunfriendly\u201d landscapes can usually be circumvented)\n\nSome concrete evaluation concerns and experiments that the authors can run to alleviate them:\n- Figure 5 shows adversarial robustness even against eta = 0.25---at this value of epsilon, the attacker should be able to create realistic images of other classes (even without PGD), so this suggests that the loss is somehow making examples hard to find rather than removing them. The authors should address this issue.\n- Showing that at a sufficiently high eta attacks start to succeed is also useful\n- Running SPSA, CarliniL2-FP, and PGD for *many* more iterations and using *many* more steps for binary search (right now it looks like the binary search is looking in a space of size 10^6 with 10 steps, which only has a granularity of about 5k, which means you never see any value < 5k in a bisection search, which casts into doubt all of these results)\n- The AUC should monotonically degrade with eta (this is another indication the attacks might not be running for long enough)\n- The method does not seem to be specific to L-infinity constraint. To this end, a version of Figure 5 in the L2 case would be extremely useful in understanding the detection method.\n\nI also apologize if some of these concerns about gradient masking seem unsubstantiated; that said, I tried to run the code given in the paper, but got several OOM and other errors (utils modules not found, and PyTorch deprecations), even on a machine with 8 12GB-memory GPUs. If the authors can provide instructions for running the code I will be happy to test it and alleviate some of my own concerns. I also tried to reimplement the approach, but did not manage to finish before the review deadline. If I am able to reimplement the approach I will update my review accordingly.\n\nSome smaller comments on the paper:\nA consolidated set of tables for attack parameters in an appendix is needed\n- Page 4 last paragraph line 4 find the subset that \u201csatisfies\u201d instead of \u201csatisfy\u201d\n- Page 5 paragraph 1 line 1 defender,for needs a space before for\n- Page 5 paragraph right before theorem 1 last line Here, for detection needs a , after detection \n- Page 7 paragraph 1 line 2 (2 hidden layers the 2 should be written two\n- Page 7 second last paragraph line 2 \u201cis chosen\u201d instead of \u201care chosen\u201d\n- Page 7 last paragraph line 2 \u201cacross attacks\u201d needs a , after \n- Page 9 table 6 label line 2 \u201cdoes not shown\u201d should be \u201cshow\u201d instead of \u201cshown\u201d \n- Page 9 last line \u201cmeasure of robustness\u201d remove \"of\"\n\n[1] https://arxiv.org/pdf/1802.00420.pdf", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}