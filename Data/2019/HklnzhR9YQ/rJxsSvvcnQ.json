{"title": "Approximate block sparse fully connected neural networks, the Barron class and the Holder class using the Residual CNNs", "review": "The authors demonstrate the function expression properties for the Residual type convolutional neural networks to approximate the block sparse fully connected neural networks. Then it is shown that such Res-CNNs can approximate any function as long as it can be expressed by the block-sparse FNNs, including the Barron class and Holder class functions. The price to pay is that the number of parameters is larger than that of the FNNs by a constant factor. \n\nThe idea for connecting the expressive ability of CNNs with FNNs is interesting, which can fully take advantage of the power of FNNs to understand CNNs. However, it is not very clear how the convolutional structure of CNNs help in the analysis of approximating FNNs. For example, in the analysis of C.1 and C.2, it will help better understand why CNNs may work from a high-level intuition when the authors construct the filters. \n\nMoreover, it will also help better understand the expressive power of CNNs if the authors can provide some extended discussion on why approximating the block-sparse FNNs rather than arbitrary feed-forward networks. Is there any fundamental reason (or a counterexample) this cannot be realized, or is there to some extent a technical barrier in the analysis? \n\nMinor issue\n\nOn page 20, \u201cBounds residual blocks\u201d -> \u201cBounds for residual blocks\u201d\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}