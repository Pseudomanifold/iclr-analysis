{"title": "The block sparse structure seems unnecessary given the results in [Schmidt-Hieber 2017].", "review": "This manuscript shows the statistical error of the ERM for nonparametric regression using the family of a Resnet-type of CNNs. Specifically, two results are showed. First, the authors show that any block-sparse fully connected neural network can be embedded in CNNs. Second, they show the covering number of the family of CNNs. Combining with the existing results of the approximation error of neural nets (Klusowski&Barron 2016, Yarotsky 2017, Schmidt-Hieber 2017), they show the L2 statistical risk. \n\nDetailed comments:\n\n1. The intuition of using block-sparse FNN seems unclear. It seems that when $M=1$, it reduces to the sparse NN considered in [Schmidt-Hieber 2017]. In the proof of Corollary 5, the authors directly use the error of approximating Holder smooth function by sparse FNN and show that the construction in [Schmidt-Hieber 2017] is actually block-sparse. Thus, it seems unclear why we should consider such block-sparse family. Can any sparse NN be embedded in the family of CNNs?\n\n2. In the Related Work, the authors only compare with 2 previous work on the approximation error of CNN. Actually, this work is more related to [Schmidt-Hieber 2017] due to borrowing the results. It would be better to see what the novelties are compared with that work, especially in terms of the proof techniques.\n\n3. The authors claim that the construction of approximator for Holder functions in [Schmidt-Hieber 2017] is block sparse. It would be nice to give more details of the construction since this is not claimed in [Schmidt-Hieber 2017].", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}