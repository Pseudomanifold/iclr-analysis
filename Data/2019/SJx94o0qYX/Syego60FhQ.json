{"title": "This paper proposes to keep a high activation/gradient flow in two special kinds of networks structures, namely ResNet and LSTM. For ResNet, the skip connections are made high-precision by adding the skip connection before quantization. For LSTM, the cell and hidden state computations are of high precision.", "review": "The proposed method is advantageous in that it only requires changes to some parts of the original ResNet or LSTM, without having to significantly change the network structure or training algorithm. It also reports empirical success of using high-precision skip connections in ResNet and cell/hidden state updates in LSTMs.\n\nHowever, it is unclear why it is necessary to keep a high-precision activation/gradient flow. What is the problem with existing quantized networks that do not have these high-precision-flow? Also, how does the high-precision flow interact with the rest of the network (with low-precision operations)?\n\nMoreover, the proposed method has limited novelty as the use of full-precision skip connections has been proposed in Bi-Real (Liu et al. 2018).\n\nMinor:\n- It is hard to tell that the weight histogram in Figure 3 is similar to a Laplacian distribution. It can also be approximated by other distributions (such as Gaussian or piecewise-linear distributions).\n- What kind of activation quantization is used?\n- In the experiments, when is the cosine similarity between the quantized and full-precision networks computed? after training or on an intermediate training step?\n- What are the axes in Figure 5? Why is there only one local minimum in Figure 5(d)? Why the training with PH converges even slower than without PH at the early stage of training?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}