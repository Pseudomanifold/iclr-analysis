{"title": "An OK paper but need more evaluation.", "review": "This paper investigates the problem of neural network quantization. The main idea is to employ an end-to-end precision highway to reduce the accumulated quantization error and meanwhile enable ultra-low precision in deep neural networks.  The experimental results on the 3- and 2-bit quantizations of ResNet-18/50 and 2-bit quantization of an LSTM model demonstrate the effectiveness of the proposed method. \n\nThis paper is well written and organized. The idea of utilizing a high-precision information flow to reduce the accumulated quantization error is technically sound. The empirical studies on accumulated quantization error, loss surface analysis, model performance, and hardware cost are quite thorough and solid. \n\nThe idea of precision highway, however, is quite similar to the skip connections used in Bi-Real Net. Therefore, it may be a good idea to provide a thorough discussion over these two different methods so as to make the distinction.\n\nIn Table 2, the results of Bi-Real Net is based upon 1 bit activation/weight quantization, while the proposed method uses 2 bit activation/weight quantization. To give a fair comparison, it may be better to provide 1 bit activation/weight quantization results of the proposed method.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}