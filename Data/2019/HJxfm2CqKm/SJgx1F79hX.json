{"title": "OK paper. Well written, but weak novelty.", "review": "Summary: This paper studies the recently problem of learning active learning (LAL). It sets up a MDP where the the state is determined by the labeled, unlabelled datasets and classifier, the acton is to query a point, the reward is linked to classifier test set performance improvement and the transition is to update the base classifier. Recent Q-learning algorithms are used to perform the optimisation. The results show that it outperforms some classic handcrafted AL algorithms and some prior LAL algorithms. A feature of this paper is that the method is relatively simple compared to some prior LAL methods, and also that it learns policies that can transfer successfully across diverse heterogenous datasets.\n\nStrengths:\n+ Good results. \n+ Nice that it works well while being simpler and faster than prior transferrable method MLP-GAL.\n+ Generally well written.\n+ Fig 4 is interesting.\n\nWeaknesses:\n- Novelty/originality is rather incremental. \n- Experiments are still on toy datasets.\n\nSpecifics:\n1. Novelty: The concept of formulating AL as a MDP for optimisation is now a standard idea. The optimisers used are recent off-the-shelf Q-learners. The result is that this method is similar to a non-myopic extension of LAL (Konyushkova\u201917) but several papers already did non-myopic AL. In particular it\u2019s very similar to the SingleRL method in (Pang\u201918). The only differences are smallish design parameters like: slightly different reward function definition, use Q-learning instead of policy-gradient optimiser, and slightly different state featurisation. The improved sample/speed-efficiency vs SingleRL is likely relatively automatic due to use of recent Q-learning optimisers, rather than vanilla PG optimiser of SingleRL. Not clear that benefit comes from something uniquely contributed here. Other limitations of various prior LAL work, such as binary classifier only, are not alleviated here.\n2. Experiments: The experiments are on toy datasets. Particularly given the small novelty, then evaluation should be much more. For example: 1. How well does it work when transferred to a relatively less toy dataset such as CIFAR. 2. To what extent can it transfer across classifiers rather than only across datasets? \n3. The state representation as a sorted list of scores is rather unintuitive. Is there any intuition on what smart decisions the model could be using this to make?\n4. The featurisations used are not very standard: Like the classifier state sorted score list, and the action featurisation (instance score, instance distance to class, instance distance to unlabelled). It would be good to evaluate this featurisation with a supervised active learner (like LAL), in order to disambiguate whether the good performance comes from these feature choices, or from the recent RL algorithms used to optimise. Similarly for the choice of reward function.\n5. How does the proposed method deal with a suite of training datasets for AL that are of greatly varying difficulty. A relatively very easy dataset needing << 100 examples to reach threshold would generate few AL training examples due to early stopping. A very hard dataset might use all 100 examples. Does it mean that easy datasets contribute less to training than hard ones? ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}