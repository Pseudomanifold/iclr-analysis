{"title": "Not ready for publication", "review": "The paper aims at justifying the performance gain that is acquired by the use of \"composite\" neural networks (e.g., composed of a pre-trained neural network and additional layers that will be trained for the new task).\n\nI found the paper lacking in terms of writing and in terms of clarity in expressing scientific/mathematical ideas especially for a theory paper.\n\nExample from the Abstract:\n\n\"The advantages of adopting such a pre-trained model in a composite neural network are two folds. One is to benefit from other\u2019s intelligence and diligence, and the other is saving the efforts in data preparation and resources\nand time in training\"\n\nThe main results of the paper (Theorem 1,2,3) are of the following nature: if you use more features (i.e., \"components\") in the input of a network then you have \"more information\", and this cannot be bad. Here are the corresponding claims in the Abstract:\n\n\"we prove that a composite neural network, with high probability, performs better than any of its pre-trained components under certain assumptions.\"\n\n\"if an extra pre-trained component is added to a composite network, with high probability the overall performance will be improved.\"\n\nHowever, this argument seems to be just about expressiveness; adding more features can be statistically problematic. \n\nFurthermore, why is it specific to pre-trained components? Essentially the theorems are about adding any features.\n\nFinally, the assumption that the pre-trained components are linearly independent is invalid and the makes the whole analysis somewhat simplistic.\n\n\nThe motivating Example 1 just shows that the convex hull of a class of hypotheses can include more hypotheses than the class itself. I don't see any connection between this and the use of pre-training.\n\nOther examples unclear statements from the intro:\n\n\"One of distinctive features of the complicated applications is their applicable data sources are boundless. Consequently, their solutions need frequent revisions.\"\n\n\"Although neural networks can approximate arbitrary functions as close as possible (Hornik, 1991), the major reason for not existing such competent neural networks for those complicated applications is their problems are hardly fully understood and their applicable data sources cannot be identified all at once.\"\n\nThere are many typos in the paper including this one about X for the XOR function:\n\"Assume there is a set of locations indexed as X = {(0; 0); (0; 1); (1; 0); (1; 0)} with the corresponding values Y = (0; 1; 1; 0). Obviously, the observed function is the XOR\"\n\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}