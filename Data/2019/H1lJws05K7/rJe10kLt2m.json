{"title": "Interesting solid work but few concerns remain", "review": "Studying properties of random networks in the infinite width limit, this work suggests guidance for choosing initialization and activation function. \n\nIn my opinion, novel contribution comes for guidance for choosing activation functions and theoretical grounds for superior performance of ```'swish\u2019 activation function. \n\nI have two main concerns :\n\nIn terms of selection on initialization, the findings seem to be mostly discussed already in Schoenholz et al (2017) [1]. In their work, Edge of Chaos is critical line separating different phases and was already shown to have power-law decay rather than exponential decay. As far as I can tell, analysis on EOC on ReLU-like activations are different from Schoenholz et al (2017) [1]. Some of the results for ReLU are already appeared in the literature e.g. Lee et al (2018) [2].\n\nAnother main concern is in the author\u2019s experimental setup. It is hard to draw conclusions when comparison experiments were done with a fixed learning rate.  As we know learning rate is one of the most critical hyperparameter for determining performance and optimal learning rate is often sensitive to architecture choice. Especially for different non-linearity and different depth/width optimal learning rate can change.\n\nPros: \n - Clearly written and easy to understand what authors are trying to say \n - Interesting theoretical support for activation function which recently got attention due to boosting performance in neural networks\n - Nice suggestion of choosing activation function for deep networks (Proposition 4)\n      -- ELU/SELU/Softplus/Swish all satisfy this suggestion\nCons:\n - Novelty may be not strong enough as the standard analysis tool from [1] was mostly used\n - Experimental setup may suffer from some critical flaw \n\nFew comments/questions:\n- P3: Is M_{ReLU} = 2 correct, from ReLU EOC, shouldn\u2019t it be \u00bd?\n- For all the works using activation functions satisfying Proposition 4 (ELU/SELU/Softplus/Swish), the initialization scheme close to EOC? Does this work\u2019s analysis actually explain performance boost over ReLU for these activation functions?\n\n[1] S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 5th International Conference on Learning Representations, 2017.\n[2] J. Lee, Y. Bahri, R. Novak, S.S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. 6th International Conference on Learning Representations, 2018.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}