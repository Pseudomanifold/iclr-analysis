{"title": "Shared video-text embedding learnt with AEs and Adversarial learning applied to three diverse tasks. The paper has novel aspects and is well evaluated.", "review": "Summary:\nThe paper aims to learn a common embedding space for video appearance and text caption features. The learned shared embedding space then allows multiple applications of zero-shot activity classification, unsupervised activity discovery and unseen activity captioning.\n\nThe method is based on two autoencoders which have a common intermediate space. The losses optimized encourage the standard unimodal reconstructions in the AEs, along with joint embedding distances (appearance and text of the same video are encoded close by) as well as cross domain mapping (video encoding generates text and vice-versa), and cycle consistency. Apart from these additional supervised losses, unsupervised losses are added with adversarial learning which aim to bring the video and text encoding distributions in the common space close, as well as the standard real and generated video/text distributions close by adding corresponding discriminators (like in GANs). The whole system is learned end-to-end in two phases, first with supervised paired data and then with all paired and unpaired data.\n\nThe experiments are shown on four datasets: ActivityNet, HMDB, UCF101, MLB-YouTube\n\nPositives:\t\n- The problem of multimodal learning is an interesting and challenging problem\n- The paper is novel; while the idea of a joint shared embedding space is not new this paper adds new losses as summarized above and shows reasonably convincing empirical results\n- The results are shown for diverse applications which highlight the generality of the method\n- The use of unpaired/unsupervised data is also relatively less explored which this paper incorporates as well\n- The empirical results given are convincing, eg Tab1 gives a good ablation study showing how the different components affect the performance. SoA comparison are given on a standard task (however see below)\n\nNegatives:\n- Comparison with state of the art result Tab2 should also contain the features used. The performances might just be higher due to the better features used (Kinetics pretrained I3D). Please give a self implemented baseline method with same features but some standard loss in the shared space to give an idea of the strength of the features.\n- Ideally features provided by previous papers\u2019 authors should be used if available and it should be shown that the proposed method improves results.\n\nOverall the paper is well written and had novel aspects which are convincingly evaluated on challenging and diverse tasks.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}