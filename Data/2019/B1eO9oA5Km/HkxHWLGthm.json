{"title": "Not easy to follow; experiments not convincing", "review": "[Summary]\nThis paper proposes an extension of the dual learning framework, with a guider network and multiple languages included: (1) Each language $i$ has a guider network $GN_i$, that can be used to reconstruct the source sentence from either the output of the encoder or the output of the decoder. (2) Multiple languages are used in this framework, where each language also has a $GN_i$  for guiding the training according to the reconstruction error. The authors work on MultiUN dataset to verify their algorithms.\n \n[Clarity]\nThis work is not easy to follow. My suggestions to revise the paper are shown as follows:\n(1) Please use the \\begin{equation}\u2026\\end{equation} environment to clearly describe your framework and training objectives, with each notation, function and hyper-parameter clearly defined. Actually, I do not find the training objective function in this paper.\nBesides, currently, in this paper, there are many undefined notations and typos, for example, (1) in section 3.1, first paragraph, what is the $n$? Then in Eqn.(1) ,what is $N$ and $M$? Also, it is very confusing to use subscripts $i$ and $j$ to distinguish the hidden states from the encoder and decoder. (2) What is the mathematical definition of $ISE_i$? (3) In page 5, 3rd line, \u201cthen ISD_i is used to reconstruct Si = GNi(ISE_i , \\theta)\u2026\u201d Should the ISE_i be ISD_i?\n(2) Please use \\begin{algorithm}\u2026\\end{algorithm} to tell the readers how your framework works.\n \n[Details]\n1. The first question is \u201cwhy this problem\u201d. In the 3rd paragraph of page 1, you mentioned that \u201cHowever, the best direction to update parameters heavily relies on the quality of sampled translations ... which may be far from real translations Y due to inaccurate translations existing in the sampled ones\u2026\u2026\u201d But in practice, dual learning as well as back-translation [ref1] works well for many language pairs. In particular, the dual learning and back-translation works for the unsupervised NMT [ref2], where no labeled data is available. Therefore, I am not fully convinced by this claim and then, the motivation of this work. What\u2019s more, this paper does not work on standard WMT dataset, while previous dual learning and back-translation work on that most commonly used dataset. Therefore, the comparison between the guider network and dual learning are not fair.\n2. I am not sure how the BDE in Eqn. (1) is related to the NMT translation quality. Any reference or theoretical/empirical proofs? \n3. It is hard to reproduce such a complex NMT system with NMT, GN and an RL scheduler. Any open-source code or any simple solutions?\n4. Do you use a single-layer LSTM or a deep LSTM? Transformer [ref3] is the state-of-the-art NMT system. Why don\u2019t you choose this system? Also, you do not work on WMT dataset to verify your GLF-2L (Table 1). Therefore, I cannot justify whether the proposed algorithm is efficient compared to the current NMT algorithms. I am not convinced by the experimental results.\n5. The connection/difference between this work and (Tu et al 2017) should be discussed clearly, and you should implement (Tu et al 2017) as your baseline.  Besides, for the 3-language setting, no multilingual baseline is implemented.\n \n[Pros & Cons]\n(+) This paper tries to extend dual learning from word level to hidden state level;\n(+) Multiple languages are involved in this framework;\n(-) Experiments are not convincing; the models are weak; many important baselines are missing; no results on widely used WMT datasets;\n(-) The paper is not easy to follow. (See [clarify] part for details);\n(-) Training process is a little complex; not easy to implement;\n \nReferences\n[ref1] Edunov, Sergey, et al. \"Understanding back-translation at scale.\" EMNLP 2018\n[ref2] Lample, Guillaume, et al. \"Phrase-Based & Neural Unsupervised Machine Translation.\" EMNLP 2018\n[ref3] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017.\n ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}