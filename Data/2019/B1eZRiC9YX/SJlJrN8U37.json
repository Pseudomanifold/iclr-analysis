{"title": "Results seem to be shallow and vague", "review": "This paper extends the definition of adversarial examples to the ones that are \u201cfar\u201d from the training data, and provides two conditions that are sufficient to guarantee the non-existence of adversarial examples. The core idea of the paper is using the epistemic uncertainty, that is the mutual information measuring the reduction of the uncertainty given an observation of the data, to detect such faraway data. The authors provided simulation studies to support their arguments.\n\nIt is interesting to connect robustness with BNN. Using the mutual information to detect the \u201cfaraway\u201d datapoint is also interesting. But I have some concerns about the significance of the paper:\n1.  The investigation of this paper seems shallow and vague. \n    (1). Overall, I don\u2019t see the investigation on the \u201ctypical\u201d definition of adversarial examples. The focus of the paper is rather on detecting \u201cfaraway\u201d data points. The nearby perturbation part is taken care by the concept of \u201call possible transformations\u201d which is actually vague.\n    (2). Theorem 1 is basically repeating the definition of adversarial examples. The conditions in the theorem hardly have practical guidance: while they are sufficient conditions, all transformations etc.. seem far from being necessary conditions, which raises the question of why this theory is useful? Also how practical for the notion of \u201cidealized NN\u201d?\n    (3). What about the neighbourhood around the true data manifold? How would the model succeed to generalize to the true data manifold, yet fail to generalize to the neighbourhood of the manifold in the space?  Delta ball is not very relevant to the \u201ctypical\u201d definition of adversarial examples, as we have no control on \\delta at all.\n2. While the simulations support the concepts in section 4, it is quite far from the real data with the \u201ctypical\u201d adversarial examples. \n\nI also find it difficult to follow the exact trend of the paper, maybe due to my lack of background in bayesian models. \n1. In the second paragraph of section 3, how is the Gaussian processes and its relation to BNN contributing to the results of this paper?\n2. What is the rigorous definition for \\eta in definition 1?\n3. What is the role of $\\mathcal{T}$, all the transformations $T$ that introduce no ambiguity, in Theorem 1. Why this condition is important/essential here?\n4. What is the D in the paragraph right after Definition 4? What is D\u2019 in Theorem 1?\n5. Section references need to be fixed. \n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}