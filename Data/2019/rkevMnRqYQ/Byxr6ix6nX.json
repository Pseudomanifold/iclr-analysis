{"title": "Original formulation of initial state exploration for robot action optimisation - Reinforcement Learning", "review": "The framework of this work is Reinforcement Learning (RL) optimisation. The data consists of states of the space where the action takes place. Actions are possible, and they lead to possible transitions in the state space. A reward function assesses how adequate a state space is.\nThe main originality of the work is to use the initial state as a key information about the features that translate many desired state of background objects in a scene. An algorithm is built to make use of this information to build an ad hoc reward function, which specifies a good landscape of desired vs non-desired states of the space. An empirical evaluation of the introduced method is presented. It is rich and interesting, although hard to fully grasp for a non-expert.\n\nKey questions/remarks:\n - how different is your approach to a Bayesian approach with the combination of a likelihood (~reward) and prior (~initial state analysis) into a posterior distribution of the space? This seems to be the case in Section 5, where your alternative formulation clearly resembles a Lasso approach (which can be cast in a Bayesian framework).\n - I quite like your decomposition of your ideas into many titled paragraphs. The drawback is that there is sometimes a lack of connections between the many ideas you combine. A would see a big figure in the form of a map as a central contribution of your work to explain the different bits. Still, I appreciate the effort to have a synthetic contribution!\n\nSmall remarks:\n - the abstract could be improved to provide an easier reading experience\n - first time IRL on p2 is mentioned, without a prior explanation of the acronym\n - the world is already optimised for human preferences: yes and no, this is one of your (strong?) assumptions. The robot could well move the vase to a location which is acceptable. Or put it back. \n - on p3, beg.  of Section 3, explain the decomposition of r(s) = \\theta^{T}f(s).\n - in IRL paragraph: say the elements of \\tau_{i} are s.t. the transitions need be possible.\n - p8 'access to a simulator': what can be simulated if very little is know about the background, but via an initial state?\n - past point of the discussion: I simply don't get it!?!", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}