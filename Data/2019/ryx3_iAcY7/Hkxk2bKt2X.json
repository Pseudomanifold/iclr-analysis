{"title": "Interesting idea, but the improvement over the baseline is not significant.", "review": "\n[Summary]\nThis paper proposes \u201ca role interaction layer\u201d (briefly, RIL) that consists of context-dependent (latent) role assignments and role-specific transformations: Given an RIL layer, different dimensions of an embedding vector are \u201cinteracted\u201d based on Eqn. (5), Eqn. (7), etc. The authors work on IWSLT De->En and WMT En->De, En->Fi to verify their proposed algorithm with case study included. \n\n[Pros]\n(+) I think the idea/thought of using a \u201crole interaction layer\u201d is interesting.  The case study in Section 5.3 demonstrates different \u201croles\u201d. Also, different RIL architectures are designed.\n(+) The paper is easy to follow.\n\n[Cons & Details]\n(1) As stated in the abstract, \u201c\u2026, but that the improvement diminishes as the size of data grows, indicating that powerful neural MT systems are capable of implicitly modeling role-word interaction by themselves\u2026\u201d (1) The main concern is that, considering RIL does not obtain significant gain on large datasets, then we cannot say that the proposed algorithm is better than the baseline. (2) Why the NMT systems trained on large dataset can \u201cimplicitly modeling role-word interaction\u201d, while small dataset cannot? Any analysis?\n\n(2) For the \u201cmatched baseline\u201d, page 5, you increase the dimensionality of the models. But an RIL is an additional layer, which makes the network deeper. Therefore, a baseline with an additional layer should be implemented. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}