{"title": "A short and concise theoretical paper, but I find the contribution limited", "review": "The paper shows that the set of global minimums of an overparametrized network with smooth activation function is almost surely a high dimensional manifold. In particular, the dimension of this manifold is exactly n-d, where n is the number of parameters and d is the number of datas. To the best of my knowledge, this is the first proper proof of such non-surprising result.  \n\nThe theoretical analysis is a straightforward combination of neural network's expressiveness result and some classical theorems in the field of differential topology. However, the assumption on the overparametrized neural network is somehow unrealistic since it requires that at least one layer has no less than d neurons, which is as many as the number of datas. This is usually not the case. Moreover, the result is to some extent \"asymptotic\", in the sense that a small perturbation in terms of data may be required in order to make the statement holds.  \n\nMore importantly, the result does not provide any useful characterization distinguishing stationary point/local minimum versus global minimum. It might be possible that the set of stationary points is also a manifold with very high dimension, which is indeed supported by the argument listed in the bottom of page 7: \"the Hessian of the loss function tended to have many zero eigenvalues, some positive eigenvalues, and a small number of negative eigenvalues\". It is possibly the case that the set of stationary points has even higher dimension. This suggests that the dimensionality itself is not right indicator, but the difference in terms of dimension between different type of stationary points that matters. \n\nOverall, the paper is short and concise but I find the contribution a bit limited.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}