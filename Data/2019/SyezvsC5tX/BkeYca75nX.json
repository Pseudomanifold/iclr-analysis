{"title": "The results from the paper are sort of known in previous literature, yet the proof in the paper was still smart and innovative. ", "review": "This paper gave an interesting theoretical result that the global minima of an overparameterized neural network is a high-dimensional sub-manifold. This result is particularly meaningful as it connected several previous observations about neural networks and a indirect evidence for why overparameterization for deep learning has been so helpful empirically.\n\nThe proof in the paper was smart and the rough logic was quite easy to follow. The minor issue is that the proof in the paper was too sketchy to be strict. For example, in proof for Thm 2.1, the original statement about Sard\u2019s theorem was about the critical values, but the usage of this theorem in the proof was a little indirect. I can roughly see how the logic can go through, but I still hope the author can give more detailed explaining about this part to make the proof more readable and strict.\n\nOverall, I think the result in this paper should be enough to justify a publication. However, there\u2019re still limitations of the result here. For example, the result only explained about the fitting on training data but cannot explain at all why overfitting is not a concern here. It also didn\u2019t explain why stochastic gradient descent can find these minima empirically. In particular, even though the minima manifold is n-d dimensional, it\u2019s still a zero-measure set which will almost never get hit with a random initialization. Of course, these are harder questions to explore, but maybe worthy some discussion in the final revision.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}