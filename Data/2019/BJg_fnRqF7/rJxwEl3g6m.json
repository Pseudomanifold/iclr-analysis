{"title": "Deep clustering based on a mixture of autoencoders", "review": "This paper presents a deep clustering based on a mixture of autoencoders - which follows the well-known idea of a mixture of experts, and the k-means. Different from k-means, in this method, each data point is allocated to a cluster based the representation error if the autoencoder network were used to represent this data point. \n\nAlthough the method is developed based on some existing concepts in the literature, the use of mixture of autoencoders for deep clustering is novel. The paper is also very well presented and I enjoyed reading the paper. I have however several questions:\n\n(1) I feel the performance of the algorithm is most likely very dependent on the pre-training for initialisation that was used by the authors. How much is the performance dependent on pre-training? Is it possible to use random initialisation, and what's going to happen if you use random initialisation? \n\n(2) Do you have to assume that you know how many clusters are in the data? This appears to be assumed known in your experiments. What happen if it is not known?\n\n(3) The proposed method is only compared with the deep clustering baselines, and k-means. Are they state-of-the-art? Have you also considered subspace clustering algorithms, such as the sparse subspace clustering algorithms and low-rank subspace clustering algorithms?\n\nAlthough the paper is well written, there are still a few typos - for example, \"should results in\", \"Once all the network parameter are\", \"Autoencodr\", \"a variant the\", etc.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}