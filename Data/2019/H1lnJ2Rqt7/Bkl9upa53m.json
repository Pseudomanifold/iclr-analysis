{"title": "interesting work, but the theoretical part is not strong enough", "review": "This paper studies the large batch size training of neural networks, and incorporates adversarial training and second-order information to improve the efficiency and effectiveness of the proposed algorithm. In particular, the authors use second-order information to automatically generate the step size and batch size in each iteration, and apply adversarial training as a regularization method to improve the test performance. Finally, the authors demonstrate their algorithm and compare it with the baseline algorithms on a wide range of datasets. This paper is clearly written and has the following strength:\n\n1.\tThis paper proposes an adaptive method for SGD training, which proves its convergence for strongly convex optimization.\n2.\tThis paper incorporates the adversarial training and robust optimization into the adaptive SGD training, and shows that this combination significantly improves the test performance.\n3.\tThe authors perform experiments on different datasets, which show that the proposed method enjoys less training time and higher accuracy when using large batch size.\n\nHowever, this paper also has the following weakness:\n\n1.\tThe theoretical analysis is somewhat trivial, and the assumption on the objective function is rather strong, which is not consistent with the nonconvex loss functions that are widely applied in training neural networks.\n2.\tTheorem 1 provides convergence rate of SGD on strongly convex objective functions. However, the authors do not carefully characterize the learning rate to ensure that the loss function achieve \\epsilon-accuracy. Moreover, in order to make the last term in (5) be smaller than \\epsilon, the learning rate \\eta_0 should be in the order of O(\\epsilon), which is no longer a tuning-free parameter.\n3.\tThe authors mention that the proposed algorithm converges faster than basic SGD, but it is not clearly demonstrated from Theorem 1.\n4.\tI am confused about how to determine the number of iterations for different algorithm as shown in Table 1s and 2? Do you stop each algorithm when they attain the same training error on the training dataset?\n5.\tIt is also confused that the number of iterations for ABS and ABSA are relatively larger than that of BL (Tables 1 and 2), but the training time of BL is longer than those of ABS and ABSA as reported in Table 3?\n6.\tSome minor flaws. In (9) it should be \\|\\nabla L(\\Theta)\\|^2; in Lemma 3, the expectation on the left side should be taken conditioned on \\theta_t.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}