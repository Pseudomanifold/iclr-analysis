{"title": "Clearly an unfinished paper", "review": "The authors propose using information from the Hessian to grow the batch size as the training progresses. It is well-known that larger batch sizes can be used for later stages of optimization (ie, https://arxiv.org/abs/1711.00489, https://arxiv.org/abs/1706.05699), but they are missing motivation as to why use Hessian information for this.\n\nFurthermore, the description of the algorithm is lacking detail and is essentially unreproducible in current form.\n\nThe main description of their method is Algorithm 1 box, which suggests to grow batch size when \"eigenvalue\" is much smaller than previous eigenvalue. Is that the top eigenvalue? How is it estimated? Why is that the criterion? Note that for stochastic least squares problem one benefits from later batch sizes in later stages of optimization even though Hessian doesn't change.\n\nIn section Section 4.3 they start talking a bit about computing Hessian, referring to non-existent figure 6 for details of block approximation.\n\nAuthors mention that Hessian computation is not supported in major frameworks but don't provide explanation of how they compute it (did they not use a major framework for ImageNet experiments?).\n\nNote that a single row of Hessian (hence full Hessian) can be computed in all major frameworks by differentiating an element of the gradient. IE, in PyTorch https://gist.github.com/apaszke/226abdf867c4e9d6698bd198f3b45fb7, and also eigenspectrum of Hessian can be approximated -- https://github.com/noahgolmant/pytorch-hessian-eigenthings", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}