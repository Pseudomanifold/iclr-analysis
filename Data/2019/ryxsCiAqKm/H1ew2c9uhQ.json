{"title": "Highly-problematic experimental setup", "review": "# Summary of the paper\n\nThis paper presents a method for graph classification based on spectral convolutional neural networks. At its heart, the paper applies a standard decomposition in terms of the graph Laplacian and uses Chebyshev polynomials to obtain an approximation of node feature convolution. This is now extended to graphs with multiple edge relation types by performing the same calculations for every Laplacian that is induced by a certain relation $r$, and either extended the Chebyshev polynomials to multiple variables, or multiplying/adding the resulting feature vectors.\n\n# Review\n\nThe main idea of this paper, i.e. improving graph classification results by exploiting different types of edges, is very interesting. In particular for data sets with additional hierarchical information, this could well increase performance significantly.\n\nHowever, in its present form, the paper suffers from several issues. The following is a brief listing; each point will be expanded on below:\n\n1. Issues with originality: at times, it is unclear in what ways previous work is extended\n2. Issues with clarity: while most of the parts of the method can be 'reconstructed' through literature knowledge, the paper is not self-contained and often uses terms without explaining them.\n3. Issues with experiments: the reported results are *not* state-of-the-art for several data sets and I doubt the correctness of the setup.\n\nIn particular the experimental issues I consider to be highly problematic at the moment.\n\n## Originality:\n\nThe paper needs to be more clear about its contributions. Some parts read like an introduction to GCNs, but it is often not clear what is novel here:\n\n- The main contribution, as far as I understand it, should be an extension of `ChebNet` to multigraphs.  However, different relations in a graph appear to be modelled by calculating multiple graph Laplacians, which in turn can be stacked; this strikes me as a rather straightforward extension. Furthermore, the operation in Eq. 5 also appears a straightforward extension that uses learned node embeddings for each relation type.\n\n- The learning of edges for each relation, described in Section 3.1, is also a straightforward formulation/re-use of previous work (the papers cites Velickovic et al. (2018) as an example), and I do not exactly what is novel here: Velickovic et al. use the same equation to predict coefficients, as claimed in the paper, but how is this different from learning a weight for all edges as in Eq. 7? In Velickovic et al., this coefficient is only calculated for some of the edges. Is the novelty then that this is calculated for _all_ edges here?\n\n## Clarity:\n\n- The explanation of the Chebyshev expansion could be improved: Kipf & Welling (2016), for example, explain this in term of signal convolution (Eq. 3--5 in their paper), while for this paper, the introduction of node features in Eq. 2 is not clear.\n\n- What is $f_0, \\dots, f_{R-1}$ in Eq. 5? Is this an input signal at the nodes of a graph or an additional function defined on it, such as in the `ChebNet` paper by Monti et al.?  \n\n- What is $f_{\\text{edge}}$ in Eq. 7?\n\n- What is $\\bar{X}^{(0)}$ in EQ. 5? The preceding section only introduces a notation for $X$ with multiple indices, but suddenly, $X$ is now indexed over the number of relations.  \n\n- p. 4: The paper seems to be contradicting itself at places: the multi-variable polynomials from Eq. 3 and Eq. 4 are said to have a great computational cost and an exponential number of parameters for many relations. But the edge feature concatenation introduced at the bottom of p. 3 is dismissed by saying that it grows _linearly_ with $R$. What am I missing here?\n\n- The paper claims that most data set, except for the image, only have a single relation type ($R = 1$); later on, 'learned edges' are added for training. While I understand this and the benefits of adding them, I think that this should be clarified more: the advantage of the current approach seems to be that it can leverage more information by treating learned edges as a separate entity. This should be highlighted better.\n\n- In general, the paper would benefit from a section that describes the full method, preferably with a few equations, so that the readers can see at one glance which parts are involved. At present, I am doubtful about the reproducibility of the method.\n\n## Experimental setup\n\nThis is the major issue for me: the paper claims to follow a 'standard approach to evaluation' (p. 6) that consists of a 10-fold cross-validation. However, only average classification accuracies are reported, even though the sources I checked (Shervashidze et al., Yanardag & Vishwanathan, Niepert et al.) provide also the *standard deviation* along with the mean accuracy. This is important to know, since often (in particular for smaller data sets), there is a lot of overlap in terms of accuracy +- sdev.\n\nMoreover, the reported accuracies are just not state of the art: the 2016 paper *On Valid Optimal Assignment Kernels and Applications to Graph Classification* by Kriege et al. describes a novel variant of the Weisfeiler-Lehman graph kernel, referred to as WL-OA. The reported accuracies +- sdev are as follows:\n\n- NCI1: 86.1\u00b10.2 (better than in the current paper)\n- NCI109 86.3\u00b10.2 (better than in the current paper)\n- MUTAG: 86.0\u00b11.7 (worse than in the current paper; WL instead of WL-OA)\n- ENYZMES: 59.9\u00b11.1 (worse than in the current paper)\n- PROTEINS: 76.4\u00b10.4 (better/equal to the current paper)\n- COLLAB: 80.7\u00b10.1 (better than in the current paper)\n\nEven accounting for standard deviations, WL-OA appears to have a good 'safety margin' in terms of accuracy here. The claim that the new method 'wins by a large margin' here is thus flatly false.\n\nFurthermore, I find the way results are reported somewhat misleading: since Table 1 shows the second-best result in bold, the Multigraph Chebnet results appear to be more important. At the very least,  the standard deviations have to reported, and the experimental setup should be described in more detail, because the graph kernel publications also clarify that they learn their parameters on an inner validation such that only training data is used for hyperparameter tuning. It is unclear whether the paper is following the same approach here.\n\n# Minor issues\n\n- p. 2: use a consistent notation for the real space, i.e. $\\mathds{R}$ or $\\mathbb{R}$\n- p. 2: the justification of Eq. 1 could be phrased more succinctly: since $U$ is an orthogonal matrix, the convolution equation simplifies the way it is described; I find the description of '...property of eigendecomposition to eliminate computationally inconvenient eigenvectors' somewhat confusing\n- I don't see the significance of Figure 1; it only expresses that matrix powers make eigenvalues smaller if $\\lambda < 1$ (this property of the Laplacian is not mentioned, by the way)\n- p. 3: the notation for the number of relations is not optimal: $\\Theta\\in X_{in} K^R X_{out}$ should be rather rephrased as '$\\Theta$ is a matrix of the following dimensions'\n- The bibliography needs to be updated: there are some inconsistencies with respect to capitalization (such as in journal titles); also, at least the citation of Kipf & Welling should be adjusted as it is *not* a pre-print any more but was published in ICLR 2017\n- p. 3: Figure 2 needs a better explanation to be fully useful: its purpose appears to show that a 2D Chebyshev polynomial captures a different sort of information than a 2-hop filter because it does use information about different edge relations. Is this not clear from the onset since multi-hop filters operate only on a _single_ view of the graph?\n- p. 3: Eq. 5 seems to be a Hadamard product, so another operator should be used (essentially a circle with a dot); at present, the operator is implying that one _composes_ features by first applying one filter, then another, and so on.\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}