{"title": "This paper presents several enhancements to spectral GCNs. I don't believe it is novel enough for ICLR, nor are its experimental results strong enough.", "review": "## Summary ##\n\nThe authors describe several improvements on existing spectral convolutional networks for graphs:\n\n1. Learning 'abstract' edges that aren't included in the graph.\n2. Incorporating multiple edge types by several different methods.\n3. Building hierarchical graphs for image tasks.\n\nThey apply these modified architectures to classification of chemical compounds, social networks and images.\n\n## Assessment ##\n\nThe topic of spectral graph convolutions is interesting, and the enhancements suggested seem promising. However, each of 1-3 above is a straightforward application of an idea that appears elsewhere to spectral graph convolutions, making the work seem incremental. What is more, the experimental results were mixed; it was hard to discern significant improvement from 1-3. I don't think this paper warrants acceptance in ICLR at this time.\n\n## Questions and Concerns ##\n\n* I think section 2.1 (describing approximate spectral graph convolutions) could be made much clearer. The section in the supplementary material was much clearer and didn't take any more space than 2.1. Obviously, the authors can't be expected to cover ChebNet in detail, but it would be nice if they could get the gist of it from section 2.1 without having to read the original paper.\n* In many of the experiments, the hierarchical/multigraph ChebNet either underperformed another architecture, or outperformed by a very narrow margin. If there aren'y big improvements on benchmarks, it would be nice to see some other way that these new features are improving the model (e.g. can we show that it implicitly learns some interesting feature of the graphs?).\n* I found the comparison of edge fusion methods (summarized in Fig 5) difficult to understand. It was difficult to distinguish which improvements come from a better architecture and which simply come from a larger receptive field.\n* In 'Graph formation for images,' the authors say they included features from the last layer of pretrained VGG-16 in the hierarchical ChebNet for PASCAL. Moreover, these features were pretrained on a separate dataset (Imagenet). It wasn't clear whether these features were available to any other architectures, but they seem like they would provide a large advantage.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}