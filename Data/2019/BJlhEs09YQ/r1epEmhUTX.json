{"title": "Nice analysis but needs strong assumptions", "review": "This paper provides theoretical guarantees for learning deep convolutional neural networks using rank-one tensor decomposition. In particular,  a tensor is first constructed from the given data. Then the authors show that the error between this tensor and its corresponding population counterpart can be bounded. They also show that the population tensor can be approximated by a rank-one tensor whose components are convolutional kernels. Finally, by doing decomposition of this tensor, the kernels can be recovered up to some error. \n\nThe paper is in general easy to read even if there are a lot of technical notations in it. The analyses also look rigorous. The tensor formulation provides some theoretical insights for deep neural networks since most existing tensor approaches for neural networks can only handle one-layer neural networks. \n\nHowever, I have several concerns about this paper. \n\n1. The assumptions are very strong and mostly unrealistic. For example, 1) the input needs to follow Gaussian distribution; 2) the stride is equal to the kernel size; 3) the input and activations are one-dimensional. 4) the data comes from a planted model. \n\n2. Another caveat of the analyses is that it assumes the constructed tensor can be approximated by a rank-one tensor. However, according to Theorem 4.7, this error highly depends on the ground truth kernels as well as the activation function. This error does not converge to zero asymptotically and is not proved to be a sufficiently small number. In other words, this error may be a significant number that invalidates the proposed tensor decomposition approach for learning deep networks. \n\nRecovery guarantees for deep convolutional neural networks are challenging. This paper provides a tensor approach with rigorous analyses. However, the assumptions are too strong and the error bound does not converge to zero asymptotically . \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}