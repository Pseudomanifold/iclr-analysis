{"title": "Good results but limited novelty. Experimental comparison could be improved.", "review": "The paper proposes a multi-objective search algorithm that designs resource-efficient convolutional architectures. The key idea is to maintain a population of networks and to iteratively approach the Pareto front through evolution. The normal & reduction cells are searched on CIFAR-10 and then transferred to ImageNet. The resulting architectures empirically lead to better trade-offs than other baselines.\n\nPros:\nThe paper is well-written and easy to comprehend.\nResults are competitive against strong baselines such as NASNet.\nResource budgets are handled in a principled manner with multi-objective optimization. \n\nCons:\n\nMy main concerns are on the technical novelty and experimental comparison.\n\nTechnical novelty:\n\nThe proposed algorithm seems highly similar to the existing multi-objective NAS algorithms, especially the ones based on Pareto optimality [1,2,3]. In Sect 2, the authors state that the main difference from prior works such as [2] and [3] is the usage of a different and larger search space and large-scale experiments. However, both aspects are of limited technical novelty.\n\nExperimental comparison:\n\nIn sect 3.3, the authors say \u201cwe noticed that the original NASNet search space can greatly benefit from extra connections from any given block\u201d. If the proposed algorithm was investigated in an enhanced version of the NASNet space, it would be unclear whether we should attribute the reported performance to the proposed multi-objective evolution or this additional search space engineering. It would be better to report the results using the original space as well for fair comparison. \n\nThe main claimed contribution is a multi-objective evolutionary algorithm. To demonstrate its effectiveness, it would be necessary to compare against existent multi-objective NAS strategies in the literature. Most of those strategies (e.g., scalarization, weighted product method) should be straightforward to implement on top of the current search space. The current results are less convincing since the authors only compared their method against single-objective baselines (e.g. NASNet, PNAS, AmoebaNet) which are completely unaware of additional dimensions of the desired objectives. \n\nThe networks are searched on CIFAR-10 and then transferred to ImageNet. Unlike most prior works (including the ones focusing on resource-constrained NAS), the authors did not the final performance of their architecture on CIFAR-10. It would be informative to report the CIFAR-10 results as well.\n\nOther suggestions & questions:\nThe authors did not report their training setup for ImageNet. It would be good to include those details to ensure the readers are informed should there are any additional augmentations.\n\n\u201cuniform mutation and a crossover probability of 0.1\u201d (sect 4.1)\nIt would be better to included more details on these evolution forces for reproducibility. These are also important component of the proposed algorithm.\n\n\u201cWe manually select 3 architectures that we will be fully train on ImageNet in Section 4.2\u201d (sect 4.1)\nI believe this part needs more clarifications since there can be a large number of architectures on the Pareto front. What\u2019s the criteria for manual selection?\n\n[1] Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. \"Multi-objective architecture search for cnns.\" arXiv preprint arXiv:1804.09081 (2018).\n[2] Kim Ye-Hoon, Reddy Bhargava, Yun Sojung, and Seo Chanwon. NEMO: Neuro-Evolution with Multiobjective Optimization of Deep Neural Network for Speed and Accuracy. ICML\u201917 AutoML Workshop, 2017.\n[3] Dong, Jin-Dong, et al. \"DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures.\" arXiv preprint arXiv:1806.08198 (2018).", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}