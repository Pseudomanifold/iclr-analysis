{"title": "Bayesian reasoning about DNN outcome", "review": "Summary\n=========\nThe paper describes a probabilistic approach to quantifying uncertainty in DNN classification tasks.\nTo this end, the author formulate a DNN with a probabilistic output layer that outputs a multinomial over the\npossible classes and is equipped with a Dirichlet prior distribution.\nThey show that their approach outperforms other SOTA methods in the task of out-of-distribution detection.\n\nReview\n=========\nOverall, I find the idea compelling to treat the network outputs as samples from a probability distribution and\nconsequently reason about network uncertainty by analyzing it.\nAs the authors tackle a discrete classification problem, it is natural to view training outcomes as samples from\na multinomial distribution that is then equipped with its conjugate prior, a Dirichlet.\n\nHowever, the model definition needs clarification. In the classical NN setting, I find it misleading\nto speak of output distributions (here called p(x)). As the authors point out, NNs are deterministic function approximators\nand thus produce deterministic output, i.e. rather a function f(x) that is not necessarily a distribution (although can be interpreted as a probability).\nOne could then go on to define a latent multinomial distribution over classes p(z|phi) instead that is parameterized by a NN, i.e. phi = f_theta(x).\nThe prior on p(phi) would then be a Dirichlet and consequently the posterior is Dirichlet as well.\nThe prior distribution should not be dependent on data x (as is defined below Eq. 1).\n\nThe whole model description does not always follow the usual nomenclature, which made it at times hard for me to grasp the idea.\nFor instance, the space that is modeled by the Dirichlet is called a simplex. The generative procedure, i.e. how does data y constructed from data x and the probabilistic procedure, is missing.\nThe inference procedure of minimizing the KL between approximation and posterior is just briefly described and could be a hurdle to understand, how the approach works when someone is unfamiliar with variational inference.\nThis includes a proper definition of prior, likelihood and resulting posterior (e.g. with a full derivation in an appendix).\n\nAlthough the authors stress the importance of the approach to clip the Dirichlet parameters, I am still a bit confused on what the implications of this step are.\nAs I understood it, they clip parameters to a value of one as soon as they are greater than one.\nThis would always degrade an informative distribution to a uniform distribution on the simplex, regardless whether the parameters favor a dense or sparse multinomial.\nI find this an odd behavior and would suggest, the authors comment on what they mean with an \"appropriate prior\". Usually, the parameters of the prior are fixed (e.g. with values lower one if one expects a sparse multinomial).\nThe prior then gets updated through the data/likelihood (here, a parameterized NN) into the posterior.\n\nClipping would also lead to the KL term in Eq. 3 to be 0 often times, as the Dir(z|\\alpha_c) often degrades to Dir(z|U).\n\nThe experiments are useful to demonstrate the application and usefulness of the approach. \nOutcome in table 3 could maybe be better depicted using bar charts, results from table 4 can be reported as text only, which would free up space for a more thorough model definition.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}