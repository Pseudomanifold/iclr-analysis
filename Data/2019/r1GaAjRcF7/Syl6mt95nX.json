{"title": "Needs major improvement to be acceptable", "review": "This paper uses a differentiable submodular function to the task of selecting evidence sentences for claims. The paper is not ready to be accepted in a top venue in its current form. Details below.\n\n1) Writing: Needs major work. There are 3.5 pages of introduction and unnecessary background on submodular functions and two paragraphs in sec 3.2 of what the method proposed in this paper does. Only mentioning what the submodular function is, and may be a couple of references for it should have been enough. Proposition 1 is redundant, just mentioning the result in a single sentence is more than enough. I do not see the benefit of talking about a substantial subset of the submodular literature. The intro itself needs major revisions to be readable, there are a lot of holes. e.g. what is \"unfolding\" of the greedy algorithm that the  authors refer to?  Use of \\tau in f() should be explicitly written as an equation, so the reader doesn\u2019t have to look at the paper by Jang et al to know that the exact expression is used in the training. The experimental task itself could have been explained better. What is  the DeepEncoder architecture? Why was it used as a baseline? Why cant diversity arguments not be made by easily altering the DeepEncoder cost function? It may not have been designed to have marginal gains over other selections, and may be selects sentences as standalone evidences. The motivation of the proposed approach is unclear, and warrants a separate point. There are minor typos (e.g. see Table 1 caption  \"the recall improves recall \"), which could be ignored for now.\n\n2) Motivation: The authors mention \"We choose the ReLU function specifically because the submodularity of the SCMM function requires non-negativity \". This is not a suitable reason to design experiments. The guarantees should follow from the function that empirically works best, the functions should not be designed to get some theoretical result and show it works \"reasonably well\" in practice. The \"attention mechanism \" motivation is very weak and does not justify the design or the greedy regularization. Why select a subset when the full problem can be tractably solved ? Again this could just be lazy writing. \n\n3) Empirical evaluation: One task and evaluation does not suggest that the proposed methodology is useful. Even ignoring the holes in writing, a single task and empirical evaluation on one dataset with precision/recall/F1 scores at different k is not evidence that the proposed method is verified. It does not even justify the title. Are there no other baselines that the proposed method could be compared against? This paper is not about \"Differentiable Greedy Networks\", it is about \"greedy evidence selection for verifying claims using Recurrent NN\". ", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}