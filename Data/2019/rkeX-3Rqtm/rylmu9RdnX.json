{"title": "Borderline paper; interesting approach but insufficient contribution to warrant acceptance", "review": "Summary:\nThe paper presents a novel combinatorial search algorithm for the discrete target propagation framework developed in Friesen & Domingos (2018). Experiments on small datasets with small models demonstrate some potential for the proposed approach; however, scalability remains a concern.\n\n  Pros:\n-\tI like the goal of the work and think that if the targeted problem were to be solved it would be an interesting contribution to the field.\n-\tThe proposed search algorithm is reasonable and works OK.\n-\tThe paper is mostly well written and clear.\n-\tThe experiments are reasonably thorough.\n\n  Cons:\n-\tThe paper states that it is a feasibility study on search methods for learning hard-threshold networks, however, it only evaluates the feasibility of one combinatorial search method. \n-\tIt\u2019s not made clear whether other approaches were also investigated or what the authors learned from their exploration of this approach.\n-\tThe actual algorithm is not very well explained, despite being the main contribution of the paper.\n-\tThe datasets and models are small and not necessarily representative of the requirements of the field.\n-\tScalability remains a serious concern with the proposed approach.\n-\tIt\u2019s not clear to me that the paper presents a sufficient enough contribution to warrant acceptance.\n\nOverall, I like the direction but do not feel that the paper has contributed enough to warrant acceptance. The authors should use the experiments they\u2019ve run and also run more experiments in order to fully analyze their method and use this analysis to improve their proposed approach. \n\n\nQuestions and comments:\n\n1.\tDid you try alternative local search algorithms or did you just come up with a single approach and evaluate it? What did you learn from the experiments and the development of this algorithm that will let you create a better algorithm in the next iteration?\n\n2.\tI think that it is unfair to say that \u201cit suggests a simpler, independent justification for the performance improvements obtained by their method.\u201d in reference to the work of Friesen & Domingos (2018), given that the straight-through estimator is not well justified to begin with and their work in fact provides a justification for it. I do agree that it is important to investigate alternative heuristics and approaches within the discrete target propagation framework, however.\n\n3.\tSections 2 and 3 do not clearly define what L_i is and where it comes from. Since these do not normally exist in a deep network they should be clearly defined.\n\n4.\t\u201cstep 2(b)\u201d is not well defined in section 3.1.1. I assume that this refers to lines 4-8 of Algorithm 1? The paper should explain this procedure more clearly in the text. Further, I question the locality of this method, as it seems capable of generating any possible target setting as a neighbor, with no guarantee that the generated neighbors are within any particular distance of the uniform random seed candidate. Please clarify this.\n\n5.\tI believe that a negative sign is missing in the equation for T_i in \u2018Generating a seed candidate\u2019. For example, in the case where |N| = 1, then T_i = sign(dL/dT_i) would set the targets to attain a higher loss, not lower. Further, for |N|=1, this seems to essentially reduce to the heuristic method of Friesen & Domingos (2018). \n\n6.\tIn the \u2018Setting the probabilities\u2019 section:\n(a) All uses of sign(h) can be rewritten as h (since h \\in {-1, +1}), which would be simpler.\n(b) The paper contradicts itself: it says here \u2018flip entries only when sign(dL/dh) = sign(h)\u2019 but Algorithm 1 says \u2018flip entries only when sign(dL/dh) != sign(h)\u2019. Which is it?\n(c) What is the value of a_h in the pseudocode? (i.e., how is this computed in the experiments)\n\n7.\tIn the experiments, the paper says that \u2018[this indicates] that the higher dimensionality of the CIFAR-10 data manifold compared to MNIST may play a much larger role in inhibiting the performance of GRLS.\u2019 How could GRLS overcome this? Also, I don\u2019t agree with the claim made in the next sentence \u2013 there\u2019s not enough evidence to support this claim as the extra depth of the 4-layer network may also be the contributing factor.\n\n8.\tIn Table 2, why are some numbers missing? The paper should explain what this means in the caption and why it occurs. Same for the bolded numbers.\n\n9.\tThe Loss Weighting, Gradient Guiding, Gradient Seeding, and Criterion Weighting conditions are not clearly defined but need to be to understand the ablation experiments. Please define these properly.\n\n10.\tThe overall structure of the algorithm is not stated. Algorithm 1 shows how to compute the targets for one particular layer but how are the targets for all layers computed? What is the algorithm that uses Algorithm 1 to set the targets and then set the weights? Do you use a recursive approach as in Friesen & Domingos (2018)?\n\n11.\tIn Figure 2, what dataset is this evaluation performed on? It should say in the caption. It looks like this is for MNIST, which is a dataset that GRLS performs well on. What does this figure look like for CIFAR-10? Does increasing the computation for the heuristic improve performance or is it also flat for a harder dataset? This might indicate that the initial targets computed are useful but that the local search is not helping. It would be helpful to better understand (via more experiments) why this is and use that information to develop a better heuristic.\n\n12.\tIt would be interesting to see how GRLS performs on other combinatorial search tasks, to see if it is a useful approach beyond this particular problem.\n\n13.\tIn the third paragraph of Section 4.2, it says \u2018The results are presented in Table 3.\u2019 I believe this should say Figure 3. Also, the ordering of Figure 3 and Table 3 should be swapped to align with the order they are discussed in the text. Finally, the caption for Table 3 is insufficiently explanatory, as are most other captions; please make these more informative.\n\n14.\tIn Section 4.3:\n(a), the paper refers to Friesen & Domingos (2018) indicating that zero loss is possible if the dataset is separable. However, what leads you to believe that these datasets are separable? A more accurate comparison would be the loss for a powerful non-binarized baseline network. \n(b) Further, given the standard error of GRLS, it\u2019s possible that its loss could be substantially higher than that of FTPROP as well. It would be interesting to investigate the cases where it does much better and the cases where it does much worse to see if these cases are informative for improving the method.\n\n15.\tWhy is there no discussion of training time in the experiments? While it is not surprising that GRLS is significantly slower, it should not be ignored either. The existence of the Appendix should also be mentioned in the main paper with a brief mention of what information can be found in it.\n\n16.\tIn Algorithm 1, line 2 is confusingly written. Also, notationally, it\u2019s a bit odd to use h both as an element and as an index into T.\n\n17.\tThere are a number of capitalization issues in the references.\n\n18.\tThe Appendix refers to itself (\u201cadditional hyperparameter details can be found in the appendices\u201d).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}