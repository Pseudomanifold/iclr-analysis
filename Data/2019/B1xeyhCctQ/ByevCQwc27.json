{"title": "Interesting work, but needs refinement.", "review": "Summary of the paper\nThis paper proposes a method for attributing the output of a neural network to bias terms. The method is restricted to networks that have piecewise-linear activation functions. Computation is recursive, starting from bias attribution to the activation of the penultimate layer, such that the final attribution is of the same size as the input data point and sums up to the bias term when the network is written as a linear function (for that input).\n\nStrengths \n- The idea of mapping the bias term back to the input is interesting as it shows a common behaviour of the network on inputs that choose the same pieces of the piecewise linear functions. \n- Other gradient-based methods overlook the bias term when piecewise-linear activations are involved, so this method closes that gap. \n\nQuestions for authors\n- The separability of the bias and gradient terms is possible only for piecewise linear activation functions, and would not generalize to other activations (e.g., LSTMs in NLP). \n- Except for the 1-2 examples pointed out by the authors, it is not clear from the visualizations that bias attribution shows something qualitatively different. For instance, in \u201cairplane\u201d, \u201chorse\u201d and \u201cfireguard\u201d, gradient also highlights a region similar to bias attributions (although, technically they are complementary). \n- While the authors qualitatively compare with other attribution methods, they only experimentally compare with gradient. It would be instructive to compare with more refined gradient-based attribution methods such as Integrated Gradients or DeepLift and show empirically that looking at bias attribution is better over simply looking at other attribution-based methods. Integrated Gradients specifically argues that it removes extraneous attribution to background (http://www.unofficialgoogledatascience.com/2017/03/attributing-deep-networks-prediction-to.html). \n- To have substantial content for a full publication, it may be good to address what insights one can derive from bias attributions. For instance, cluster inputs based on bias attributions, and show how different sets of inputs may be affected by different kinds of biases. For e.g., do all images of birds show similar bias attributions? Or, for instance, does a picture of a house (which may have the same kind of edges as a chair) have the same bias attribution as that of a chair? \n\nConclusion\nI think the paper is interesting, but for a full publication, a thorough comparison with other methods is required, as well as showing more insights as to how bias attribution is useful (another e.g., can it be combined with gradient-attribution for a \u201cricher\u201d visualization?) \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}