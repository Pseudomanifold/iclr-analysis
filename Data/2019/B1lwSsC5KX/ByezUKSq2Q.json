{"title": "review of \"an empirical evolution of the memorisation properties of Convents\"", "review": "Summary of the paper:\n\n\nThe paper has two intertwined goals. These goals are to illuminate the\ngeneralization/memorization properties of large and deep ConvNets in\ntandem with trying to develop procedures related to identifying\nwhether an input to a trained ConvNet has actually been used to train the\nnetwork. The latter task is generalized to detecting if a\nparticular dataset has been used to train a ConvNet. These goal are\ntackled empirically with multiple sets of experiments on largescale\ndatasets such as ImageNet22k and modern deep ConvNets architectures\nsuch as VGG and ResNet.\n\n\n\nPaper's positive points\n\n+ The paper has a very comprehensive set of references in the areas it\ntouches upon.\n\n+ Some of the experimental results presented are quite\ninteresting. They show that regularization data-augmentation helps\nprevent a network from explicit memorization and could be used as a\nway to help make training data more anonymous.\n\n+ Large scale experiments are reported on modern architectures.\n\n\nPaper's negative points\n\n- The paper makes use of a result from the David MacKay textbook\n  which defines the capacity of a single layer network to memorize the\n  labelling of $n$ inputs in $d$-dimensional space. If I'm not\n  mistaken, from this result the authors extrapolate that the capacity\n  of a (deep) neural network is proportional to the number of\n  parameters in the network. This is true, but there are a\n  couple of caveats. The first is that the coefficient of\n  proportionality must depend very much on the number of layers in the\n  network. Increasing the network's depth increases the efficiency of\n  the representation (i.e. fewer total parameters needed to have the\n  same representational power as a shallow network). And as MacKay\n  also says in his book (chapter 44 quoting findings from Radford\n  Neal) that for MLPs what determines the complexity of the typical\n  function (once the network has a large enough width) represented by\n  the MLP is the \"characteristic magnitude of the weights\". So the\n  regularization technique applied is very significant in the\n  controlling the effective capacity of a network. This paper\n  experimentally shows that is the case multiple times as it is shown\n  that with increasing degrees of regularization (figure 1, figure 2)\n  it becomes harder and harder to memorize the positive training\n  images. It would be great if the paper also made some attempt to\n  consider these connections. Or at least comment on how these factors\n  could be incorporated into a more sophisticated analysis of the\n  capacity of a network.\n\n\n\n- There is a slight oxymoron in the premise of the first set of\n  experiments. The network is forced to memorize a set of\n  positive examples relative to the negative set it sees during\n  training. What is memorized I presume depends a lot on the negative\n  set used for training (its diversity, closeness to the positive set\n  and how frequently each negative example is seen during\n  training). This issue is not really commented upon in the paper. Is\n  there a training task which would allow one to more explicitly\n  memorize the image (some sort of reconstruction task) as opposed to\n  an in/out classification task?\n\n- This paper is a slightly difficult read - not because of the\n  language or the presentation of the material but more because there\n  is not one main coherent argument or goal for the paper. This is\n  reflected in the \"Related work\" section where 4 different\n  issues/tasks are referred to. Each one of these topics is worthy of\n  a paper in itself, but this paper dips into each one and then\n  swiftly moves onto the next one. For example in section 3 the paper\n  explores if a network can be forced to explicitly memorize a set of\n  images and how the size of this set is affected by the number of\n  parameters in the network and data augmentation. High-level\n  conclusions are made: more parameters in the network implies more\n  images can be memorized and data-augmentation makes explicit\n  memorization more difficult. Then it is off to considering\n  pre-trained networks and determining whether by analyzing the\n  statistics of the responses at different layers one can decide if a\n  set of images was used for training or not (or similar tasks). Yes\n  the different sections are related but it is does not feel like they\n  build upon each other to help form a clearer picture of memorization\n  within neural networks.\n  \n\n- The conclusions focus on the importance of section 3 and\n  the results of the experiments performed. Do the conclusions accurately\n  reflect the opinions of the author? If yes, would\n  it better to re-organize the paper and devote more of it to the\n  material presented in section 3 and filling this out with more\n  analysis and experiments to perhaps explore the issue of the\n  capacity of a network in more \n\n\nQueries/ points that need some clarification\n\n- I'm a little unclear when data-augmentation is included in the\n  training phase whether the goal is to be able to also recognise\n  perturbed versions of the input images at test time. In section 3 is\n  a perturbed positive image considered a positive training image? And\n  in the testing phase are only unperturbed versions of the positive\n  images given to the ConvNet as input?\n\n- Last paragraph page 4: \"when the accuracy gets over 60\\% and again\n  at 90\\%\". Is this training or validation accuracy?\n\n\n\n\nTypos possible errors spotted along the way:\n\n* First paragraph page 5: \"more shallow\" --> \"shallower\"\n* Page 7, first paragraph of section 5.: \"is ran\" --> \"is run\"\n* Using \"scenarii\" for the plural of \"scenario\" I would say is pretty\n  non-standard and most people would use \"scenarios\"", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}