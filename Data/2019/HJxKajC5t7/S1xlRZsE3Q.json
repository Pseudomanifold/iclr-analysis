{"title": "Not enough contribution and not very clear experiments", "review": "This paper presnets a different way to train a binary network. Instead of using the sign function for the forward pass and the straight trough approximation for the backward pass, as in previous work, it uses continuation, i.e. weights and activations are converted to the range [-1,1] with a hyperbolic tangent, with a multiplicative factor that initially is low, and then though iterations becomes larger and forces the output of htan to be close to the binary values. The paper also uses a different way to compute batch normalization (BN), which manages to use 8 bits fixed point instead of 32 bits floating point. Results are shown on CIFAR 10, 100 and ImageNet, and compared to previous approaches.\n\nPros:\n- The paper is well written\n- The idea of fully avoiding float32 is intriguing, but it would be much more interesting if it can be done also at training time.\n\nCons:\n- Reading the introduction gives a high expectation about a method that can actually learn directly on binary values. Then, in practise, the real contribution of the paper is just a different binarization of the network and a different binary BN.\n- The first contribution of the paper is to use continuation instead of straight trough for using back-propagation. To me it makes a lot of sense, but in the experimental part I could not clearly see if the improvement in performance is due to this representation of the binarized BN.\n- The second contribution of the paper is on batch nomralization. The authors show another way to compute jointly the network activation and batch normalization using a 8 bit fixed point instead of a 32 bit floating point. This can help on practical implementations to use the network on machines that do not have floating point operations. However, I do not see this contribution enough for a ICLR paper.\n- The presentation of the results seems biased. In tab. 1, the authors show that their approach requires only 1 bit for the weights and 1 bit for their binary BN. However, they also use a 8 bit fixed point for BN, but it is not shown. \n- In Fig. 3, the improvement in computational cost and memory is only for the binarized BN. Globally I think that this is not affecting much the entire network speed and memory. Can the authors say more about that? \n\nGlobal Evaluation:\nI thinks that a representation of a binary net based on a continuation approach is quite interesting. However, in the experimental results the authors did not really clarify if the obtained improvements are really due to this part or not. More experiments and an ablation study is necessary. The second contribution about binarized BN does not seem very important to me. It can reduce the BN computational cost and memory consumption, but globally,in terms of the full network it does not change much.\n\n\nAdditional comments:\n- Fig. 1 is quite confusing. There are many bars and it is not clear what they represent in the network.\n- I do not think it is necessary to describe all the method for reducing networks computation in related work\n- in related work: \"which requires only one bit to represent\" needs an object or a passive form.\n- missing the very relevant reference to: \"How to Train a Compact Binary Neural Network with High Accuracy?\" Wei Tang, Gang Hua, Liang Wang.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}