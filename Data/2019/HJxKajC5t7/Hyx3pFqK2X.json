{"title": "Self-Binarizing Networks", "review": "This is a good paper on an important topic of low precision representation in neural network weights and signal representations. As we move more and more into making neural networks deployed in low power applications such as mobile and wearable devices, this topic is of increasing importance. This is an area explored by other researchers -- and the manuscript does a good job of surveying related work. The novel contribution in the paper is to hide neural network weights and activation at nodes behind use hyperbolic tangent functions, thereby working in a continuous space in which differentiation is straightforward. The sharpness of hyperbolic tangent is gradually increased (in an ad-hoc way every epoch), so that gradually everything become binary. A clearly written paper. However, the empirical work reported is somewhat weak: (a) there is no uncertainty provided in the results of Table 1. When the differences in performance quoted are so small, it is important to give uncertainties (by cross validation) so that a reader can judge the significance of the results; (b) very little effort is made to carry out an analysis of errors; where do the gains come from? If, for example, one looks at the confusion matrix of the ten-class problem, can one identify where the differences are -- are they random or is there anything systematic one can pull out? Lack of error analysis is particularly striking when one notes that in both the CIFAR10 and CIFAR100 problems, there are instances where the binary low precision method actually outperforms the full precision method. How is this so? (c) following on from this, what does the paper bring to this particular conference on \"learning representations\"? Is there anything we can pull out from the work of representing features in a binary space that is specific to the problems considered? \nIn conclusion, the paper has a novel idea I like, it is explained clearly, but the work has to mature a bit more in terms of  empirical work and interpretation of results.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}