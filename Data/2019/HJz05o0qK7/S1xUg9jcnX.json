{"title": "Solid paper on an interesting topic - some questions as to generalization in other settings", "review": "This paper describes a framework - Tree Reconstruction Error (TRE) - for assessing compositionality of representations by comparing the learned outputs against those of the closest compositional approximation. The paper demonstrates the use of this framework to assess the role of compositionality in a hypothetical compression phase of representation learning, compares the correspondence of TRE with human judgments of compositionality of bigrams, provides an explanation of the relationship of the metric to topographic similarity, and uses the framework to draw conclusions about the role of compositionality in model generalization.\n\nOverall I think this is a solid paper, with an interesting and reasonable approach to quantifying compositionality, and a fairly compelling set of results. The reported experiments cover reasonable ground in terms of questions relevant to compositionality (relationship to representation compression, generalization), and I appreciate the comparison to human judgments, which lends credibility to applicability of the framework. The results are generally intuitive and reasonable enough to be credible as indicators of how compositionality relates to aspects of learning, while providing some potential insight. The paper is clearly written, and to my knowledge the approach is novel.\n\nI would say the main limitation to the conclusions that can be drawn from these experiments lies in the necessity of committing to a particular composition operator, of which the authors have selected very simple ones without comparing to others. There is nothing obviously unreasonable about the choices of composition operator, but it seems that the conclusions drawn cannot be construed to apply to compositionality as a general concept, but rather to compositionality when defined by these particular operators. Similar limitations apply to the fact that the tests have been run on very specific tasks - it is not clear how these conclusions would generalize to other tasks.\n\nDespite this limitation, I'm inclined to say that the introduction of the framework is a solid contribution, and the results presented are interesting. I think this is a reasonable paper to accept for publication.\n\nMinor comment:\np8 typo: \"training and accuracies\"\n\n------\n\nReviewer 2 makes a good point that the presentation of the framework could be much clearer, currently obscuring the central role of learning the primitive representations. This is something that would benefit from revision. Reviewer 2's comments also remind me that, from a perspective of learning composition-ready primitives, Fyshe et al. (2015) is a relevant reference here, as it similarly learns primitive (word) representations to be compatible with a chosen composition function. \n\nBeyond issues of presentation, it seems that we are all in agreement that the paper's takeaways would also benefit from an increase in the scope of the experiments. I'm happy to adjust my score to reflect this.\n\nReference:\nFyshe et al. (2015) A compositional and interpretable semantic space.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}