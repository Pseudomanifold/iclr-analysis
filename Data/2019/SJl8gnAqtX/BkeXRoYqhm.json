{"title": "small technical contribution", "review": "The paper proposed a hierarchical framework for problem embedding and intended to apply it to adaptive tutoring. The system first used a rule-based method to extract the concepts for problems and then learned the concept embeddings and used them for problem representation. In addition, the paper further proposed negative pre-training for training with imbalanced data sets to decrease false negatives and positives. The methods are compared with some other word-embedding based methods and showed 100% accuracy in a similarity detection test on a very small dataset. \n\nIn sum, the paper has a very good application but not good enough as a research paper. Some of the problems are listed as follows:\n1.\tLack of technical novelty.  It seems to me just a combination of several mature techniques. I do not see much insight into the problem. For example, if the rule-based concept extractor can already extract concepts very well, the \u201cproblem retrieval\u201d should be solved by searching with the concepts as queries. Why should we use embedding to compare the similarity? Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap. \n2.\tData size is too small, and the baselines are not state-of-the-art. There are some unsupervised sentence embedding methods other than the word-embedding based models. \nSome clarity issues. For example, Page 6. \u201cis pre-trained on a pure set of negative samples\u201d\u2014 what is the objective function? How to train on only negative samples?\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}