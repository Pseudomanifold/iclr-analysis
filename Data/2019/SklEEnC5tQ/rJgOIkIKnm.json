{"title": "Nice experimental paper (with theory backing) ", "review": "In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.\n\nThe paper is interesting and well written. However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc.  \n\n", "rating": "7: Good paper, accept", "confidence": "1: The reviewer's evaluation is an educated guess"}