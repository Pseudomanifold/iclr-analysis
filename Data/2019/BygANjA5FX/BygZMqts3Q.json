{"title": "Issues of clarity and comparison", "review": "This work proposes an ensemble method for convolutional neural networks wherein each convolutional layer is replicated m times and the resulting activations are averaged layerwise.\n\nThere are a few issues that undermine the conclusion that this simple method is an improvement over full-model ensembles:\n\t1. Equation (1) is unclear on the definition of C_layer, a critical detail. In the context, C_layer could be weights, activations before the nonlinearity/pooling/batch-norm, or activations after the nonlinearity/pooling/batch-norm. Averaging only makes sense after some form of non-linearity, otherwise the \u201censemble\u201d is merely a linear operation, so hopefully it\u2019s the latter.\n\t2. The headings in the results tables could be clarified. To be sure that I am understanding them correctly, I\u2019ll propose a new notation here. Please note in the comments if I\u2019ve misunderstood! Since \u201cm\u201d is used to represent the number of convolutional layer replications, let\u2019s use \u201ck\u201d to represent the number of full model replications. So, instead of \u201cCNL\u201d and \u201cIEA (ours)\u201d in Table 1 and \u201cEnsemble of models using CNL\u201d and \u201cEnsemble of models using IEA (ours)\u201d in Table 2, I would recommend a single table with these headings: \u201c(m=1, k=1)\u201d,  \u201c(m=3, k=1)\u201d,  \u201c(m=1, k=3)\u201d,  and \u201c(m=3, k=3)\u201d, corresponding to the columns in Tables 1 and 2 in order. Likewise for Tables 3-6.\n\t3. Under this interpretation of the tables---again, correct me if I\u2019m wrong---the proper comparison would be \u201cIEA (ours)\u201d versus \u201cEnsemble of models using CNL\u201d, or  \u201c(m=3, k=1)\u201d versus \u201c(m=1, k=3)\u201d in my notation. This pair share a similar amount of computation and a similar number of parameters. (The k=3 model would be slightly larger on account of any fully-connected layers.) In this case, the \u201couter ensemble\u201d wins handily in 4 of 5 cases for CIFAR-10.\n\t4. The CNL results, or \u201c(k=1,m=1)\u201d, seem to not be state-of-the-art, adding more uncertainty to the evaluation. See, for instance, https://www.github.com/kuangliu/pytorch-cifar. Apologies that this isn\u2019t a published table. A quick scan of the DenseNets paper and another didn\u2019t yield a matching set of models. In any case, the lack of data augmentation may account for this disparity, but can easily be remedied.\n\nGiven the above issues of clarity and that this simple method seems to not make a favorable comparison to the comparable ensemble baseline (significance), I can\u2019t recommend acceptance at this time. \n\nOther notes:\n\t* The wrong LaTeX citation function is used, yielding the \u201cauthor (year)\u201d form (produced by \\citet), instead of \u201c(author, year)\u201d (produced by \\citep), which seems to be intended. It\u2019s possible that \\cite defaults to \\citet.\n\t* The acronyms CNL and FCL hurt the readability a bit. Since there is ample space available, spelling out \u201cconvolutional layer\u201d and \u201cfully-connected layer\u201d would be preferred.\n\t* Other additions to the evaluation could or should include: a plot of test error vs. number of parameters/FLOPS/inference time; additional challenging datasets including CIFAR-100, SVHN, and ImageNet; and consideration of other ways to use additional parameters or computation, such as increased depth or width (perhaps the various depths of ResNet would be useful here).\n", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}