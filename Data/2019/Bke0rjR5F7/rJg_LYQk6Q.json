{"title": "Motivation and contribution are not very clear", "review": "In this paper, the authors propose a method to generate predictions under fairness constraints by optimizing quadratic penalties over non-convex loss functions. The main idea is to replace the linear fairness constraints by the second-order penalty. Meanwhile, an efficient method is derived to compute the gradients associated with the second-order penalties in stochastic mini-batch settings. Finally, the experimental results show the effectiveness of their proposed method empirically. \n\n(1) The authors argued that their experimental results in a more practical training procedure in non-convex, large-data settings. However, in Section 6, the sample size of the data-sets they used are small and the loss functions of learning models are convex. I think they need to provide more experimental results to make their proposed method more convincing. \n\n(2) It is a little difficult to follow the motivation and contributions of this paper. I would recommend the authors to improve the presentation by providing more context for the use of double integral or sampling method and other mostly relevant works in this area. \n\n(3) From the optimization viewpoint, the second-order penalty in Eq. (3) is convex with respect to d. Why replacing the linear penalties with quadratic penalties to solve the shortcomings of using linear penalties or Lagrange multipliers. Isn't the square of an expectation always an expectation of pairs? In other word, Eq. (4) is always equivalient to Eq. (7) without additional conditions?\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}