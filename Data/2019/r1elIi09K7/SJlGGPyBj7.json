{"title": "The method needs more discussions and more comparison baselines.", "review": "This paper deals with the open set classification problem, where in addition to the known classes, the method should also be able to recognize the unknown class. The main idea is based on two parts: learning a discriminative representation, and a threshold based detection rule. To learn the embedding, the authors propose to minimize the inner class distance (between each instance to its center) and enlarge the distance between centers. The outlier score of an instance is computed as the minimum distance between known class prototypes. Experiments on various datasets show the ability of the learned method.\n\nI'm not completely sure whether the whole approach is novel or not in the open set recognition domain, but both parts are not novel enough. Pulling similar instances together and pushing dissimilar ones away is the main idea in embedding learning. The ii-loss is similar to the triplet-center loss in the paper \"He et al. Triplet-Center Loss for Multi-View 3D Object Retrieval. CVPR18\". \n\nAlthough in the experiments the proposed method achieves good results in most cases, the reviewer suggests the authors comparing with more baselines to make the work solid.\n1. Comparing with other embedding learning methods with the same outlier detection score. \nThe authors should prove that the proposed embedding is important enough in the open set case. For example, using the center loss (Wen et al. A discriminative feature learning approach for deep face recognition. ECCV16), triplet-center loss, triplet loss (computing class centers after embedding).\n\n2. Discuss more on the outlier score part. \nHow to differentiate the known class outlier and new class? Will the problem be more difficult when the unknown class contains more heterogeneous classes? The authors can also apply existing open set recognition rule on the learned embedding.\n\nSome detailed questions:\n1. What's the difference between \"the network weights are first updated to minimize on ii-loss and then in a separate step updated to minimize cross entropy loss\" and optimize both loss terms simultaneously?\n2. \"We assume that a certain percent of the training set to be noise/outliers\", how to determine the concrete value? Is 1% the helpful one for all cases?\n3. Since there is not optimize over the unknown classes in training, could the reason for \"the unknown class instances fully occupy the open space between the known classes\" is the unknown classes are randomly sampled from the whole class set? For example, if classes about animals are known classes and classes about scene compose the unknown class, will the unknown class also occupy the whole space in this case?\n4. What is the motivation of making \"the unknown class instances fully occupy the open space between the known classes\"?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}