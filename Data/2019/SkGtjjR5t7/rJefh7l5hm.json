{"title": "A reasonable approach for self-driving vehicle control.", "review": "Summary.\nThe paper proposes a vehicle\u2019s trajectory planner that iteratively predict next-step (longitudinal and latitudinal) position of an ego-vehicle. Instead of using a raw image, a set of handcrafted features (i.e., the status of traffic lights, route, roadmap, etc) are mapped onto a fixed-size of bird-eye view map, which is then fed into the recurrent neural network. Additional regularizing loss terms are explored for the robustness of the model. The effectiveness of the method is demonstrated in simulation and real-world experiment.\n\nStrengths.\n- Impressive demonstrations in simulation and real-world experiments.\n- The paper is generally well-written and easy to follow.\n\nvs. Existing motion planning approaches.\nThere exists a large volume of papers on vehicle motion planning, which has largely been explored for controlling self-driving vehicles. Some of them successfully demonstrated their effectiveness for navigating a vehicle in typical driving scenarios, including \u201cslowing down for a slow car\u201d.\nA notable survey may include:\n\n[1] Paden et al., \u201cA survey of motion planning and control techniques for self-driving urban vehicles,\u201d IEEE Transactions on intelligent vehicles, 2016. \n\nHowever, the paper provides neither any works of literature on existing motion planners nor any types of comparison with them. This makes hard to judge the proposed learning-based motion planner outperforms others including conventional optimization-based methods. \n\nMissing data collection details.\nThis work depends hugely on its own human-designated oracle-like map, which provides driving-related features, such as lane, the status of traffic lights, speed limits, desired route, dynamic objects, etc. Generating this map would not be a trivial task, but details are missing on (1) how this data collected and (2) how this data can be collected during the testing time (especially for dynamic objects/traffic light status). Section 6.2 should be explained more in detail.\n\nA weak novelty of using intermediate-level input/output representation.\nThere exist similar approaches that utilized similar representations to determine a vehicle\u2019s behaviour, examples may include:\n\n[1] Lee et al., \u201cConvolution Neural Network-based Lane Change Intention Prediction of Surrounding Vehicles for ACC,\u201d IEEE ITSC 2017.\n[2] We et al., \u201cModeling trajectories with recurrent neural networks,\u201d IJCAI, 2017.\n\nMissing evaluation details.\nIn Section 6.2, (though not mentioned) it seems that a training dataset is collected from 60-days of real-world driving (given the context). But, in the testing phase, it seems that the authors used a simulator to evaluate different driving scenarios with various initial condition (i.e., speed, heading angle, position, etc). Can authors clarify details of the evaluation environment?\n\nMinor concerns.\nA paragraph of contribution summary (in Introduction section) will help. \nTypos (e.g., Section 2 line 17: \u2018off of\u2019)", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}