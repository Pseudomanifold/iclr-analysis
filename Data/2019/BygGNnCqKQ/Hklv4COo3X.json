{"title": "Interesting idea but the approach is not rigorous", "review": "The paper presents a way to compress the neural network architecture. In particular, it first extracts some characteristics for the neural network architecture and then learns two mapping functions, one from the encoded architecture characteristics to the expected accuracy and the other from the same encoded architecture characteristics to the number of parameters. In the meanwhile, the proposed approach learns the encoding and the decoding for the architecture characteristics. \n\nPros:\n1. The idea of converting the architecture characteristics, which is discrete in nature, to continuous variables is interesting. The continuity of the architecture characteristics can help architecture search tasks.\n\nCons:\n1. My main concern is the validity of the compression step, Procedure COMPRESS, in Algorithm 1. First, is only one step gradient descent applied? If it is, why not minimize the L_c until convergence?  Second, it seems that minimizing L_c cannot guarantee that both error and the number of parameters are reduced. It is possible that only one of them is reduced. \n2. The writing of the paper needs to be improved. Some notations are not consistent with each other. For example, the loss notations in Line 19 in Algorithm 1 are different from those defined in Sec. 4.3. \n3. There is no step size, \\eta in Line 20 in Algorithm 1, but there is a step size in the last equation on Page 6. \n4. It is unclear to me how the hyperparameters, such as the step size and \\lambda's, are chosen. \n5. More experimental results are needed to support the proposed approach. \n\nIn summary, I think this paper is not ready to be published. \n\n ==== After rebuttal ====\nThe authors' feedback clarified some of my concerns. But my main concern about why minimizing the objective function can reduce both error and the number of parameters still remains. So I changed my rating to 4 from 3. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}