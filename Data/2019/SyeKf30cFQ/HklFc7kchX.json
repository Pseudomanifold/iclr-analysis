{"title": "The authors propose a framework that utilizes the teacher-student setting and give some impressive evaluations on deep neural networks. This paper has rigorous theoretical analysis, but lacks necessary experiments.", "review": "The authors propose a framework that utilizes the teacher-student setting to evaluate deep locally connected ReLU network. The framework explicitly formulates data distribution, which has not been considered by previous works. The authors also show that their framework is compatible with Batch Normalization and favors disentangled representation when data distributions have factorizable structures. Based on this framework, the authors re-explain some common issues of deep learning, such as overfitting. \n\nMy major concerns are as follows.\n\n1. The framework is based on the teacher-student setting, and the authors claim that \"the teacher generates classification label via a hidden computational graph\". However, how the teacher can be designed is not clear in the paper.\n\n2. The data distribution included in this paper is $P(z_{\\alpha}, z_{\\beta})$, where $z_{\\alpha}$ and $z_{\\beta}$ are all summarization variables. From this perspective, it only has an indirect connection with original data distribution $P(x)$ or $P(x_{\\alpha}, x_{\\beta})$, and thus it could be questionable whether $P(z_{\\alpha}, z_{\\beta})$ is a convincing representation.\n\n3. The authors may want to conduct more experiments to better support their claims.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}