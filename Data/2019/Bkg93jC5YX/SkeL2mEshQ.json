{"title": "a semi-supervised algorithm for bilingual lexicon induction problem", "review": "Summary: \n\nThe paper propose a semi-supervised algorithm for bilingual lexicon induction (BLI) problem. Prior works on BLI problem usually impose orthogonality constraint on the linear transformation in order to obtain a \"reversible\" mapping and to preserve the monolingual performance. However, from both modeling and generalization perspective, recent works do not impose this constraint while learning the mapping (Doval et al 2018, Jawanpuria et al 2018, Joulin et al 2018, Sogaard et al 2018, among others). The present work argues for the removal of the orthogonality constraint when language spaces are non-isometric, and proposes to employ the Gromov Hausdroff (GH) distance to validate this condition. Overall, the paper employs  an objective function which is the sum of the (unsupervised) adversarial distribution matching objective (Lample et al 2018b), (supervised) the BLI loss function (typically the square loss), and a consistency loss (Hoshen and Wolf 2018). Empirically, the proposed method shows better results than unsupervised method of Lample et al (2018b) and the Procrustes solution.\n\nThe pros:\n\n- Existing works have shown that some BLI techniques perform better than the other in *some* pair of languages. Hence, it seems that there may not be \"one size fit all\" BLI technique. The proposed usage of GH distance is in the direction to quantitatively categorize pairs of languages. Based on a carefully crafted metric, the practical systems may chose to use one BLI algorithm over another for a given pair of languages.\n\nThe cons:\n\n- From modeling perspective, the utility of weak orthogonality constraint in the objective function is unclear. Does it improve generalization performance? Is it for preserving monolingual performance? The cited works (in the above summary) show that removing the strong/weak orthogonality constraint improves the BLI accuracy while preserving the monolingual performance.\n- The baselines chosen for experiments are not state-of-the-art. In addition, Artetxe et al. (2017, 2018) results are with NN/ISF retrieval procedure. These baselines should be rerun with CSLS retrieval procedure (codes are available in the author's website), which is now a standard for BLI task. Refer to Artetxe et al (2018b), Joulin et al (2018), Jawanpuria et al (2018), Gravel et al (2018) for state-of-the-art (semi-supervised/ unsupervised) results on MUSE and Vecmap datasets.\n- Experiments with varying data (Table 6) does not provide a clear picture without discussing unsupervised/semi-supervised baselines.\n- The logic behind experiments on GH distance (Table 2) is unclear. Why should a high correlation with *a baseline* suggest that GH distance correlates well with the degree of isometry of the two languages? Does GH distance has high correlation with *any* baseline for BLI?\n\n\nArtetxe et al (2018b): A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.\nJoulin et al (2018): Loss in translation: Learning bilingual word mapping with a retrieval criterion.\nJawanpuria et al (2018): Learning multilingual word embeddings in latent metric space: a geometric approach.\nHoshen and Wolf (2018): Non-adversarial unsupervised word translation.\nDoval et al (2018): Improving cross-lingual word embeddings by meeting in the middle.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}