{"title": "The work proposes a variational approach to meta-learning that employs latent variables corresponding to task-specific datasets, but is presented in a misleading and imprecise manner. The experimental improvements are not well-motivated by the methodology introduced in the paper.", "review": "Strengths:\n+ A variational approach to meta-learning is timely in light of recent approaches to solving meta-learning problems using a probabilistic framework.\n+ The experimental result on a standard meta-learning benchmark, miniImageNet, is a significant improvement.\n\nWeaknesses:\n- The paper is motivated in a confusing manner and neglects to thoroughly review the literature on weight uncertainty in neural networks.\n- The SotA result miniImageNet is the result of a bag-of-tricks approach that is not well motivated by the main methodology of the paper in Section 2.\n\nMajor points:\n- The motivation for and derivation of the approach in Section 2 is misleading, as the resulting algorithm does not model uncertainty over the weights of a neural network, but instead a latent code z corresponding to the task data S. Moreover, the approach is not fully Bayesian as a point estimate of the hyperparameter \\alpha is computed; instead, the approach is more similar to empirical Bayes. The submission needs significant rewriting to clarify these issues. I also suggest more thoroughly reviewing work on explicit weight uncertainty (e.g., https://arxiv.org/abs/1505.05424 , http://proceedings.mlr.press/v54/sun17b.html , https://arxiv.org/abs/1712.02390 ).\n- Section 3, which motivates a combination of the variational approach and prototypical networks, is quite out-of-place and unmotivated from a probabilistic perspective. The motivation is deferred to Section 5 but this makes Section 3 quite unreadable. Why was this extraneous component introduced, besides as a way to bump performance on miniImageNet? \n- The model for the sinusoidal data seems heavily overparameterized (12 layers * 128 units), and the model for the miniImageNet experiment (a ResNet) has significantly more parameters than models used in Prototypical Networks and MAML.\n- The training and test set sampling procedure yields a different dataset than the one used in e.g., MAML or Prototypical Networks. Did the authors reproduce the results reported in Table 1 using their dataset?\n\nMinor points:\n- abstract: \"variational Bayes neural networks\" -> variational Bayesian neural networks, but also this mixes an inference procedure with just being Bayesian\n- pg. 1: \"but an RBF kernel constitute a prior that is too generic for many tasks\" give some details as to why?\n- pg. 2: \"we extend to three level of hierarchies and obtain a model more suited for classification\" This is not clear.\n- pg. 2: \" variational Bayes approach\" -> variational Bayesian approach OR approach of variational Bayes\n- pg. 2: \"scalable algorithm, which we refer to as deep prior\" This phrasing is strange to me. A prior is an object, not an algorithm, and moreover, the word \"deep\" is overloaded in this setting.\n- pg. 3: \"the normalization factor implied by the \"\u221d\" sign is still intractable.\" This is not good technical presentation.\n- pg. 3: \"we use a single IAF for all tasks and we condition on an additional task specific context cj\" It might be nice to explore or mention that sharing parameters might be helpful in the multitask setting...\n- Section 2.4 describes Robbins & Munro style estimation. Why call this the \"mini-batch\" principle?", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}