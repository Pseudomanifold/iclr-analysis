{"title": "Review", "review": "In this paper the authors propose DL2 a system for training and querying neural networks with logical constraints\n\nThe proposed approach is intriguing but in my humble opinion the presentation of the paper could be improved. Indeed I think that the paper is bit too hard to follow. \nThe example at page 2 is not clearly explained.\n\nIn Equation 1 the relationship between constants S_i and the variables z is not clear. Is each S_i an assignment to z?\n\nI do not understand the step from Eq. 4 to Eq. 6. Why does arg min become min?\n\nAt page 4 the authors state \"we sometimes write a predicate \\phi to denote its indicator function 1_\\phi\". I\u2019m a bit confused here, when is the indicator function used in equations 1-6?\n\nWhat kind of architecture is used for implementing DL2? Is a feedforward network used? How many layers does it have? How many neurons for each layer? No information about it is provided by authors.\n\nIt is not clear to me why DL2/training is implemented in PyTorch and DL2/querying in TensorFlow. Are those two separate systems? And why implementing them using different frameworks?\n\nIn conclusion, I\u2019m a bit insecure about the rating to give to this paper, the system seems interesting, but several part are not clear to me.\n\n[Minor comments]\nIt seems strange to me to use the notation L_inf instead of B_\\epsilon to denote a ball.\n\nIn theorem 1. \\delta is a constant, right? It seems strange to me to have a limit over a constant.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}