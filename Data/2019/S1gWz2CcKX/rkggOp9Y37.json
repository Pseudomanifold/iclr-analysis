{"title": "Official Review: a multiplayer environment. Lacks comparison with related settings, many arbitrary choices, needs rewriting.", "review": "The paper presents a new evaluation platform based on massive multiplayer games, allowing for a huge number of neural agents in persistent environments.\nThe justification evolves from MMO as a source of complex behaviours arguing that these settings have some characteristics of life on earth, being a \u201ccompetitive game of life\u201d. However, there are many combinations with completely different insights and implications. The key characteristics for the setting in this paper seem to be:\n1.\tCognitive evolution with learning, rather than physical or just genetic evolution (all bodies and architectures are equal)\n2.\tChanging environments (tasks), between parameter updates\n3.\tSurvival-oriented rewards\nAnd for some experiments some agents share policy parameters to simulate \u201cspecies\u201d.\nFrom the introduction and the rest of the paper, it\u2019s not clear whether the same platform can be used with agents that are not neural, or even agents that are hardcoded (for the sake of diversity or to analyse specific behaviours). This is an important issue, as other platforms allow for the definition of some baseline agents, including random agents, agents with simple policies, etc.\nThe background and related work section covers MMO and artificial life, but has some important omissions, especially those ideas in the recent literature that are closest to this proposal.\nFirst, why can\u2019t Yang et al., 2018 be extended with further tasks? \nSecond, conceptually, the whole setting is very similar to the Darwin-Wallace setting proposed in Orello et al. 2011:\n@inproceedings{hernandez2011more,\n  title={On more realistic environment distributions for defining, evaluating and developing intelligence},\n  author={Hern{\\'a}ndez-Orallo, Jos{\\'e} and Dowe, David L and Espa{\\~n}a-Cubillo, Sergio and Hern{\\'a}ndez-Lloreda, M Victoria and Insa-Cabrera, Javier},\n  booktitle={International Conference on Artificial General Intelligence},\n  pages={82--91},\n  year={2011},\n  organization={Springer}\n}\n\nThe three characteristics mentioned before are the key elements of this evaluation setting, which changes environments between generations. Also, the setting is presented in the context of evaluation and experimentation, as this manuscript.\n\nThird, regarding multi-agent evaluation setting, Marlo over Minecraft (Malmo) is covering this niche as well. \n\nhttps://marlo-ai.github.io/\n\nAlthough it is episodic and the number of agents is limited, this should be compared too.\n\nNevertheless, the authors should make a more convincing argument about why we need *massively* multiplayer settings. Why is it the case that some behaviours and skills appear with thousands of agents but cannot appear with dozens of examples? In evolutionary game theory, for instance, some complex situations emerge from very few agents.\n\nFinally, the use of agents that have to survive with \u201chealth, food and water\u201d and its use as experimental setting can be found in Stranneg\u00e5rd et al. 2018.   \nhttps://www.degruyter.com/downloadpdf/j/jagi.2018.9.issue-1/jagi-2018-0002/jagi-2018-0002.pdf\nFigures are not very helpful. Especially the captions do not really explain what we see in the figures. For instance, Figure 2 doesn\u2019t show much. Figure 3 left and middle show some weird dots and patterns, but they are not explained. Also, the one on the right tries to show \u201cghosting\u201d, but colours and their meaning are not explained. Similarly, it is not clear what the agents see and process. I assume it is a local grid as the one seen in figure 4. But this is quite an aerial view, and other grid options might do the job as well.\nSimilarly, some actions are mentioned (it seems that N, S, E, W and \u201cPass\u201d? plus some attack options, but they are not described). In the end, I understand many choices have to be made for any evaluating setting, but many choices are very arbitrary (end of section 3 and especially experiments) and there is a lot of tuning, so it\u2019s unclear whether some of the observations happen just in a particular combination of choices, but are more general. The authors end up with many inconclusive observations and doubts (\u201cperhaps\u201d) about small changes, at the end of section 5.\nOther things such as the \u201cspawn cap\u201d and the \u201cserver merge\u201d are poorly explained, with clear definitions and proper justification of their role. Similarly, I\u2019m not sure about how reproduction takes place or not, and if so, whether weights are inherited or reinitialised. Something related is said about species.\nI found the statement about multiagent competition being a curriculum magnifier, not a curriculum itself, very interesting, but is this really shown in the paper or elsewhere?\nIn general, I miss many details and justifications for the whole architecture and mechanism of this neural MMO.\nPros:\n-\tDesigned to be scalable\n-\tGoes in the right direction of benchmarks that can capture generally variable (social) behaviour.\nCons:\n-\tPoor comparison with existing platforms and similar ideas.\n-\tToo many arbitrary decisions for the setting and the experiments to make it work or show complex behaviours\n-\tThe paper needs extensive rewriting, clarifying many details, with the figures really helping for the understanding.\nTypos and minor things: \n-\t\u201cSusan Zhang 2018\u201d is named a couple of times, but the reference is missing. Also, it is quite unusual to use the given name for this researcher while this is not done for any other of the references.\n-\t\u201cas show in Figure 2\u201d -> shown\n-\t\u201cimpassible\u201d -> \u201cimpassable\u201d\n\n****************************\nI've read the new comments from the authors and the new version of the paper. I think that the paper has improved significantly in terms of presentations, coverage of related work. I still see that the contribution is somewhat limited, but I'm updating the score to better account with this new version of the paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}