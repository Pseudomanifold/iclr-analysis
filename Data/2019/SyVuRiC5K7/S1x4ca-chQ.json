{"title": "Novel idea, but important details and deeper analysis are missing", "review": "Summary\nThis paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.  The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results. \n\nPros. \n-This paper is well-motivated. Studying label propagation in the meta-learning setting is interesting and novel. Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low. \n-The empirical results show improvement over the baselines, which are expected. \n\nCons.\n-Some technical details  are missing. In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing. Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?\n-Does episode training help label propagation? How about the results of label propagation without the episode training? \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}