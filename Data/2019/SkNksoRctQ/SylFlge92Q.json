{"title": "Inspired by statistical mechanics, the authors derive the stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in SGD. They further use the relations to set training schedule adaptively and analyze the loss-function landscape. However, the analysis about the stationarity assumption is insufficient and some experiments are weak.", "review": "The authors establish a stationary fluctuation-dissipation theorem and derive two specific fluctuation-dissipation relations. The authors use the first relation to check the stationarity and the second relation to delineate the shape of the loss-function landscape.\nTo verify their claim, the authors further use the relations to set the learning-rate schedule adaptively in SGD. \n\nMy major concerns are as follows.\n\n1. The experiments in subsection 3.3 are not convincing. The authors compare the proposed adaptive training schedule with a preset training schedule. However, the improvement by the proposed schedule is insignificant.\nTo make this paper more convincing, the authors may want to compare the proposed adaptive training schedule with other approaches that have dynamic learning rates, such as those mentioned in [1].\n\n2. The derived relations are based on the stationarity assumption. However, there are few discussions on when this assumption will hold. The authors may want to analyze the conditions for the assumption to hold and explain why imposing L^2-regularization can ensure stationarity.\n\nThis paper will be more convincing if the above issues are addressed properly, and I will be happy to raise my score.\n\n[1] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv: 1609.04747, 2017.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}