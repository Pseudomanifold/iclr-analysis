{"title": "Interesting idea, but heuristical.", "review": "Summary\nThis paper proposes label propagation network (LPN), a neural network to learn label prediction and similarity measure (weights) between data points simultaneously in semi-supervised setting. The proposed method simulates label propagation steps with the forward pass of LPN, enabling backpropagation through label propagation steps.\n\nStrong points\n- Learning both weights and label predictions in SSL seems to be novel (provided that the author's claim in the related work section is right).\n- Good performance.\n- The paper is generally well written.\n\nConcerns\n- Replacing the label propagation by forward pass of a neural network is an attractive idea, but because of that the convergence guarantee is lost.  As Figure 4 shows, LPN without bifurcation mechanism seems to suffer from convergence issue as the number of evaluation step grows. I guess that the algorithm may go wrong even with bifurcation mechanism for some data, for example if the bifurcation rate grows too fast/slow.\n- The original label propagation works with weights without entropy. Does introducing entropy term (e(h_i;theta)) is always helpful? For instance, if some data points erroneously get certain during initial iterations, the whole algorithm may fail.\n- The performance reported for GCN is quite different from what is presented in the GCN paper, and authors explain that this is due to the different experimental setting. For me the performance gap is quite significant to be originated from different experimental setting. Could you elaborate on this? Also, how many GCN layers were used?\n- Too many hyperparameters to tune.\n\nMinor points\n- I think the line above Eq (4) should be like \\tilde w_ij = w_ij / sum_k w_ik.\n- Eq (10) is quite misleading. The original weight w_ij should be symmetric (w_ij = w_ji), but this is not. Also, considering the intuition behind the label propagation, I think Eq (10) should be like alpha_ij(h_i, h_j) = exp(e(h_j) + d(h_i, h_j)), not e(h_i) as written the paper.\n- In the experiments setting, the authors calling their algorithm as DeepLP_alpha and DeepLP_phi. I guess these should be LPN_alpha and LPN_phi.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}