{"title": "Method for non-linear label propagation while learning the network weights simultaneously. Insufficient comparison to related methods, insufficient experimental evidence and explanation for the results. ", "review": "**** After Revision ***********\nI thank the authors for diligently revising the paper according to the reviewers' suggestions. I have increased my score for the paper. I still think the experimental evaluation can be more thorough. For example, it would be good to show the effect of varying the \\tau parameter and the number of available labels (k). It would also be good to experiment with the Flickr graph without any sparsification and to add uncertainty estimates to the results in Table 1. \n**** After Revision ***********\n\nThis paper proposes a framework for non-linear label propagation where the weights are learned simultaneously. There are model specific and experimental setup design decisions that require justification. There also needs to be a number of ablation studies to justify the effectiveness of the different components of this framework.  Finally, there seems to be an insufficient comparison (both experimentally and theoretically) to the large amount of related literature. \n- What is the total number of parameters in the proposed network? Please clarify how this is \"relatively few parameters\" as compared to other methods. \n- Please compare how your method for learning weights relates to the following papers and the references therein [1,2]\n[1] Online Learning of Multiple Tasks and Their Relationships. Saha et al, AISTATS, 2011. \n[2] Convex Learning of Multiple Tasks and their Structure, Cilberto et al, 2015. \n- It would be good to have an ablation study in order to discern what is the contribution of learning the weights vs propagating labels (instead of embeddings). \n- For clarity, please specify that \\theta are the parameters to be learned. \n- Please explain the intuition of using entropy and KL divergence for the attention weights. Shouldn't the attention for an edge be inversely proportional to the entropy i.e. the attention should be higher if the neighboring node's label is more certain?\n- Instead of the bifurcation mechanism proposed in section 3.2, isn't it possible to use a threshold to round the resulting prediction to a hard label?\n- In equation 13, are the hyper-parameters a, b tuned using cross-validation? Can't we learn the \\tau in the same training procedure? Please justify this design decision?\n- What is the performance if the loss in equation 14 is replaced by the standard empirical loss? There needs to be an ablation study on this. \n- If the node features are available, how are they used in this framework?\n- In the experimental section, why is k chosen to be equal to 1%? Please show results while varying this. \n- Please justify the line \"parameterizes w using a small number (~20) of informative features based on the raw features (e.g., dimensionality reduction), the graph (e.g., edge betweenness), and the labeled set (e.g., distance from labeled nodes). \" Isn't it possible to get similar performance by reducing the number of parameters so that model doesn't overfit?\n- Please clearly state what is the difference in the framework from the Kipf and Welling, 2016 paper?\n- Why isn't there a comparison to methods like Graph-Sage?\n- Please explain this line \"LPNnobif degrades with large T, and even \\tau slightly above 1 makes a difference\"\n- Finally, please explain the trend in the results in Table 1. For example, why is the performance of the proposed method poor on the Flickr dataset, but better on the DBLP dataset?\n- It would good to have uncertainty estimates for the results reported in Table 1. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}