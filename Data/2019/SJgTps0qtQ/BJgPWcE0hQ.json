{"title": "The papers proposes methods to robustify reinforcement learning algorithms against environment uncertainty which arises due to parametric variability. This is a interesting paper with promising results. What would make this paper a clear accept is the addition of experiments with high dimensional systems with more unknown parameters.  ", "review": "- Does the paper present substantively new ideas or explore an under-explored or highly novel question? \n\nThe paper claimed that there is limited work on the investigating the sensitivity of RL caused by the physics variations of the environment, such as object weight, surface friction, arm dynamics, etc. So the paper proposed learning a stochastic curriculum, guided by episodic reward signals (which is their contribution compared with previous related work) to develop policies robust to environmental perturbation.  Overall the combination of ideas is novel but the experimental results are limited in scope. \n\n- Does the results substantively advance the state of the art?\n\nThe results advance the state of the art, since they are compared against : 1) the best results observed via a grid search (oracle) on policies trained exclusively on specific individual environment settings; 2) Policies trained under a mixed training structure, where the environment settings are varied every episode during training, with the episode settings drawn uniformly at random from a list of values of interest. Their 3 experiment results are competitive with 1) and much better than 2).\n\n- Will a substantial fraction of the ICLR attendees be interested in reading this paper? \n\nYes, because the robustness of RL policies to changes in the physic parameters of the environment has not been well explored. Although previous investigations exist, and this paper\u2019s algorithm is the combination of EXP3 and DDPG, it is still interesting to see them combined together to solve model uncertainty problem of RL with very good simulation results.\n\n- Would I send this paper to one of my colleagues to read? \n\n  I would definitely send the paper to my colleagues to read. \n\n\n- In terms of quality:  \n\nClear motivation; substantiated literature review; but the algorithms proposed are not novel and the question of whether the method will scale to more unknown parameters is not answered. \n\n- I terms of clarity:  \n\nEasy to read.\u2013Experimental evaluation is clearly presented.\n\n- Originality:  The problem of developing an automated curriculum for learning generalization over environment settings for a given RL task is formulated as a multi-armed bandit problem, and EXP3 algorithm is used to minimize regret and maximize the actor\u2019s rewards. Itis a very interesting application of EXP3, although such inspiration is drawn from a former multi-task NLP paper Graves et al. (2017).\n\n- In terms of significance:  \n\n The paper is definitely interesting and presents an  promising  direction. The significance is  limited because of the simplicity of the examples considered in the experimental session. It would be interesting to see how this method performs in problems with more states and more unknown parameters.   \n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}