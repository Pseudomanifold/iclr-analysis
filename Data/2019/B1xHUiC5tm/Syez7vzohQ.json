{"title": "Interesting paper; needs more comparisons, methodological development", "review": "Quality: the paper is overall okay, but I would like to see significantly more comparisons and methodological development.\n\nClarity: the paper is mostly clear, though the presentation could be improved in various parts, as I mention below.\n\nOriginality: as far as I am aware the approach itself is novel, but I am having trouble identifying a core methodological advance that you have made.\n\nSignificance: multivariate time series forecasting and spatiotemporal forecasting is important, and you have clearly made a contribution in this area, but I would like more development. I will try to make some suggestions about where to go next below.\n\nSuggestions for improvements:\n\n1) I know the use of the term \"context\" is standard in parts of ML, but I have never been convinced that there is a truly new concept here that cannot be addressed using existing statistical ideas: coding \"context\" simply as indicator variables (covariates) is an obvious starting point, even better to pursue hierarchical modelling (i.e. fixed, random, and mixed effects). Stated another way: in the case of actual time series models, I don't see how \"context\" is a different concept than covariates, which are of course a standard part of time series modelling. And in the case of spatial models, there are a number of approaches which you do not mention (CAR and Gaussian processes being the leading approaches) which explicitly model spatial dependence. \n\n2) Your related work section could be significantly improved to make it clearer what your contribution is.\n\n3) Following up on point #1 above, your data is fundamentally a set of counts observed over space and time. Various appropriate statistical models exist for handling this (VAR with a Poisson or Negative Binomial likelihood, GP regression with a Poisson / NegBinom likelihood, CAR, etc.); certainly there is room to improve all of these statistical methods. And switching from a distribution appropriate for count data (e.g. Poisson) to the squared error loss (aka Gaussian) is potentially a reasonable approximation with high counts. However, I think you need to make this case, at least by mentioning other approaches and possibly by explicitly comparing. Perhaps none of them are scalable! Or your neural net does much better! But we don't know...\n\n4) Spatial RNNs have been proposed before--perhaps you are doing something that distinguishes your approach? You should state this.\n\n5) Returning to the idea of hierarchical modelling---there is a large amount of information that you could be building into the model, for example the data presumably has very marked periodicity throughout the day. Similarly, weekdays are most likely similar to each other, and so are weekends. I do not see any of this being built into the model. Maybe that's ok, you have enough data to learn this, but I would at least like to see plots showing that indeed your model is capturing these important features.\n\n6) How can I interpret the RMSEs? I suppose that after you scaled everything to [0,1], fit the model, and made your predictions, you rescaled back to the true values and calculated RMSE? Thus a baseline RMSE of 32 means that in a 15 minute window the units of 32 are in number of people entering the subway? Reducing this from 32 to 25 is certainly an improvement; were half a million parameters really necessary to do that? A slightly more realistic baseline (i.e. one that takes yesterday's level of activity into account) would be helpful. Your claim is that the model is robust to unseen values, but I'm not sure what unseen values you're referring to. You also claim your model can handle the anomaly on 4 November, but in preprocessing your data you excluded 15 days that were anomalous.\n\n7) Your random train / test split is stratified based on day of week. But since you are randomly leaving out days, your results may be overoptimistic in the fact that a given day that is left out of the data is similar to other days that are left in, i.e. the day right before and right after. This may not matter as much as you're not training a standard time series setting, where interpolation might be very easy but forecasting might be hard. Nevertheless it would be useful to try a harder train/test split, leaving out blocks of, say, 3 days.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}