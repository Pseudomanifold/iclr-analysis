{"title": "lack of contributions and limited comparisons", "review": "The authors applied the external memory module proposed by Graves et al. (2016) to the image segmentation task. SHAMANN is an extension to allow memory sharing between directions. \n\nAuthors claimed that one of the contributions is a reformulation of the semantic segmentation problem as a sequence learning task.\nThere are many previous works done in this direction,\n- \"Multi-Dimensional Recurrent Neural Networks\", 2007\n- \"Scene Labeling with LSTM Recurrent Neural Networks\", 2015\n- \"ReSeg: A Recurrent Neural Network-Based Model for Semantic Segmentation\", 2016\n- \"Robust, Simple Page Segmentation Using Hybrid Convolutional MDLSTM Networks\", 2017 \nand many more.\nAuthors should compare with those LSTM-based image segmentation approaches as well.\n\nTheir second contribution is a network with a shared external memory module between directions. However, the experiments are not enough to show the benefits of it. See the details below.\n\nHandling long-range dependencies:\n- In Section 3.3.2, authors mentioned that \"One limitation of Bi-LSTM is that the number of network parameters grows proportionally to the memorization capacity, making it unsuitable for sequences with long-range dependencies.\". \nHowever, the experiments are not with long range sequences: 169 sequence length for X-ray dataset and 49 length for MNIST. A classic LSTM (not bi-directional) is known to handle up to 200 timesteps. Some comparison/analysis of handling long-range dependencies of Bi-LSTM, Bi-MANN, and SHAMANN are needed (ideally on high-resolution real images).\n\nDataset:\n-  Authors compared 3 models only on MNIST. The structure on MNIST is simple, and the resolution of images is small to show the benefit of using (shared) external memory module instead of individual memory cells. It is not surprising that the reported performance difference is small. Authors could have reported such a comparison on X-ray dataset too but they did not. I would recommend authors pick another high-resolution real-image dataset and compare the performance of these 3 models.  \n\nAdditional comparisons:\n- Various patch size\n- Longer sequence length\n- Especially a trade-off between the patch size and the sequence length on the high resolution images (larger patch size with a shorter sequence length or shorter patch size with a longer sequence length)\n\n- A comparison of Bi-LSTM with sharing weights will also be a good baseline. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}