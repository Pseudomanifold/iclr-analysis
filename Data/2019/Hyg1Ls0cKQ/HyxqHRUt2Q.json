{"title": "Really confusing", "review": "This paper proposes to learn a manifold of deep generative models using a pre-trained VAE. To generate samples with desired features, this paper proposes to learn an embedding of each feature in the hidden space using VAE. Then the learned hidden space is used to train a GAN.\n\nHowever, the method in this paper and main contributions are not clearly represented. I can hardly understand the motivation of this paper. In the introduction part, this paper mentions \u201clarge scale of latent space\u201d lots of times, but does not make it clear that why a large latent space hinders the deep generative models. In Fig.1, it demonstrates that for some manifold, L2 distance cannot be applied directly. However, for most DGMs, the hidden space is defined in Euclid Space, and L2 distance is a valid distance for them. \n\nIn Sec. 3, the method is not presented clearly and the notation is confusing. In Sec. 3.2.1, Eqn (8) is not an objective function and it is confusing how to optimize the generator using it. In Sec. 3.2.2, the notation is really confusing and I can hardly understand the proof the Theorem 2.\n\nThe experimental results are not solid where no well-known metrics, such as Inception Score, FID, are used to evaluate the generated samples. For compression rate, the size of bottleneck has not been mentioned above, and the experimental setting of each baseline is not ignored which makes the experimental results incomparable.\n\nOverall, this paper is not a qualified paper for ICLR.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}