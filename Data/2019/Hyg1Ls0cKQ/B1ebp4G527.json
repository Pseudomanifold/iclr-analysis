{"title": "Poorly written draft with minimal technical novelty", "review": "The paper proposes a generative model that combines VAE and GAN. The main idea of the paper is to replace the standard normal distribution used in VAE with a normal distribution centered at a feature representation of the input image. In other words, the prior distribution is data adaptive. The paper compares the proposed generative model to DCGAN and EBGAN for image generation quality using the CelebA dataset and reports better human preference score.\n\nOverall, the paper is poorly written with incorrect technical descriptions and vague expositions.  The two baselines (DCGAN and EBGAN) are also quite out-dated. Beating these two baselines are insignificant, particularly there are GAN methods that  can generate high quality images without an encoder. For example, the Progressive GAN by Terro et. al. (ICLR 2018), SNGAN by Miyato et. al.(ICLR 2018), and GAN with zero-center gradient penalty by Mescheder et. al. (ICML 2018). The paper also fails to give a literature overview of effort in combining VAE and GAN. For example, Zhiting et. al. ICLR 2018 and Liu et. al. NIPS 2017.\n\nTechnical errors\n\n- In the related works section, the paper states that VAEs and GANs are both based on maximum likelihood. This statement is incorrect as GANs are based on distribution matching. \n\nVague exposition\n\n- In Section 2, the paper states that \"Larsen et al. (2015) used both VAE and GAN in one generative model. As they just mixed two models and did not analyzed a latent space, so that the manifold of data was hidden to us.\" Isn't the data manifold in this case a multivariate Gaussian distribution. The paper fails to explain what it means by the sentence.\n\n- In Section 3.1, the paper states that \"Since any supervision is not in training process, the manifold constructed is hidden to us.\" Again, the reviewer fails to understand what the paper means.\n\n- In Section 3.2.1, the paper states that \"it is not efficient to pre-train the G , because it depends on the parameters of the D.\" This sentence is confusing. Isn't pretraining just meaning using a pretrained decoder weight to initialize G?\n\n\n\n\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}