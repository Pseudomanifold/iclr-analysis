{"title": "Review: Diminishing Batch normalization. ", "review": "\nIn this work, the authors propose a generalization of the batch normalization (BN) technique often used in training neural networks, and analyzed this convergence. In particular, a one hidden layer and one BN hidden layer fully connected network is considered, and a deterministic gradient descent algorithm with certain kind of BN has been considered in this work. The proposed \u201cgeneralized\u201d BN strategy is devised on the deterministic setting, but it is a slight generalization of the original BN by introducing a moving average operation. Classical results of Bertsekas is leveraged to show the asymptotic convergence of the algorithm. \n\nI have the following three main comments about the paper. \n1)\tOnly deterministic setting is considered, but in this case every time the entire data set will be used to perform the averaging, it appears to be much easier to analyze than the stochastic setting. Further the reviewer has doubt on whether the resulting deterministic algorithm has any practice value. \n2)\tBecause the authors have used the Bertsekas/Tsitsiklis (B/T) argument, only asymptotic convergence is shown. It is not clear, even in the deterministic case, whether some kind of sublinear convergence rate can be obtained. \n3)\tOnly one hidden layer of neural network with one BN operation is considered. It is not clear whether the analysis can be extended to multiple layers, despite the statement of the author saying that \u201cthe technique presented can be extended to more layers with additional notation\u201d. In particular, when there are multiple layers, the BN layers will be further composite together across multiple nonlinear operations. \n4)\tThe authors have mentioned that the derivative is always taking w.r.t. theta. However, in (9) is appears that the derivative is taken with respect to lambda, in order to get the Lipschitz condition on \\lambda. This is a bit confusing. Also it is not clear how the gradient in Assumption 5 is defined. \n5)\tAssumption 5 does not make sense. Problem (1) is a constrained problem with both variables being confined in compact feasible sets. And this condition is important in Assumption2. Now the authors say that at stationary solution the gradient is zero? Please specify functions when this will happen. I will suggest that the authors use a proper definition of stationarity solution for constrained problems. \n6)\tFollow up on the previous point. The analysis builds upon B/T argument for unconstrained optimization. However it is not suitable for the constrained problems that the authors started out at the beginning of the paper. The authors may consider develop new analysis tools to understand the problem at hand, rather than assuming away the difficulties. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}