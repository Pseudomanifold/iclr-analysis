{"title": "Biased particle-based gradient estimator; no comparison to related algorithms", "review": "The paper introduces a new training algorithm for generative latent variable models called Wake-Sleep-Remember (WSR). The authors note that it is related to Wake-Sleep (WS) and is supposed to improve on it. \n\nWSR uses samples from p(x, z) to train the recognition model q(z|x), similarly to WS.  The authors note that the gradient estimator of decoder parameters in WS is biased with respect to the ELBO, because samples are taken from q(z|x) instead of intractable p(z|x). To alleviate this issue, in WSR samples from q are cached (remembered), and p(x, z) is trained on samples from the cache instead of being trained on samples from q directly, which is the case in WS. In particular, the cache has a fixed size of k samples per data point, and samples are weighted according to the log probability under the model p(x, z). Whenever any of the samples in the cache is less probable under the model than a new sample from q (for that data point), it gets replaced by that new sample. The authors argue that this procedure reduces the bias of the decoder update of WS, because the decoder is trained only on \u201cgood\u201d samples. \n\nIt is also noted in \u00a72.3\u00b62 that optimising the ELBO in VAEs and WS relies on the ability of q to match the true posterior exactly - a failure to do so leads to a modification of p.  IWAE [1] and RWS [2] alleviate this issue by importance sampling. Interestingly, WSR could also mitigate this issue, but authors do not comment on that.\n\nWSR is strongly related to RWS, but the authors seem to have missed the connection. Both algorithms require evaluation of p(x, z) on k samples from q at every training iteration. The only difference seems to be that WSR can populate the cache with samples that have very high probability under the model and can reuse the same sample from the cache multiple times to train p. RWS, on the other hand, draws new samples at every training iteration. While both algorithms feature the same computational cost, the approach of RWS is based on importance sampling and the resulting gradient estimator is unbiased (but see [3]). This is unlike WSR, in which samples in the cache are updated with samples that have higher log probability under the model. This can lead to a scenario where the cache contains samples from only one mode of the posterior, or even where it is filled with virtually the same sample replicated k times. This leads to a biased gradient estimator.\n\nThe proposed algorithm is evaluated on three problems: (1) learning cellular automata, (2) composing gaussian process kernels and (3) discovering text concepts, all of which involve discrete latent variables. WSR is compared WS, a VAE with a control variate and a WSR model without a recognition model. The last baseline corresponds to a lesioned version of WSR to gauge how important the cache (memory) is. WSR achieves the highest quantitative results and produces the best looking qualitative results from all considered baselines. Experimental evaluation would be sufficient, if not for the fact that it does not feature comparison to other particle-based inference algorithms like IWAE or RWS, which are closely related to WSR. It is hard to say whether WSR is useful despite its biased gradient estimates, since both IWAE and RWS tend to achieve much better results than VAE and WS, respectively [4].\n\nThe paper is generally well written, but it is confusing in a few places. For instance, the \u201cmemory\u201d used in WSR is a type of cache and is unrelated to other (working/external/episodic) memory used in literature - it is purely algorithmic, and not architectural component. It would be clear if it was treated as such. For instance, I the \u201cmemory\u201d column in Table 2 confusing.  Moreover, it calls the latent variable \u201cz\u201d a stochastic program, which is not necessary - it is just a discrete latent variable. \u201cELBo\u201d is typically written as \u201cELBO\u201d.\n\nIt is an interesting paper, but the authors does not seems to realize that they have introduced significant biased by updating the cache at every training iteration, nor do they notice links to RWS. I believe the paper should be rejected, as it might otherwise be confusing for inexperienced readers.\n\n[1] Burda, Y., Grosse, R.B., & Salakhutdinov, R. (2015). Importance Weighted Autoencoders. CoRR, abs/1509.00519.\n[2] Bornschein, J., & Bengio, Y. (2014). Reweighted Wake-Sleep. CoRR, abs/1406.2751.\n[3] Tucker, G., Lawson, D., Gu, S., & Maddison, C. J. (2018). Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives. arXiv preprint arXiv:1810.04152.\n[4] Le, T.A., Kosiorek, A.R., Siddharth, N., Teh, Y.W., & Wood, F. (2018). Revisiting Reweighted Wake-Sleep. CoRR, abs/1805.10469.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}