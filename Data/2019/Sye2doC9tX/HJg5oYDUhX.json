{"title": "Nice paper", "review": "The authors develop a new algorithm for reinforcement learning based on adding another agent rewarded by both the extrinsic environment reward and the TD-errors of the original agent, and use the state-action pairs visited by the added agent as data for the original.\n\nThis co-evolutionary process could be more broadly described as a student agent that learns from the trajectories of a teacher agent, which visits trajectories with high reward and/or high student TD-error.\n\nThe algorithm is not proved to converge to the optimal Q.\nAlgorithm (1) by not using epsilon-greedy on Q_hat\nhas an initialization-based counter-example in the tabular bandit case\ne.g.\n  MDP being a bandit with two arms with rewards 100 and 1000 respectively\n  Q_hat that initially is X for first arm and Y for second arm, with X > 100 > Y\nThis could be solved by, for example, adopting epsilon greedy.\n\nPrioritized Experience Replay (Schaul et al. https://arxiv.org/pdf/1511.05952.pdf), which suggests using the TD-error to change the data distribution for Q-learning, should be also a baseline to evaluate against.\n[Speculative: It feels like a way to make prioritized experience replay that instead of being based on time-steps (state, action, reward, new state) is based on trajectories, and this is done by doubling the number of model parameters.]\n\nOn quality:\nThe evaluation needs different experience replay baselines.\n\nOn clarity/naming:\nHere 'uncertainty in reward space' is used to refer to TD-error (temporal difference), I found that confusing.\nHere 'intrinsic motivation' is used but the 'intrinsic motivation' proposed depends on already having an externally defined reward.\n\nPros:\n+ \"Prioritized Experience Replay for Trajectories with Learning\"\nCons:\n- Not evaluated against experience replay methods.\n- No plots showing number of gradient descent steps (as the proposed method has double gradient descent updates than the baselines)\n- No proof of correctness (nor regret bounds).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}