{"title": "A well-intentioned piece of work... but the understanding of prior work / the exploration problem is lacking", "review": "This paper suggests an exploration driven by uncertainty in the reward space.\nIn this way, the agent receives a bonus based on its squared error in reward estimation.\nThe resultant algorithm is then employed with DQN, where it outperforms an e-greedy baseline.\n\nThere are several things to like about this paper:\n- The idea of exploration wrt uncertainty in the reward is good (well... actually it feels like it's uncertainty in *value* that is important).\n- The resultant algorithm, which gives bonus to poorly-estimated rewards is a good idea.\n- The algorithm does appear to outperform the basic DQN baseline on their experiments.\n\nUnfortunately, there are several places where the paper falls down:\n- The authors wrongly present prior work on efficient exploration as \"exploration in state space\" ... rather than \"reward space\"... in fact prior work on provably-efficient exploration is dominated by \"exploration in value space\"... and actually I think this is the one that makes sense. When you look at the analysis for something like UCRL2 this is clear, the reason we give bonus on rewards/transitions is to provide optimistic bounds on the *value function*... now for tabular methods this often degenerates to \"counts\" but for analysis with generalization this is not the case: https://arxiv.org/abs/1403.3741, https://arxiv.org/abs/1406.1853\n\n- So this algorithm falls into a pretty common trope of algorithms of \"exploration bonus\" / UCB, except this time it is on the squared error of rewards (why not take the square root of this, so at least the algorithm is scale-invariant??)\n\n- Once you do start looking at taking a square root as suggested (and incorporating a notion of transition uncertainty) I think this algorithm starts to fall back on something a lot more like lin-UCB *or* the sort of bonus that is naturally introduced by Thompson (posterior) sampling... for an extension of this type of idea to value-based learning maybe look at the line of work around \"randomized value functions\"\n\n- I don't think the experimental results are particularly illuminating when comparing this method to other alternatives for exploration. It might be helpful to distill the concept to simpler settings where the superiority of this method can be clearly demonstrated.\n\nOverall, I do like the idea behind this paper... I just think that it's not fully thought through... and that actually there is better prior work in this area.\nIt could be that I am wrong, but in this case I think the authors need to include a comparison to existing work in the area that suggests \"exploration by uncertainty in value space\"... e.g. \"deep exploration via randomized value functions\"", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}