{"title": "lack of details and proper comparisons", "review": "This paper proposes to use GAN to disentangle style information from speech content. The presentation of the core idea is clear but IMO there are some key missing details and experiments.\n\n* The paper mentions '....the model could simply learn to copy the waveform information from xaud to the output and ignore s....' \n--  Did you verify this is indeed the case? 1) The style embedding in Skerry-Ryan et al.'18 serves as a single bottleneck layer, which could prevent information leaking. What dimension did you use, and did you try to use smaller size? 2) The GST layer in Wang et al.'18 is an even more aggressive bottleneck layer, which could (almost) eliminate style info entangled with content info. \n\n* The sampling process to get x_{aud}^{-} needs more careful justifications/ablations.\n-- Is random sampling enough? What if the model samples a x_{aud}^{-} that has the same speaking style as x_{aud}^{+}? (which could be a common case).\n\n* Did you consider the idea in Fader Netowrks (Lample et al.'17)', which corresponds to adding a simple adversarial loss on the style embedding? It occurs to be a much simpler alternative to the proposed method.\n\n* Table 1. \"Tacotron2\" is often referred to Shen et al.'18, not Skerry-Ryan et al.'18. Consider using something like \"Prosody-Tacotron\"?\n\n* The paramerters used for comparisons with other models are not clear. Some of them are important detail (see the first point above)\n\n* The author mentioned the distance between different clusters in the t-SNE plot. Note that the distance in t-SNE visualizations typically doesn't indicate anything.\n\n* 'TTS-GAN' is too general as the name for the proposed method.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}