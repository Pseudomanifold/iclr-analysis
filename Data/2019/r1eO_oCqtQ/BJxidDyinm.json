{"title": "Simple ideas but unconvincing results", "review": "The work takes inspiration from a recent work on phased LSTM, and proposes to add a Gaussian gate based on time to LSTM cells. With this additional gate, the network can skip updating the states by closing the time-gate, as a result enabling longer memory persistence, and better gradient flow. The authors also propose to add a budget term to force the time-gate to be closed most of the time as a way to save compute. Empirical results suggest the Gaussian-gated LSTMs perform better than regular LSTMs on tasks with long temporal dependencies. The authors also propose to use a curriculum training schedule in which the variance of the gaussian gates is continuously increased to speed up training of LSTMS.  \n\npros:\n1. The paper is clearly written and easy to follow;\n2. The way the authors introduce time-dependent gating into LSTM is easy to follow and re-implement;\n3. Experiments on various tasks of long temporal dependencies do show improvement over the standard LSTM cells;\n4. The experiments on the adding task does show gLSTM is less sensitive to initialization than the phased LSTM;\n5. The experiments on setting curriculum training schedule to improve convergence on LSTMs are insightful. \n\ncons:\n1. The work was framed as an easier-to-optimize alternative to the time-based gating mechanism introduced in phased LSTMs, which takes a parametrization form that is much harder to learn, the gating mechanism covered by the new model gLSTM however, is much more limited. The parametrization introduced in Phased LSTMs allows the memory cells and outputs of LSTMs to be updated periodically. gLSTMs on the other hand only allows updates within a single window over the entire sequence; As a result, one would expect phased LSTM to outperform gLSTM on tasks with periodical temporal dependencies;  \n2.  The empirical results are not convincing enough. gLSTM performs noticeably worse than several state-of-the-art work on improving long-term dependencies in RNNs. The authors did not give any explanation in the performance gap.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}