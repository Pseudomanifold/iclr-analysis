{"title": "Promising results, but could use some more experiments", "review": "This work proposes an alternative loss function to train models robust to adversarial attacks. Specifically, instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix. I believe the high-level idea behind this work is that changing the target labelspace is a more effective means of defending against adversarial attacks than modifying the underlying architecture, as the loss-level gradients will be strongly correlated across all architectures in the latter scenario.\n\nPros:\n-Paper was easy to follow\n-Using orthogonal encodings to decorrelate gradients is an interesting idea\n-Benchmark results appear promising compared to prior works\n\nCons:\n-This work claims that their RO-formulation is fundamentally different from 1-of-K, but I'm not completely sure that's true. One could train a classification model where the final fully connected layer (C inputs K output logits) were a frozen matrix (updates disabled) of K orthogonal basis vector (ie, the same as the C_{RO}) codebook they propose. The inputs to this layer would probably have to be L2 normalized, and the output logits would then proceed through a softmax-crossentropy layer. Would this be any less effective than the proposed scheme?\n-Another baseline/sanity test that should probably included is how does the 1-of-K softmax/cross-entropy compare with the proposed method where encoding length l = k and the C_{RO} codebook is just the identity matrix? \n-Some of the numbers in Table 4 are pretty close. Since the authors are replicating Kannan et al, it would be best to included error bars when possible to account for differences in random initializations.\n-It is unclear the extent to which better classification performance on the clean input generalizes to datasets such as ImageNet\n\nOverall, I think the results are promising, but I'm not fully convinced that similar results cannot be achieved using standard cross-entropy losses with 1-hot labels.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}