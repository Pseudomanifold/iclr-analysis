{"title": "Clear writing but only mild improvement for computational cost.", "review": "This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.\n\nIn active learning, there is generally a trade-off between data efficiency and computational cost. This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both. The improvements over basic ensembling are rather minimal, at the cost of extra computation. More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so. On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments). As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout? At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?\n\nThe novelty of this method is minimal. The technique basically fills out the fourth entry in a Punnett square.\n\nThe paper is well-written, has good experiments, and has a comprehensive related work section.\n\nOverall, this paper is good, but is not novel or important enough for acceptance.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}