{"title": "interesting work, but the theory is not very deep", "review": "This paper studies the geometry of adversarial examples under the assumption that dataset encountered in practice exhibit lower dimensional structure despite being embedded in very high dimensional input spaces. Under the proposed framework, the authors analyze several interesting phenomena and give theoretical results related to the necessary number of samples needed to achieves robustness. However, the theory in this paper is not very deep.\n\nPros:\n\nThe logic of this paper is very clear and easy to follow. Definitions and theories are illustrated with well-designed figures.\n\nThis paper shows the tradeoff between robustness under two norm and infinity norm for the case when the manifolds of two classes of data are concentric spheres.\n\nWhen data are distributed on a hypercube in a k dimensional subspace, the authors show that balls with radius \\delta centered at data samples only covers a small part of the \u2018\\delta neighborhood\u2019 of the manifold. \n\nGeneral theoretical results on robustness and minimum training set to guarantee robustness are given for nearest neighbor classifiers and other classifiers.\n\nCons:\n\nMost of the theoretical results in this paper are not very general. The tradeoff between robustness in different norms are only shown for concentric spheres; the \u2018X^\\epsilon is a poor model of \\mathcal{M}^\\epsilon\u2019 section is only shown for hypercubes in low dimensional subspaces. \n\nSection 5 is not very convincing. As is discussed later in the paper, although $X^\\delta$ only covers a small part of \\mathcal{M}^\\delta, robustness can be achieved by using balls centered at samples with larger radius.\n\nMost of the analysis is based on the assumption that samples are perfectly distributed to achieve the best possible robustness result. A more interesting case is probably when samples are generated on the manifold following some probabilistic distributions. \n\nTheorems given in Section 6 are reasonable, but not very significant. It is not very surprising that nearest neighbor classifier is more robust than \u2018x^\\epsilon based\u2019 algorithms, especially when the samples are perfectly distributed. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}