{"title": "Well motivated use of James-Stein estimator to RL problems", "review": "The paper suggest a shrinkage-based estimator (James-Stein estimator) to compute policy gradients in reinforcement learning to reduce the variance by trading some bias. Two versions are suggested: The on-policy gradients is shrinked either towards (i) model based gradient, or towards (ii) a delayed average of previous on-policy gradients. Empirically, both methods have better performance than the baseline.\n\nThe paper is clearly written and well motivated. Some details are lacking that would be of interest to the reader and to make the results reproducible. For example how is \\hat Q estimated? The trick that is referred to in the end of page about only simulating short horizon trajectories deserves more detail. I would suggest providing more details, in the text and/or in the two algorithms.\n\nThe authors claim that JS estimator for gradient estimation in RL has not been used before. I am also not aware of any other work, but have also not been looking after that line of work. The paper seems to be a good contribution to the ever increasing literature of how to improve deep RL.\n\nMinors:\n\\hat \\theta on RHS in eq (7) should be \\bar \\theta ? Otherwise, what is \\hat \\theta?\nsection 4.2 Ww -> We\n\n======= After revision =========\n\nI still think this is a very interesting, novel and relevant idea that desires attention. However, on the same time, I agree with the points raised by the other two reviewers which are all well-motivated and relevant concerns. Therefore, I join the view that the paper is not yet ready for publication but I do encourage the authors to improve their work and resubmit to another venue.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}