{"title": "Decent paper, but not very novel, sparse on details.", "review": "The paper performs model-based reinforcement learning. It makes two main contributions. First, it divides training into two phases: the unsupervised phase for learning transition dynamics and the second phase for solving a task which comes with a particular reward signal. The scope of the paper is a good fit for ICLR.\n\nThe paper is very incremental: the ideas of using an ensemble of models to quantify uncertainty, to perform unsupervised pre-training and to explore using an intrinsic reward signal have all been known for many years.\n\nThe contribution of the paper seems to be the combination of these ideas and the way in which they are applied to RL. I have the following observations / complaints about this.\n\n1. The paper is very sparse on details. There is no pseudocode for the main algorithm, and the quantity v^i_t (the epistemic variance on page 5) isn't defined anywhere. Without these things, it is difficult for me to say what the proposed algorithm is *exactly*.\n\n2. Sections 1 and 2 of the paper seem unreasonably bloated, especially given the fact that the space could have been more meaningfully used as per (1).\n\n3. The experimental section misses any kind of uncertainty estimates. If, as you say, you only had the computational resources for three runs, then you should report the results for all three. You should consider running at least one experiment for longer. This should be possible - a run of 50K steps of HalfCheetah takes about one hour on a modern 10-core PC, so this is something you should be able to do overnight.\n\n4. The exploration mechanism is a little bit of a  mystery - it isn't concretely defined anywhere except for the fact that it uses intrinsic rewards. Again, please provide pseudocode.\n\nAs the paper states now, the lack of details makes it difficult for me to accept. However, I encourage the authors to do the following:\n1. Provide pseudocode for the algorithm.\n2. Provide pseudocode for exploration mechanism (unless subsumed by (1)).\n3. Add uncertainty estimates to evaluation or at least report all runs.\n\nI am willing to re-consider my decision once these things have been done.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}