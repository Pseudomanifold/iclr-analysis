{"title": "Review for Adversarial Domain Adaptation for Stable Brain-Machine Interfaces", "review": "Here the authors define a BMI that uses an autoencoder -> LSTM -> EMG. The authors then address the problem of data drift in BMI and describe a number of domain adaptation algorithms from simple (CCA to more complex ADAN) to help ameliorate it. There are a lot of extremely interesting ideas in this paper, but the paper is not particularly well written, and the overall effect to me was confusion. What problem is being solved here? Are we describing using latent variables (AE approach) for BMI?  Are we discussing domain adaptation, i.e. handling the nonstationarity that so plagues BMI and array data?  Clearly the issue of stability is being addressed but how?  A number of different approaches are described from creating a pre-execution calibration routine whereby trials on the given day are used to calibrate to an already trained BMI (e.g. required for CCA) to putting data into an adversarial network trained on data from earlier days.  Are we instead attempting to show that a single BMI can be used across multiple days?\n\n\nThis paper is extremely interesting but suffers from lack of focus, rigor, and clarity.  \nFocus : \nAE to RNN to EMG is that the idea to compare vs. Domain adaptation via CCA/KLDM/ADAM.  \nOf course a paper can explore multiple ideas, but in this case the comparisons and controls for both are not adequate.\n\nRigor: \nWhat are meaningful comparisons for all for the AE and DA portions? The AE part is strongly related to either to Kao 2017 or Pandarinath 2018 but nothing like that is compared.  The domain adaptation part evokes data augmentation strategies of Sussillo 2016 but that is not compared.\n  \nIf I were reviewing this manuscript for a biological journal a rigorous standard would be online BMI results in two animals.  Is there a reason why this isn\u2019t the standard for ICLR? Is the idea that non-biological journals / conferences are adequate to vet new ideas before really putting them to the test in a biological journal?  The manuscript is concerned with the vexing problem of BMI stability of time, which seems to be a problem where online testing in two animals would be critical. (I appreciate this is a broader topic relevant to the BMI field beyond just this paper, but it would be helpful to get some thinking on this in the rebuttal).\n\nClarity : \nThis paper needs to be pretty seriously clarified.  The mathematical notation is not adequate to the job, nor is the motivation for the varied methodology. I cannot tell if the subscript is for time or for day. Also, what is the difference between z_0 vs. Z_0? I do not know what exactly is going into the AE or the ADAN.\n\nThe neural networks are not described to a point where one could reproduce this work. The notation for handling time is inadequate.   E.g. despite repeated readings I cannot tell how time is handled in the auto-encoder, e.g. nxt is vectorized vs feeding n-sized vector one time step at a time?\n\n\nQuestions \n\nWhat is the point of the latent representation in the AE if it is just fed to an LSTM? Is it to compare to not using it? \n\nPage 3, how precisely is time handled in the AE?  If time is just vectorized, how can one get real-time readouts? In general there is not enough detail to understand what is implemented in the AE. If only one time slice is entered into AE, then it seems clear AE won\u2019t be very good because one desires latent representation of the dynamics, not single time slices.\n\nHow big is the LSTM used to generate the EMG?\n\nIt seems like a the most relevant baseline is to compare to the data perturbation strategies in Sussillo 2016.  If you have an LSTM already up and running to predict EMG, this seems very doable.\n\nPage 4, \u201cWe then use an ADAN to align either the distribution of latent variables or the distributions of the residuals of the reconstructed neural data, the latter a proxy for the alignment of the neural latent variables.\u201d  This sentence is not adequate to explain the concepts of the various distributions, the residuals of reconstructed neural data (where do the residuals come from?), and why is one a proxy for the other.  Please expand this sentence into a few sentences, if necessary to define these concepts for the naive reader. \n\nPage 5, What parameters are minimized in equation (2)? Please expand the top sentence of page 5.\n\nPage 6, top - \u201cIn contrast, when the EMG predictor is  trained simultaneously with the AE\u2026\u201d Do you mean there is again a loss function defined by both EMG prediction and AE and summed, and then backprop is used to train both in an end-to-end fashion?  Please clarify.\n\nPage 8, How do the AE results and architecture fit into the EMG reconstruction \u201cBMI\u201d results? Is that all decoding results are first put through the AE -> LSTM -> EMG pipeline? I.e. your BMI is neural data -> AE -> LSTM -> EMG?  If so, then how does the ADAN / CCA and KLDM fit in?  You first run those three DA algorithms and then pipe it through the BMI? \n\nPage 8, How can you say that the BMI improvement of 6% is meaningful to the BMI user if you did not test the BMI online?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}