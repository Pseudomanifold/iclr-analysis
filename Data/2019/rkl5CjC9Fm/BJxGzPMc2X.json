{"title": "Exploiting importance sampling in the latent space of auto-encoder to alleviate mode collapse", "review": "This paper proposed a new regularizer for the objective of GAN\u2019s generator, with the purpose of alleviating the mode collapse problem. More specifically, with a pretrained auto-encoder, the regularizer is defined as a weighted distance between the latent codes of data samples and generated samples. Two different weights are used for data samples and generated samples, respectively. Accordingly comes the name \u201cDual Importance Weight GAN\u201d.\n\nEven though experimental results seem convincing, the paper is not considered well-written. Detailed comments are listed below.\n\n(1) Notations are confusing. For example, it\u2019s hard to tell vectors from scalars.\n(2) I think the introduced weights, w_r and w_k, play an important part in the proposed algorithm. However, there are no related analysis or experiments to discuss their influence.\n(3) In the paragraph before Sec. 4, the authors mentioned \u201cwe calculate distances among all real samples to all generated samples and assign pair one by one minimizing average distance\u201d What does that mean? Mini-batch learning is used in the experiments, right? \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}