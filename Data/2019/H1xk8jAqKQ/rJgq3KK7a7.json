{"title": "sensible method, but limited novelty and evaluation is lacking", "review": "The paper presents a strategy for solving sparse reward tasks with RL by sampling initial states from demonstrations. The method, Backplay, starts by sampling states near the end of a demonstration trajectory, so that the agent will be initialized to states near the goal. As training progresses, the initial state distribution is incrementally shifted towards earlier steps in the demonstration, until the agent is trained starting from the original initial state. The authors further provide an analysis of the sample complexity of this method on a simple MDP. The method is demonstrated on a maze navigation task and a challenging game Pommerman.\n\nThe method is simple and sensible, but not particularly novel. As the authors pointed out, a very similar strategy for using demonstrations was previously presented in an OpenAI blogpost, Learning Montezuma\u2019s Revenge from a Single Demonstration. However since that work was not published, it should not be held against this paper. That being said, sampling initial states from demonstrations is a tried-and-true strategy in RL, and the manually designed curriculum is also not particularly novel. Therefore the method is mainly a minor tweak to longstanding techniques. The paper has also acknowledges these previous works. As such, a more thorough evaluation with previous methods, such as those for automatic curriculum generation (e.g. Florensa et al. 2017 and Aytar et al. 2018) is vital, but is very much lacking in the current set of experiments.\n\nThis work can also benefit from a more diverse set of tasks to better evaluate the effectiveness of the method, and provide more insight on when such a strategy is beneficial. The experiments were conducted only on discrete grid world tasks, and additional experiments in continuous domains could be valuable. In the maze task, Backplay is not significantly better than uniform. Pommerman is a much more compelling task and shows more promising improvements from backflip. However, training seems to have been terminated fairly early, before the performance for most policies have converged. In particular, the standard dense policy in figure 3c seems to be doing pretty well, will it catch up to the backplay policy with more training? It is also pretty unexpected that the uniform policies are doing so poorly, worse than the standard policy for the Pommerman experiments. Do the authors have any intuition on why this might be the case?\n\nIn figure 3, what is the initial state distribution used to evaluate the various methods? Are all policies initialized to the original initial state of a task, or are initial states sampled from the demonstrations? Given the periodic drops in performance for the backplay policies, it appears that the initial states might be changing according the curriculum during evaluation. If that is the case, it might not be a fair comparison for the other policies, especially for the standard policies, which are trained under different initial states.\n\nAs detailed in the appendix, the sliding windows for the curriculum do not seem to have a lot of overlap. This might be a potentially problematic design decision, since the sudden change in the initial state distribution, may cause the policy to \u201cforget\u201d about strategies learned for previous initial states. Has the authors experimented with other more gradual transition strategies?\n\nI think this paper in its current form does not yet meet the bar for ICLR. But this line of work could be a potentially promising direction. More thorough evaluation, better baselines, and more diverse tasks can help to strengthen this work. Further analysis on the effects of different initialization strategies could also make for a compelling contribution.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}