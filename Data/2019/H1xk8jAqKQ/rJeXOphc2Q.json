{"title": "Request for some clarifications. ", "review": "Thanks for your submission.\n\nThe  authors present a very elegant strategy of using Backplay, that learns a curriculum around a suboptimal demonstration. The authors show the technique reaches an upper bound on sample complexity especially in sparse reward environments. The strength of the paper is the ability to learn from even 10 sub-optimal demonstrator trajectory thereby achieving optimality in reaching the goal. The biggest limitation of the method as with other vanilla model free RL is the lack of generalization. \n\nA bit more motivation on the simplified assumption that function approximation would have been better. Although, such a simplification seems to be a natural candidate to be upper bounded by the longest shortest path from v_0 to v_*; consideration of such simplicaton on the neighbourhood structure of the graph with respect to the maximum vertex degree seems to be missing or cliques. Although, the authors comment about the strong assumptions being made to aid the analysis. \n\nThe authors explain the analysis in a very precise and the analysis seems to work. Although, the part of the analysis where connections are drawn to the reciprocal spectral gap is not very clear. \n\nThe authors discuss the limitation of the analysis in the case of the binary tree, that follows from the arguments before.\n\nIt will be great to see a more systematic approach to deciding how fast/slow the window should be updated to unify some of the findings from the empirical experiments as that seems to affect the way the agent trains using Backplay.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}