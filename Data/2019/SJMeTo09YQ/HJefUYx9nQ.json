{"title": "A simple but nice idea. However, there are issues with the algorithm in the continuous action case and the evaluation could be more exhaustive. ", "review": "The paper introduces permissible actions to reinforcement learning problems. A action is non-permissible if it is known to not lead to the optimal solution. The agent can, after executing an action a_t in state s_t and ending up in s_t+1, estimate whether the action is a_t is non-permissible. This data is used to train a new classifier that predicts the permissibility of an action in a state. The exploration of the RL algorithm can now be guided by the permissibility estimate, i.e., non-permissible actions are not executed. \n\nThe paper is well written and presents a simple, but promising idea to simplify reinforcement learning methods. I so far have not seen the definition of non-permissible actions in the literature so I believe this is novel and makes intuitively also sense, as permissible actions can be identified in many scenarios. However, the paper has a few issues that I want the authors to address:\n- The amount of newly introduced hyperparameters is quite big and I am not sure whether the improved performance justifies the increased number of hyperparameters justifies. \n- How many trials have been used to generate the results? Fig3 says \"Avg. reward over past 100 training steps\". Does that mean only one trial and you average over the last 100 rewards? In order to be significant, at least 5 to 10 trials have to be used as deep RL is known to show highly varying results depending on the random seed. Please also report error bars.\n- Why are there no learning curves for Flappy Bird?\n- The method for creating the action set if the selected action is permissible seems very adhoc for me, at least in the continuous action case. Would it not make more sense to include the gradient of the classifier into the actor update of DDPG such that the policy would also learn to avoid non-permissible actions? The presented method is in my opinions very hard to scale to higher dimensional action spaces (>2), which is quite a limitation of the approach.\n- The description of Section 4, in particular of the construction of the candidate actions could be made more clear. \n- Results are only shown for a rather low dimensional action set (driving) and a discrete action example. 1-2 more illustrations where AP1 could be useful would be highly appreciated. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}