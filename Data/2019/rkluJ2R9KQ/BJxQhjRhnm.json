{"title": "Review for A new dog learns old tricks: RL finds classic optimization algorithms ", "review": "This paper introduces a new framework to solve online combinatorial problems using reinforcement learning. The idea is to encode the current input, the global parameters, and a succinct data structure (to represent current states of the online problem) as MDP states. Such a problem can then be solved by deep RL methods. To train such models, the authors use a mixture of input distributions. Some come from hard distributions which are used to prove lower bounds in the TCS community, and the others are carefully constructed distributions to fool a specific set of algorithms. The authors made an important point that their algorithms are uniform in the TCS sense, i.e., the algorithm does not depend on the input length.\nFurthermore, the authors show that the learned algorithms have similar behavior as those classical optimal algorithms that are first obtained in the online combinatorial optimization community. \n\nThe idea of using hard distributions to train the model is reasonable, but not extremely interesting/exciting to me since this is by now a standard idea in TCS. Moreover, in many cases (especially those problems that don't have a lower bound), it is even very hard to construct a hard distribution. In general, how should we use construct the input distribution in those cases? Can the proposed methods still generalize if we don't have an appropriate hard distribution? I would like to see some discussion/experiments along this line. \n\nMoreover, it is unclear to me whether the methods proposed here can really learn *uniform* algorithms for the ADWORDS problem. To make the state length independent of the number of advertisers (n), the authors discretized the state space (see Appendix B). This approach might work for small (constant) n. But as we increase n to infinity, it seems to me that due to precision issues this approach will fail. If this is true, then in order to make this algorithm work, we also need to increase the precision of the real numbers used to describe the state as we increase n. If it is the case, why is the algorithm still uniform? If it is not the case, the authors need to provide extra experimental results to show that the effectiveness of the learned algorithm keeps unchanged, even if we keep increasing n and do not change the representation of the states.\n\nIt is an interesting observation that deep RL methods can actually learn an algorithm with similar behavior as optimal classical combinatorial optimization algorithms. However, there is no explanation for this, which is a little bit frustrating. Would this phenomenon be explained by the use of hard distributions? The paper can be strengthened by providing more discussions along this line. \n\nMinor comments:\n\nThe caption of Table 2 seems to contradict its explanation (on top of page 14). Is the state space discretized or the number of advertisers changed?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}