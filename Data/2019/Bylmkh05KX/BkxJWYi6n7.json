{"title": "Success on practical task using unsupervised learning, interesting problem, comprehensive study and experiments. ", "review": "This paper proposes fully unsupervised learning algorithm for speech recognition. It involves two alternating trained component, a phoneme classifier, and a boundary refining model. The experiment results demonstrate that it achieves first success on speech recognition that approaches the supervised learning performance.  \n\nPros:\n+ The paper propose to use a frame-wise smoothing term J_FS added on J_ODM cost. In the new cost function, J_ODM controls the coarse-grained inter-segment distribution using a prepared language model P_LM, while J_FS controls the fine-grained intra-segment distribution. It is actually benefit to take use of this hierarchical 2-level scopes than only 1-level scope on evaluate the distribution mismatch in the cost function. Because otherwise, if only focus on fine-grained frame level,  much larger number of frame labels and longer N-gram have to be considered to evaluate the distribution of phoneme. Consequently, the computation can be exploding. \n+ The proposed unsupervised phoneme classification method is superior to the baseline (Liu et al., 2018) because the baseline relies on a clustering which is upper-bounded by cluster purity. Directly optimize on \\theta using an end-to-end scheme is preferred. \n+ I like the idea to use an iterative training algorithm to jointly improve classifier parameter \\theta and segment boundaries b. \n+ It is quite impressive that unsupervised learning system get close to performance of supervised system on speech recognition. The proposed system also outperforms state-of-the-art baseline with large margin. \n+ The settings of experiments are rather comprehensive. Especially the \u201cnon-matching language model\u201d, tests the case where language model cannot directly estimated from training set.  \n\nQuestions:\n1.\tIn Appendix B you mentioned that for the N-gram you choose N=5. So the original language model P_LM can be a high-dim matrix with exactly 39^5 elements. How sparse is the original P_LM? It describes that 10000 elements are chosen, which are only 0.001%(=10000/39^5) of elements in the original one. How representative are they?\n\n2.\tI notice for the balance weight of J_FS in (3), you empirically take the best \\lambda=1e-5 during experiment. To me, the scale of optimal \\lambda is such small value maybe because the order of J_FS is improperly determined. My suggestion is, could you try using square root on the current J_FS, or using standard deviation of intra-segment outputs. The reasons are, first, minimizing std is a more interpretable penalty on diversion in a same segment; second, since you have used mean of outputs in J_ODM, then it is better to use a same dimension statistics, such as std of outputs in J_FS rather than sum of squared differences, when you combine J_ODM and J_FS in a uniform cost.\n\n3.\tWhat is the time complexity of running a comparable supervised speech recognition task with unsupervised learning method? \n\nMinor issues:\nMaybe it is a typo that the second term of Eqn (2) should be \u201c-p_\\theta(y_(t+1)=y|x_(t+1))\u201d instead? Since the p_\\theta is defined as posterior probability of the frame label given the corresponding input. \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}