{"title": "Novel loss function but experiments are lacking", "review": "Motivated by the observation that most of previous dimensionality reduction methods focus on preserving \nlocal pairwise neighboring probabilities and lack in preserving global properties, this paper proposes a \nmethod called TriMap to optimize a loss function preserving similarities among triplets of data points. A large \nnumber of triplets are sampled either based on nearest neighbor calculations or random sampling. Experimental \nresults on several datasets show that TriMap identifies outliers and preserves global data properties better \nthan previous approaches based on pairwise data point comparisons.\n\nMajor:\n\nThe idea in this paper is well motivated and the loss function based on probability ratio is novel. However, \nthere are some major concerns about method analyses and experimental evaluations,\n\n1. Data embedding based on triplets has been presented in (van der Maaten and Weinberger, 2012). The authors \nneed to present detailed explanations and formal analysis why the proposed method significantly outperforms the \nprevious one. A recent dimensionality reduction method compares data points only to data cluster centers (Parametric \nt-distributed stochastic exemplar centered embedding, Min et al., 2018), does it preserve global data properties? Does \nits trivial combination with standard t-SNE well preserve both local and global data properties?\n\n2. Preserving local pairwise neighborhood structure is often the most important part in high-dimensional data \nvisualization, because only local similarities can be confidently trusted in a high-dimensional space. Even if preserving \nglobal data properties is important, the very local neighborhood structure should also be preserved. However, the \nproposed method TriMap is significantly worse than t-SNE according to AUC under the precision-recall curve. \n\n3. Standard quantitative evaluations based on 1NN error rate and quality scores (van der Maaten & Hinton 2008, Min \net al. 2018) should be added to the experiments. For preserving global data properties, quantitative evaluations on all \nthe datasets will make the experiments much more convincing.\n\n4. In the abstract, the claim that TriMap scales linearly is inaccurate, the triplet sampling requires nearest neighbor \ncalculations, which has computational complexity of at least O(nlogn)\n\n5. This paper proposed two variants of triplet sampling, nearest neighbor triplets and random triplets. Detailed experimental \ncomparisons about them should be provided in the paper.\n\n\n6. The running time comparisons in Table 1 must be wrong or highly biased with improper hyperparameter setting. Based \non tree accelerations, t-SNE can produce impressive visualization on MNIST-scale datasets within 15 minutes (please \ncheck the experimental details PP. 3235-3238 in van der Maaten, Journal of Machine Learning Research 2014).\n\n7. The authors mentioned partial observation, outliers and subclusters in the global information, but the authors do not specifically define \nwhat the global information should be rigorously, and the paper does not theoretically prove or explain via experiments how the global \ninformation is kept by TriMap.\n\n8. In the experiments, the authors applied PCA before TriMap to reduce the dimensionality while PCA is not applied in tSNE and LargeVis. The authors do not explain why the settings are different in the three methods.\n\nMinor:\n\n9. In the algorithm, the authors show different equations for different t and t\u2019, but are not evaluated in experiments.\n\n(After reading the rebuttal, I raised the rating from 5 to 6.)", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}