{"title": "This paper does not have high enough quality and novelty", "review": "The paper looks at the problem of reducing the communication requirement for implementing the distributed optimization techniques, in particular, SGD. This problem has been looked at from multiple angles by many authors. And although there are many unanswered questions in this area, I do not see the authors providing any compelling contribution to answering those questions or providing a meaningful solution.\n\nFirst of all the solution that is proposed in the paper is just a heuristic and does not seem to have a very justified basis. For example, is SUM, and PARTIAL ways of combining the local gradients the authors justify the formulas by not counting things twice. But then in the ERROR they do not care about the same issue. This tells me that it is highly probable that they have tried many things and this is what has kind of worked. I do not think this is acceptable as we would like to know why something works as opposed to just trying every possibility (which is impossible) and then finding out something works (may be only for a set of limited problems).\n\nIn addition to that, I think a much simpler version of what they are proposing has been already proposed in [1]. In [1] the authors propose to run SGD locally and then average the models every once in a while. They empirically show that such a method works well in practice. As it is clear, [1] does not do the weird heuristic of combining the gradients, but still manages to save time and converge relatively fast. So, I am not quite sure if this heuristic combining that is proposed by this paper is any useful.\n\n\n\n[1] McMahan, H. Brendan, et al. \"Communication-efficient learning of deep networks from decentralized data.\" ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}