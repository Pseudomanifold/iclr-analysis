{"title": "sparse gradient does not seem justified, and the speedup is small", "review": "This paper proposes a 3 modes for combining local and global gradients\nto better use more computing nodes. The paper is well-written and well-motivated.\n\nI question why it is a good idea to use sparse gradient in a neural network where all coordinates (i.e. hidden units) are\nexchangeable.\nThis paper claims that \"gradient values are skewed, as most are close to zero\",\nbut there is no references nor statistics. The only source of sparsity\nfor a typical MT model are those for word vectors, but in that case, perhaps it would be better to develop algorithms that specifically make use of that type of sparsity.\nThe quality degradation in methods like gradient dropping seems to suggest that\nsome arbitrary cutoff for sparsity is not a good idea.\n\nFor the results, the proposed method running on 4 nodes is compared to synchronous SGD, and\nonly got 25% and 45% speedups on the 2 tests.\nNotably, they do not compare to asynchronous SGD with stale gradients. \nSuch a test would help offer empirical support/refutation\non the assumption that the sparse gradient is a good idea in the first place.\nI think these results are not good enough to justify\nthe extra complications in the proposed method.\n\nThe paper is well-written,\nhowever AllReduce seems undefined, and I assumed it aggregates sparse gradients from all nodes.\nI also a bit confused about the characteristics of ApplyOptimizer. In gradient dropping,\nApplyOptimizer takes a sparse gradient, but in the proposed methods, it takes a dense gradient.\nI am a bit confused on how exactly the synchronous update is done, and I'd appreciate it if you can make your model of parallel updates more explicit.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}