{"title": "Experimentally weak with the results not justifying the increased computation. No comparison to other methods doing non-uniform sampling and mini-batch selection. ", "review": "###### Post-Revision ########################\nThank you for revising the paper and addressing the reviewers' concerns. The updated version reads much better and I have updated my score. \n\nUnfortunately, I still think that the experimental analysis is not enough to warrant acceptance. I would encourage the authors to have a more detailed set of experiments to showcase the effectiveness of their method and have ablation studies to disentangle the effects of the different moving parts. \n###### Post-Revision ########################\n\nThis paper considers arranging the examples into mini-batches so as to accelerate the training of metric embeddings. The\n- The paper doesn't have sufficient experimental evidence to convince me that the proposed method is useful. There is no comparison against baselines. The paper is not clearly written or well organized. Detailed comments below:\n- For example, when introducing focus and context entities, it would be helpful to give examples of this to make it clearer. \n- In section 3, please clarify that after drawing both positive and negative examples, what is the size of the minibatch for which the gradient is calculated? \n- How do you choose the size of the microbatches? If the microbatch size is too small, then the effect of associating examples is small. \n- In the line, \"Instead, we use LSH modules that are available at the start of training and are only a coarse proxy of the target similarity\" Why are you not iteratively refining the LSH modules as the training progresses? Won't this lead to an improvement in the performance? \n- In the line \"The coarse embedding can come from a weaker (and cheaper to train) model or from a partially-trained model. In our experiments we use a lower dimension SGNS model.\" Could you please clarify what is the additioanal computational complexity of the method? This involves additional computational cost? It doesn't seem to me that the results justify this increased computation. Please justify this. \n- In Lemma 3.2, the term s_i is undefined\n- \"In early training, basic coordinated microbatches with few or no LSH refinements may be most effective. As training progresses we may need to apply more LSH refinements. Eventually, we may hit a regime where IND arrangements dominate.\" This explanation is vague and has no theoretical or empirical evidence supporting it. Please clarify this. \n- Please fix the size of the axes and the legend in all the figures. \n- For figure 1, how is the step-size chosen? What is the dimensionality of the examples?\n- From figure 3, it is not clear that the proposed methods lead to significant gains over the independently sampling the examples? Are there any savings in the wall clock time for the proposed methods? Why is there no comparison against other methods that have proposed non-uniform sampling of examples for SGD (like Zhang, 2017)? Are the hyper-parameters chosen in a principled way for these experiments? ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}