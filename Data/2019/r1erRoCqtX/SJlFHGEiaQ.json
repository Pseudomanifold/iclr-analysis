{"title": "Requires further clarification and empirical justification", "review": "The paper presents a method for improving the convergence rate of Stochastic Gradient Descent for learning embeddings by grouping similar training samples together. The basic idea is that gradients computed on a batch of highly associated samples encode related information in a single update that independent samples might take multiple updates to capture. These structured minibatches are constructed by independently combining subsets of positive examples called \u201cmicrobatches\u201d. Two methods are presented for constructing these microbaches; first by grouping positive examples by shared context (called \u201cbasic\u201d microbatches), second by applying Locality Sensitive Hashing to further partition the microbatches into groups that are more likely to contain similar examples.\n\nThree datasets are used for experimental analysis: a synthetic dataset generated using the stochastic block model, and two large scale recommendation datasets. The presented algorithms are compared to a baseline of independently sampled minibatches using the cosine gap and precision for the top k predictions. The authors show the measured cosine gaps over the course of training as well as the gains in training performance for several sets of hyperparameters.\n\nThe motivation and basic intuition behind the work is clearly presented in the introductory section. The theoretical justification for the structured minibatches is reasonably convincing and invites empirical verification.\n\nGeneral concerns:\nAny method for improving the performance of an optimization process via additional preprocessing must show that the additional overhead incurred from preprocessing the data (in this case, organizing the minibatches) does not negate the achieved improvement in convergence time. This work presents no evidence that this is the case. I expected to see 1) time complexity analysis of each new algorithm proposed for preprocessing and 2) experimental results showing that the overall computation time, including the proposed preprocessing steps, was reduced by this method. Neither of these things are present in this work.\n\nFurthermore, the measured \u201ctraining gains\u201d are, to my knowledge, not clearly defined. I assume that the authors are using the number of epochs or iterations before convergence as their measure of training performance, but this should be stated explicitly rather than implicitly.\n\nFinally, the experimental results presented do not seem to entirely support the authors\u2019 conclusions. Figures 2, 3, and 4, as well as several of the figures in the appendix, show some parameter settings for which the gains over the baseline are quite limited. This makes me suspect that perhaps the coordinated minibatches aren\u2019t the only variable affecting performance.\n\nI have organized my remaining minor concerns and requests for clarification by section, detailed below.\n\nSection 1\n- In the last paragraph, the acronym SGNS is mentioned before being defined. You should either state the full name of the method (with citation) or omit the mention altogether.\n\nSection 2\n- I would like a few sentences of additional clarification on what \u201cfocus\u201d entities vs. \u201ccontext\u201d entities are in the more general case. I am familiar with what they mean in the context of Skip Gram, but I think more discussion on how this generalizes is necessary here. Same goes for what kappa (\u201cassociation strength\u201d) means, especially considering that this concept isn\u2019t really present (to my understanding) in Skip Gram.\n- Grammar correction:\n\u201cThe negative examples provide an antigravity effect that prevents all embeddings to collapse into the same vector\u201d\n\u201cto collapse\u201d -> \u201cfrom collapsing\u201d\n\nSection 3\n- Maybe this is just me, but I find the mu-beta notation for the microbatch distributions rather odd. Why not just use a single symbol?\n- I would like a bit more clarification on the proof for lemma 3.1, specifically on the last sentence, \u201cthe product of these events \u2026\u201d; that statement did not follow obviously to me.\n\nSection 3.1\n- Remove the period and colon near kappas at the end of paragraph 3. It\u2019s visually confusing with the dot index notation right next to them.\n\nSection 4\n- Typo: \u201cWe selects a row vector \u2026\u201d -> \u201cWe select a row vector \u2026\u201d\n\nSection 5\n- I don\u2019t understand what Figure 1 is trying to demonstrate. It doesn\u2019t do anything (as far as I can tell) to defend the authors\u2019 claim that COO provides a higher expected increase in cosine similarity than IND.\n\nSection 6\n- All figures in this section should have axis labels. The captions don\u2019t sufficiently explain what they are.\n\nSection 6.2\n- How is kappa computed for the recommendations datasets? This isn\u2019t obvious at all.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}