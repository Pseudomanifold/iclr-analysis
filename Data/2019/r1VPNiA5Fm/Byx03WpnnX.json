{"title": "approximation theory using relu networks", "review": "This paper describes results regarding approximations of certain function families using ReLU neural networks. The authors emphasize two points about these networks: finite width and depth that is logarithmic in the approximation error parameter $\\epsilon$. \n\nThe first result concerns approximation of polynomials, which is used as a building block for all subsequent results. This result itself is quite simple and mostly follows from simple observations or known results, though it is possible that these have not been explicitly written in this form anywhere. The other results concern smooth functions, and some kinds of non-smooth functions such as the Weirstrass function. There are two neat observations (i) using the sawtooth function to approximate sinusoidal ones and (ii) using overlapping \"approximation\" to simulate an indicator. \n\nThe paper is refreshingly well-written and pleasant to read. Most of the results are tailored to work for either periodic functions, or can be expressed as: if piecewise polynomials are a good approximation, then so are constant depth neural networks with ReLU. I'm not sure that ICLR is the best venue for these kinds of results, as any connection with learning is at best tenuous, and the kind of approximation results don't seem to have any direct bearing on machine learning.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}