{"title": "Solid work, but the presentation is not self-contained and hard to follow", "review": "This paper has two main contributions: \n 1) it extends normalizing flows to discrete settings (exploiting relaxation ideas from Jang et al and Maddison et al).\n 2) it presents an approximate fixed-point update rule for autoregressive time-series that can exploit GPU parallelism.\n\nOverall, I think the work is solid. Contribution 1 isn't very novel, but is useful and the authors did a good job there.\n\nContribution 2 seems more interesting, but is not as well studied. When is the fixed point update expected to work? \nWhat assumptions does it imply? How does performance improve with the number of steps K? Does simulating for a finite steps emphasize the effect of early z's?\nI'm a bit surprised that the authors did not attempt to study this part of their algorithm in isolation. They make a claims \nbut never look at this in detail.\n\nThat said, the authors do a good job showing the method \"works\", and figures 3F and 3G are particularly nice. \nIn 3G, is \"autoregressive\" supposed to converge to flow eventually?\nWhy don't the authors also use time as the x-axis in figure 2F (like 3F)?\n\nMy biggest complaint about the paper is the writing, which does not introduce and present ideas in a clear sequential \nmanner, making the paper hard to read. I realize ELBO is standard, but at least some description of the setup in equation 1 \nis warranted. What is x,z,\\theta etc? Any paper should aim to be minimally self-contained. This continues throughout the paper, which does not really attempt to place the contribution in the larger literature, but rather just reports what the authors did and observed.\n\nSome more examples:\n\nPage 3: \"so we need to evaluate \\hat{Q}\". This isn't defined. The authors should mention what \\hat{Q} and \\bar{Q} are.\nSimilarly for P. After a couple of passes through the paragraph, I could figure what the authors meant, but they \nshould introduce the notation they use.\n\nIn section 2, while defining their model, they do not mention the dimension of z_t until after equation 8\n(and even here, it has to be inferred).\n\nWhat is x in 6b? What is the generative model they are doing inference on?\n\nSection 2.2: it's not clear to me how convergence is defined even in the discrete case. I feel this discussion \nalso really belongs to section 2.1\n\nWhile I can understand what section 2.3 is trying to say, I could not really follow the notation.\n\nI could not understand figure 1E and the associated sentence in section 2.4\n\nWhat is the take-away of section 2.3 and 2.4? The authors seem to imply working with the discrete model is \nbetter in their experiments. Maybe forewarn the reader here?\n\nThe experiments are a bit hard to follow. It is inspired by a neuroscience application, but uses only simulated data. This is fine, but rather than describe the setup in mathematical/time-series language, it is complicated the with neuroscience jargon. As such, it feels disjointed and disconnected from the rest of the paper. I already complained that earlier sections do not describe the modeling setup, this is one way the paper could be improved.\n\nIn figure 2A and 3A, are the s's actually z's?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}