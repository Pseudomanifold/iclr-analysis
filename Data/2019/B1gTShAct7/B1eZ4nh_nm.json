{"title": "Good paper, more RL experiments and ablations would improve it substantially", "review": "The paper considers a number of streaming learning settings with various forms of dataset shift/drift of interest for continual learning research, and proposes a novel regularization-based objective enabled by a replay memory managed using the well known reservoir sampling algorithm.\n\nPros:\nThe new objective is not too surprising, but figuring out how to effectively implement this objective in a streaming setting is the strong point of this paper. \n\nTask labels are not used, yet performance seems superior to competing methods, many of which use task labels.\n\nResults are good on popular benchmarks, I find the baselines convincing in the supervised case.\n\nCons:\nDespite somewhat frequent usage, I would like to respectfully point out that Permuted MNIST experiments are not very indicative for a majority of desiderata of interest in continual learning, and i.m.h.o. should be used only as a prototyping tool. To pick one issue, such results can be misleading since the benchmark allows for \u201ctrivial\u201d solutions which effectively freeze the upper part of the network and only change first (few) layer(s) which \u201cundo\u201d the permutation. This is an artificial type of dataset shift, and is not realistic for the type of continual learning issues which appear even in single task deep reinforcement learning, where policies or value functions represented by the model need to change substantially across learning.\n\nI was pleased to see the RL experiments, which I find more convincing because dataset drifts/shifts are more interesting. Also, such applications of continual learning solutions are attempting to solve a \u2018real problem\u2019, or at least something which researchers in that field struggle with. That said, I do have a few suggestions. At first glance, it\u2019s not clear whether anything is learned in the last 3 versions of Catcher, also what the y axis actually means. What is good performance for each game is very specific to your actual settings so I have no reference to compare the scores with. The sequence of games is progressively harder, so it makes sense that scores are lower, but it\u2019s not clear whether your approach impedes learning of new tasks, i.e. what is the price to pay for not forgetting?\n\nThis is particularly important for the points you\u2019re trying to make because a large number of competing approaches either saturate the available capacity and memory with the first few tasks, or they faithfully model the recent ones. Any improvement there is worth a lot of attention, given proper comparisons. Even if this approach does not strike the \u2018optimal\u2019 balance, it is still worth knowing how much training would be required to reach full single-task performance on each game variant, and what kind of forgetting that induces. \n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}