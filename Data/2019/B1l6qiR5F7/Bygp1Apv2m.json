{"title": "A very interesting new proposal, thoroughly explored, with at least middling good results", "review": "Quality\n - Pro:\n   o This paper was in general a quality effort. It had a thorough bibliography of both older and recent relevant research contributions\n   o Providing useful, well done experimental results on four tasks was also a sign of this good thoroughness\n - Con: none observed\n\nClarity\n - Pro:\n   o The paper was generally well-written and clear. Results were clearly presented.\n - Con:\n   o Notwithstanding the half page of explanation of the intuition behind the new ON-LSTM update rules (top of p.5), it wasn't really enough for my old brain to get a good sense of what was going on \u2013 though I'm sure younger, smarter people will have made more sense of it. :) It would really help to try to provide more intuition and understanding here. Things that would probably really help include a worked example and diagrams.\n   o There were minor English/copyediting problems, but nothing that interfered with understanding. E.g., \"monotonously\" on p.4 should be \"monotonically\" (twice).\n\nOriginality\n - Pro\n   o This was REALLY NEAT! This paper had a real, clear, different idea that appeared interesting and promising. That puts it into the top half of accepted papers right there.\n   o The basic idea of the different update time scales, done flexibly, controlled by the master forget/input gates seemed original, flexible, and good.\n - Con: Nothing really observed; there are clearly a bunch of slightly related ideas, well referenced in this paper.\n\nSignificance\n - Pro\n   o If this idea pans out well, it would be a really interesting new structural prior to add to the somewhat impoverished vocabulary of successful techniques for building deep learning systems.\n   o Has an original, promising approach. That has the opportunity for impact and significance.\n - Con:\n   o The results so far are interesting, and in places promising, but not so clearly good that this idea doesn't need further evaluation of its usefulness.\n   o All the results presented are on small datasets (Penn Treebank WSJ (1 million words) size or smaller). What are the prospects on bigger datasets?  It looks like in principle this shouldn't be a big obstacle \u2013 except for not having a highly tuned CuDNN implementation, it looks like this should basically be fairly efficient like an LSTM and not hard to scale like, e.g., an RNNG.\n\nOther comments:\n - Some of the wording on page 1 seemed strange to me. Natural language has a linear overt form as spoken and (hence) written. It's really not that the sequential form is just how people conventionally \"present\" it. That is, it's not akin to a chemical compound which is really 3 dimensional but commonly \"presented\" by chemists in a convenient sequential notation.\n - p.2 2nd paragraph: Don't RNNs \"explicitly impose a chain structure\" not \"implicitly\"?!?\n - I wasn't sure I was sold on the name \"Ordered Neurons\". I'm not sure I have the perfect answer here, but it feels more like \"multi-timescale units\" is what is going on.\n - The LM results look good.\n - Because of all the different datasets, etc. it was a little hard to call the grammar induction results, but they at least look competently strong.\n - The stronger results on long dependencies in targeted syntactic evaluation look promising, but maybe you need a bigger hidden size so you can also do as well on short dependencies?\n - The logical inference results were promising \u2013 they seem to suggest that you capture some but not all of the value of explicit tree structure (a TreeLSTM) on a task like this.\n - The tree structures in Appendix A look promisingly good.\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}