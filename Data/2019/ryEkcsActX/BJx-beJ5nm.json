{"title": "confusingly written paper that also lacks some intuition", "review": "The paper proposes a new performance metric for neural architecture search based on the similarity of internal feature representations to a predefined fixed teacher network.\n\n\nThe idea to not only use performance after each epochs as a signal to guide the search procedures is sensible. \nHowever, the description of the  proposed method is somewhat confusing and lacks some intuition: \n\n1) Why should a new architecture necessarily mimic the internal representation of the teacher network? Wouldn't the best configuration simply be an exact copy of the teach network?\n  A well-performing networks could still have a low TG score, simply because its internal representation does not match the teacher layer-wise.\n\n2) Probably, in the most scenarios on new tasks, a teacher network is not available. This somewhat contradicts the intention of NAS / AutoML, which aims to automatically find well-performing networks without any human intervention or prior knowledge.\n\n3) It is unclear to me how to compute the correlation of RDMs in cases where the architecture space is not as structured as in the paper (=either just sequential models or cell search space)\n\n4) Figure 1, middle: while the overall correlation of 0.62 is ok, it seems that the correlation for high-performing models (=the region of interest),say P+TG > 0.5, is rather small/non-existing\n\n\n\nMinor comments:\n\n - Section 3.3 first sentence says: \"Sequential Model-Based Optimization approaches are greedy numerical method\" : this is not correct since they use an acquisition function to pick new candidates which trades-off exploration and exploitation automatically. Furthermore, under some assumptions, Bayesian optimization converges to the global optimum.\n\n- I think, arguing that one can use the human brain as a teacher network is a bit of a stretch. Also, the authors do not give any explanation how that could be realized.\n\n- Section 3.3 says that TPE fits a Gaussian Mixture Model, however, it actually fits a kernel density estimator.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}