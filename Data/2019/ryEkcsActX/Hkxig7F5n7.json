{"title": "Interesting idea and explorations", "review": "They propose a method of accelerating Neural Architecture Search by using a Teacher Network to predict (combined with immature performance)  the mature performance of candidate architectures, allowing them to train these candidates an order of magnitude faster (e.g. 2 epochs rather than 20). \n\nIt achieves 10x speed up in NAS. Interesting exploration of what is predictive of mature performance at different epochs. For example, TG(L1) is most predictive up to epoch 2, TG(L2/L3) are most predictive after, but immature performance + similarity to teacher (TG) is most predictive overall.\n\nIt has a very well-written related work section with a clear story to motivate the work.\n\nThe baselines in Table 2 should be updated. For example, NASNet-A on CIFAR10 (https://arxiv.org/pdf/1707.07012.pdf) reports an error of 3.41 without cutout and 2.65 with cutout, while the #Params is 3.3M. A more fair comparison should include those as baselines.\n\nThe experiments only consider 10 and 20 layer ConvNet.\n\nThe paper has lots of typos and missing articles or verbs. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}