{"title": "Review for Neural Model-Based Reinforcement Learning for Recommendation", "review": "The authors propose a deep reinforcement learning based recommendation algorithm. Instead of manually designing reward function for RL, a generative adversarial network was proposed to learn the reward function based on user's dynamic behavior. The authors also try to provide an efficient combinatorial recommendation algorithm by designing a cascade DQN. The authors hold their experiments on the Movielens and Ant Financial news dataset. The authors adopt logistic regression (LR) and collaborative competitive filtering(CCF) as comparison baseline to evaluate recommendation performance. The authors also compared their proposed RL policy CQDN with LinUCB.\n\n[Pros in Summary]\n1. Recommendation in the deep neural network based RL is a hot topic.\n\n2. The motivation for using a self-learned rewards function and provide efficient combinatorial recommendation is interesting.\n\n[Cons in Summary]\n1. The motivations/claimed contributions are not well supported/illustrated by the proposed algorithm or experiments.\n\n2. Some assumptions may not be realistic.\n\n3. The experiment is not sufficient without enough state of art baselines.\n\n4. The writing of this paper needs improvement.\n\n[Thoughts, Questions, and Problems in Details]\n1. The idea of using a learned reward function instead of manually defined one sound sweet. But based on (7) and (8), the reward function is essentially giving more rewards for the action that the user really clicks on. How much difference is there compared with traditional manual reward design of giving a click with a reward of 1, especially given the circumstance that a lot of manual intervention is actually used in designing loss function like (7)?\n\nMoreover, in the experiment, there is no comparison experiment evaluating the difference between using a self-learned reward function vs. a traditional manual designed reward function.\n\n2. The assumption \"in each pageview, users are recommended a page of k items and provide feedback by clicking on only one of these items; and then the system recommends a new page of k items\" does not sound realistic. What if the users click on multiple items?\n\n3. The combinatorial recommendation is useful in the recommendation setting. But it is also important to get the correct ranking order for items from the recommendation list, ie, the best item should rank on the top of the list. Is this principal guaranteed in the combinatorial recommendation proposed in this paper? It is not discussed in this paper.\n\n4. The authors claim to provide an efficient combinatorial recommendation but fail to provide any computational complexity analysis or providing any analysis on training or serving time. Is the proposed algorithm computationally practical to be deployed in a real system?\n\n5. The experiments are too weak because the baselines are old and state of art methods are missing from the comparison.\n\n6. Typos and grammar errors across the paper, to name a few\n\n\"we will also estimate a user behavior model associate with the reward function\"\n\n\"a model for the sequence of user clicking behavior, discussion its parametrization and parameter estimation.\"", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}