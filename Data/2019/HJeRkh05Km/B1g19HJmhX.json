{"title": "fine paper; some questions", "review": "This paper explores the use of semantic priors for semantic navigation. The semantic priors are derived from language datasets (in the form of word embeddings, which assign similar feature vectors to related words) and from visual datasets (Visual Genome, which represents relationships between objects that co-occur in scenes).\n\nThe general ideas are reasonable. The experimental protocol is sound and uses recent best practices. The results are fine.\n\nI'm a bit puzzled by the way the GCN is used. Figure 2 implies that the GCN doesn't actually use information from the current image. I.e., the GCN input doesn't change as the agent navigates the scene. (In Figure 2, the GCN path appears similar to the Word Embedding path. The Word Embedding path doesn't update when the agent moves, so the reader can infer that the GCN path doesn't update either.) But then I don't quite understand how the GCN incorporates information from the current scene.\n\nFigure 4 implies that the GCN is re-evaluated when the agent moves and the input image changes. But how is information from the image fed into the GCN? The text implies that an ImageNet classification model is run on the image. But why image classification and not object detection? It seems that what one would really want is to understand what objects are in the scene. And how is output of the image classification network supplied to the GCN? Is the target object type used as well? Overall it's not clear to me exactly how information from the current image is supplied to the GCN, why this mechanism is right, and what the GCN is expected to do. (I do understand how GCNs work, just not how exactly they are used here and why this precise usage is right for this application.) I hope the authors can clarify in the response.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}