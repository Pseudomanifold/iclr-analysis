{"title": "a good idea to try but the paper is not yet ready", "review": "In summary, I think this paper contains some reasonable results based on a reasonable, moderately novel, idea, but unfortunately, it is not yet ready for publication. Reading it made me rather confused. \n\nGood things:\n- The main idea is sensible, though distilling into the same architecture (sFEED) is not that novel. I think the pFEED is probably the more novel part.\n- The numerical results are quite good.\n- It's a fairly simple method. If others reproduced these results, I think it would be useful.\n\nProblems:\n- Some parts of the paper are written in a way that makes the reader confused about what this paper is about. For example the first paragraph. Some motivations I just did not understand.\n- Some parts of the paper are repeating itself. For example \"introduction\" and \"related works\". The section on related work also includes some quite unrelated papers.\n- The references in the paper are often pointing to work that came much later than the original idea or some pretty random recent papers. For example the idea of model compression (or knowledge distillation) is much older than Hinton et al. I believe it was first proposed by Bucila et al. [1] (which the authors mention later as if knowledge distillation and model compression were very different ideas), it definitely doesn't come from Kim et al. (2018). Learning from intermediate representations of the network is at least as old as Romero et al. [2]. Compression into a network of the same architecture is definitely older than Furnarello et al. (2018). It was done, for example, by Geras et al. [3]. The paper also cites Goodfellow et al. (2016) in some pretty random contexts. I don't want to be too petty about references, but unfortunately, this paper is just below a threshold that I would still find acceptable in this respect.\n- The comparison in Table 6 would make more sense if the same architectures would be clearly compared. As it is, it is difficult to be certain where the improvement is coming from and how it actually compares to different methods.\n\nTypos: Titap X, ResNext, prarphraser.\n\nReferences:\n[1] Bucila et al. Model Compression. 2006.\n[2] Romero et al. FitNets: Hints for Thin Deep Nets. 2014.\n[3] Geras et al. Blending LSTMs into CNNs. 2016.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}