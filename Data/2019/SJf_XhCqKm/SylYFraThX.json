{"title": "small number of hyperparameters, comparison with spearmint not strong enough", "review": "I reviewed the same paper last year. I am appending a few lines based on the changes made by authors.\n\nThe authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al. (2011). The k-DPP sampling algorithm and the concept of k-DPP-RBF over hyperparameters are not new, so the main contribution here is the empirical study. \n\nThe first experiment by the authors shows that k-DPP-RBF gives better star discrepancy than uniform random search while being comparable to low-discrepancy Sobol sequences in other metrics such as distance from the center or an arbitrary corner (Fig. 1).\n\nThe second experiment shows surprisingly that for the hard learning rate range, k-DPP-RBF performs better than uniform random search, and moreover, both of these outperform BO-TPE (Fig. 2, column 1).\n\nThe third experiment shows that on good or stable ranges, k-DPP-RBF and its discrete analog slightly outperform uniform random search and its discrete analog, respectively.\n\nI have a few reservations. First, I do not find these outcomes very surprising or informative, except for the second experiment (Fig. 2, column 1). Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20. The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al. (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al. (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\n\nCOMMENTS ON THE CHANGES SINCE THE LAST YEAR\n\nI am not convinced by the comparison with Spearmint added by the authors since the previous version. It is unclear to me if the comparison of wall clock time and accuracy holds for larger number of hyperparameters or against Spearmint with more parallelization.\n\nIn addition the authors do not compare against more recent work, e.g., \n\n@INPROCEEDINGS{falkner-bayesopt17,\n author    = {S. Falkner and A. Klein and F. Hutter},\n title     = {Combining Hyperband and Bayesian Optimization},\n booktitle = {NIPS 2017 Bayesian Optimization Workshop},\n year      = {2017},\n month     = dec,\n}\n\n@InProceedings{falkner-icml-18,\n  title =        {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},\n  author =       {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n  booktitle =    {Proceedings of the 35th International Conference on Machine Learning (ICML 2018)},\n  pages =        {1436--1445},\n  year =         {2018},\n  month =        jul,\n}", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}