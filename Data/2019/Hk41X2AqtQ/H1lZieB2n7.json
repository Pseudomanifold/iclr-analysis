{"title": "Nice samples but lack of comparison to existing hierarchical VAEs", "review": "This paper proposes using a hierarchical VAE to text generation to solve the two problems of long text generation and mode collapse where diversity in generated text is lost.\n\nThe paper does this by decoding the latent variable into sentence level latent codes that are then decoded into each sentence. The paper shows convincing results on perplexity, N-gram based and human qualitative evaluation.\n\nThe paper is well written though some parts are confusing. For example, equation 4 refers to q as the prior distribution but this seems like it's the posterior distribution as it is described just below equation 5. p(z_1|z_2) is also not well defined. It would be clearer to specify the full algorithm in the paper.\n\nThe work also mentions that words are generated for each sentence until the _END token is generated. Is this token always generated? What happens to a sentence if that token is not generated?\n\nThe novelty of this paper is questionable given the significant amount of existing work in hierarchical VAEs. It's also unclear why a more direct comparison can't be made with Serban et. al in terms of language generation quality and perplexity. If a downstream model is only able to make use of one latent variable, can't multiple variables simply be averaged?\n\nIt's also unclear how this work is novel with regards to the works below.\n\nHierarchical Variational Autoencoders for Music\nRoberts, et. al\nNIPS 2017 creativity workshop\nThis seems to have a similar hierarchical structure where there is an initial 16 step decoder that decodes the latent code for the lower level note level LSTMs to use during generation.\n\nUnsupervised Learning of Disentangled and Interpretable Representations from Sequential Data\nHsu, et. al\nNIPS 2017\nThis proposes a factorized hierarchical variational autoencoder which also has a double latent variable hierarchical structure, one that is conditional on the other.\n\nMinor comments\n- Typo in page 3 under Hierarchical structures in NLP: characters \"from\" a word\n- Typo above section 4.3: hierarhical\n\n=== After rebuttal ===\nThanks for the response.\n\nI believe that Reviewer2's criticism about the similarity to Park et. al isn't sufficiently addressed by the authors. Even if the hierarchical structure is different it's unclear whether this alternative structure is superior to Park et. al. There appears to be no evidence that the latent variables contain more global information relative to VHCR (Park et. al). These claims aren't tested and the results in the paper aren't comparable since the authors don't evaluate on the same datasets as Park et. al.\n\nIn general, I think the claims of a superior hierarchical structure to models such as the factorized hierarchical VAE paper needed to be tested to show evidence of a more powerful representation for hier-VAE.\n\nI will keep my score.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}