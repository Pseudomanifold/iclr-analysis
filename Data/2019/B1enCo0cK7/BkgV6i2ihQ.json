{"title": "No real value of the work", "review": "This paper is about understanding robustness of neural networks w.r.t. adversarial examples.\n\nThe authors argue that a good model should be built upon a small subset of input features, with the rest being ignored as noise. If there are many more noisy features (referred as redundant ones), it is harder to find the good relevant features, hence the model becomes more susceptible to any noise introduced in the unfiltered features. This argument is presented theoretically using the notion of minimal sufficient statistics; it is stated that a model that is robust to adversary attacks must compress the input features to minimal sufficient statistics. As an example, even if there is one bit more than the minimal sufficient statistics, the model can overfit, and so the paper title.\n\nWhile the paper is a nice read to understand a new perspective about adversarial examples, it doesn't solve the existing issues with neural networks. At present, it doesn't seem like a complete work to be published.\n\nAs per the definition of adversarial examples cited in the paper, if x' has a very small l2 norm distance w.r.t. x, while x' having a class label different from x, x' is considered to be an adversarial example. As per the decades of machine learning literature, x's should not be considered an adversary example as such. I suppose it is deemed to be a reasonable definition specific to the known and unknown capabiltiies of neural networks.\n\nDepending upon a perspective, one may also find the argument on redundnancy counter intuitive. Ensemble classifiers take an advantage of redundancy while avoiding the problem of adversarial examples by learning each classifier on a subset of features or a subset of dataset. In that context, to understand adversarial examples in a principle manner as this paper attempts, one may also want to relate to the old literature for bagging, boosting, etc.\n\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}