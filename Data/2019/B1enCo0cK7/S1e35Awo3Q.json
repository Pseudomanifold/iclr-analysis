{"title": "Discuss an important issue, but empirical investigation is not convincing and discussion is at times incorrect", "review": "Summary:\nIn this work, the authors study the problem of adversarial examples in ML from an information-theoretic point of view. They explore connections between feature redundancy and adversarial perturbations. They provide a combination of empirical and theoretical studies to justify this connection.\n\nQuality, Originality and Novelty:\nWhile the high level idea studied in this paper is interesting, however it has been suggested before in prior work [Tsipras et al., 2018].  Thus, I do not find the analysis in this paper novel or convincing. There also seem to be some technical flaws in the examples used to corroborate the claims. The experimental justification provided is lacking, as analysis is largely restricted to simple models and does not include state-of-the-art attacks. \n\nSpecific Comments:\n\nSection 2:\n- The authors are misinformed about the lack of prior work to theoretically understand the cause of adversarial examples. Many aspects of this problem have been analyzed in previous work. Some (non-exhaustive) related work - [Papernot et al., 2016, Fawzi et al., 2018, Schmidt et al., 2018, Bubeck et al., 2018, Tsipras et al., 2018].\n\n-  I do not understand the comment on optical illusions, etc. There seems to be a misunderstanding about the definition of adversarial examples. In the standard setting, adversarial examples are defined as small perturbations, which are imperceptible to humans, that cause models to classify otherwise benign inputs incorrectly. Note that the ground truth label for adversarial perturbations is determined by human classification and thus instances such as optical illusions are not adversarial examples since humans do not consider them to be part of the same class.\n\nSection 3:\n\n- The setting discussed by the authors in Figure 2a does not fit in to the standard adversarial threat model. If you consider the colored regions to be ground truth labels for the points then the colored points are not adversarial examples, but are perturbations that truly switch the underlying label of the data point. The goal of adversarially robust machine learning is not to be resilient to arbitrary magnitudes of noise, but specifically small perturbations that do not change the underlying class of a data point. In fact, in the example provided, if an ML model truly learns the linear separators then it is robust to adversarial examples.\n\n- The authors state that \u201crandom noise overflows the separation capability of a network\u201d. As mentioned in comment 1 above, the colored points in Figure 2a are not adversarial examples. Hence it is not clear why this comment about *large* random noise applies to imperceptible adversarial perturbations. Further, it is important to note that the \u201ctrue\u201d decision boundary of the task is not affected by the presence of adversarial examples. In fact, we expect adversarial examples to capture invariances that we believe would be present in a classifier that was able to learn this true boundary. \n\n- The authors seem to conflate random noise with adversarial perturbations. It is well known that random noise is not an adversarial perturbation for state-of-the-art ML models. \n\n- Section 3.1: The example about the boolean circuit that the authors provide is extremely confusing and also likely incorrect. It is unclear what threat model the authors assume here. The authors seem to consider inputs for which the model predicts an incorrect output as adversarial examples. But these do not come from benign inputs unless we allow very large perturbations (for instance an l_inf eps = 1). Under such large perturbations, a model with both w_1, w_2 = 0 could also be made to \u201cfail\u201d by flipping x_1 or x_2.\n\n- Section 3.2: \n-> I had a hard time trying to understand the discussion in this section. The authors use phrases such as \u201cbit eraser\u201d and \u201coverflowing decision making\u201d which have not been properly defined in the paper. It is also unclear what the threat model being considered is. It seems as though the authors consider standard ML models as bit erasers that can already disregard redundant inputs. If this is the case, it is not clear to me what the authors mean by: \u201cFor a black box adversarial attack, we therefore just need to add enough irrelevant input to overflow this bit erasure function.\u201d Is the attacker allowed to append an arbitrary bits to the input at test time? This is not true of standard attack models where the number of bits of the input data are fixed, and hence not clear why adversarial examples would need more bit erasures. \n-> It is well-known that training with random noise is not enough to build robust models. \n-> The authors make a broad claim about transferability, but do not provide sufficient evidence to support it. Given that we train ML models with millions of parameters on a much smaller set of training samples, it is unclear to me why different models would consistently identify the same set of redundant features.\n\n- Section 3.3: The claim that the existence of adversarial examples is a failure of standard generalization is incorrect. In the standard ML setting, we seek to minimize expected population loss on samples from the distribution. In the adversarial setting, we seek to minimize the loss on *worst-case* perturbations of inputs drawn from the distribution. This is a different and much more challenging statistical problem and likely requires more data [Schmidt et al, 2018] and compute [Bubeck et al, 2018] as has been studied out prior works.\n\n-Section 4: \n--> Section 4.1: I do not entirely understand the setup of this experiment. Do the authors retrain after setting weights to 0? Or do they simply try setting different fractions of weights to zero until the train error hits eps. It is not clear to me that setting random weights to zero is a good way to evaluate the complexity of robust models (i.e., those trained on adversarial examples). It has been noted in prior work that these robust models already tend to be sparser than standard models, so setting random weights to zero could indeed have a larger impact on these models. Did the authors verify how many of the weights are away from zero for robust models before clipping the weights? If these were included, the plots might change. The plots in Figure 4 actually contradict the authors claim that adversarial examples are harder to memorize, since the training accuracy plots look very similar for all datasets. The test accuracy plots are indeed different, but this could be because robust learning is a statistically more complex problem and needs more data as has been studied in prior work [Schmidt et al, 2018]. \n--> Section 4.2: The authors should provide more details as to how they estimate the various statistics, at least in the appendix. It is hard to draw inferences from these tables without details about the procedure to generate these statistics, how these adversarial examples were constructed (what was the epsilon, did the authors use code released by prior art) and what the models were on which these examples were generated.\n--> Section 4.3: Similar insights about feature redundancy in robust models have been observed in prior work [Ross et al., 2017, Tsipras et al., 2018].\n\n- Section 5: Using lossy compression as a defense against adversarial examples has been tried in prior art and is known to be unsuccessful [Athalye et al., 2018].\n\n\nReferences:\n\nPapernot, Nicolas, et al. \"Towards the science of security and privacy in machine learning.\" arXiv preprint arXiv:1611.03814 (2016).\n\nFawzi, Alhussein, Hamza Fawzi, and Omar Fawzi. \"Adversarial vulnerability for any classifier.\" arXiv preprint arXiv:1802.08686 (2018).\n\nBubeck, S\u00e9bastien, Eric Price, and Ilya Razenshteyn. \"Adversarial examples from computational constraints.\" arXiv preprint arXiv:1805.10204 (2018).\n\nSchmidt, Ludwig, et al. \"Adversarially Robust Generalization Requires More Data.\" arXiv preprint arXiv:1804.11285 (2018).\n\nRoss, Andrew Slavin, and Finale Doshi-Velez. \"Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients.\" arXiv preprint arXiv:1711.09404 (2017).\n\nTsipras, Dimitris, et al. \"Robustness May Be at Odds with Accuracy.\" arXiv preprint \tarXiv:1805.12152 (2018).\n\nAthalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" arXiv preprint arXiv:1802.00420 (2018).\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}