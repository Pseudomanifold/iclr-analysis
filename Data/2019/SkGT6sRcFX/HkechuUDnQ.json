{"title": "a weight initialization approach to enable infinitely deep and infinite-width networks, experimental results on small datasets", "review": "Pros:\n\nThis paper uses kernel mappings between any two layers for weight initialisation. Using the representer theorem, a proper distribution for weights is constructed in H_{k_i} instead of being learned by \\phi_i, and then is formulated as a GP. \n\nCons:\n\nHowever, there are some key issues.\n1. The so-called \u201cinfinite width\u201d is just yielded by kernels in RKHS for weight initialization. For practical implementation, the authors use this scheme with random Fourier features to construct finite-width network. A key issue is that how to guarantee that the approximated weights are still in the same space? For example, weights can be in RKHS, but their approximation might be not in RKHS. See in [S1] for details.\n \n[S1] Generalization Properties of Learning with Random Features, NIPS 2017.\n \n2. Experimental part is not very convincing. First, the authors just compare different initialization schemes. The used architectures are simple and not representative. Second, the overall performance is not satisfactory, and the compared classification datasets are quite small. Overall, the experimental results are inadequate and unconvincing.\n\nSummary:\nThe paper attempts to proposal a weight initialization scheme to enable infinite deep infinite-width networks. However, there are some key issues not address such as whether the approximated weights are still in the same space and the limited experimental results.\n\nResponse to rebuttal:\nThe authors have addressed my question about the weights being still in the same RKHS. I still think the motivation and experiments are not very satisfactory. \n\nTherefore the paper is very borderline. However, I would like to bump my rating a bit higher.\n\n \n \n ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}