{"title": "Interesting Work", "review": "==========\nStrengths:\n==========\n\n- Out-of-vocabulary VQA is an interesting and timely task. \n\n- The presented approach leverages region annotations (Visual Genome attributes and objects) and descriptions (COCO Captions) to learn a task-conditioned visual classifier. This seems fairly intuitive and an efficient reuse of existing data to learn visual groundings for out-of-vocabulary words. \n\n- Overall the qualitative and quantitative results support the proposed method's effectiveness. \n\n- I really appreciate the extended results in the supplement.\n\n- The paper is generally written well with clear descriptions and useful figures. Thanks!\n\n- Thank you for using multiple random seeds and reporting variance. \n\n==========\nConcerns:\n==========\n\n[A] How is the initial answer space of the task-conditioned classifier set? The dataset construction paragraph is somewhat vague about what a 'pretrained visual word' is. Is it the 3000 objects and 1000 attributes from Visual Genome referenced in the pretraining paragraph?\n\n[B] The task construction for visual descriptions could be clearer. From my understanding, you select a caption and randomly remove a word, replacing it with the <blank> symbol. This word becomes the answer and the RNN encoded blanked-caption is the task. How is the word selected? \n\n[C] Minor point: I think the VQA score listed in 5.1 is incomplete. If I understand correctly, the calculation should be run over all 10 choose 9 sets of answers and the mean reported.\n\n[D] Small request: Could the authors add a list of answers in train and test in the supplement? Aside from academic curiosity, I would also like to get a sense for how many of these answer refer to novel concepts / entities / relationships as opposed to being synonyms for existing ones in train. Any attempt to quantify this would also be appreciated!\n\n==========\nOverview:\n==========\n\nI'm overall positive on this paper. I think the problem is interesting and the approach leverages existing source of visual grounding in a exciting way. The results in the main paper and supplement paint a convincing picture of the method's efficacy. There are a couple of things that I would like more clarity on in the submission.\n\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}