{"title": "Approach to \u201czero-shot\u201d VQA which decouples task prediction from answer prediction for generation; requires additional and more careful experimental evaluation.", "review": "The paper looks at the task of extending VQA to a novel answer space, for which there are no corresponding Questions-Image-Answer Triples available at training time, but separate Text and Image classification information exists for these answers. The paper proposes a model which decouples task prediction from answer prediction to allow the model to generalize to novel answers.\n\nStrength:\n1.\tInteresting direction of extending supervised VQA to a novel answer space.\n2.\tModel successfully allows transfer from different task data.\n3.\tOverall novel model which aims to decouple task prediction from answer prediction.\n\nWeaknesses:\n4.\tRelated work: \n4.1.\tThe paper mentions the zero-shot VQA work from Teney & Hengel, however, given that is one of the most related works, I expect a more thorough discussion of the similarity in dataset and method. Is it possible to run the approach from Teney & Hengel on the dataset of the authors or run the authors approach on the zero-shot answer split?\n4.2.\tThe paper is related to zero-shot recognition, which is an established field in computer vision, however this line is not discussed sufficiently, see e.g. Xian et al. for a discussion on the topic.\n4.3.\tWhile the paper discusses the setting of Agrawal et al. (2018), it misses to discuss how this works related to the GVQA model proposed by Agrawal et al, which has a several similar aspects to the model proposed in this work (although it is overall different).\n5.\tEvaluation\n5.1.\tThe paper only evaluates on the novel classes, but it is unclear what happens if the model encounters known \u201cclasses\u201d, i.e. has to answer with known answers from the training set. I think it is critical to also evaluate this scenario. For image classification this is sometimes referred to as \u201cGeneralized zero shot\u201d, see e.g. Xian et al. CVPR 2017, or for just evaluating including known classes as distractors see Rohrbach CVPR 2010.\n5.2.\tIt also would be good to include a baseline/ablation which does not look at the question, i.e. without task prediction.\n5.3.\tThe paper evaluation consists of a plot which shows the performance over training iterations. Instead the paper should pick the iteration on a validation set and report results on a test set as a single number, best in a table, which allows clear comparison for future work.\n5.4.\tThe paper uses a \u201ctest-validation\u201d set which includes the novel answers, which are also in the test set. However, as the validation set is part of model optimization, i.e. training of the model, any validation set, including \u201ctest-validation\u201d should not include novel answers included in the test set. Instead I suggest the dataset should split a \u201ctest-validation\u201d which does not overlap with test nor with the training set, to do proper validation without conflict of the test set.\n\n\nConclusion: Overall a great direction and interesting approach but requires more careful experimental setup and evaluation and discussion of related work for acceptance.\n\n\nReferences: \nXian, Y., Schiele, B., & Akata, Z. (CVPR 2017). Zero-shot learning-the good, the bad and the ugly. \nAnd/or the TPAMI version Zero-shot learning - A comprehensive evaluation of the good, the bad and the ugly; 2018\n\nRohrbach, M., Stark, M., Szarvas, G., Gurevych, I., & Schiele, B. (CVPR 2010). What helps where \u00e2?? and why? Semantic relatedness for knowledge transfer.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}