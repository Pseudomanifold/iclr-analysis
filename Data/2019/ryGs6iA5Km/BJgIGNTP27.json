{"title": "Reviewer comment", "review": "This paper presents a very interesting investigation of the expressive capabilities of graph neural networks, in particular focusing on the discriminative power of such GNN models, i.e. the ability to tell that two inputs are different when they are actually different.  The analysis is based on the study of injective representation functions on multisets.  This perspective in particular allows the authors to distinguish different aggregation methods, sum, mean and max, as well as to distinguish one layer linear transformations from multi-layer MLPs.  Based on the analysis the authors proposed a variant of the GNN called Graph Isomorphism Networks (GINs) that use MLPs instead of linear transformations on each layer, and sum instead of mean or max as the aggregation method, which has the most discriminative power following the analysis.  Experiments were done on node classification benchmarks to support the claims.\n\nOverall I quite liked this paper.  The study of the expressive capabilities of GNNs is a very important problem.  Given the popularity of this class of models recently, theoretical analysis for these models is largely missing.  Previous attempts at studying the capability of GNNs focus on the function approximation perspective (e.g. Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction by Hertiz et al. which is worth discussing).  This paper presents a very different angle focusing on discriminative capabilities.  Being able to tell two inputs apart when they are different is obviously just one aspect of representation power, but this paper showed that studying this aspect can already give us some interesting insights.\n\nI do feel however that the authors should make it clear that discriminative power is not the only thing we care, and in most applications we are not doing graph isomorphism tests.  The ability to tell, for example, how far two inputs are, when they are not the same is also very (and maybe more) important, which such isomorphism / injective map based analysis does not capture at all.  In fact the assumption that each feature vector can be mapped to a unique label in {a, b, c, ...} (Section 3 first paragraph) is overly simplistic and only makes sense for analyzing injective maps.  If we want to reason anything about the continuity of the features and representations, this assumption does not apply, and the real set is not countable so such a mapping cannot exist.\n\nIn equation 4.1 describes the GIN update, which is proposed as \u201cthe most powerful GNN\u201d.  However, such architecture is not really new, for example the Interaction Networks (Battaglia et al. 2016) already uses sum aggregation and MLP as the building blocks.  Also, it is said that in the first iteration a simple sum is enough to implement injective map, this is true for sum, but replacing that with mean and max can lose information very early on.  Another MLP on the input features at least for mean or max aggregation for the first iteration is therefore necessary.  This isn\u2019t made very clear in the paper.\n\nThe training set results presented in section 6.1 is not very clear.  The plots show only one run for each model variant, which run was it?  As the purpose is to show that some variants fit well, and some others overfit, these runs should be chosen to optimize training set performance, rather than generalization.  Also the restrictions should be made clear that all models are given the same (small) amount of hidden units per node.  I imagine if the amount of hidden units are allowed to be much bigger, mean and max aggregators should also catch up.\n\nAs mentioned earlier I quite liked the paper despite some restrictions anc things to clarify.  I would vote for accepting this paper for publication at ICLR.\n\n--------\n\nConsidering the counter-example given above, I'm lowering my scores a bit.  The proof of theorem 3 is less than clear.  The proof for the first half of theorem 3 (a) is quite obvious, but the proof for the second half is a bit hand-wavy.\n\nIn the worst case, the second half of theorem 3 (a) will be invalid.  The most general GNN will then have to use an update function in the form of the first half of 3(a), and all the other analysis still holds.  The experiments will need to be rerun.\n\n--------\n\nUpdate: the new revision resolved the counter-example issue and I'm mostly happy with it, so my rating was adjusted again.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}