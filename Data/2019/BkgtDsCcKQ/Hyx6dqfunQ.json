{"title": "This is an interesting paper that seems to make an important contribution but its technical presentation needs to be improved. As of now, it is somewhat hard to appreciate how significant the proposed solution is. I hope the authors could help me clarify the below points so I can converge to a final rating.", "review": "PAPER SUMMARY:\n\nThis paper proposes a new POVI method for posterior inference in BNN. Unlike existing POVI techniques that optimize particles in the weight space which often yields sub-optimal results on BNN due to its over-parameterized nature, the new POVI method aims to maintain and update particles  directly on the space of regression functions to overcome this sub-optimal issue.\n\nNOVELTY & SIGNIFICANCE:\n\nIn general, I am inclined to think that this paper has made an important contribution with very promising results but I still have doubts in the proposed solution technique (as detailed below) and am not able to converge to a final rating at this point.\n\nTECHNICAL SOUNDNESS:\n\nThe authors claim that the new POVI technique operates directly on the function-space posterior to sidestep the over-parameterized issue of BNN but ultimately each function particle is still identified by a weight particle (as detailed in Eq. (2)). In terms of high-level ideas, I am not sure I understand the implied fundamental differences between this work and SVGD and how significant is it.\n\nOn the technical level, the key difference between the proposed work and SVGD seems to be the particle update equation in (2): The gradient flow is multiplied with the derivative of the BNN evaluated at the corresponding weight particle (in SVGD, the gradient flow was used alone). The authors then mentioned that this update rule results from minimizing the difference between f(X, theta) and f(X, theta) + \\epsilon * v(f(., theta))(X). I do not follow this step -- please elaborate.\n\nThe theoretical justification that follows Eq. (3) is somewhat incoherent: What is \\Epsilon(q(f(x)))? This has not been defined before or anywhere in the main text. Furthermore, the paragraph that follows the theoretical justification implies the computation of the gradient flow in (3) involves the likelihood term -- why is that?\n\nIn Algorithm 1, why do we sample from both the training set and some measure \\mu? I am sure there must be a reason for this but I could not find it anywhere except for a short statement that \"for convenience, we choose \\mu in such a way that samples from \\mu always consists a mini-batch from X\". Please elaborate.\n\nWill the proposed POVI converge?\n\nCLARITY:\n\nI think this paper has clarity issue with the technical exposition. The explanation tends to be very limited and even appear coherent at important points. For example, see\nmy 3rd point above. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}