{"title": "This paper proposes a unified framework for speeding up sparse regression algorithms by adapting fast nearest-neighbour search algorithms for updating the support. ", "review": "The paper is very well-written, readable, with the ideas and derivations clearly explained. \n\nThe literature review is comprehensive and informative. I do feel however that the review could be improved, for example, by discussing the recent papers by Chinmay Hegde and Piotr Indyk on \"head\" and \"tail\" approximate projections to speed up recovery algorithms. The problem under study is indeed important and the contribution is interesting. \n\nMy biggest concern is that the technical contribution is too modest. Theorem 1 serves more as a decorative technical result (the assumption \"And for any vector v...\" seems out of the blue and too convenient) and the paper does not answer the many questions that come to mind here. For example, what is the intrinsic dimension of common random measurement matrices? Or how do any wrongly detected nearest neighbours propagate through the iterations of the algorithm? How does the measurement noise change the intrinsic dimension? We should intuitively lose stability in return for faster recovery. How would this be quantified in what you've proposed.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}