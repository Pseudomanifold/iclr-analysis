{"title": "good work but can be improved", "review": "This paper continues the study of the signSGD algorithm due to (Balles & Hennig, Bernstein et al), where only the sign of a stochastic gradient is used for updating. There are two main results: (1) a slightly refined analysis of two results in Bernstein et al. The authors proved that signSGD continues to converge at the 1/sqrt(T) rate even with minibatch size 1 (instead of T as in Bernstein et al), if the gradient noise is symmetric and unimodal; (2) a similar convergence rate is obtained even when half of the worker machines flip the sign of their stochastic gradients. These results appear to be relatively straightforward extensions of those in Bernstein et al.\n\nClarity: The paper is mostly nicely written, with some occasionally imprecise claims. \n\nPage 5, right before Remark 1: it is wrongly claimed that signSGD converges to a critical point of the objective. This cannot be inferred from Theorem 1. (If the authors disagree, please give the complete details on how the random sequence x_t converges to some critical point x^*. or perhaps you are using the word \"convergence\" differently from its usual meaning?)\n\nPage 6, after Lemma 1. The authors claimed that \"the bound is elegant since ... even at low SNR we still have ... <= 1/2.\" In my opinion, this is not elegant at all. This is just your symmetric assumption on the noise, nothing more...\n\nEq (1): are you assuming g_i > 0 here? this inequality is false as you need to discuss the two cases. \n\n\"Therefore signSGD cannot converge for these noise distributions, ..... point in the wrong direction.\" This is a claim based on intuitive arguments but not a proven fact. Please refrain from using definitive sentences like this.\n\nFootnote 1: where is the discussion?\n\n\nOriginality: Compared to the existing work of Bernstein et al, the novelty of the current submission is moderate. The main results appear to be relatively straightforward refinements of those in Bernstein. The observation that majority voting is Byzantine fault tolerant is perhaps not very surprising but it is certainly nice to have a formal justification.\n\nQuality: At times this submission feels like half-baked:\n-- The theoretical results are about signSGD while the experiments are about sigNUM\n-- The adversaries must send the negation of the sign? why can't they send an arbitrary bit vector?\n-- From the authors' discussion \" we will include this feature in our open source code release\", \"plan to run more extensive experiments in the immediate future and will update the paper...\", and \"should be possible to extend the result to the mini-batch setting by combining ...\"\n\nSignificance: This paper is certainly a nice addition to our understanding of signSGD. However, the current obtained results are not very significant compared to the existing results: Theorem 1 is a minor refinement of the two results in Bernstein et al, while Theorem 2 at its current form is not very interesting, as it heavily restricts what an adversary worker machine can do. It would be more realistic if the adversaries can send random bits (still non-cooperated though).\n\n\n\n##### added after author response #####\nI appreciate the authors' efforts in trying to improve the draft by incorporating the reviewers' comments. While I do like the authors' continued study of signSGD, the submission has gone through some significant revision (more complete experiments + stronger adversary). ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}