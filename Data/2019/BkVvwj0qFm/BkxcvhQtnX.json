{"title": "Review for the paper Geometric Operator Convolutional Neural Network", "review": "I appreciate the responses authors provided to my comments. Many of my main concerns have been addressed there. \n\nHaving said this, I do not consider the contribution of this paper substantial given the fact the a more involved framework (CKN) has been already known. I honestly believe that the findings of the authors are interesting but doubt that we use Gabor features in the first layer of CNNs from now on. \n\nIf the paper is accepted, I kindly encourage the authors to reflect their answers to my comments in their work. For the sake of completeness, I have incorporated my comments and authors responses below.\n\nGood luck\n\n-------------------------------------------------------------------------------------------------------------\nI would like to get my head around a few points and appreciate clarifications on \n\n- To me the title is a bit misleading, especially the word geometric seems a bit far-stretched. can you explain why geometric is essential here?\n\n- In experiments, given that the Gabor filters have less tunable parameters, did the authors keep the number of channels similar to their CNN counterparts or increased the number of channels (Gabor filters) to match the number of tunable parameters in CNNs?\n\n- have authors evaluated their solution on large scale problems, say image classification on image-net?\n\n- what will happen if more than 1 layer of Gabor filters is considered? I am curious to know whether the performance decreases or not\n\n- how is your proposal different from previous studies where learning the parameters of a kernel function (e.g., CKN by Mairal et al.) has been discussed? I am not very much convinced that the proposed approach is brand-new.\n\n- can you explain how you initialize the Gabor filters? Can you also report [min,max] accuracies over say 10runs if random initialization is used?   \n\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nRe: Some clarifications  \nICLR 2019 Conference Paper273 Authors\n25 Oct 2018ICLR 2019 Conference Paper273 Official CommentReaders:  Everyone\nComment: Q1 feedback: \n  The title of our paper is originated from geometric operators in traditional image process algorithms. Geometric properties in geometric operators, such as symmetry, account for the word formation. The kernels in the first convolutional layer in AlexNet \\cite{krizhevsky2012imagenet} show similarity with Gabor operator kernels in vision, and there is redundancy of parameters in CNNs, which inspire us to combine geometric operators in the traditional image process algorithms into CNNs.\n\nQ2 feedback:\n  In our experiments, the output channel number of the first convolutional layer keeps the same between common CNN and the corresponding GO-CNN. Therefore, the number of trainable parameters of common CNN is larger than that of the corresponding GO-CNN. There is theoretical guarantee in our article that, under less parameter degrees of freedom, the approximation ability of the Geometric Operator Convolutional Network is not worse than that of common CNN, and the parameter redundancy of common CNN is further verified.\n\nQ3 feedback:\n  The main purpose of this article is to validate and analysis the potential of the proposed architecture in it, and experiments on large scale dataset are not considered in this article. We are going to study applications by the proposed architecture on large scale datasets in the future. \n\nQ4 feedback:\n  One of the motivations in this paper is to observe that the visualization of the first layer of the AlexNet convolution kernel has certain geometric characteristics, such as symmetry and volatility, and parameter redundancy. The visualization of other layer convolution kernels does not show obvious geometric characteristics (\\cite{krizhevsky2012imagenet} ). In addition, in the MNIST experiment, the experimental effect of replacing the two-layer ordinary convolution kernel with the geometric operator kernel is the same as that of replacing the first-layer convolution kernel. At the same time, in order to reduce the computational complexity and the convenience of theoretical analyses, only the results of replacing one layer are considered in this paper, and the research of replacing multiple layers will be carried out in the future.\n\nQ5 feedback:\n  There are many related studies that have similarities with the method presented in this article, which are discussed in the part of related work. As for CKN mentioned, there are at least three differences with the GO-CNN proposed in this article:\n1. The number of trainable parameters about convolution kernels is unchanged in CKN comparing to common CNN, however that of the corresponding GO-CNN is smaller.\n2. CKN calculates the convolution kernel by approximating the Gaussian kernel with linear functions, and GO-CNN directly learn proper parameters of the geometric operator functions from training samples to get the convolution kernel.\n3. CKN uses an energy function as loss for better performance in approximation, and GO-CNN uses directly cross entropy as loss for better performance in classification.\nTherefore, GO-CNN proposed in this article is very different from CKN, and other similar studies discussed in the related work section.\n\nQ6 feedback:\n  In this article, we use random uniform initializer to initialize the five parameters in Gabor kernel function, wherein \\phi_{init} ~ U(-\\pi, \\pi), \\lambda_{init} ~ U(2, 10), \\theta_{init} ~ U(0, 2\\pi), \\sigma_{init} ~ U(0, 2\\pi), and \\gamma_{init} ~ U(0, 1).\n  The model\u2019s accuracy rates over ten experiments on the Cifar-10/100 test set if random initialization is used are listed as following:\n1. GO-ResNet18 achieves accuracy of 95.17%\u00b10.13% in Cifar-10 and 77.59%\u00b10.04% in Cifar-100.\n2. GO-ResNet34 achieves accuracy of 95.77%\u00b10.14% in Cifar-10 and 78.26%\u00b10.03% in Cifar-100.\n3. GO-ResNet50 achieves accuracy of 94.72%\u00b10.08% in Cifar-10 and 79.50%\u00b10.06% in Cifar-100. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}