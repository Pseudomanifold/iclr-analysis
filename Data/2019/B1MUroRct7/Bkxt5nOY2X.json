{"title": "the algorithm is not efficient for large p", "review": "This paper proposes an online learning algorithm for supervised dimension reduction, called incremental sliced inverse regression (ISIR). The key idea is converting the SIR problem into PCA problem by using the inverse of covariance matrix. After the transformation, we can use incremental PCA to compute the top eigenvector and obtain the approximate solution of SIR in streaming way. The authors also extend ISIR to overlapping case.\n\nThe motivation of this paper is reasonable, but I have some concerns as follow.\n\n1. The computation of ISIR is dependent on maintaining the matrix \\hat \\Sigma\u2019 (or its inverse), which requires O (p^2) time and space. In my opinion, this complexity is too expensive for high dimensional datasets which makes the main result of this paper is not strong. Maybe we can use low-rank approximation and its variants to improve the efficiency.\n\n2. For large dataset, the covariance matrix may be ill-conditioned with more and more data arriving even we use warm start strategy at first. It is more reasonable to introduce a ridge term to make the algorithm more stable.\n\n3. The experiments only evaluate on some small datasets. It is not enough to show the advantage of the proposed algorithms. There are also many other strategy can be used into this problem such as random sampling, random projections and frequent directions, but this paper does not provide sufficient discussion.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}