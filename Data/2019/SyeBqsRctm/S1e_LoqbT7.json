{"title": "Replacement for an original reviewer", "review": "The paper proposes a modification of the saliency map/gradient approach to explain neural networks.\n\n# Method summary\n\nThe approach is as follows:\nFor each layer, the gradient w.r.t. it's input layer is computed for multiple images concurrently.\nThen for conv layers, the activations are averaged per feature map (over space).\nAs a result, for both fully connected and convolutional layers there is a 3D feature map.\nFrom these at most b positive outliers are selected to be propagated further. \nWhat is a bit strange is that in the results section, guided backpropagation is mentioned and clearly used in the visualizations but not mentioned in the technical description.\n\n# Recommendation\n\nThe current evaluation is definitely not sufficient for acceptance. \nThe evaluation is done in a purely qualitative matter (even in section 4.1 Quantitive justification of outliers as relevant neurons). The results appear to be interesting but there is no effort done to confirm that the neurons considered to be relevant are truly relevant. On top of that, it is also evaluated only on a single network and no theoretical justification is provided.\n\n# Discussion w.r.t. the evaluation\n\nTo improve section 4.1,  the authors could for example drop out the most important neurons and re-evaluate the model to see whether the selected neurons have a larger impact than randomly selected neurons. Since the network is trained with dropout, it should be somewhat robust to this. This would not be a definitive test, but it would be more convincing than the current evaluation. Furthermore high values do not imply importance. \n\nIt might be possible that I misunderstood the experiment in Figure 2. So please correct me if this is the case in the reasoning below. \nIn figure 2, FC2 is analyzed. This is the second to last layer. So I assume that only the back-propagation from logits (I make this assumption since this is what is done commonly and it is not specified in the paper) to FC2 was used. Since we start at the same output neuron for a single class, all visualisations will use the same weight vector that is propagated back. The only difference between images comes from which Relu's were active but the amount if variability is probably small since the images were selected to be classified with high confidence. Hence, the outliers originate from a large weight to a specific neuron. \n\nThe interpretation in the second paragraph of section 4.2.1 is not scientific at all. I looked at the German Shepherd images and there are no teeth visible. But again, this is a claim that can be falsified easily. Compare the results when german Shepherds with teeth visible are used and when they are not. The same holds for the hypothesis of the degree of danger w.r.t. the separation. \n\nFinally, there is no proof that the approach works better than using the magnitude of neuron activations themselves, which would be an interesting baseline. \n\nAdditional remarks\n---------------------------\n\nThe following is an odd formulation since it takes a 3D tensor out of a 5D one and mixes these in the explanation:\n\"... the result of equation for is a 5D relevance tensor $\\omega^l_{n,i,..} \\in R^{H\\times W\\times K} .....\"\n\nThe quality of the figures is particularly poor. \n- Figure 1 b did not help me to understand the concept.\n- Figure 2 The text on the figure is unreadable. \n- Figure 4a is not readable when printed. ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}