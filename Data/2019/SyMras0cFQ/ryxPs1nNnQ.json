{"title": "Well written paper, with good literature review and interesting experiments showing faster unsupervised learning", "review": "This paper proposed a bio-inspired sparse coding algorithm where iterations\nfor dictionary updates take into account the past updates. It is argued\nthat time takes a crucial rule in learning.\n\nThe paper is quite well written and contains an extensive literature review\ndemonstrating a good understanding of previous literature in both ML/DL and biological\nvision.\n\nThe idea of using a \"non-linear gain normalization\" to adjust atom selection\nin sparse coding is interesting and as far as I know novel, while providing\ninteresting empirical results: The system learns in an unsupervised way faster.\n\nMisc:\n\n- Using < > for latex brakets is not ideal. I would recommend: $\\langle\\,,\\rangle$\n\n- \"derivable\" I guess you mean \"differentiable\"\n\n- Oliphant and Hunter are cited for Numpy/scipy and matplotlib but the\nreference to Pedregosa et al. for sklearn is missing.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}