{"title": "Solid work but the importance unclear", "review": "Please consider this rubric when writing your review:\n1. Briefly establish your personal expertise in the field of the paper.\n2. Concisely summarize the contributions of the paper.\n3. Evaluate the quality and composition of the work.\n4. Place the work in context of prior work, and evaluate this work's novelty.\n5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.\n6. Provide a summary judgment if the work is significant and of interest to the community.\n\n1. I am a researcher working at the intersection of machine learning,\ncomputational neuroscience and biological vision.  I have experience\nwith neural network models and visual neurophysiology.\n\n2. This paper develops and tests an adaptive homeostatic algorithm for\nunsupervised visual feature learning (for example for learning models\nof early visual processing/V1).\n\n3.The work spends a lot of pages describing the general problem of\nunsupervised feature learning and the history of the base algorithms.\nThe literature review is quite extensive.  The new content appears to\nbe in section 2.2 (Histogram Equalization Homeostasis - HEH), where a\nsimple idea to keep all units with balanced activity over the set of\nnatural images.  The authors also develop a computationally cheaper\nversion they call HAP (Homeostasis on Activation Probability) The\nauthors show that their F function is optimized quicker with the HEH\nand HAP algorithms.  I would like to see how these curves vary with\nthe number of neurons (e.g. can you add X% more neurons and get\nsimilar convergence speed -- and if so which is more computationally\ncostly)?\n\n4. Many groups have developed various homeostatic algorithms for\nunsupervised learning, though I have not seen this exact one before.\n\n5.  The experiments reveal the resulting receptive fields and show the \ndecrease in the F function (error function).    The resulting receptive fields\ndo not seem that different to me between the different methods.  I am also not\nthat convinced that the faster convergence as a function of learning step is that important\nespecially as the learning steps may be more computationally expensive for this method.\n\n6. I am not sure how interesting this work will be for the ICLR audience,\nas it is not clear how important the faster convergence and more even\nutilization of neurons is (and how it would compare computationally\nwith just having more neurons).\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}