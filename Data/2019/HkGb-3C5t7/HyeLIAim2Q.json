{"title": "Review", "review": "This paper studies adversarial discriminative domain adaptation (ADDA) and proposes several modifications to improve its empirical performance on two standard datasets for unsupervised domain adaptation. In its current form, the submitted paper is very incremental and fails to deliver justifications or insights on why these specific modifications help generalization. Overall the paper is quite easy to understand, but it is written in a form focusing on what have been done by the authors rather than why these modifications should be applied. Empirically, the authors show that the proposed model achieves better results on 2 out of 4 domain adaptation experiments on digits, but without proper statistical testing it is not clear whether the improvement is significant or not. There are quite a few notations that are used without introductions and several inconsistent use of the same notations, see more details below. \n\nThe proposed framework for unsupervised domain adaptation in this paper is the same as the original one studied in the ADDA paper. The only difference lies in the design of specific loss functions on the domain discriminators: 1). Instead of using the binary classification loss over source and target domains, the authors use the K+1 classes instead. 2). Besides the domain classification error, the authors propose to use a denoised reconstruction loss. However, despite the demonstrated improvement in the digit experiment, to me the first point does not really make sense, because intuitively in this case a large discriminative error can be caused by the confusion between the first K classes rather than by invariant representations between the source and the target. The second point is also not justified: it is not clear to me why adding a reconstruction loss on the discriminator may lead to better generalization? \n\nIn experiments, when training the shared encoder with both instances from source and target, the selection procedure of \\hat{X}_S plays an important role in the final result, but the authors didn't describe how this part is being done. \n\nMinor comments:\n1.  In section 3, when X_S is not empty set, what's the meaning of D_B? In this case since X_S and X_T are from different distributions, how do you define D_B? \n2.  In the first paragraph of section 3, H_S is used to denote the joint distribution while in section 3.2.1 H_S is used to mean the marginal distribution. \n3.  To me a better notation for function composition is D = C_d \\circ E_d rather than C_d(E_d).\n4.  In section 3.1, \\phi_s should be \\theta_s. \n5.  In the end of section 3.2.1, \\theta_t should be \\theta_b.\n6.  In section 3.2.2, q_s should be q_b. \n7.  In Eq. (3), for h_b, why can you have access to y_b in the unsupervised domain adaptation scenario?\n8.  In Eq. (6), there is no j inside the argmax, so what's the meaning of this equation here?\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}