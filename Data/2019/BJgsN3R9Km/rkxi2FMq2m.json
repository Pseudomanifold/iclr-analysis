{"title": "Good design", "review": "This paper proposed to use sparse low-ranking compression modules to reduce both computation and memory complexity of RNN models. And the model is trained using knowledge distillation. \nclarity:\nI think Fig1a can be improved. Initially I don't understand how the shuffle part works. It will be more clear if the mx1 vectors have the same length and the two (m x1) labels are in the same height.\noriginality:\nThe method is quite interesting and should be interesting to many people. \npros:\n1) The method reduces computation and memory complexity at the same time.\n2) The result looks impressive.  \ncons:\n1) Is the training of AntMan models done on GPU or CPU? How is the training time. It seems efficient implementation of the model on GPU can be challenging. \n2) It seems the modules can be used to replace any dense matrix in the neural networks. I'm not sure why it is applied on RNN only.\n3) I think another baseline is needed for comparison, a directly designed small RNN model trained using knowledge distillation. In this way, we can see if the sparse low-rank compression provides new values. ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}