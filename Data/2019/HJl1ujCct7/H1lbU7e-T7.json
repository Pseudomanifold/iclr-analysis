{"title": "Review", "review": "The paper presents an anomaly detection method called MMOCGAN which is claimed to work well on high-dimensional datasets with limited, multimodal data. The proposed idea is to train a GAN generator to simulate anomalies in the data in order to provide the one-class classifier with more negative examples. Overall I find that the paper is not clear and reproducible enough for me to recommend its acceptance:\n- Results are presented on a single private dataset, and I don\u2019t see any indication that the dataset will be shared with the community. This is problematic because there is no way for the community to reproduce and validate these results. I don\u2019t think results on private datasets should systematically be rejected, but they should at least be presented alongside results on public benchmarks to enable some form of reproducibility.\n- Given the small number of non-pass products in the dataset (22), it\u2019s unclear to me whether a held-out test set was used, or if hyperparameter selection was performed on the full set of non-pass products.\n- The use of a pre-trained, domain-specific feature extractor is briefly mentioned but no details (what is the architecture, on which data it\u2019s been trained, etc.) are provided.\n- The central idea in the paper is to have the generator capture the \"complementary distribution\" of the data-generating distribution. The way in which this distribution is defined is not specific enough (it depends on hyperparameters C and epsilon for which there is no clear prescribed value). On a conceptual level it seems to me that for a data-generating distribution corresponding to a low-dimensional manifold embedded in a high-dimensional space the complementary distribution will essentially be uniform random noise, and in that case it\u2019s unclear to me how it\u2019s supposed to \"simulate anomalies\".\n- The way the proposed method is presented makes it look ad-hoc: several moving parts (InfoGAN, generator loss term encouraging it to learn the \"complementary distribution\", feature matching regularization term, pull-away loss term, discriminator entropy term) are connected together and their individual inclusion in the final loss is loosely justified. In practice, looking at the results it\u2019s impossible for me to tell which term is necessary and which is not.\n- The word \"modal\" is used throughout as a noun. I\u2019m not sure if the authors mean \"model\", \"mode\", or \"modality\", but based on the context I assume they mean \"mode\" as in \"mode of the distribution\".\n- The use of an InfoGAN architecture and loss is not credited clearly enough to Chen et al. and may give the impression to a casual reader that the idea is novel to this paper. The paper also does not make it clear how the number of categories or modes for the latent variable should be chosen, and what was the value used for the experiments.\n- The paper is legible, but there are several grammatical errors and typos throughout that make it harder to read than necessary.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}