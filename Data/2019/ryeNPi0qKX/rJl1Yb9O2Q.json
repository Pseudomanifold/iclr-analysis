{"title": "official review", "review": "This is nicely written paper analyzing the effect of various pre-training methods and shows that language models are very effective on sequence tagging tasks (POS, CCG). The experiments are well motivated and well described.\n\nRegarding Table 1: which one of the \"LM forward\" models was used in the subsequent experiments? \n\nAre the input embeddings for the random init LSTM pre-trained or are they also randomly initialized?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}