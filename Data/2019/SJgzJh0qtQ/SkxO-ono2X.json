{"title": "This paper proposes a PCA based analysis of a pre-trained network to determine the number of neurons that are relevant in a Deep network", "review": "\nClaims in the paper are there is no need for iterations or retraining and proposing design heuristics for optimal network design. \nThe analysis is not only about width but also depth without compromissing accuracy.\n\nFrom the paper, one of the findings is a consistent answer independently of the architecture. \n- What sounds interesting to me is finding the depth. However, does seem quite heuristic and not really relevant. \n- The idea of using PCA to analyze the relevance of each filter has been approached in several papers as postprocessing and some others  (for instance Compression-aware training of Neural nets, NIPS 2017) during training and therefore, in this case, no finetuning would be required. \n- How does this differ from those?\n\n- What is really the take home message from the analysis of the layers? In the end, after the architecture is found, there is a need to retrain (as it is still an approximation)\n\n- Results on imagenet are interesting but based on VGG networks that, from related works, are extremely overparameterized and therefore easy to prune. \n\n- As in the conclusions, what will happen with relus and batch norms? I have to say those conclusions seem more a discussion than conclusions\n\n\n- One other claim is this could be used while designing the network for new data. I do not see that point. Why? In the end, you need the network to converge and have proper accuracy, and if that is the case, worth trying to just do regularization, compression aware pruning or simple pruning.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}