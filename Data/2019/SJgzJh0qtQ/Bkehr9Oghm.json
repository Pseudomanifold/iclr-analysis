{"title": "Interesting investigation based on reasonable heuristics. Benchmarking and comparison with respect to other compression methods is needed ", "review": "This paper presents a framework for optimising neural networks architectures through the identification of redundant filters across layers. \nThe intuition behind this work is that the redundancy in a given layer can be modelled through linear transformation of a low-dimensional set of basis functions. This motivates the use of PCA at each layer to identify a novel low-dimensional representation of the filters. \nThe PCA is actually performed on the set of concatenated outputs obtained by convolving a given input patch with each filter. The practical implementation of the scheme is based on the computation of PCA on the activations obtained from a small sample of training images. \n\nThe results provide some interesting insights. The pruning seems to be more effective for deeper layers, where the overall accuracy seems to be less sensitive to filters removal. The experimental validation shows that re-training a network by using the reduced filter set results in a minimal accuracy drop. Overall, the paper is based on a nice intuition and points to interesting research directions. However, the development looks quite heuristic and not completely intuitive. Moreover, the work completely lacks the comparison with respect to the state of the art.\n\nDetailed comments.\n\n- One may argue that the heuristics 1-4 provided in section 3.1 would be enough to effectively reduce the networks parameters. The authors should have compared the proposed framework with respect to the network reduction obtained by using these approaches (even without retraining).  \n\n- The logic from section 3.1 to 3.2 is counterintuitive. The idea is introduced by studying the spatial correlation analysis across filters. \nHowever, the  actual dimensionality reduction is performed on stacked convolution outputs, which are non-trivially related to the filter appearance. Again, the impression is that the development of the work is not clearly motivated from the methodological perspective.\n\n- The procedure based on iterative convolutions on randomised training samples is quite tedious, and again completely dependent on heuristics. Moreover, the required sample size may be prohibitive for many real world applications. \n\n- The post-hoc assessment of the redundancy may lacks of consistency, as it would be beneficial to prune filters and weights directly during training. For example, it is not clear under which sense the proposed compression is optimal. Since no relationship across layers is taken into account it is difficult to assess the impact of pruning across different layers.  \n\n- One of the major issue of this work concerns the lack of comparison with respect to the state of art. For example, one would expect a comparison with respect to some of the several proposed techniques on network compression. \n\n- Section 6. Figure 9 is mentioned, but the authors are perhaps referring to 8b and 8c?\n\n- I found the paper presentation not optimal. The focus of the first part of the paper (Section 3.1) provides some evidence about the paper\u2019s idea. However it distracts the reader from the contribution of the work. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}