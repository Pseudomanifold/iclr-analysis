{"title": "Good paper, accept", "review": "Overall impression: \nI think that this is a well written interesting paper with strong results. One thing I\u2019d have liked to see a bit more is an explanation of why self imitation is more effective than standard policy gradient? Where does the extra supervision/stability come from, and can this be explained intuitively? I\u2019ve suggested some small changes/clarifications to be made inline, and a few more comparisons to add. But overall, I very much like this line of work and I recommend accepting this paper. \n\n\nAbstract:\nWe demonstrate its effectiveness on a number of challenging tasks. -> be more specific.\n\nThe term single-timestep optimization is not very clear. Can this be clarified?\n\nthey are more widely applicable in the sparse or episodic reward settings -> it is likely important to mention that they are agnostic to horizon of the task.\n\nRelated works: \nGuided Policy Search also does divergence minimization. GAIL considers the imitation learning work as a sort of divergence minimization problem as well, which should be explicitly mentioned. Other work for good exploration include DIAYN (Eysenbach et al 2018). The difference in resulting updates between (Oh et al) and this work should be clearly discussed in the methods section. \n\n\u201cwe learn shaped, dense rewards\u201d-> too early in the paper for this to make sense. can provide some contextt\n\nSection 2.2:\nfully decides the expected return -> clarify this a bit. I think what you mean is that the dynamics are wrapped into this already, so it accounts for this, but this can be made explicit.\n\nSmall typos in appendix 5.1 (r should be replaced by the density ratio)\n\nThe update in (3) seems quite similar to what GAIL would do. What is the difference there? Or is the difference just in the fact that the experts are chosen from \u201cself\u201d experiences. \n\nHow is the priority list threshold and size chosen?\n\u2028Would a softer version of the priority queue update do anything useful? Or would it just reduce to policy gradient when weighted by rewards?\n\nAppendices are very clear and very informative while being succinct!\n\nI would have liked to see Appendix 5.3 in the main text (maybe a shorter form) to clarify the whole algorithm \n\nWhat is psi in appendix 5.3? The algorithm remains a bit unclear without this clarification\n\nExperiments. \nOnly 1 question to answer in this section is labelled? Put 2) and 3) appropriately. \n\nCan a comparison to Oh et al 2018 be added to this for the sake of completeness? Also can this be compared to using novelty/curiosity based exploration schemes?\n\nCan the authors comment on why the method reaches higher asymptotic performance but is often slower in the beginning than the other methods in Fig 3. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}