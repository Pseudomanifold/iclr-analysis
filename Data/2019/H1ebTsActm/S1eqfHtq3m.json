{"title": "Paper that establishes minimax optimal rates for deep network models over Besov spaces", "review": "This paper makes two contributions:\n* First, the authors show that function approximation over Besov spaces for the family of deep ReLU networks of a given architecture provide better approximation rates than linear models with the same number of parameters.\n* Second, for this family and this function class they show minimax optimal sample complexity rates for generalization error incurred by optimizing the empirical squared error loss.\n\nClarity: Very dense; could benefit from considerably more exposition.\n\nOriginality: afaik original. Techniques seem to be inspired by a recent paper by Montanelli and Du (2017).\n\nSignificance: unclear.\n\nPros and cons: \nThis is a theory paper that focuses solely on approximation properties of deep networks. Since there is no discussion of any learning procedure involved, I would suggest that the use of the phrase \"deep learning\" throughout the paper be revised.\n\nThe paper is dense and somewhat inaccessible. Presentation could be improved by adding more exposition and comparisons with existing results.\n\nThe generalization bounds in Section 4 are given for an ideal estimator which is probably impossible to compute.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}