{"title": "AR3: Hindsight policy gradients", "review": "The authors present HPG, which applies the hindsight formulation already applied to off-policy RL algorithms (hindsight experience replay, HER, Andrychowicz et al., 2017) to policy gradients.\nBecause the idea is not new, and formulating HPG from PG is so straightforward (simply tie the dynamical model over goals), the work seems incremental. Also, going off policy in PG is known to be quite unstable, and so I'm not sure that simply using the well known approach of normalized importance weights is in practice enough to make this a widely useful algorithm for hindsight RL.\n\n\nEvaluation      3/5 How does HPG compare to HER? The only common experiment appears to be bit-flipping, which it appears (looking back at the HER paper, no reference to HER performance in this paper) to signifcantly underperform HER. In general I think that the justification for proposing HPG and possible advantages over HER need to be discussed: why should we generalize what is considered an on-policy algorithm like PG to handle hindsight, when HER seems ideally suited for such scenarios? Why not design an experiment that showcases the advantages of HPG over HER? \nClarity              4/5 Generally well explained.\nSignificance    3/5 The importance of HPG relative to off-policy variants of hindsight is not clear. Are normalized importance weights, a well established variance reduction technique, enough to make HPG highly effective? Do we really want to be running separate policies for all goals? With the practical need to do goal sub-sampling, is HPG really a strong algorithm (e.g. compared to HER)? Why does HPG degrade later in training sometimes when a baseline is added? This is strange, and warrants further investigation.\nOriginality     2/5 More straightforward extension of previous work based on current presentation. \n\nOverall I feel that HPG is a more straightforward extention of previous work, and is not (yet at least) adequately justified in the paper (i.e. over HER). Furthermore, the experiments seem very preliminary, and the paper needs further maturation (i.e. more discussion about and experimental comparision with previous work, stronger experiments and justification).\nRating          5/10 Weak Reject\nConfidence      4/5\n\nUpdated Review: \n\nThe authors have updated the appendix with new results, comparing against HER, and provided detailed responses to all of my concerns: thank you authors.\n\nWhile not all of my concerns have been addressed (see below), the new results and discussion that have been added to the paper make me much more comfortable with recommending acceptance. The formuation, while straightforward and not without limitations, has been shown in preliminary experiments to be effective. While many important details (e.g. robust baselines and ultimate performance) still need to be worked out, HPG is almost certainly going to end up being a widely used addition to the RL toolbox. Good paper, recommend acceptance.\n\nEvaluation/Clarity/Originality/Significance: 3.5/4/3/4\n\nRemaining concerns: \n- The poor performance of the baselines may indeed be due to lack of hindsight, but this should really be debugged and addressed by the final version of the paper.\n- Results throughout the paper are shown for only the first 100 evaluation steps. In many of the figures the baselines are still improving and are highly competitive... some extended results should be included in the final version of the paper (at least in the appendix).\n- As pointed out, it is difficult to compare the HER results directly, and it is fair to initially avoid confounding factors, but Polyak-averaging and temporal difference target clipping are important optimization tricks. I think it would strengthen the paper to optimize both the PG and DQN based methods and provide additional results to get a better idea of where things stand on these and/or possibly a more complicated set of tasks.\n\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}