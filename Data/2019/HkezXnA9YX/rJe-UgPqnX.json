{"title": "Review", "review": "This paper presents a targeted empirical evaluation of generalization in models\nfor visual reasoning. The paper focuses on the specific problem of recognizing\n(object, relation, object) triples in synthetic scenes featuring letters and\nnumbers, and evaluates models' ability to generalize to the full distribution of\nsuch triples after observing a subset that is sparse in the third argument. It\nis found that (1) NMNs with full layout supervision generalize better than other\nstate-of-the art visual reasoning models (FiLM, MAC, RelNet), but (2) without\nsupervised layouts, NMNs perform little better than chance, and without\nsupervised question attentions, NMNs perform better than the other models but\nfail to achieve perfect generalization.\n\nSTRENGTHS\n- thorough analysis with a good set of questions\n\nWEAKNESSES\n- some peculiar evaluation and presentation decisions\n- introduces *yet another* synthetic visual reasoning dataset rather than\n  reusing existing ones\n\nI think this paper would have been stronger if it investigated a slightly\nbroader notion of generalization and had some additional modeling comparisons.\nHowever, I found it interesting and think it successfully addresses the set of\nquestions it sets out to answer. If it is accepted, there are a few things that\ncan be done to improve the experiments.\n\nMODELING AND EVALUATION\n\n- Regarding the dataset: the proliferation of synthetic reasoning datasets is\n  annoying because it makes it difficult to compare results without downloading\n  and re-running a huge amount of code. (The authors have, to their credit, done\n  so for this paper.) I think all the experiments here could have been performed\n  successfully using either the CLEVR or ShapeWorld rendering engines: while the\n  authors note that they require a \"large number of different objects\", this\n  could have been handled by treating e.g. \"red circle\" and \"red square\" as\n  distinct atomic primitives in questions---the fact that redness is a useful\n  feature in both cases is no different from the fact that a horizontal stroke\n  detector is useful for lots of letters.\n\n- I don't understand the motivation behind holding out everything on the\n  right-hand side. For models that can't tell that the two are symmetric, why\n  not introduce sparsity everwhere---hold out some LHSs and relations?\n  \n- Table 1 test accuracies: arbitrarily reporting \"best of 3\" for some model /\n  dataset pairs and \"confidence interval of 5\" for others is extremely\n  unhelpful: it would be best to report (mean / max / stderr) for 5. Also, it's\n  never stated which convidence interval is reported.\n\n- Table 1 baselines: why not run Conv+LSTM and RelNet with easier #rhs/lhs data?\n\n- How many MAC cells are used? This can have significant performance\n  implications. I think if you used their code out of the box you'll wind up\n  with way bigger structures than you need for this task.\n\n- I'm not sure how faithful the `find` module used here is to the one in the\n  literature, and one of the interesting claims in this work is that module\n  implementation details matter! The various Hu papers use an attentional\n  parameterization; the use of a ReLU and full convolution in Eq. 14 suggest\n  that that one here can pass around more general feature maps. This is fine but\n  the distinction should be made explicit, and it would be interesting to see\n  additional comparisons to an NMN with purely attentional bottlenecks.\n\n- Why do all the experiments after 4.3 use #rhs/lhs of 18? If it was 8 it would\n  be possible to make more direct comparisons to the other baseline models.\n\n- The comparison to MAC in 4.2 is unfair in the following sense: the NMN\n  effectively gets supervised textual attentions if the right parameters are\n  always plugged into the right models, while the MAC model has to figure out\n  attentions from scratch. A different way of structuring things would be to\n  give the MAC model supervised parameterizations in 4.2, and then move the\n  current MAC experiment to 4.3 (since it's doing something analogous to\n  \"parameterization induction\".\n  \n- The top-right number in Table 4---particularly the fact that it beats MAC and\n  sequential NMNs under the same supervision condition---is one of the most\n  interesting results in this paper. Most of the work on relaxing supervision\n  for NMNs has focused on (1) inducing new question-specific discrete structures\n  from scratch (N2NMN) or (2) finding fixed sequential structures that work well\n  in general (SNMN and perhaps MAC). The result this paper suggests an\n  alternative, which is finding good fixed tree-shaped structures but continuing\n  to do soft parameterization like N2NMN.\n\n- The \"sharpness ratio\" is not super easy to interpret---can't you just report\n  something standard like entropy? Fig 4 is unnecessary---just report the means.\n\n- One direction that isn't explored here is the use of Johnson- or Hu-style\n  offline learning of a model to map from \"sentences\" to \"logical forms\". To the\n  extent that NMNs with ground-truth logical forms get 100% accuracy, this turns\n  the generalization problem studied here into a purely symbolic one of the kind\n  studied in Lake & Baroni 18. Would be interesting to know whether this makes\n  things harder (b/c no grounding signal) or easier (b/c seq2seq learning is\n  easier.)\n\nPRESENTATION\n\n- Basically all of the tables in this paper are in the wrong place. Move them\n  closer to the first metnion---otherwise they're confusing.\n\n- It's conventional in this conference format to put all figure captions below\n  the figures they describe. The mix of above and below here makes it hard to\n  attach captions to figures.\n\n- Some of the language about how novel the idea of studying generalization in\n  these models is a bit strong. The CoGenT split of the CLEVR dataset is aimed\n  at answering similar questions. The original Andreas et al CVPR paper (which btw\n  appears to have 2 bib entries) also studied generalization to structurally\n  novel inputs, and Hu et al. 17 notes that the latent-variable version of this\n  model with no supervision is hard to train.\n\nMISCELLANEOUS\n\n- Last sentence before 4.4: \"NMN-Chain\" should be \"NMN-Tree\"?\n\n- Recent paper with a better structure-induction technique:\n  https://arxiv.org/abs/1808.09942. Worth citing (or comparing if you have\n  time!)", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}