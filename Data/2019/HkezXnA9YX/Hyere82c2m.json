{"title": "Interesting observations but limited experiments; also doubtful how experiments and learning can be generalized to more complex tasks", "review": "Summary: The paper focuses on comparing the impact of explicit modularity and structure on systematic generalization by studying neural modular networks and \u201cgeneric\u201d models. The paper studies one instantiation of this systematic generalization for the setting of binary \u201cyes\u201d or \u201cno\u201d visual question answering task.  They introduce a new dataset called in which model has to answer questions that require spatial reasoning about pairs of randomly scattered letters and digits in the image. While the models are evaluated on all possible object pairs, they are trained on a smaller subset. They observe that NMNs generalize better than other neural models when an appropriate choice of layout and parametrization is made. They also show that current end-to-end approaches for inducing model layout or learning model parametrization fail to generalize better than generic models.\n\nPros:\n- The conclusions of the paper regarding the generalization ability of neural modular networks is timely given the widespread interest in these class of algorithms. \n- Additionally, they present interesting observations regarding how sensitive NMNs are to the layout of models. Experimental evidence (albeit on specific type of question) of this behaviour will be helpful for the community and hopefully motivate them to incorporate regularizers or priors that steer the learning towards better layouts.  \n- The authors provide a nice summary of all the models analyzed in Section 3.1 and Section 3.2. \n\nCons:\n- While the results on SQOOP dataset are interesting, it would have been very exciting to see results on other synthetic datasets. Specifically, there are two datasets which are more complex and uses templated language to generate synthetic datasets similar to this paper:\n    - CLEVR environment or a modification of that dataset to reflect the form of systematic the authors are studying in the paper. \n    - Abstract Scenes VQA dataset introduced in\u201cYin and Yang: Balancing and Answering Binary Visual Questions\u201d by Zhang and Goyal et al. They provide a balanced dataset in which there are a pairs of scenes for every question, such that the answer to the question is \u201cyes\u201d for one scene, and \u201cno\u201d for the other for the exact same question. \n- Perhaps because the authors study a very specific kind of question, they limit their analysis to only three modules and two structures (tree & chain). However, in the most general setting NMN will form a DAG and it would have been interesting to see what form of DAGs generalize better than other. \n- It is not clear to me how the analysis done in this paper will generalize to other more complex datasets where the network layout NMN might be more complex, the number of modules and type of modules might also be more. Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets. \n\nOther Questions / Remarks:\n- Given that the accuracy drop is very significant moving from NMN-Tree to NMN-Chain, is there an explanation for this drop? \n- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper. \n- Small typo in the last line of section 4.3 on page 7. It should say: This is in stark contrast with \u201cNMN-Tree\u201d \u2026..\n- Small typo in the \u201cLayout induction\u201d paragraph, line 6 on Page 7:  \u2026 and for $p_0(tree) = 0.1$ and when we use the Find module   \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}