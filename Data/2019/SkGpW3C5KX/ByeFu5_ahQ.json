{"title": "Heated-Up Softmax Embedding", "review": "This paper presents an interesting idea to improve the softmax embedding performance with heated-up strategy. It is well-written and the proposed method is easy to implement. Several experiments on metric learning datasets demonstrate the effectiveness of the proposed method.\n\nThe motivation to find a balance between the compactness and \"spread-out\" embedding is reasonable. The major weakness is the intermediate temperature selection, it might be a little tricky. How to generalize it to other applications?\n\nThe authors claim that \"heated-up\" strategy produces well generalized feature, but the rationale behind is unclear. And there is no quantitative analysis to support this point. \n\nThe starting temperature aims at pushing the \u201cincorrect\u201d samples to \u201cboundary\u201d samples and pushing the \u201cboundary\u201d samples to \u201ccentroid\u201d samples. I would like to see the ratio of #incorrect/total and #boundary/total changed with different temperature in training process, i.e., alpha = 16, 4, 1. This experiment may help to verify the idea.\n\nAs mentioned in Section 3, multiple strategies could be defined to increase the temperature. It is interesting to design a multiple heat-up strategy. Does it help to improve the learning speed?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}