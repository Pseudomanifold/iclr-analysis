{"title": "An incremental method on lifelong learning.", "review": "This paper summarizes previous lifelong learning methods and identifies three different continual learning scenarios. Based on that, it draws a conclusion that DGR+distill outperforms other methods on all these scenarios. Further, the paper proposes unified model that combines a replay generator and a classification model. The proposed RTF model achieves comparable performance with DGR+distill and is approximately two times faster than DGR+distill.\n\nMy biggest concern is the novelty of the model, since RTF is still a replay-based method that is very similar as DGR+distill. Empirically it can be expected that RTF should behave similar as DGR+distill as well. And the result in this paper justifies that. So the main contribution comes from the efficiency boost by the integrated model strategy. That is, by replacing a separate generative model by a symmetric VAE. Besides that, there seem to be no significant contribution of the proposed model.\n\nIn my opinion, this paper look somewhat incremental. The first five pages are mostly reviews of previous methods, and the model it propose behave very similar to a previous method.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}