{"title": "Provide an additional bitstring to the discriminator which can swap the label for observed and generated samples.", "review": "Authors argue that the main issue with stability in GANs is due to the discriminator becoming too powerful too quickly. To address this issue they propose to make the task progressively more difficult: Instead of providing only the samples to the discriminator, an additional (processed) bitstring is provided. The idea is that the bitstring in combination with the sample determines whether the sample should be considered true or fake. This in turn requires the decision boundary of the discriminator to become more complicated for increasing lengths of the bitstring. In a limited set of experiments the authors show that the proposed approach can improve the FID scores.\n\nPro:\n- A simple idea to make the problem progressively more difficult.\n- The writing is relatively easy to follow.\n- Standardized experimental setup.\n\nCon:\n- Ablation study of the training tricks is missing: (1) How does the proposed approach perform when no progressive scheduling is used? (2) How does it perform without the linear model for increasing p? (3) How does the learning rate of G impact the quality? Does one need all of these tricks? Arguably, if one includes the FID/KID to modify the learning rates in the competing approaches, one could find a good setup which yields improved results. This is my major issue with this approach.\n- Clarity can be improved: several pages of theory can really be summarized into \u201clearning the joint distribution implies that the marginals are also correctly learned\u2019 (similar to ALI/BIGAN). This would leave much more space to perform necessary ablation studies. \n- Comparison to [1] is missing: In that model, it seems that the same effect can be achieved and strongly improves the FID. Namely, they introduce a model in which observed samples pass through a \"lens\" before being revealed to the discriminator thus balancing the generator and discriminator by gradually revealing more detailed features.\n- Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing? In some approaches such as Wasserstein GAN, we should train the discriminator to optimality in each round. Why is the proposed approach more practical then approaches such as [2]?\n\n[1] http://proceedings.mlr.press/v80/sajjadi18a.html\n[2] https://arxiv.org/abs/1706.08500", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}