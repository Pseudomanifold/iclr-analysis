{"title": "Clear description , lacking in comparison with relevant prior work.", "review": "In this paper the authors describe a quantization approach for activations of the neural network computation to improve the memory efficiency of neural network training and thus training efficiency of a single worker.\n\nPrior work\n-----------------\nThey compare the proposed method with other approaches involving the quantization of gradients or recomputation of activations in a sub-graph during back-propagation. However the literature survey lacks survey of more relevant quantization techniques e.g. [1]. \n[1] : Hubara, Itay, et al. \"Quantized neural networks: Training neural networks with low precision weights and activations.\" The Journal of Machine Learning Research 18.1 (2017): 6869-6898.\n\nexperimental setup\n-----------------------------\nA more formal description of experimental setup assuming a general reader not familiar with the specific toolkits is advised. Any toolkit specific details like how the layer-wise forward & backward propagation is done via separate sess.run calls can be delegated to an appendix or footnote. Further given that the authors have chosen not to utilize the auto-diff functionality or other computation graph optimization features provided by Tensorflow; and given that they are even manually managing the memory allocation it is not clear why they are relying on this toolkit.  Irrespective of this choice, this section could be re-written to make the implementation description more accessible to a general reader and toolkit specific details could specified separately.\n\nReg. manual memory management - The authors specify how common buffers are being used for storing activations and gradients across layers. Given that typical neural network models need not be composed of homogenous layer types which can actually share the buffers it would be useful to add a detail on how much efficiency is achieved by reducing the memory allocation calls for the architectures being used in this paper.\n\n\nresults\n-----------\nComparisons with prior work using other quantization methods to achieve memory efficiency is lacking.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}