{"title": "GVFs for Knowledge", "review": "This paper advocates general value functions (gvfs) as a unifying and binding technology for building AI systems, and points out how existing ideas like successor representations, policy gradient, and options can be well-thought of as gvfs. The paper uses these connections to develop new ideas for policy gradient and feudal networks, illustrating these ideas with experiments in Atari and a gridworld environment.\n\nI am leaning to reject this paper. On the one hand I agree with the authors on the importance of gvfs and their potential is spread across numerous papers making it difficult, currently to make these connections. On the other hand many of the insights here are simple and in many cases well-known, but perhaps not yet formally written down. The current paper is not particularly well organized and the experiments are a bit confusing, and not particularly convincing in the case of the Atari results. All taken together, the paper needs a bit more work to achieve the ambitious goals it sets for itself. It seems another round of editing, tightening, and improving the results could indeed make this a very solid paper.\n\nExplanation for rating. This paper is trying to achieve something fairly difficult: elevate the importance of gvfs as a unifying concept for AI systems. This is needed because the Horde and Nexting papers did not provide or attempt to spell out all the connections to other things that can be well-thought of as a collection of gvfs, while later work that used these ideas, like the UNREAL paper, did a poor job of assigning credit and clearly discussing how the gvf idea was key. To do this well the writing of the paper must proceed very carefully, and this submission falls a little flat here. For example the abstract states \u201cIn this paper, we argue that value functions are also a very natural way of providing a framework for knowledge representation for reinforcement learning agents.\u201c This was the whole objective of the Horde paper, so one has to explain what is missing without sounding redundant. The introduction has similar problematic sentences of this form: \u201cIn this paper, we highlight the fact that value functions can in fact be used as the main building block for knowledge representation in RL agents.\u201c  To do this well the paper has to be very precise, and sometimes it fails to do so. For example: \u201cSutton et al (2011)\u2026illustrates in fact the successful large-scale learning of GVFs about a diversity of signals and at many different time scales, in parallel, from a single stream of data.\u201c. This is not correct, scale was demonstrated in the Nexting paper, not the Horde paper. Furthermore, the related work section is strange because it talks about two architectures that use the gvf formalism. These seem to be examples supporting your argument, not things to contrast against. In fact, it is not clear what the related work section of such a paper should be given that this paper is primarily a review (with unifications that should always be the outcome of an extensive and insightful review) and position paper.\n\nIn the end I am not sure this paper has a clear identify because of the somewhat distracting focus on new algorithms and experiments trying to illustrate their utility\u2014but the experiments don\u2019t make a serious attempt at illustrating a contribution as discussed below. This could all be addressed with a change in the pitch and tone of the paper, but the authors should ask themselves \u201cwhat are we adding beyond what can be found in the work of Sutton et al (2011), Modayil et al (2013), White (2015), and Sutton\u2019s numerous writings on Predictive approaches to AI?\u201d\n\nThe connection between nexting (and thus gvfs) and Successor features is well known, while theorem 1 follows directly from the original policy gradient theorem. This is not to say these connections are not interesting and should not be talked about, but they are true by construction and that is indeed why using gvfs for knowledge is such a good idea. There must be a way to discuss these things without pitching the resulting new algs as the main contribution of the paper. It seems the paper is trying to do too many things at once. If the new algs are interesting write a paper about just them and provide clearer evidence of their utility rather than hiding it inside a large narrative about gvfs for AI.\n\nThe experiments are difficult to understand. Section 4.1 appears to describe an experiment, but in the end is describing an algorithm described in Fig 1. I think the alg requires rollouts which is not ideal, but this is mentioned in passing. There is an algorithm block in the paper but it is never referenced: I can\u2019t quite see how it connects to experiment #1. Section 4.3 finally describes the experiment (after the the main figure, which does not indicate the task or what the baselines are). What is being learned here: the 4 rooms problem was originally designed for option learning. Are you learning policies for each room, it is never stated. What is the behavior policy here? I don\u2019t understand the y-axis of these graphs it says \u201cvalue function\u201d but the caption mentions L1 norm. I assume up is good but I have no context to understand if this is good final performance and fast learning. A well explained baseline would help. It is really great that you show all the parameter combinations and their performance, good job.\n\nThe second experiment compares against Kondas AC calling it a a policy gradient method for larger or continuous environments. This was not explained further, and it was evaluated on a small discrete domain. I cannot tell if this experiment was fair, even consulting the plots in the appendix. It raises several questions:  How did you decide the ranges of the meta parameters to sweep? The AC baseline\u2019s performance gets better and better with increasing alphas. What about even larger alphas?  Did that make the two approaches tie.  I don\u2019t understand why the baseline totally fails in the larger gridworld. Is this expected or perhaps it is harder to get the baseline working on the larger domain? Because the paper does not comment on the outcome, the reader is left to wonder.\n\nThe results of the 3rd experiment don\u2019t paint a clear picture either. Sometimes the new approach helps a small amount, sometimes it hurts\u2014basically the same overall. Perhaps this\u2014atari\u2014was not a good domain to illustrate the merit of this idea.\n\nOverall the paper is too rushed, and too unpolished. I look forward to the author respond so that I can refine both my understanding and assessment of this work. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}