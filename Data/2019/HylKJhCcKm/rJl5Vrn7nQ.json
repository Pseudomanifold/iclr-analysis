{"title": "Reinvention of what is already proposed by Sabour et al.", "review": "The paper deals with the idea to generalize the CapsNet architecture from Sabour. Under generalization the authors mean, to define a routing procedure without an iteration parameter.\n\nIn general, your paper has a good length, is well explained and good organized. To be honest, I don\u2019t like your writing style. It seems to be a bit too casual and not formal enough for a scientific work. Additionally, note that:\n-\tYou are not staring with Fig. 1 in the introduction\u2026the counting should start by one.\n-\tAC-GAN is the abbreviation for? \n-\tEq. 2 is outside the page space.\n\nI have several concerns about the contribution. My major concern is that it isn\u2019t new in general. If I break down your method, it is just the basic Dynamic Routing procedure with:\n-\tthe number of iterations defined to be one;\n-\ttrainable initial routing coefficients;\n-\tno softmax normalization over routing coefficients.\nThe usage of trainable initial routing coefficients was already mentioned by Sabour. Thus, the only thing which is new in your method is that you skip the normalization and I\u2019m not sure that this has a positive effect on the process. \n\nMinor concerns/minor mistakes:\n1.\tYou mentioned that the code is public available. Where is the link to a respective repository?\n2.\tPage 1: \u201c[\u2026] so it makes sense to believe that CapsNet has a better generalization ability.\u201d Compared to what?\n3.\tPage 2: \u201cThe routing iterations is a meta-parameter that needs to be set manually which limits the scalability of CapsNet [\u2026].\u201d Why it should limit the scalability? It has no effect on the model size, etc. It\u2019s just a parameter which has to be defined.\n4.\tPage 3: Are you sure that a linear transformation of a hyper-cube is defined in that way in general?\n5.\tPage 4: What is T_k?\n6.\tPage 7: \u201cHinton et al. (2018) claimed that CapsNets [\u2026]\u201d Are you aware that Hinton worked on Matrix Capsules and not on the CapsNet architecture of Sabour?\n7.\tPage 8: How you can guarantee the convergence of your method? Moreover, the convergence to what?\n8.\tCould you add some histograms plots of your c_ij values after the training?\n9.\tWhy are your performance values so bad compared to CapsNet and Matrix Capsules?\n10.\tCould you add to your tables the inference, training times? If you remove the iteration parameter I would assume that your method should be faster, or?\n11.\tIs the parameter lambda in Eq. 2 the same as in Eq. 6? How you tune that parameter?\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}