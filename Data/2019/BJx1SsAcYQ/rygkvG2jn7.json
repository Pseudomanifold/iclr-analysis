{"title": "Review comments on \u201cDiscovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference\u201d", "review": "\nSummary:\nThis paper proposes three methods to improve the performance of the low-precision models. Firstly, to reduce the number of training iterations, the authors propose to do quantization on pre-trained models rather than training from scratch. Secondly, the authors propose to use large batches size and proper learning rate annealing with longer training time to reduce the gradient noise introduced in quantization. Experimental results demonstrate the effectiveness of the proposed methods.\n\nContributions:\n1.\tThe authors hypothesize that noise introduced by quantization is the limiting factor for training low-precision networks and present empirical evidence to support this hypothesis.\n2.\tThe authors formulate the error as equation (1) and propose two techniques (large batches size and proper learning rate annealing) to minimize the final error.\n3.\tThe authors conduct a series of experiments to demonstrate the effectiveness of the proposed methods.\n\nCons:\n1.\tThe novelty of this paper is limited. Firstly, fine-tune the pre-trained model is a well-known method in quantization. Secondly, using large batches size and proper learning rate annealing are more like tricks in hyper-parameter tuning rather than a method. \n\n2.\tIn table 2, the performance of the model in the second row (batch size=400) is worse than the baseline ones (batch size=256). In order to keep the same number of weight updates, the author increases the number of epochs during training, which results in performance improvement. Do large batches size really contribute to performance improvement? Whether the performance gain is due to the large batches size or more sampling data?\n\n3.\tThe authors claim that large batches size can reduce the gradient noise introduced by quantization. It would be better to show the introduced noise with different batch sizes in figure 1. \n\n4.\tThis paper is not the first time for ResNet-50 with 4-bit quantization to outperform the full-precision network. EL-Net[1] has trained a 4-bit precision network, which leads to no performance degradation in comparison with its full precision counterpart.\n\n5.\tThe title in experiments part is too long and confusing. It will be better to keep the short and meaningful title.\n\n\nReferences\n[1] Zhuang B, Shen C, Tan M, et al. Towards Effective Low-bitwidth Convolutional Neural Networks[J]. 2017.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}