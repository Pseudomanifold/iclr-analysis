{"title": "interesting paper and result is promising, but lack of novelty", "review": "This paper proposes a fine-tuning scheme for quantized network which can achieve higher accuracy ( in 8bits case) than the original full-precision (32 bits) network. The main finding/motivation of this paper is that in order to make the fine-tuning works, the retrain needs to overcome the gradient noise that is introduced by weight quantization. Therefore, it considers several typical retraining techniques: large batch size training, retraining from full-precision network instead of quantized one from scratch,  lower weight decay. \n\nI think it is an interesting paper, and the result is quite promising. In fact, I have not seen any quantized network that can perform better than the original full-precision network. While in terms of novelty, no new techniques/algorithms are proposed, and it is combing standard strategies used in retrain networks. In addition, I have several questions for this work:\n\n1) It seems that training longer time will benefit the fine-tuning a lot. What if we can also train the original model for some additional amount of training time(like 165 epochs in Table 2), and then quantize this full-precision network without retrain, will the proposed scheme still have better accuracy than this naive way? \n\n2) Will these fine-tuning strategies/findings be generalized to other datasets or other models? In this paper, only results in ImageNet are shown.\n\n3) Can I use these fine-tuning strategies to improve other quantization methods? For example, I could use larger batch size when training for other fine-tuning methods, and will it also make their quantized models better than the original precision model?\n\n4) As mentioned in the paper, the proposed quantized network is used to  speed up the inference time. Some results for inference time using the proposed quantized network will be super interesting.\n\nOverall, the proposed fine-tuning scheme has promising results. My main concern for this paper is its novelty and whether it can be generalized to other models.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}