{"title": "I thought this was an excellent paper - very clear, an important problem, a useful set of techniques and results.", "review": "The paper describes a VAE-based approach to semi-supervised learning\nof dependency parsing. The encoder in the VAE is a neural edge-factored\nparser allowing inference using Eisner's dynamic programming algorithms.\nThe decoder generates sentences left-to-right, at each point conditioning\non head-modifier dependencies specified by the tree. A key technical \nstep is to develop a method for \"differentiable\" sampling/parsing,\nusing a modification of the dynamic program, and the Gumbel-max trick.\n\nI thought this was an excellent paper - very clear, an important \nproblem, a very useful set of techniques and results. I would strongly\nrecommend acceptance.\n\nSome comments:\n\n* I do wonder how well this approach would work with orders of magnitude\nmore unlabeled data. The amount of unlabeled data used is quite small.\n\n* Similarly, I wonder how well the approach works as the amount of\nunlabeled data is decreased (or increased, for that matter). It should\nbe possible to provide graphs showing this.\n\n* Are there natural generalizations to multi-lingual data, for example\nsettings where supervised data is only available for languages other\nthan the language of interest?\n\n* It would be interesting to see an analysis of accuracy improvements\non different dependency labels. The \"root\" case is in some sense just\none of the labels (nsubj, dobj, prep, etc.) that could be analyzed.\n\n* I wonder also if this method would be particularly helpful in \ndomain transfer, for example from Wall Street Journal text to\nWikipedia or Web data in general. The improvements could be more\ndramatic in this case - that kind of effect has been seen with \nELMO for example.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}