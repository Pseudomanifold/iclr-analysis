{"title": "Paper needs a major revision, too many unjustified claims are made", "review": "The paper is overall unclear and not well-written. There are lots of typos, grammar mistakes, misuse of terminology,... that obscure the clarity of the paper. More importantly, the contribution is unclear to me, and I think the paper needs a major rewrite in order to comply with the type of standard we expect from an international publication in a machine learning venue. Below I give some comments that I hope will help the authors to revise their submission.\n\n1) \u201cIn addition, it is unclear whether the method has advantages in generalization performance.\u201c\nThere is some theoretical  results that are worth citing such as\nBottou, L., & Bousquet, O. (2008). The tradeoffs of large scale learning. In Advances in neural information processing systems (pp. 161-168).\nUnder certain assumptions, second-order methods are known to generalize more poorly.\n\n2) Page 2, \u201cpositive matrix G\u201d\nYou mean positive-definite matrix since G is a covariance matrix. Positive and Positive-definite matrices are different concepts.\n\n3) Assumption section 3\nThe authors make the assumption \u201cthere exists no stationary point except for a minima\u201d. This is a rather strong assumption which of course does not apply do deep neural networks. The authors should discuss how to relax this assumption.\n\n4) Derivation section 3\nThe authors drop the first term in the decomposition of the Hessian-momentum term without providing any justification. Why are you making this simplification? It is probably worth pointing out this resembles the Gauss-Newton approximation and you are left with a positive-definite approximation of the Hessian, therefore ignoring any negative curvature.\n\n5) Computation of the matrix\n\u201c One of the basic methods to construct such a projection... requires only the matrix multiplication\u201d\nI would also suggest mentioning that these methods usually construct sparse matrices such as tri-diagonal matrices, see e.g. Nocedal, J., & Wright, S. J. (2006). Numerical optimization 2nd.\n\n6) Suggested approach\na) The whole derivation is rather unclear, the author seem to arrive to a second-order equation similar to the one derived for Nesterov accelerated gradient in Su et al. 2014. You should contrast how your equation differs from theirs.\nb) What are the convergence guarantees for your approach? Consider deriving a proof of convergence as in Su et al. 2014 (at the very least I would expect an asymptotic result)\nc) Second-order ODES are difficult to discretize (see discussion in Su et al.) and even if the continuous method is guaranteed to converge, the discretization procedure might not have such guarantees. You need to add a discussion about discretization and explain what approach you used in practice.\n\n7) Deterministic vs Stochastic setting\nThe derivation presented in the main text assumes full gradients are computed but of course, in practice, one would only use mini-batches. The authors make some rather bold and unjustified claims regarding the ability of their method to generalize to a stochastic setting. In particular they claim \u201csetting the hyperparameter k small makes EDF be compatible with stochastic\napproaches and take their advantages\u201d. This statement requires a solid justification. I do not believe this is true. If you add noise to your differential equation, you need to ensure the noise has certain properties (vanishing noise in the limit or bounded noise) if you want to guarantee convergence. I recommend the authors read the relevant literature, for instance:\nLi, Q., Tai, C., et al. (2015). Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv preprint arXiv:1511.06251.\nKrichene, W., Bayen, A., and Bartlett, P. L. (2015). Accelerated mirror descent in continuous\nand discrete time. In Advances in neural information processing systems, pages 2845\u20132853.\n\n8) Experiments\nChoice of hyper-parameters: the authors need to explain how they pick the hyper-parameters. Simply listing what values are used is not sufficient. You need to show you\u2019ve tried different settings for the competing methods. You said\n\u201cThe step sizes for Momentum, NAG, and Adam were fixed to 0.01, 0.001, and 0.001, respectively\u201d. How did you pick these values?\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}