{"title": "Nice idea with some interesting results but needs more work", "review": "OVERVIEW:\nThe paper presents an interesting approach to unsupervised (more accurately weakly supervised) object detection that uses motion as training cue. At a high level, the authors propose to learn what an object looks like from two input videos: (1) a positive example containing the object of interest undergoing some motion and (2) a negative example of the same background scene without the object. They use a spatial encoder network in conjunction with three losses (variation loss, slowness loss, presence loss) to learn the appearance of an object based on motion cues. The only supervision here is at the video level with a label indicating the positive example. Then, given a new video, they are able to detect the object under some nuisance factors. They show four experiments on (1) detection with static camera showing written text, (2) detection of a moving objects like a roomba, drone and toy car with a moving camera, (3) detection of multiple objects, and (4) comparison with baselines of object tracking and template matching.\n\nPROS:\n1. The authors are able to show object detection with video level labels solely using motion cues. This is an interesting idea which can have significant impact with the abundance of unlabeled data in the wild and where video level annotations are much cheaper than bounding box level or even image level annotations.\n2. The spatial encoding (not a contribution of this work) to represent object location is simple but powerful and I feel can be extended to foreground segmentation. The authors use it neatly with reasonable loss functions to formulate the detection problem in an interesting way.\n3. The experiments show qualitatively that their proposed approach works reasonably well under simple settings (distinct objects on uniform background).\n\nCONS:\nThis is a nice idea and some preliminary experiments show positive qualitative results. However, for a publication especially at ICLR, it needs more work. The below points are some negative aspects of the current submission but also things I feel the authors can work on in their next submission:\n1. The Problem Setup: I think the scenario where you have one video of a single moving object in the scene and another video of the same scene without the object is not common. That is not to say that there might be some practical applications where this kind of setup in natural (maybe drones?) but you haven't convinced me about it. It will work in your favor if you are able to convince the reader where this kind of a setup is easily present and data is easily available.\n2. Concern about optimization: You mention in the second paragraph of Sec. 3.3 some weird stuff happening during optimization and how you counter it. It feels very hand-wavy and you need to either (1) investigate this behavior further and resolve the problem or (2) provide a more convincing answer as to why your proposed solutions make sense.\n3. Lack of quantitative evaluation: The experimental evaluation shows qualitative performance. I think the paper will look better (and more mature) with some quantitative metrics, say percentage of frames where the object was detected correctly, AP scores using bounding box annotations on test videos, experiments on larger number of videos with some averaged metrics.\n4. Stronger baselines: This is a continuation of the previous point. The only quantitative evaluation the authors have is MSE loss on position, between their proposed approach and baselines of template matching & object tracking. In my opinion, these baselines are very weak. Seeing the example images/videos, I am almost tempted to say that video registration followed by background subtraction with median image might also work pretty well. An out-of-the-box object detection system, trained on COCO (that hasn't seen these categories) might still return \"good\" bounding boxes with bad category labels. Basically, convince the reader with some stronger experiments than just proof-of-concept (for this kind of idea).\n5. Lack of variety in data: The data looks simple with one or few objects on plain background. I would trust the algorithm more if you show that it works on harder data, say textured background (drone in sky/field, car on road), appearance changes (train on one car, test on another), viewpoint changes, etc. \n6. One Shot Learning: The problem setup sounds very close to one-shot learning where you are seeing a video of the object once before trying to learn it. Some discussion along this direction in related work might be needed.\n\nOVERALL:\nI am rejecting the paper in it's version right now. I feel this is an interesting idea and can be resubmitted (possibly to a Vision conference) with additional work. If this was to be a vision paper, I can see a new dataset introducing this task with your approach and a stronger baseline comparison.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}