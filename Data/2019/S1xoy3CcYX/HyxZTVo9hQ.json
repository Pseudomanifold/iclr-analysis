{"title": "not very interesting conclusion", "review": "The paper tries to make a connection between the functionality of Gaussian noise to adversarial examples. It shows that data augmentation with added Gaussian noise could also improve the model robustness. \nMeanwhile, it shows that in a high dimensional space, even with a small (error) set, its bounding ball of \\epsilon l_p distance could be large. It explains why even with a small test error, the model could still be vulnerable to adversarial examples. \n\nAlthough the paper has some good intuitions and some nice experiments, I find the main conclusion of this paper not very interesting. \nAlthough I partially buy the second point about the high dimensional geometry, this is an obvious observation and does not give rise to much meaningful result for the future work. It would be more interesting to see the different geometry structure of robust versus not robust models. \n\nMeanwhile, though a formal definition of error set is not presented in the paper, it seems the authors are simply dividing the data space to the set where the model gives a correct label, and the \u201cerror set\u201d the other way around. However, since the paper is considering a data distribution (q) rather than a dataset, the separation could be more complicated than that. For instance, a noise image should also in your data space, but does it belong to an error set or not? It isn\u2019t necessarily attached to any labels. Or does your model only consider meaningful images? But what if adding noise simply get you out of the space? It\u2019s better to make this concept clearer. \n\n\nIt is not a surprising result that there is one randomly chosen direction mimicking the performance of adversarial examples as in Figure 2. By \u201ccarefully crafted imperceptible noise\u201d, I assume it means choosing one random sample that will change the model output the most. This is exactly a way of choosing an adversarial example. Since even in high dimensional space, out of a lot of random vectors , one could approximate a target (adversarial) direction.\n\nSimilar explanations also apply to the training with error part. How much more data do you use for the data augmentation? If you use much more data with Gaussian noise than what your use for adversarial training, it is not surprising at all to get a more robust network, with a similar argument as above.\n\n\nminor issue: Section 3 should not be an isolated section.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}