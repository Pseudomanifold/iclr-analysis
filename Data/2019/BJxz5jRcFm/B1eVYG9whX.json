{"title": "review", "review": "The paper proposes to extend VAT by decomposing the regularization term into two spaces, the tangent space and the normal space. The tangent space is spanned over the columns of the Jacobian matrix of a function that maps latent (manifold) variables to data, and the normal space is defined as orthogonal to the tangent space. Power iteration was employed to optimize the inner-loop function.\n\nPros\n1. Positive signals have been shown on multiple datasets.\n2. The proposed regularization terms seem to be novel and technically sound.\n\nCons and questions\n1. How did you obtain the results of VAT in the experiments? It seems the results in Table 3 could not be found in Miyato et al. It is important to obtain a proper baseline to substantiate the main claim of the paper. Moreover, it might not be appropriate to claim the proposed method is state-of-the-art (SoTA) on SVHN and CIFAR-10 given that the numbers in Table 3 are different from Miyato et al. To claim SoTA, you would need to at least implement your model in a setting that is comparable to the numbers in Miyato et al (e.g. using ZCA), rather than running your model in a setting that was not considered in Miyato et al (e.g. removing ZCA).\n2. It seems that the proposed method introduces a bunch of additional hyper-parameters, including lambda, two epsilons, and the alphas that weigh different terms. Is it difficult to tune these hyper-parameters in practice? What values did you actually use for these hyper-parameters?\n3. The paper would be further improved if an ablation study could be performed on SVHN and/or CIFAR-10, since these are more popular datasets for semi-supervised learning. It would become clearer how the proposed method is better than the previous ones.\n4. The paper claimed that \"TAR inherits the computational efficiency from VAT\". Does power iteration bring additional computation costs? What is the actual computational time, compared to VAT?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}