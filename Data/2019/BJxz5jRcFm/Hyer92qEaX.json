{"title": "Limited novelty and not fully convince by the improvement in empirical evaluation", "review": "As I was asked for an emergency reviewer for this paper rather late, I refrain myself from reading at other reviewers comment when writing this review for an unbiased judgment. Please forgive me if the concerns or issues I raise here already been asked by other reviewers.\n\n- summary\nThis paper proposes to improve semi-supervised learning with two manifold regularizations: the tangent adversarial regularization (TAR) and normal adversarial regularization (NAR), hence naming their method Tangent-Normal Adversarial Regularization (TNAR). The author implements the TAR and NAR by applying virtual adversarial training (VAT) techniques on the manifold space, which is trained separately by either variational autoencoder (VAE) or localized GAN (LGAN) to obtain the encoder and decoder mapping between data space and manifold space. The proposed TNAR shows improvement on the controlled synthetic dataset; demonstrates the effectiveness of both TAR and NAR via the ablation study on the FashionMNIST dataset; claims to outperform state-of-the-art SSL methods on both SVHN and CIFAR-10 datasets. \n\nEvaluation\n- The writing of the paper is in general ok, but reading the introduction that categorizes the SSL by three streams seem somehow unnatural to me. Perhaps it would be more clear to directly start from manifold space assumption and add more details or connection to other manifold space regularization related works.  \n\n-The technical derivation in section 3 is sound as far as I can tell because it mainly involves existing techniques that have been studied in VAT [7]. This also makes me feel the contribution in terms of novelty is rather limited, since the tangent space regularization is not new [4]. The only novel part seems to add an additional normal adversarial regularization that is perpendicular to the tangent noise.\n\n- My biggest concern is in the empirical evaluation. The experiment setup closely follows [7], using the model architecture Conv-small appeared in [1,3,4,5,6,7] and Conv-large appeared in [2,6,7], while not applying ZCA whitening as data preprocessing. This possibly leads to an inconsistency between the results in Table 3 of this paper and table 4 of [7] for both Conv-small and Conv-large setting. Also, it would also be an unfair comparison that only TNAR and VAT include entropy regularization while other SOTA methods [1,3,4,5,6] did not. As shown in Table 4 of [7], entropy-regularization significantly improve accuracy. I suggest the author include results of TNAR w/o entropy-regularization;  results of ZCA preprocessed data;  setting alpha_2 and alpha_3 as zero to see the difference between TNAR and FM-GAN [5].\n\nMinor questions:\n- What is the difference between TNAR and [5]? Is it that TNAR finding the tangent vector r where TNAR applies power methods as like VAT while [5] find the tangent vector r differently?\n\n- How important is the GAN/VAE training for good semi-supervised learning task? Do you also find the bad generator leading to better SSL task as shown in [8]? I am also curious that how different encoder/decoding mapping would affect the SSL downstream task, as different prior works adopt different settings.\n\n- TNAR is claimed to be computational efficient compared other manifold regularization based on tangent propagation or manifold Laplacian norm. Do you have empirical evidence?\n\n- I also suggest reporting the mean/std of the experiment results by running like five different random seed, as most of the works did.\n\n[1] T. Salimans et al., Improved techniques for training gans. NIPS 2016\n[2] S. Laine and T. Aila. Temporal Ensembling for Semi-supervised Learning. ICLR 2017\n[3] V. Dumoulin et al., Adversarial Learned Inference. ICLR 2017\n[4] A. Kumar et al, Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference, NIPS 2017\n[5] B. Lecouat et al., Semi-supervised Learning with GANs: Revisiting Manifold Regularization. ICLR 2018 Workshop\n[6] Guo-Jun Qi et al, Global versus Localized Generative Adversarial Nets. CVPR 2018\n[7] T. Miyato et al., Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning, PAMI 2018\n[8] Z. Dai etal ., Good Semi-supervised Learning That Requires a Bad GAN, NIPS 2017\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}