{"title": "elegant construction; interesting phenomenon", "review": "\nThe authors provide a clean and easily understood sufficient\ncondition for spurious local minima to exist in networks with\na hidden layer using ReLUs or leaky ReLUs.  This condition,\nthat there is not linear transformation with zero loss,\nis satisfied for almost all inputs with more examples than\ninput variables.\n\nThe construction is elegant.  The mathematical writing in the paper,\nespecially describing the proof of Theorem 1, is very nice -- they\nexpose the main ideas effectively.\n\nI do not know of another paper using a similar proof, but I have not\nstudied the proofs of the most closely related papers prior to doing\nthis review, so I have limited ability to vouch for this paper's\ntechnical novelty.\n\nThe authors also show that networks using many other popular\nactivation functions have spurious local minima for a very\nsimple dataset.  All of these analysis are unified using a\nsimple, if technical, set of conditions on activation function.\n\nFinally, the authors prove a somewhat technical theorem about\noptima in deep linear networks, which generalizes some\nearlier treatments of this topic, providing an checkable\ncondition for global minimality.\n\nThere is extensive discussion of related work.  I am not aware of\nrelated work not covered by the authors.\n\nIn some cases, when the authors discuss previous work, they write as\nif restriction to the realizable case is an assumption, when it seems\nto me to be more of a constraint.  In other words, it seems harder to\nprove the existence of spurious minima in the realizable case.\nThey seem to acknowledge this after their statement of their Theorem 2,\nwhich also uses a realizable dataset.\n\nAlso, a few papers, including the Venturi, et al paper cited by\nthe authors, have analyzed whether spurious local minima exist\nin subsets of the parameter space, including those likely to\nbe reached during training with different sorts of initializations.\nIn light of this work, the authors might want to tone down claims\nabout how their work shows that results about linear networks do\nnot generalize to the non-linear case.  In particular, to make\ntheir construction work in the case of wide networks, they\nneed an overwhelming majority of the hidden units to be \"dead\",\nwhich seems as it is unlikely to arise from training with\ncommonly used initializations.\n\nOverall, I think that this paper makes an interesting and\nnon-obvious contribution on a hot topic.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}