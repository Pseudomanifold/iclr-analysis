{"title": "Review of \"Small nonlinearities in activation functions create bad local minima in neural networks\"", "review": "Paper represents theoretical analysis of the loss surface of neural networks. The authors supplied interesting results about local minima properties of neural networks. The paper is written quite well and easy to follow. Furthermore, authors made a comprehensive literature survey and connected their paper to already existing literature. The proofs seem correct (please note that paper is quite long and it requires more time then conference review period, hence, I stated \u201cseem correct\u201d). \nAlthough, paper provides novel theorems, I have several concerns, and these are:\n\u2022\tIsn\u2019t it clear that (generally speaking) a non-convex problem will have many local minima? Previous paper in neural network community is (in my opinion) not theorems. I believe one should read those statements such that in practice (please note here practice means that architectures, e.g. resnet50, inceptionv3, used in day to day life) researchers don\u2019t not observe those \u201creally bad\u201d local minima.\n\u2022\tIn ML literature, we have convex machines which are guaranteed to converge to global optimum. Given image dataset, or text datasets supervised deep learning in in general much better than convex methods e.g. SVMs. The paper clearly shows that there exist, in some cases, exponentially many local minima however  current training methods are able to find better solutions than convex methods (I am completely aware that functions classes are different however success metric, accuracy, is the same). Hence, how relevant are the results without taking in to account architectural choices or optimization methods for deep learning? May be structural risk minimisation is a better approach than empirical risk minimization for quantifying the performance of deep neural networks,\nIn conclusion, paper is interesting however I believe it need to be improved. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}