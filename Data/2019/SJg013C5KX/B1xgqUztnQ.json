{"title": "General framework, insufficient comparison with prior work", "review": "Summarization: This paper studies how to inject structured prior knowledge into the teaching model for machine learning. The authors propose a very general framework called `teach to teach\u2019, in which 1) the knowledge is distilled via subset selection that matches the teacher decision distribution 2) the distilled knowledge is then transferred into the teacher model by reweighting the contributions of teacher objective and coherence constraints. Extensive experiments are conducted on image classification, unsupervised domain adaptation and sequence learning.\n \nQuestions: 1) Can the teacher models, like those in L2T, be successfully transferred? For example, the teacher model trained with task 1 (with prior knowledge 1) successfully applied to task 2?\n2) I\u2019m not that clear about the relationship with knowledge distillation (Hinton et. al 2015). Per my understanding, the authors seem to make the distribution of the knowledge (specified by the set prior function F) coherent with teacher model and let the both influence each other (in section 3.2). In that sense I do not know what is the `dark\u2019 knowledge here.\n \nPros: In general I think this paper is a decent work that the structural prior knowledge is elegantly combined with teaching strategy (a.k.a. the teacher models in curriculum learning). The proposed method is intuitive and natural. The empirical verifications are deep and comprehensive to demonstrate the effectiveness of the `teaching to teach\u2019 framework.\n \nCons: 1) I think the authors should compare with self-paced learning with diversity (SPLD) since you also take diversity as a form of structural knowledge.\n2) The writing needs to be significantly polished. First, please simply the writing both in terms of general logic and language. I spent quite a few efforts in figuring out the meaning of some notations and complicated terms such as `curriculum-routed\u2019 and `g_i\u2019. Furthermore, I see no reason of putting so much fancy decorations on an essentially iterative algorithm (the bottom part of page 5 and all page 6). Second,  I suggest the authors give more intuitive and concrete examples towards what is the structural prior knowledge at the earlier phase of the paper, rather than putting most of them into appendix. Last but not least, please use more clear citation formats: currently quite a few citations are missing of publishing venues such as Fan et.al 2018 and Furlanello et.al 2018.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}