{"title": "Compelling approach to an important problem in FDR-controlled feature selection, with only minor weaknesses.", "review": "This manuscript describes an extension of the knockoff framework, which is designed to carry out feature selection while controlling the FDR among selected features, to settings in which the generative distribution of the features is not Gaussian. Specifically, the authors employ a GAN (with several modifications and additions) in which the generator produces knockoffs and the discriminator attempts to identify which features have been swapped between the original and the knockoff.\n\nThe method works as follows: (1) A conditional generator takes random noise and the real features as input, and outputs knockoff features. (2) A modified discriminator is used in such a way that the generator learns to generate knockoffs satisfying the necessary swap condition, so as to control the FDR of the knockoff procedure. (3) A power network uses Mutual Information Neural Estimation (MINE) to estimate the mutual information between each feature and its knockoff counterpart, so as to maximize the power of the knockoff procedure.\n\nResults are provided on synthetic and real data. As for the synthetic data, when the underlying feature distribution is Gaussian, the proposed method, KnockoffGAN, performs almost as well as the original knockoff and outperforms the BHq method; when the underlying feature distribution is non-Gaussian, KnockoffGAN dominates both the original knockoff and BHq methods. As for the real data, the authors claim to identify nine relevant features for cardiovascular disease and eight relevant features for diabetes, whereas the original knockoff procedure identifies zero features from the same data.\n\nGeneral comments: \n\nThis is an extremely impressive piece of work. The manuscript itself is a pleasure to read, and the results clearly demonstrate that the proposed KnockoffGAN both controls FDR and achieves power comparable to the original knockoff procedure in the Gaussian setting and much better than the original knockoff when the underlying distribution is not Gaussian.\n\nStrengths: \n\nThe combination of GANs and knockoff filter is a very promising and intriguing idea.\n\nThe use of the modified discriminator to ensure that the generated knockoffs satisfy the necessary swap condition is novel and intuitively sound.\n\nThe use of MINE to maximize power by maximizing the mutual information between each feature and its knockoff counterpart is also interesting.\n\nThe paper is well written, reads smoothly and the ideas are well exposed.\n\nThe illustrative figure is straightforward.\n\nWeaknesses:\n\nIntuitively, the modified discriminator and the power network should conflict with each other. I expect it was tricky to achieve a good tradeoff between two, but the authors failed to elaborate on these details.\n\nThe authors do not provide the design details of the neural networks. How dependent on the specific parametrization of the network architecture is the performance? How does the training order of four networks matter to the performance? \n\nThe manuscript should cite [[ Jaime Roquero Gimenez, Amirata Ghorbani, and James Zou. \"Knockoffs   for the mass: new feature importance statistics with false discovery   guarantees.\" arXiv:1807.06214, 2018. ]] which proposes a way to generate knockoffs for a Gaussian mixture model, and this method should be included in the relevant supplementary figure.\n\nIn Section 5.1.4, I would like to know, for a fixed data set, how the regularization affects the final values of the other loss terms.\n\nThe analysis of real data in Section 5.2 is unsatisfying in several respects.\n\nFirst, there is an unfortunate oversight in Table 1: the text refers to three features that are \"trivial,\" but only one of these is marked with an asterisk.  This leaves open the question of whether there are other trivial features beyond the three mentioned in the text. In addition, it is not clear exactly what it means for a feature to be \"trivial\" in this context.\n\nThis point gets to a deeper problem with the evaluation, which is that we are told, with no evidence, that these features are supported by literature in PubMed.  I would like to see two things here.  First, it seems obvious to me that if you are going to say that there is support in PubMed, you are obliged to actually report the citations that supposedly give this support. This could be done in the appendix. Equally importantly, there is a potential here for ascertainment bias which should be combatted in some fashion.  Presumably, some human expert had to do the PubMed searches to make this assessment.  I would like to know how \"permissive\" this assessor is.  To assess this, one could give the assessor a collection of terms, some of which were selected by KnockoffGAN and some at random, and then report the results. Obviously, some features that are significant may not be in the list of selected terms (because KnockoffGAN does not achieve 100% power) and so may appear as false negatives. But without some assessment like this, I have trouble believing this assessment.\n\nA related point is that it seems quite unfortunate that the authors chose a data set that cannot be described at all due to the anonymity constraint.  At the very least, it seems that we should be told the dimensionality of the data set. The Knockoff literature contains real data sets that could have been used here.\n\nMinor comments:\n\nOn the first page, the sentence beginning \"On the other hand,\" should clarify that this is only in expectation.\n\np. 3: Missing right paren after [7].\n\np. 5: Write out \u201cGaussian process.\u201d\n\np. 5: \"as little\" -> \"as little as possible\"\n\np. 6: \"to show that in\" -> \"to show, in\"\n\nIn Figures 2-5, add a horizontal line at 10% FDR for reference.\n\np. 10: \"features ones\" -> \"features\"\n\nNote to program committee:\n\nI did not review the technical details of the proof in the appendix.\n\n\n", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}