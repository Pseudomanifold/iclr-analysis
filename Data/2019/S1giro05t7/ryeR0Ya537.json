{"title": "Review", "review": "The paper proposes two ideas for reducing overconfident wrong predictions:\n- Method 1: \u201cG-distillation\u201d of an ensemble with extra unsupervised data\n- Method 2: Novelty Confidence reduction (NCR) using novelty detector\n\nThe paper is well-written and was a pleasure to read. In particular, I really enjoyed reading the introduction and related work. My main concern is that some of the contributions claimed were already shown in previous work (see method 1 below for details), and the novelty feels a bit limited. That said, I like the simplicity of the method and think that the extensive experiments on a variety of datasets and architectures is useful to the community.\n\nMethod 1:\n- The paper claims \u201cDraw attention to a counter-intuitive yet important problem of highly confident wrong predictions when samples are drawn from a unknown distribution that is different than training\u201d as one of the contributions. Note that previous work has already shown that single models are overconfident on unknown classes and ensembles are less overconfident, e.g. see Section 3.5 of the paper: \nSimple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\nhttps://arxiv.org/pdf/1612.01474.pdf\n- If I understand correctly, the key difference is that the proposed method 1 also uses ensemble prediction on unlabeled data for distillation, which could make the distilled model more robust. Ensembling on unlabeled data for robustness does seem novel to me, however, the text needs to be updated to clarify the novelty.\n\n\nMethod 2:\n- By off-the-shelf, do you mean a pre-trained network released by ODIN? Or did you train ODIN-based novelty detector on your dataset? \n- There was a recent paper that proposed to reduce confidence on novel inputs, which might be worth discussing:\nReliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors\nhttps://arxiv.org/pdf/1807.09289.pdf\n\n\nMinor issues:\n- Figures 3,4 are a bit small and hard to see\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}