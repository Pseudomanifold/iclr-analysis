{"title": "Well-presented but some shortcomings in experiments", "review": "Overview:\n\nThis paper proposes an approach to document classification in a low-resource language using transfer learning from a related higher-resource language. For the case where limited resources are available in the target low-resource language (e.g. a dictionary, pretrained embeddings, parallel text), multi-task learning is incorporated into the model. The approach is evaluated in terms of document classification performance using several combinations of source and target language.\n\nMain strengths:\n\n1. The paper is well written. The model description in Section 2 is very clear and precise.\n2. The proposed approach is simple but still shows good performance compared to models trained on corpora and dictionaries in the target language.\n3. A large number of empirical experiments are performed to analyse different aspects and the benefits of different target-language resources for multi-task learning.\n\nMain weaknesses:\n\n1. The application of this model to document classification seems to be new (I am not a direct expert in document classification), but the model itself and the components are not (sequence models, transfer learning and multitask learning are well-established). So this raises a concern about novelty (although the experimental results are new).\n\n2. With regards to the experiments, it is stated repeatedly that the DAN model which are compared to uses \"far more resources.\" The best ALL-CACO model also relies on several annotated but \"smaller\" resources (dictionaries, parallel text, embeddings). Would it be possible to have a baseline where a target-language model is trained on only a small amount of annotated in-domain document classification data in the target language? I am proposing this baseline in order to answer two questions. (i) Given a small amount of in-domain data for the task at hand, how much benefit do we get from additionally using data from a related language? (ii) How much benefit do we get from using target-language resources that do not address the task directly (dictionaries, embeddings) compared with using a \"similar\" amount of data from the specific task?\n\nOverall feedback:\n\nThis is a well-written paper, but I think since the core of the paper lies in its empirical evaluation, the above experiments (or something similar) would greatly strengthen the work.\n\nEdit: I am changing my rating from 5 to 6 based on the authors' response.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}