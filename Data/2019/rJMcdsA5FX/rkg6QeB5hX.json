{"title": "this paper seems to be timely for this line of work ", "review": "===========================\nSince the authors did not provide a proper response to my questions, I have lowered my score from 7 to 6. I think this paper will have a good chance to be a good paper if evaluated more comprehensively, as suggested by reviewers. \n===========================\n\nContributions:\n\nThe main contribution of this paper is the study of the currently adopted evaluation metrics for textual GAN models. It was shown that BLEU and Self-BLEU scores used by previous work are insufficient to evaluate textual GAN models, and the authors propose that Frechet Distance and reverse Language Model scores can be a good complement to the above BLEU score evaluations. \n\nDetailed Comments:\n\n(1) Novelty: It seems to me that this paper is timely, as developing GAN models for text generation gains more and more attention in the research community, and it is indeed much needed to provide good evaluation methods. The proposed new metrics seem proper, and the observation that most GAN models do not yield obviously better results than conventional LM is also insightful. \n\n(2) Presentation: This paper is generally well-written and easy to follow. However, when discussing related work in section 3.1, I think one literature [*] is missed. It uses annealed softmax to approximate argmax for textual GAN. \n\n[*] Adversarial Feature Matching for Text Generation, ICML 2017\n\n(3) Evaluation: The experiments are generally well-executed, with some questions listed below.\n\nQuestions:\n\n(1) I have some concerns in terms of human evaluation. Though human evaluation is the golden metric, it seems that presenting individual sentences to human raters does not account diversity into consideration. Therefore, systems that generate high quality samples but with less diversity will get a high score in terms of human evalution. Can the authors provide some discussion on this? And if this is the case, how will this change the conclusions in this paper?\n\n(2) I understand why the authors use simplified GAN models for evaluation. However, if the models are not simplified, what the performance will be for LeakGAN and MaskGAN, for example? This seems to be relatively easy to evaluate since the code is open sourced. \n\nMinor issues:\n\n(1) I think the citation format needs to be changed. For example, in many places, it is more natural to use \"(Hassan et al., 2018)\" than \"Hassan et al. (2018)\" for example. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}