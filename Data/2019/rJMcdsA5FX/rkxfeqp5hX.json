{"title": "Valuable goal, but very limited executions with incorrect claims", "review": "The paper sets out to improve the evaluation of GAN models as e.g. the previously used BLEU score is not sensitive to semantic deterioration of generated texts. The paper claims to \u201cpropose alternative metrics that better\ncapture the quality and diversity of the generated samples\u201d.\n\n\nStrengths:\n-\tThe paper has a valuable goal\n-\tSome of the evaluations are interesting.\n\n\nWeaknesses:\n1.\tThe claim of the paper to \u201cpropose alternative metrics that better capture the quality and diversity of the generated samples\u201d is not met in multiple ways:\na.\tThe paper seems not to propose any new metrics but evaluate existing ones.\nb.\tThe metrics are not extensively compared to human judgments, e.g. by computing correlation. In fact, Figure 5 suggests that they are not very well correlated.\nc.\tThe diversity is not explicitly studied on generated text samples.\n2.\tThe paper concludes that the human eval \u201cassigns better scores to the Language Model\u201d, which is incorrect as Seq gan scores 3.49 vs. 3.37 for language model (even if the seq gan has higher variance).\n3.\tThe metrics are not very well defined, e.g. with formulas, although this is one of the central points of the paper. e.g. what are the reference the blue score is computed against?\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}