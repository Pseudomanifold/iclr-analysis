{"title": "Poor presentation and weak technical contents.", "review": "Summary: \nThis paper presents a non-linear channel-aggregation layer (NCAL) for deep neural networks.\nTo globally aggregate channel features, NCAL first projects the feature map along the channel dimension into the new feature space of the same channel dimensionality by means of trigonometric functions like discrete cosine transform (DCT).\nSo-transformed feature map is multiplied with the original one in a point-wise manner (Hadamard Product), and then passed to the next layer in the neural network.\nThe experimental results on action classification using UCF101 and HMDB51 show that the NCAL slightly boosts the performance.\n\n\nComments:\nThe reviewer regards this paper falls below the border due to the following negatives.\n\n1. Poor presentation\n- This paper is written in poor English, containing considerable amount of grammatical errors. The reviewer strongly recommends the authors to proof-read the manuscript before submission.\n\n- As to mathematical presentation, the notations are conflicted and/or confusing; especially, in Eq.(1), the notations of i and j are conflicted and thus it is difficult to understand its mathematical meaning at first glance, though it actually shows a simple \"3D\" conv. And, some functions that compose the method, such as G and H, are not clearly defined throughout the paper.\n\n- The term of \"hand-crafted rule\" is found several times in this paper, but it is unclear since the authors do not give any definition nor examples for the term. What does it actually mean?\n\n- It is trivial to show Fig.5 which merely explains trigonometric functions.\n\n2. Technical content\n- First of all, the reviewer guesses that the authors might misunderstand the \"3D\" operations, such as 3D-conv and 3D-pool. In the literature of action recognition, the 3D operations generally work in the spatio-temporal domain (X-Y-T = \"3\"D); thus, the 3D pooling aggregates features across X-Y-T space and the 3D conv filters are usually applied to the whole feature channels, except for grouped conv. This misunderstanding would be a critical flaw of this work, leading to the incorrect comparison described below.\n\n- It is unclear why the global channel aggregation (3) is necessary; the theoretical reason/motivation is not found in this manuscript. In ConvNet, feature channels encode different characteristics and thus it makes less sense to aggregate those heterogeneous information; note that the 3D-pooling aggregates them along X-Y-T, NOT along channel. And, why do the authors employ trigonometric functions in (3)? Such a transformation is closely related to DCT that maps a feature \"sequence\" into \"frequency\" domain. From this perspective, the reviewer has two concerns about the forms (3,4).\n First, extracting frequency characteristics along channel dimension via (3) would be less effective. The frequency information is valuable only for the signals that are sequenced in structured order such as on temporal and/or spatial domain. The CNN feature channels, however, do not exhibit such structural order since we can arbitrarily swap the feature channels by manipulating the filter weights.\n Second, it is not clearly discussed what kind of physical characteristics are extracted through the multiplication of the original feature and the frequency feature in (4). The reviewer does not understand how useful so-combined features are for classification.\n As to the second issue, the multiplication (Hadamard Product) can also be seen in the literature of bilinear model [a]. In that framework, the multiplication form is naturally derived from the low-rank approximation of bilinear weights through introducing trainable weights in contrast to this work that employs the fixed weights (W^1 and W^2).\n\n- The temporal information of the feature vector sequence {V_k} is not considered in this work, though the objective of this paper is \"action\" recognition as the title says. The channel aggregation along the temporal axis is discussed with thorough experiments in [b].\n\n[a] Jin-Hwa Kim, Kyoung Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. \"Hadamard Product for Low-rank Bilinear Pooling\". In ICLR2017.\n[b] Carreira, J., & Zisserman, A. \"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\". In CVPR2017.\n\n3. Performance\n- The performance improvement is quite limited, compared to the standard CNN. To convince the readers of the improvement by the method, the authors have to show the statistical significance such as by reporting the standard deviation of test accuracies over three splits in those datasets. And, as mentioned above, the method of \"3D convolution\" would be wrongly implemented.\n\n- Although the authors state many times that the method makes convergence in training faster, the charts in Fig.3~4 do not support the authors' claim; from those charts, all the methods are converged around 300 iterations. Note that the convergence point can be found when the training loss does not decrease any more, and the training loss value itself is not a proper measure for comparing the performance of the models; we can easily reduce training loss by employing the more complex models, though they poorly work on test samples due to overfitting.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}