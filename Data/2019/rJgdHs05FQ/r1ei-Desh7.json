{"title": "Nothing particularly new", "review": "First the paper is poorly written. The abstract contains the glaring typo \"optical convergence\", presumably meant to be \"optimal convergence\" and the last two paragraphs of page 2 are near identical repeats. Besides these egregious errors that could easily be fixed with better proofreading, the text and mathematics are difficult to follow and not precise. For example, in Equation (1), i and j appear both as subscripts and feature map indices.\n\nSecond, the approach is not particularly well motivated nor analysed. Why for example use cosine and sine functions? Is this some connection to Fourier domain analysis? Moreover, since neural networks can approximate other functions, is there an equivalent (possibly slower and deeper) multi-layer perceptron equivalent to the proposed aggregation functions?\n\nLast, the experimental results are not very compelling with state-of-the-art on HMBD51 now around 72% and UCF101 now around 95% (Feichtenhofer et al., 2017). The faster convergence time (demonstrated as more rapid decrease in loss as a function of training iteration) is also underwhelming.\n\nAll up more work needs to be done in the paper for it to make a valuable contribution.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}