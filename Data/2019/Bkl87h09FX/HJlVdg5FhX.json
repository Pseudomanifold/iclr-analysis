{"title": "Valuable systematic study of pre-training tasks' influence on downstream task performance", "review": "This paper presents a thorough and systematic study of the effect of pre-training over various NLP tasks on the GLUE multi-task learning evaluation suite, including an examination of the effect of language model-based pre-training using ELMo. The main conclusion is that both single-task and LM-based pre-training helps in most situations, but the gain is often not large, and not consistent across all GLUE tasks.\n\nThis paper represents an impressive amount of experimentation. The study and the experimental results will be useful and interesting to the community. The result that some tasks' performance are negatively correlated with each other is surprising. The paper is clearly written. \n\nOne clarification question I have is about what the \"Single-task\" pre-training means. The paper seems to suggest that it consists of pre-training a model on the same task on which it is later evaluated. I'm confused by what this means, and how this is different from just training on that task. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}