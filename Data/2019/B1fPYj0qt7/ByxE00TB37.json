{"title": "A paper on Riemannian optimization, needs to fix some math and improve experiments", "review": "This paper proposes an algorithm for optimizing neural networks parametrized by Tensor Train (TT) decomposition based on the Riemannian optimization and rank adaptation, and designs a bidirectional TT LSTM architecture.\n\nI like the topic chosen by the authors, using TT to parametrize layers of neural networks proved to be beneficial and it would be very nice to exploit the Riemannian manifold structure to speed up the optimization.\n\nBut, the paper needs to be improved in several aspects before being useful to the community. In particular, I found the several mathematical errors regarding basic definitions and algorithms (see below the list of problems) and I\u2019m not happy with lack of baselines in the experimental comparison (again, see below).\n\nThe math problems\n1) In equations (1), (2), (7), and (8) there is an error: one should sum out the rank dimensions instead of fixing them to the numbers r_i. At the moment, the left-hand side of the equations doesn\u2019t depend on r and the right-hand side does.\n2) In two places the manifold of d-dimensional low-rank tensors is called d-dimensional manifold which is not correct. The tensors are d-dimensional, but the dimensionality of the manifold is on the order of magnitude of the number of elements in the cores (slightly smaller actually).\n3) The set of tensors with rank less than or equal to a fixed rank (or a vector of ranks) doesn\u2019t form a Riemannian (or smooth for that matter) manifold. The set of tensors of rank equal to a fixed rank something does.\n4) The function f() minimized in (5) is not defined (it should be!), but if it doesn\u2019t have any rank regularizer, then there is no reason for the solution of (5) to have rank smaller then r (and thus I don\u2019t get how the automatic rank reduction can be done).\n5) When presenting a new retraction algorithm, it would be nice to prove that it is indeed a retraction. In this case, Algorithm 2 is almost certainly not a retraction, I don\u2019t even see how can it reduce the ranks (it has step 6 that is supposed to do it, but what does it mean to reshape a tensor from one shape to a shape with fewer elements?).\n6) I don\u2019t get step 11 of Alg 1, but it seems that it also requires reshaping a tensor (core) to a shape with fewer elements.\n7) The rounding algorithm (Alg 3) is not correct, it has to include orthogonalization (see Oseledets 2011, Alg 2).\n8) Also, I don\u2019t get what is r_max in the final optimization algorithm (is it set by hand?) and how the presented rounding algorithm can reduce the rank to be lower than r_max (because if it cannot, one would get the usual behavior of setting a single value of rank_max and no rank adaptivity).\n9) Finally, I don\u2019t get the proposition 1 nor it\u2019s proof: how can it be that rounding to a fixed r_max won\u2019t change the value of the objective function? What if I set r_max = 1? We should be explained in much greater detail.\n10) I didn\u2019t get this line: \u201cFrom the RSGD algorithm (Algorithm 1), it is not hard to find the sub-gradient gx = \u2207f(x) and Exp\u22121 x (y) = \u2212\u03b7\u2207xf(x), and thus Theorem 3 can be derived.\u201d What do you mean that it is not hard to find the subgradient (and what does it equal to?) and why is the inverse of the exponential map is negative gradient?\n11) In general, it would be beneficial to explain how do you compute the projected gradient, especially in the advanced case. And what is the complexity of this projection?\n12) How do you combine optimizing over several TT objects (like in the advanced RNN case) and plain tensors (biases)? Do you apply Riemannian updates independently to every TT objects and SGD updates to the non-TT objects? Something else?\n13) What is E in Theorem 3? Expected value w.r.t. something? Since I don\u2019t understand the statement, I was not able to check the proof.\n\nThe experimental problems:\n1) There is no baselines, only the vanilla RNN optimized with SGD and TT RNN optimized with your methods. There should be optimization baseline, i.e. optimizing the same TT model with other techniques like Adam, and compression baselines, showing that the proposed bidirectional TT LSTM is better than some other compact architectures. Also, the non-tensor model should be optimized with something better than plain SGD (e.g. Adam).\n2) The convergence plots are shown only in iteration (not in wall clock time) and it\u2019s not-obvious how much overhead the Riemannian machinery impose.\n3) In general, one can decompose your contributions into two things: an optimization algorithm and the bidirectional TT LSTM. The optimization algorithm in turn consist in two parts: Riemannian optimization and rank adaptation. There should be ablation studies showing how much of the benefits come from using Riemannian optimization, and how much from using the rank adaptation after each iteration.\n\nAnd finally some typos / minor concerns:\n1) The sentence describing the other tensor decomposition is a bit misleading, for example CANDECOMP can also be scaled to arbitrary high dimensions (but as a downside, it doesn\u2019t allow for Riemannian optimization and can be harder to work with numerically).\n2) It\u2019s very hard to read the Riemannian section of the paper without good knowledge of the subject, for example concepts of tangent space, retraction, and exponential mapping are not introduced.\n3) In Def 2 \u201cdifferent function\u201d should probably be \u201cdifferentiable function\u201d.\n4) How is W_c represented in eq (25), as TT or not? It doesn\u2019t follow the notation of the rest of the paper. How is a_t used?\n5) What is \u201cscore\u201d in eq (27)?\n6) Do you include bias parameters into the total number of parameters in figures?\n7) The notation for tensors and matrices are confusingly similar (bold capital letters of slightly different font).\n8) There is no Related Work section, and it would be nice to discuss the differences between this work and some relevant ones, e.g. how is the proposed advanced TT RNN different from the TT LSTMs proposed in Yang et al. 2017 (is it only the bidirectional part that is different?) and how is the Riemannian optimization part different from Novikov et al. 2017 (Exponential machines), and what are the pros and cons of your optimization method compared to the method proposed in Imaizumi et al. 2017 (On Tensor Train Rank Minimization: Statistical Efficiency and Scalable Algorithm).\n\n\nPlease, do take this as a constructive criticism, I would be happy to see you resubmitting the paper after fixing the raised concerns!\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}