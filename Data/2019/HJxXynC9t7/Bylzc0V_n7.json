{"title": "Interesting results, but \"insights\" seem misleading", "review": "The authors propose the notion of \"expressiveness\" of state representation by simply checking the effective rank of the state vectors formed by a sampled trajectory. The authors then propose a regularizer that promotes this expressiveness. Experiments on a large set of Atari games show that this regularizer can improve the performance of reinforcement learning algorithms.\n\n1. The authors motivate this notion through the example in section 2 where the model with higher capacity performs better. The authors note that the learned state matrix of the higher-capacity model has higher rank. But this is completely expected and well-known in machine learning in the regime where the models have insufficient capacity. Imagine the case where the true state representation consists of only a two-dimensional vector (say, the location vector), and an image is produced through a linear map. Now suppose we try to learn a K-dimensional state representation for K>>2 and promote a high-rank matrix. What do we expect to get from this?\n\n2. It seems that promoting a small gap between the singular values gives a performance improvement in the case of Atari games. One can easily interpret this as a regularizer that simply prevents overfitting by equalizing the magnitudes in most parameter values. In this sense, I do not see a fundamental difference between this regularizer and say, the L2 norm regularizer. Did the authors try comparing this with an L2 norm regularizer?\n\n\nThe authors present very interesting empirical results, but I am not convinced that the proposed notion of \"expressiveness\" properly explains the performance improvement in this set of tasks. I am also not convinced that this is the right notion to promote in general. In this sense, I am afraid I cannot recommend acceptance.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}