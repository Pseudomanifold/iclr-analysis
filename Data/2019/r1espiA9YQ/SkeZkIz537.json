{"title": "Review", "review": "This paper proposes a particle-based inference algorithm, the optimal update for each particle is the summation of the standard SGLD direction and SVGD velocity.  The work further analyzes non-asymptotic properties of SPOS. The results appear theoretically interesting and of potential practical value in designing inference algorithms. I did not go through the proofs in the supplementary. \n\n[Experimental results are not convincing] \n\n[BNN] I noticed the test RMSE and test LL of SVGD are directly copied from the original SVGD paper. However, the performance critically depends on:\n1.    Running time, or training epochs\n2.    Data partitions\nTo be a fair comparison, the authors should keep at least the training epochs and random partitions the same. Especially for the dataset Year, for which only one random partition is conducted. It\u2019s highly likely that the performance gain is due to favored data partition rather than the superiority of the algorithm.\n\n[RL] Average rewards are significantly lower than the scores reported in the original SVPG paper?\n1.    From figure 3, SPOS only outperforms SVPG on envs Cartpole Swing Up and Double Pendulum. The best reward for env Cartpole Swing Up reported in this paper is around 200. However, the score is ~400 in the original SVPG paper. For the env Double Pendulum, there\u2019s also very large performance gap. I am aware the code for SVPG is now publicly available, the authors may consider conducting the experiments with the same settings (e.g. same seed?). Otherwise, it\u2019s hard to tell whether the performance gain is significant while the baseline is much worse than it should be.\n2.    Only 3 envs are reported, the authors may also consider reporting all the envs are used in the SVPG paper\n\n[Figure 1] The authors may consider reporting the exact settings of this case, otherwise, it\u2019s hard to believe that SVGD would collapse on a simple 1D case.\n\nIf the authors can fully address the concerns above, I will consider changing the scores.  \n\nOther comments:\n\n-    Related papers:\n     Stein Variational Message Passing for Continuous Graphical Models,  Wang et al., ICML18 (https://arxiv.org/abs/1711.07168)\n     Stein Variational Gradient Descent as Moment Matching, Liu et al., NIPS18 (https://arxiv.org/abs/1810.11693)\n\n-    Page 30 crashes my browser all the time\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}