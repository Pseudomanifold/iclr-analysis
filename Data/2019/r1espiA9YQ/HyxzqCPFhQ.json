{"title": "I have multiple concerns regarding proof of Theorem 3", "review": "This paper considers the problem of Bayesian inference using particle optimization sampler. Similarly to SGLD, authors propose Stochastic Particle Optimization Sampler (SPOS), augmenting Stein Variational Gradient Descent (SVGD) with diminishing Gaussian noise, replacing the hard-to-compute term of the Chen et al. (2018) formulation. Various theoretical results are given.\n\nThis paper was a pleasant read until I decided to check the proof of Theorem 3. I was not able to understand transitions in some of the steps and certain statements in the proof seem wrong.\n\nTheorem 3:\n\"Note that $\\theta^i_t$ and $\\hat \\theta^i_t$ are initialized with the same initial distribution \u00b50 = \u03bd0 and we can also set $\\theta^i_0$ to be independent of $\\hat \\theta^i_0$, we can have $\\gamma(0) = 0$. $\\gamma(0) = E \\|\\theta^i_0 - \\hat \\theta^i_0 \\|^2$.\" - this doesn't seem right to me. Expectation of squared difference of two independent and identically distributed random variables is not 0, assuming expectation is with respect to their joint density.\n\n\"Then according to the Gronwall Lemma, we have\" - I don't see how the resulting inequality was obtained. When I tried applying Gronwall Lemma, it seems that authors forgot to multiply by $t$ and  $\\lambda_1$. Could you please elaborate how exactly Gronwall Lemma was used in this case.\n\n\"... some positive constants c1 and c2 independent of (M, d)$ - in the proof authors introduce additional assumption \"We can tune the bandwidth of the RBF kernel to make \u2207K \u2264 H_\u2207K, which is omitted in the Assumption due to the space limit.\" First, there is a missing norm, since \u2207K is a vector and H_\u2207K is I believe a scalar constant. Second, c1 = H_\u2207K + H_F, which both bound norm of d-dimensional vector and hence depend on d. I also suggest that all assumptions are included in the theorem statements, especially since authors have another assumption requiring large bandwidth. Additionally, feasibility of these both assumptions being satisfied should be explored (it seems to me that they can hold together, but it doesn't mean that part of assumptions can be moved to the supplement).\n\nI find using Wasserstein-1 metric misleading in the theorem statement . This is not what authors really bound - from the proof it can be seen that they bound W_1 with W_2 and then with just an expectation of l2 norm. Moreover I don't understand the meaning of this bound. Theorem is concerned with W_1 distance between two atomic measures. What is the expectation over? Note that atom locations are supposed to be fixed for the W_1 to make sense in this context (and the expectation is over the coupling of discrete measures defined by weights of the atoms, not atom locations).\n\n\"Note the first bullet indicates U to be a convex function and W to be ... \" I think it should be K, not W.\n\nTheorems 3-6 could be lemmas, while there should be a unifying theorem for the bound.\n\nFinally, I think notation should be changed - same letter is used for Wasserstein distance and Wiener process.\n\nOther comments:\n\nExample in Figure 1 is somewhat contrived - clearly gradient based particle sampler will never escape the mode since all modes are disconnected by regions with 0 density. Proposed method on the other hand will eventually jump out due to noise, but it doesn't necessarily mean it produces better posterior estimate. Something more realistic like a mixture of Gaussians, with density bounded away from zero across domain space, will be more informative.\n\nIt is not sufficient to report RMSE and test log likelihood for BNNs. One of the key motivating points is posterior uncertainty estimation. Hence important metric, when comparing to other posterior inference techniques, is to show high uncertainty for out of distribution samples and low for training/test data.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}