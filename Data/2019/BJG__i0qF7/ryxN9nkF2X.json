{"title": "A new dataset for 3D understanding", "review": "The authors present a large synthetic dataset for 3D scenes with templated descriptions.  They then use the model of Eslami 2018 to this new domain.  The previous work appears to already introduce all the necessary mechanisms for 3D generalization from multiple viewpoints, though this work embeds language instead of a scene in the process.  Minor note: A bit more discussion on this distinction would be appreciated.  Also, it appears that the previous work includes many of the rendered scenes also present here, so the primary focus of this paper is on the use of a language encoder (not necessarily a trivial extension).\n\nThe model appears to perform well with synthetic data though very poorly with natural sentences.  This may be in part due to the very small dataset size.  It would be helpful to know how much of the performance gap is due to scaling issues (10M vs 5.6K) versus OOVs, new syntactic constructions, etc.  In particular, the results have ~two deltas of interest (NL vs SYN) and the gap in the upper bound from 0.66 to 0.91.  What do new models need to be able to handle to close these gaps?\n\nRegarding the upper bound, there is some discussion that annotators might have had a strict definition of a perfect match.  Were annotators asked about this? The current examples (B.2), as the authors note, are more indicative of failings with the synthetic language than the human annotators.  This may again be motivation for collecting more natural language which would resolve some of the ambiguity and pragmatics of the synthetic dataset.\n\nIt would also be helpful to have some ablations included in this work. The most obvious being the role of $n$ (number of scene perspectives).  How crucial is it that the model has access to 9 of 10 perspectives?  One would hope that given the limited set of objects and colors, the model would perform well with far fewer examples per scene, learning to generalize across examples.\n\nSince the primary contributions of the paper are a language dataset and a language encoder for the existing model of Eslami 2018, those should be discussed and ablated in the paper rather than relegated to the appendix.\n\nMinor note:  the related work mentions grounding graphs which are core to work from Tellex and Roy, but omits existing fully neural end-to-end models in grounding (e.g. referring expressions work).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}