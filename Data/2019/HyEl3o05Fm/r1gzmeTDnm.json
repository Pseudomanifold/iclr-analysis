{"title": "VAE-GAN model for video prediction", "review": "\nSummary:\nThe authors present a video prediction model called SAVP that combines a Variational Auto-Encoder (VAE) model with a Generative Adversarial Network (GAN) to produce more realistic and diverse future samples.\n\nDeterministic models and certain loss functions such as Mean Squared Error (MSE) will produce \nblurry results when making uncertain predictions. GAN predictions on the other hand usually are more visually appealing but often lack diversity, producing just a few modes. The authors propose to combine a VAE model with a GAN objective to combine their strengths: good quality samples (GAN) that cover multiple possible futures (VAE).\n\nStrengths:\n[+] GANs are notoriously unstable to train, especially for video. The authors formulate a VAE-GAN model and successfully implement it.\n\nWeaknesses:\n[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).\n\n[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts. For example, the experiment of Figure 5 does not show SAVP being significantly more diverse than GANs for KTH (as compared to VAEs). Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.\n\nWhile the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better. Since a direct application of video prediction is model-based planning, it seems that plausibility might be as important as sample quality. This work proposes to combine VAEs and GANs in a single model to get the benefits of both models. However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results. While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.\n\nIn order to better assess this model and compare it to its individual parts and other VAE models, could the authors:\n\n1) Compare SAVP to the SVG-LP/FP model on a controlled synthetic dataset such as Stochastic Moving MNIST (Denton & Fergus, 2018)?\n2) Comment on the plausibility of the samples generated by SAVP? Do some samples show imagined objects \u2013 implausible interactions for the robotic arm dataset? If so, what would be the advantage over blurry but plausible generations of a VAE?", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}