{"title": "the paper proposes an extension of VAE based video prediction models and produces an extensive evaluation. The model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.", "review": "The paper introduces a generative model for video prediction. The originality stems from a new training criterion which combines a VAE and a GAN criteria. At training time, the GAN and the VAE are trained simultaneously with a shared generator; at test time, prediction conditioned on initial frames is performed by sampling from a latent distribution and generating the next frames via an enhanced conv LST . Evaluations are performed on two movement video datasets classically used for benchmarking  this task - several quantitative evaluation criteria are considered.\n\nThe paper clearly states the objective and provides a nice general description of the method.  The proposed model extends previous work by adding an adversarial loss to a VAE video prediction model.  The evaluation compares different variants of this model to two recent VAE baselines. A special emphasis is put on the quantitative evaluation: several criteria are introduced for characterizing different properties of the models with a focus on diversity. w.r.t. the baselines, the model behaves well for the \u201crealistic\u201d and \u201cdiversity\u201d measures. The results are more mitigated for measures of accuracy. As for the qualitative evaluation, the model corrects the blurring effect of the reference SV2P baseline, and produces quite realistic predictions on these datasets. The difference with the other reference model (SVG) is less clear.\n\nWhile the general description of the model is clear, details are lacking. It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences. This would also help to explain the difference of performance/ behavior  w.r.t. these models (Fig. 5).\n\nIt seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.  Similarly, you did not indicate what the deterministic version of your model is.\nThe generator model with its warping component makes a strong hypothesis on the nature of the videos: it seems especially well suited for translations or for other simple geometric transformations characteristics of the benchmarking videos .  Could you comment on the importance of this component? Did you test the model on other types of videos where this hypothesis is less relevant? It seems that the baseline SVG makes use of simpler ConLSTM for example.\n\nThe description of the generator in the appendix is difficult to follow. I missed the point in the following sentence: \u201cFor each one-step prediction, the network has the freedom to choose to copy pixels from the previous frame, used transformed versions of the previous frame, or to synthesize pixels from scratch\u201d .\nAlso, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.\n\nOverall, the paper proposes an extension of VAE based video prediction models and produces an extensive evaluation. While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}