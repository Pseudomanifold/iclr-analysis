{"title": "Lack of novelty", "review": "Pros:\nThis paper targets an interesting and important task, i.e. general text filling, where the incomplete sentence can have arbitrary number of blanks and each blank can have arbitrary number of missing words.  Convincing experiments are conducted both qualitatively and quantitatively.\n\nCons:\n1. Lack of novelty in the proposed method. Specifically, such impression mainly comes from the writing in Sec 3.2. When discussing details of the proposed method, authors keep referring to [A], indicating heavily that authors are applying an existing method, i.e. yet another application of [A].  This limits the novelty. On the other hand, this also limits the readability for anyone that not familiar with [A]. Moreover, this prevents authors discuss the motivation behinds their architectural choices. Whether [A] is the optimal choice for this task, and can there be alternative options for its components. For example,  could we use a rnn to encode x_template_i and use the encoding as a condition to fill  s_i? \n\n[A] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017.\n\n2. Discussion in Sec 3.2 and Figure 2 are not clear enough. I didn't get a picture of how the proposed method interacts with requirements of the task. For example, in sec 3.2.3, what represent queries, keys and values respectively are unclear. And authors mention \"template-decoder attention\" layers, where  I didn't find in Figure 2.\n\n3. Is not very straightforward that how the baselines Seq2Seq and GAN are applied to this task, where necessary information is missed in the experiment section.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}