{"title": "Interesting to read but might lack depth", "review": "I read the paper and understand it, for the most part. The idea is to interpret some regularization technics as a from of noisy bottleneck, where the mutual information b tween learned parameters and the data is limited through the injection of noise. \n\nWhile, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.\n\nI'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156)\n\nPro: nicely written, clear interpretation of regularization as a noise injection technics, explicit link with information theoery and Shanon capacity.\n\nCon: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}