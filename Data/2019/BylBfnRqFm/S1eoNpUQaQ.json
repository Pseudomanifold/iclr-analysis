{"title": "incremental idea, weak experimental evidence", "review": "Summary\nCAML is a gradient-based meta-learning method closely related to MAML. It divides model parameters into disjoint sets of task-specific parameters $\\phi$ which are adapted to each task and task-independent parameters $\\theta$ with are meta-learned across tasks. $\\phi$ are then interpreted as an embedding and fed as input to the model (parameterized by $\\theta$). Experiments demonstrate that this approach performs on par with MAML while adapting far fewer parameters. An additional benefit is that this approach is less sensitive to the adaptation learning rate and is easier to implement and faster to compute.\n\nStrengths\nWhile not really explained in the paper, this work connects gradient-based to embedding-based meta-learning approaches. Adaptation is via gradient descent, but the adapted parameters are then re-interpreted as an embedding.\nThe method has the potential to perform on par with MAML while being simpler and faster.\nThe paper is well-written.\n\nWeaknesses\nThe field of meta-learning variants is crowded, and this paper struggles to carve out its novelty. \nRusu et al (LEO) optimize a context vector, which is used to generate model parameters. Reducing the generative model to a point estimate, how is this different from generating the FiLM parameters as a function of context as done in CAML? \nLee and Choi (MT-nets) propose a general formulation for learning which model parameters to adapt. CAML is simpler in that the model parameters to adapt are chosen beforehand to be inputs. \nSnell et al. / Oreshkin et al. are prototype-based methods infer context via a neural network rather than optimizing for it.\n\nIn this context, CAML appears to be yet another point drawn from the convex hull of choices already explored in episodic meta-learning (these choices can be broadly grouped into task encoding and conditional inference). The paper must then rest on its experimental results, which are at present unconvincing.\n\nOn the whole, the experimental results seem weak and analysis results largely uninformative. The method is benchmarked on the toy tasks of sinusoid regression and a 2-D point mass, as well as mini-ImageNet few-shot classification. The sinusoid and point mass navigation are toy and compared only to MAML, so it is hard to draw conclusions from those experiments. For mini-ImageNet, while CAML outperforms MAML, it seems that the pertinent comparison is with MT-NET (which CAML does not outperform) and LEO (missing fair comparison?).\n\nQuestions regarding experiments\n - CAML is robust to the adaptation learning rate, but isn\u2019t this true of any scheme that separates meta-learned and adapted parameters into disjoint sets? (e.g. also true of Lee and Choi?) \n - The visualizations of the context parameters are nice, but interpreting much higher dimensional context vectors (which would be necessary for harder tasks) is more difficult, so I\u2019m not sure what to take away from this? It\u2019s very unsurprising that the 2-D context vector encodes x and y position in the point mass experiment, for example. \n - I am confused by the comparison between adapting input parameters versus subsets of nodes at each layer or entire layers for the sinusoid regression task. Adapting subsets of nodes at each layer roughly corresponds to Lee and Choi, yet the reported numbers are quite different? \n - In Table 3, which CAML is a fair comparison (in terms of network size and architecture) to MT-NET? \n\nEditorial Notes\nIntro paragraph 3: fine-tuning image classification features for a semantic segmentation task is not a good example of task independent parameters, since fine-tuning end-to-end gives significant improvements.\nRelated work paragraph 2: Initializing context parameters to zero is not the only difference with Rei et al (2015), and seems a strange thing to highlight?\nTables 1 and 2: state what the task is in the caption\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}