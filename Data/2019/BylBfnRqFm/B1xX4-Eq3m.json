{"title": "Good paper in general", "review": "They are proposing a meta-learning method inspired by previous method, MAML. Their idea is separating the parameters in to two groups of context and shared parameters. The context parameters are learned through back-propagation of inner-loop and represents embedding for individual task. Shared-parameters on the other hand are shared between all tasks, and are learned in the outer-loop. \n\nCompared to MAML, the pros of their method is as follows:\n- Less sensitive to learning rate: thus more robust to hyper parameters.\n- Does not prone to overfit as MAML does.\n- It is easier to implement, more efficient from memory view point.\n\nCons in general,\n-  In Mini-ImgeNet data set, although they are beating MAML, but they are not able to beat other competitors in 5-shot classification.\n- They could have explored applying their method to deep residual networks and compare their results.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}