{"title": "Reasonable approach to an important problem, but missing baselines, novelty a bit low", "review": "The submission describes a way to differentiate through the solution of a minimum cost multicut problem, thus enabling a multicut solver to be used as a module in a neural network, with applications to segmentation tasks in computer vision.  The method starts by approximating the solution to the multicut problem with an unconstrained, penalized version.  This objective is then identified with the log-likelihood of a CRF, at which point the standard trick of differentiating through a small number of mean field updates is applied.  Experiments show modest gains on the task of multiple-human pose estimation.\n\nPros:\n+ Since the minimum multicut problem is a nice abstraction for many computer vision problems, being able to jointly train multicut inference with deep learning is a well-motivated problem\n+ It is a nice observation that the multicut problem has a tractable approximation when the graph is fully connected, which is a useful case for computer vision\n+ The proposed penalized approximation of the problem is simple and straightforward to try\n+ Experiments indicate that the method works as advertised\n\nCons:\n+ There are other perhaps more straightforward ways to differentiate through this problem that were not tried as baselines\n+ Experimental gains are very modest compared to an independently-trained CRF\n+ Novelty is a bit low, since the trick of differentiating through mean-field updates is well-known at this point\n+ Efficiency: the number of variables in the CRF scales with the square of the number of variables in the original graph, which makes this potentially intractable for large problems\n\nDifferentiating the minimum-cost multicut problem is well-motivated, and I think the multiple-human pose estimation problem is a nice application that seems to fit this abstraction well.  It is also not hard to imagine many other computer vision problems where integrating a multicut solver would be useful.  The proposed solution is straightforward.  The experimental results for the pose estimation problem show that the method can be applied to a real-world problem, though the gains are pretty modest.\n\nOne important point that is sort of glossed over in the paper is that the proposed solution is only tractable for complete graphs, since it relies on enumeration of all the chordless cycles in the graph.  This does not seem to be a huge limitation for typical computer vision problems, and it is kind of an interesting observation that this is an instance where the ostensibly harder problem (complete graphs) is easier than the ostensibly simple problem (sparse graphs).  On the other hard, the efficiency argument is a double-edged sword.  Although all chordless cycles can be enumerated in a complete graph, there are O(N^2) edges in a complete graph, which means that the complexity scales at least as N^2, where N is the number of nodes in the graph.  If I\u2019m not mistaken, mean-field updates would then cost O(N^4)\u2014since we must do one update for each of the N^2 edges, and each edge variable is involved in O(N^2) cliques\u2014which would not be feasible for many computer vision problems.  A frank discussion of computational efficiency with some complexity analysis should be included.\n\nAnother significant shortcoming is that this may not even be the most straightforward way to solve this problem.  Since the minimum-cost multicut problem has a straightforward linear programming relaxation, methods such as [A,B] (see citations below) could be used as a straightforward way to differentiate through the solution of the problem.  Furthermore, these methods could potentially leverage existing solvers, and thus be more efficient and accurate.  So, not discussing these methods or comparing to them as baselines seems like an omission.\n\nThe experiments are also a bit disappointing.  First, the MNIST experiments seem a bit strange.  I don\u2019t really understand the motivation for using the validity of the edge labeling as loss function.  In any real problem, we wouldn\u2019t be interested in the accuracy of inference\u2014we would be interested in the task loss.  Using the accuracy of inference as a loss seems odd.  Also, presumably, we can get more accurate inference trivially, by increasing the penalties (gamma), so it is unclear whether this is learning anything useful.\n\nThe human pose experiments are much more interesting, as that problem is perfectly well motivated.  However, the results are disappointing, since the mean AP improvement over the \u2018post-processing\u2019 CRF seem very small.  Standard error bars should probably be computed for these metrics.  I\u2019m afraid that these small improvements may not be significant.  Also, the difference between tables 4 and 5 is unclear.  Table 4 is not referenced in the paper (I guess the last ref. to table 5 on page 7 should refer to table 4).  Is table 4 showing validation set results, whereas table 5 is showing test set results?  This is not clear.\n\nOverall, I feel that this is a reasonable approach to solve a problem with wide-ranging applications.  However, the novelty of the solution is a bit on the low side, and one could argue that other ways of differentiating through optimization problems [A-B] are important missing references and baselines for the experiments.  Since the experimental results are also not that compelling compared to even the independent CRF and efficiency could also be a problem, I\u2019m afraid potential impact may be limited at this time.\n\n[A] Amos, Brandon, and J. Zico Kolter. \"Optnet: Differentiable optimization as a layer in neural networks.\" arXiv preprint arXiv:1703.00443 (2017).\n[B] Schulter, Samuel, et al. \"Deep network flow for multi-object tracking.\" Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}