{"title": "A good work; limited novelty but solid results", "review": "This paper investigates speaker adaption with a few samples based on an existing (pre-trained) multi-speaker TTS system. The three approaches in this paper are almost the same as the voice cloning work in Arik et al. (2018). However, it is still very beneficial to demonstrate these approaches for linguistic feature conditioned WaveNet.\n\nDetailed comments:\n\n1) This manuscript is not self-contained, as it omits the important details for acquiring linguistic features (e.g., phoneme duration model) and fundamental frequency (F0) at training and test time. The only information is that it uses existing model (Zen et al., 2016) to predict linguistic features and F0. What type of linguistic features are used in this work? Is the existing model (Zen et al., 2016) trained on the same training set as WaveNet model?\n\n2) It seems the only speaker-dependent part of the system is the embedding table for WaveNet. Actually, both linguistic features (e.g., phoneme duration) and fundamental frequency sequence are highly speaker-dependent. The authors normalize F0 to make it as speaker-independent as possible. What about the speaker-dependent linguistic features? Why not keep them as speaker dependent, and do speaker-adaption for the new speaker at inference?\n\n3) In my opinion, it\u2019s a bit superfluous to name fine tuning as non-parametric few-shot adaption, and auxiliary network (speaker encoding) as parametric few-short adaption. Both ideas are quite natural as in Arik et al. (2018).\n\n4) The abbreviations SEA-ALL, SEA-EMB and SEA-ENC are appeared without explanation. \n\n5) It would be better to provide more details about early termination criterion in Section 3.1. Is it simply the validation loss?\n\n6) In Table 1, the MOS from Arik et al. (2018) and Jia et al. (2018) are not comparable. The experimental settings are different. Perhaps more importantly, these MOS evaluations are done by different group of people.\n\n7) In Section 5.3, Nachmani et al. (2018) and Arik et al. (2018) have also used speaker verification model as an objective evaluation.\n\nOverall, this is a good work with limited novelty but solid results. However, it can be improved in many ways as detailed  in previous comments. I would like to raise my rating if these comments can be addressed properly.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}