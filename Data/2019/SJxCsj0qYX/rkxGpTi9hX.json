{"title": "Interesting view of GANs from game-theory perspective, but algorithmically the Stackelberg GAN is similar with previous multiple-generator GANs", "review": "This paper proposes the Stackelberg GAN framework of multiple generators in the GAN architecture. The architecture is similar with previous multiple-generator GANs (MAD-GAN and MGAN). In fact, it's even simpler in the sense that Stackelberg GAN has simpler loss function for the discriminator compared with the previous two. The authors prove that the minimax duality gap shrinks as the number of generators increases. And this proof has no assumption on the expressive power of generators and discriminator. With this proof, the authors argues that because the duality gap shrinks as the number of generators increases, the training of GANs gets more stable.\n\nFrom the algorithm part, I think the algorithm is very similar (and even simpler) than MAD-GAN and MGAN. The MAD-GAN and MGAN even proposed some specific loss for the discriminator so that it will encourage different generator to generate different modes in the target distribution. The Stackelberg GAN does not do this, but \"partially\" achieved the same goal. However, from Figure 9, we see that the simpler the generator is, the easier different generator will capture different modes. I think that this is due to the simplicity of discriminator loss. Therefore, on the algorithm part, the author may want to address the difference between Stackelberg GAN and MAD-GAN and MGAN. On the experiment part, we need to see more comparison between these three methods. In the current experiment, MGAN result is very similar to the proposed method, and MAD-GAN result is missing. Personally, I think that on cifar dataset (or larger datasets), these three methods should have very similar behavior. \n\nFrom the theoretical part, the authors derived a bound of the minimax duality gap for the Stackelberg GAN, without the assumption on the expressive power of generators and discriminator. Although the bound may not be practical, these are nice efforts. There are many typos in the paper (and appendix), which make me difficult to follow the proofs. For example, \"Let clf (bclf) be the convex(concave) closure of f, which is defined as the function whose epigraph (subgraph) is the convex\n(concave) closed hull of that of function f.\" Do we have concave closed hull of subgraph of function f? What is the concave closed hull of a set? The usage of sub(sup)-script is also very confusing, like in the definition of h_i(u_i). The authors may want to correct typos and improve the presentation. In the conclusion, the authors conclude \"We show that the minimax gap shrinks to \\eps as the number of generators increases with rate e O(1/\\eps).\" This is an over-claim, because the authors only proved this under the assumption of concavity of the maximization w.r.t. discriminators. \n\nFinally, the authors may want to provide some simple results of the Stackelberg GAN from the perspective of density approximation, even assuming infinite capacity of the discriminator set, as other GANs does. Whether the distance defined by the maximization problem a distance or divergence. If we exactly minimizing that objective function, do we get the target distribution? \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}