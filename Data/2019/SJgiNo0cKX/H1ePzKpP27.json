{"title": "Interesting idea should be explained better, and lacks objective comparison to baselines", "review": "The paper addresses the problem of pixel-wise segmentation of lanes from images taken from a vehicle-mounted camera. The proposed method uses multiple passes through encoders decoders convnets, thereby allowing extract global features to inform better local features, and vice versa. Only qualitative baseline comparisons are presented by manually comparing the output of the network to reported results of other methods in [Pan et al.2017].\nIt is unclear to me if the proposed multiple encoder-decoder network is a novel architecture, or a known architecture applied to a novel use case. In case of the former, more details should be given on the design of the network, how it is trained, etc. for reproducibility. The biggest problem however is the subjective manual comparison to existing methods, which the authors do in favor of a quantitative comparison using well-understand objective metrics. While they point out problems with evaluating segmentation with conventional accuracy metrics, no attempt is made to make a better objective measure. We are left to judge the results on only a few selected example frames. \nIt is also unclear how the method and evaluation strategy compares to methods which predict lanes as splines or other parameterized functions. E.g. see surveys on existing approaches, and discussion of different evaluation strategies, e.g. \"Recent progress in road and lane detection: a survey\" [Hillel et al.2014] and \"Visual lane analysis and higher-order tasks: a concise review\" [Shin, 2014].\nThroughout the paper, various fuzzy and unclear statements are made (see detailed comments below). The paper would be in a better shape if more time is spend to improve the writing, provide more details on the method, and extend the experiments.\n\nPros:\n+ multiple encoder-decoder stages could be beneficial for lane segmentation\n\nCons:\n- lacking evaluation and comparison to baseline methods\n- missing details on proposed network architecture, making it hard to reproduce\n- unclear what colors in figures for qualitative evaluation represent: are individual lanes also distinguished?\n\nBelow are more detailed comments and questions:\n* Abstract\n\t* \"the capability has not been fully embodied for\" \u2192 Fuzzy statement, I don't understand what this means.\n\t* \"In especial\" \u2192 check grammar\n* Sec 1.: Introduction\n\t* \"the local information of a lane such as sharp, edges, texture and color, can not provides distinctive features for lane detection\" local edges are not distinctive for lanes? Possibly local edges alone are not sufficient, but various lanes detection approaches rely on edge extraction as features. This statement therefore seems too strong.\n\t* \"End-to-end CNNs always give better results than systems relying on hand-crafted features.\". It is not possible to say that one type of classifier categorically better than another. The 'best' classifier depends on the problem at hand, valid assumptions that can be made, and the amount of training data avaiable, among others. For instance, \"How Far are We from Solving Pedestrian Detection?\" [Zhang,CVPR16] demonstrates that CNNs do not always give better results than hand-crafted features for some tasks and datasets. The paper should be more careful with such strong statements.\n\t* \"Highly hand-craft features based methods can only deal with harsh scenarios.\". I don't understand, is this statement intended as an argument against hand-crafted features? Isn't it good to deal especially with harsh scenarios?\n\t* \"but less explored on Semantic Image Segmentation due to strong prior information is needed.\" CNNs are extensively used for semantic image segmentation, e.g. see the well-known Cityscapes benchmark.\n\t* \"recent methods have replaced the feature-based methods with model-based methods.\". Not sure why the paper call CNNs \"model-driven methods\", but refer to the earlier classical methods with highly designed representations (Kalman filter, B-snakes, ...) as \"feature-based methods\". This seems diferent from what I typically see, where CNNs are referred to as 'data-driven methods', and the classical methods as \"model-driven\".\n* Sec 1.2: Contributions\n\t* \"First, reduced localization accuracy due to the weak performance of combining the local information and global information effectively and efficiently\". Instead of presenting a first contribution, the paper presents a problem. Do the authors mean that they \"tackle the problem of reduced localization accuracy ...\" ? That would still not make this contribution very concrete though ...\n\t* \"We make our attempts to rethink these IoU based methods.\" \u2192 Please argue in favor of your new method. An in-depth comparison of evaluation methods, and why some metrics fail or could be redesigned would be good. However, the paper currently fails to present a new metric, and convince that it tackles shortcomings of established metrics.\n\n* Sec 2.: Multiple Encoder-Decoder Nets\n\t* Figure 2: Is this the first paper to propose this multiple encoder-decoder net? Or is the idea taken from other work, and is the novelty to apply it to this problem? If this general architecture was already proposed (for semantic segmentation?), please add citations and discuss it as related work. If this network design is completely novel, I would expect more details on how the network is constructed (e.g. dimensions of each layer, non-linear activation function used, batch normalization, strides, etc.). \n\t* \"the following loss function:\". Since it is a binary classification problem, and not a regression problem, why not use a (binary) cross entropy loss instead of a mean squared error?\n\n* Sec 3: Experiments\n\t* Figure 3: What is the \"Baseline\" method ? Where are the references to the other works, or is the reader required to read [Pan'2017] to understand your figures?\n\t* Figure 3: How are the colors in these figures determined? Is this also an instance segmentation problem? From your methodology section I though only binary classification was considered. Do you do some post-processing to separate individual lanes? I find this confusing, as I thought that the task was limited to binary segmentation.\n\t* \"Recent works evaluated ...\" please cite the works you refer to.\n\t* \"we have compared more than 500 probmaps of each level nets manually and count the accuracy of these probmaps as shown in figure 7.\" So if I understand correctly, instead of using an objective evaluation metric, you have reverted to manual labor to visually judge lane detection quality. This is not really a metric, and not really a solution that 'rethinks IoU based methods.' Problems of your approach is that it is unclear on what criteria results are judged, your evaluation is not objectively reproducible by others, and does not scale well for novel future evaluations. Why is this even needed? E.g. why not use some chamfer distance or Gaussian smoothing of the edge map if you want to evaluate near coverage instead of hard boundaries? Or, fit a function through the boundary, and evaluate distance (in meters) to true lane. I find the proper discussion and motivation for manual evaluation over objective metric evaluation lacking.\n\t* Figure 6: What are the Ground Truth images of each row ? E.g. in the fourth row from the top, should the right-most yellow lane be present or not? As it stands, I can't interpret the columns and see which x times is visually 'better'.\n* Sec 3.4:\n\t* \"To improve ability of the network, we propose a small quantity of channel to reduce overfitting by considering inter-dependencies among channels.\" To improve relative to what? Where are the results comparing large amounts vs small amount of channels? Note that Figure 8 is not referred to in the text, and confusingly compares \"18 layers\" to \"1 layers\". Do you mean channels instead of layers? And, how many channels were to obtained the results in the preceding sections?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}