{"title": "Interesting idea but not made rigorous/precise", "review": "The paper analyzes the empirical spectral density of DNN layher matrices and compares them with the traditionally-regularized statistical models, and develop a theory to identify 5+1 phases of training based on it. The results with different batch sizes illustrate the Generalization Gap pheneomena, and explains it as being causes by implicit self-regularization.\n\nHowever, the paper seems a little bit handwavy to me, without any serious theoretical justification. For example, why are \\mu=2 and 4 chosen as the threshold between weakly/moderately/very heavy-tailed? In addition, the paper is build upon o the 5+1 model as in Figure 2 and the graphical comparison between the empirical ESD and the expected ESD of the five models in Table 1, and they lack any mathematical/rigorous definition---see table 2. The simulations are performs over a particular data set and a particular setting, and I wonder if the observations would be different for a different data set and a different setting. \n\nAs a result, it may give some important intuition, but the content is not sufficiently rigorous to my knowledge.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "1: The reviewer's evaluation is an educated guess"}