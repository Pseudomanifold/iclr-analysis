{"title": "The idea is intuitive, and the paper has many experimental analysis.", "review": "The motivation of this paper is clear, since the sentence of same meaning in different languages should have similar representation in the high level latent space. The experimental analysis of the proposed alignment method is detailed. \n\n1. The method ``Pivot through English``: does it mean to use the baseline model decoding twice,  source->en and en->target? If true, because you already have the two src<->en and tat<->en NMT systems, you can fully use your model and should consider the baseline as the pivot method, whereas the disadvantage is the doubled latency. So the power of your model is not increased, your contribution is to make it more efficient during inference.\n2. Figure 2 is interesting. Even when you are trying to optimize the cosine distance, the actual cosine distance is better when using adversarial training.\n3. In your pre-training optimizer, the learning rate is either 1.0 or 2.0. If Adam is used, it is abnormal.\n4. Another concern is the paper did not compared with another zero-shot NMT or pivot method, e.g., https://arxiv.org/pdf/1705.00753v1.pdf . \n5. In section 4.4, the lambda to balance alignment loss is 0.001. Since the cosine distance is usually less than 1. I am wondering how significant this extra regularizer is in such case.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}