{"title": "No proper comparison with previous work", "review": "This paper proposes an explicit alignment loss for the encoder representations in multilingual NMT, which substantially improves the zero-shot translation performance.\n\nWhile the paper contains some interesting ideas, I think that it does a poor job at comparing the proposed method with existing techniques. In particular, the paper takes Johnson et al. (2016) as its only baseline, and tends to ignore all the other relevant literature in its experiments and most of the discussion. This makes it difficult to put this work into perspective, and I was left with the impression that it oversells to some extent. I would appreciate if the authors could address my concerns below, and I am open to modify my score accordingly.\n\nFirst of all, the claim that \"the quality of zero-shot translation has lagged behind pivoting through a common language by 8-10 BLEU points\" seems deceptive if not wrong. For instance, Chen et al. (2017) (http://aclweb.org/anthology/P17-1176) claim that their method \"significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs\", while your method only matches pivot-based performance. If not a direct comparison, this requires at least some discussion, but the current version does not even cite it. You could argue that, unlike Chen et al. (2018), your method is multilingual, but even from that perspective I think that the paper needs to do a better job at putting the proposed method into context.\n\nIn addition to that, Ha et al. (2017) and Lu et al. (2018) also focus on zero-shot multilingual translation as you do. While these two papers are cited, the proposed method is not compared against them. I think that some further discussion is necessary, and a direct comparison would also be desirable.\n\nSestorain et al. (2018) (https://arxiv.org/abs/1805.10338) also seems like a very relevant reference, but it is not even cited. Again, I think that some discussion (if not a direct comparison) is necessary.\n\nOther comments:\n\n- In your IWSLT experiments, you point out that \"the data that we train on is multi-way parallel, and the English sentences are shared across the language pairs\", which \"may be helping the model learn shared representations with the English sentences acting as pivots\". Something similar could also happen in your WMT experiments, as some of the underlying training corpora (e.g. Europarl) are also multi-way parallel. I do not aim to question the experimental settings, which seem reasonable, but this raises some flags to me: the proposed evaluation is not completely realistic, as (some) training corpora are indirectly aligned, and the system could be somehow exploiting that. I think that it would be great to see some further discussion on this and, ideally, having experiments that control this factor would also be desirable.\n\n- Your analysis reveals that, to a great extent, the poor performance of the baseline is caused by the decoder generating its output in the wrong language. While your proposed method does not suffer from this issue, this suggests that the baseline is very weak, as one could think of different hacks to mitigate this issue with minimal effort (e.g. simply filtering an n-best list with language detection should greatly improve results). Going back to my previous point, it seems hard to believe that such a weak baseline represents the current state-of-the-art in the topic as the paper seems to suggest.\n\n- Artetxe et al. (2018) do not use adversarial training with a language detecting discriminator as stated in page 3.\n\n- Artetxe et al. (2018) is duplicated in the references.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}