{"title": "Interesting approach for learning domain-invariant features + dataset selection", "review": "== Originality ==\nThe idea of matching features/representations across the source domain and target domain is an old idea, but it is executed in an interesting new way in this paper.\n\nIn this approach, feature representations are learned by training a neural classifier on the source domain, and an RL agent influences the feature representation by iteratively adding/removing examples from the source training data. The RL agent receives reward when the resulting feature representation causes the source domain data and target domain data to look more similar in distribution in feature space. To efficiently estimate the improvement in feature matching, a nice data bucketing strategy is used.\n\nThe novelty of the approach is the main strength of this paper.\n\n== Quality of results ==\nThe experimental results seemed overall positive, but I felt that they could have been stronger.\n\nFor POS tagging, the authors don't compare against the domain adaptation methods mentioned in their related work section. Instead, they compare against Bayesian optimization using several heuristic criteria, and it was unclear where this baseline comes from. This made it hard to see whether the new approach represents a true improvement over existing techniques.\n\nFor dependency parsing, it appears that the proposed approach is outperformed by simply training on all of the source domain data.  It would be interesting to know whether this is because feature-matching is not a good proxy for target domain performance (objective mismatch) or whether the RL system converged to a poor local optima (optimization failure).\n\n== Clarity ==\nI felt that the abstract and introduction were vague in describing the main conceptual contribution.\n\nHowever, Section 2 (The Approach) was clearly written and I came away understanding exactly what the authors are doing.\n\n== Minor comments ==\n- Algorithm 1 seems to have a typo: the definition of \\nabla \\tilde{J}(W) on the second to last line seems to be missing \\nabla \\log \\pi\n- Many citations throughout the paper need to be wrapped in parentheses\n\n== Conclusion ==\nThis paper presents an interesting new approach for dataset selection and learning domain-invariant representations.\n\nPros:\n- originality of the approach\n\nCons:\n- Experiments could have been more convincing:\n    - should compare against at least one other state-of-the-art domain adaptation method\n    - results on dependency parsing (the most challenging task they consider) were mostly negative\n    - evaluation on other more recent multi-domain NLP tasks would have been nice (e.g. MultiNLI)\n- Abstract and intro could provide better description of the conceptual contribution, as well as motivation", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}