{"title": "Interesting idea but somewhat lacking in novelty and in the experimental design", "review": "The authors propose methods to address a novel task of transfer learning for estimating the CATE function.  The methods proposed are several different neural network architectures and training strategies, which vary in how they leverage information from one task to another: learning joint representations and then different task-specific \"heads\", warm-starting from one task to another, or aggregate gradient updates across experiments. \nExperimental evaluation is always challenging for CATE tasks. In this paper the evaluation of the method is done using both a synthetic setting based on MNIST, and a real-world experimental dataset from a large Get Out The Vote experiment.\n\n\nThe task itself is of high interest, and should definitely be a subject of research. The paper is mostly clearly written and technically competent. My main concerns are as follows:\n\n1. The degree of novelty is not very high. While the task is new, I believe the technical innovation is not profound. The techniques are mostly straightforward re-use of features. The multi-head architecture has been introduced in the single experiment setting by Shalit et al. (2017). The meta-learner approach is directly adapted from Nichol et al. (2018). \n\nAlso, for this task, I would have liked to see more engagement with the many other transfer learning / domain adaptation methods out there:\nIn the very least, re-weighting based methods (which incidentally bear strong connections to propensity-score re-weighting commonly used in causal inference), as well as at least some well-known deep-learning transfer-learning methods such as Bengio \"Deep learning of representations for unsupervised and transfer learning.\" (2012), Ganin et al. \"Domain-adversarial training of neural networks.\"(2016), Long, Mingsheng, et al. \"Unsupervised domain adaptation with residual transfer networks.\" (2016), or similar work. \n\n2. The experimental evaluation can be improved in two ways:\n2.a. I would like to see more baselines. For example, in the GoTV task, how well does RF do when the \"ground truth\" is a linear model, and vice-versa? Also, Causal Forest (Wager & Athey 2017) and BART (Chipman et al. 2011) have emerged as two very strong and widely successful methods for CATE estimation. Both can be trivially adapted to the transfer learning task, e.g. by learning each task separately or by learning them jointly with an indicator feature for each task.  \n2.b. In the GoTV task, the original data is obtained from an RCT. That means that evaluation can be done using a much more powerful approach than simulating potential outcomes: one can instead evaluate the policy risk implied by the CATE estimator, as done in Shalit et al. (2017) using the LaLonde dataset. \n\n\nOverall I think this paper introduces an interesting problem and some valid approaches, but can be significantly improved both in the methods it presents and the experimental setup.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}