{"title": "Lack of clarity and weak experimental evaluation", "review": "On the positive side, I think it's a good idea to experiment with various approaches to defend DNNs against adversarial attacks, like the Background Check approach considered in this manuscript (which hasn't gotten a lot of traction in the Machine Learning community so far).\n\nHowever, the manuscript has a number of shortcomings which in my opinion makes it a strong rejection. My main concern is about the experimental evaluation: \n- The authors should test their approach on Carlini & Wagner's attack which allows for explicit control of logit differences and thus could entirely defeat the Background Check.\n- Moreover, any paper on this topic should evaluate defenses in a complete white-box setting, i.e. the adversary is aware of the detection method and actively tries to bypass it. \n- A comparison with other detection methods from the literature is missing, too, and the two-class classifier setting is very limited.\n\nBesides that, I find there is a general lack of clarity:\n- It really becomes clear only towards the end of the paper what the Background Check is applied for, namely, the detection of adversarial samples. This should be clearly articulated from the beginning.\n- Notation isn't always properly introduced (e.g. in the formula for 3-class average recall on page 6), and the same goes for \nsome acronyms (e.g. what is TPR?).\n- Where does Table 2 show a \"mean reduction in average recall of 11.6\", and what does that mean exactly?", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}