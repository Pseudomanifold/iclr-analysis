{"title": "interesting idea but ends up tackling a simple case and many elements are unclear", "review": "The paper aims at developing a methodology that uses the idea of MDP homomorphisms to transform a complex MDP with a continuous state space to a simpler one. \n\nThis idea is interesting but overall the paper ends up tackling a rather simple case, which is solving a deterministic MDP with one reward also called the goal. The problem was first formulated with a MDP with sparse rewards but then, without much explanation, all discussions (and all experiments) are done with \"one goal\". The paper states for instance that the \"agent can choose the best state-action block to execute based on the length of the shortest path to the goal from each node\". So there is in fact no possibility to deal with the case of multiple rewards. Indeed, looking for the shortest path to the closest reward isn't the strategy that will maximize the cumulative discounted returns in general.\n\nIn addition, I have doubts about the scalability of the algorithm. For instance, to deal with the difficult exploration part that is required for the algorithm, the paper uses the vanilla version of the deep Q-network (section 5.4). However DQN is known to have difficulties with tasks that have sparse rewards. And if the task ends up being solved quite well with DQN, it is unclear what the motivation is for the MDP homomorphisms (first sentence in the introduction is \"Abstraction is a useful tool for effective control in complex environments\").\n\nIn addition, the descriptions of the experiments and results seem to lack key elements to  understand them (see comments below).\n\nDetailed comments:\n- It is not clear to me why the word \"online\" is emphasized in the title. In the introduction, you also mention that the \"algorithm does not depend on a particular model\u2013we replace the convolutional network with a decision tree in one experiment\". In that case, you maybe do not need \"deep learning\" in the title either.\n- (minor) Why do you state that you use a \"one hot encoding\" for a binary variable (description of the puck stacking)? You could just use one input that is either 0 or 1 (instead of 01 and 10 that is implied by the idea of one hot encoding)?\n- (minor) It is stated that the replay buffer is limited to 10000 transitions due to memory constraints. But one transition consists in a frame of (28*C)^2 where I did not find the value of C but I guess it would reasonably be less or equal to 4 so that one frame takes less than 10kB. If that is right, 10000 of them would be less than 100MB, which seems a rather low memory constraint?\n- (minor) Section 6.2: a grid of size 3x3 is mentioned but the examples in Fig 1(d) seems to be on a 4x4?\n- Figure 1 (a) and (b): why isn't there any information relative to variance between the results over different random seeds?\n- Figure 2: I'm puzzled by the shape of the cumulative reward curve obtained through the epochs. Why is it that smooth, while there are instabilities usually. In addition, why is there a significant different regime at exactly 1000 and 1500 epochs, respectively for figure (a) and (b)?\n- The discussion on transfer learning/exploration (section 5.5) is not clear to me. In addition discussing settings such as the following do not seem of broad interest/novelty \"If we have the prior knowledge that the goal from the previous task is on the path to the goal, (...)\".", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}