{"title": "A Bayesian GAN with where data distribution is an equilibrium", "review": "Mode collapse in the context of GANs occurs when the generator only learns one of the \nmultiple modes of the target distribution. Mode collapsed can be tackled, for instance, using Wasserstein distance instead of Jensen-Shannon divergence. However, this sacrifices accuracy of the generated samples.\n\nThis paper is positioned in the context of Bayesian GANs (Saatsci & Wilson 2017) which, by placing a posterior distribution over the generative and discriminative parameters, can potentially learn all the modes. In particular, the paper proposes a Bayesian GAN that, unlike previous Bayesian GANs, has theoretical guarantees of convergence to the real distribution.\n\nThe authors put likelihoods over the generator and discriminator with logarithms proportional to the traditional GAN objective functions. Then they choose a prior in the generative parameters which is the output of the last iteration. The prior over the discriminative parameters is a uniform improper prior (constant from minus to plus infinity). Under this specifications, they demonstrate that the true data distribution is an equilibrium under this scheme. \n\nFor the inference, they adapt the Stochastic Gradient HMC used by Saatsci & Wilson. To approximate the gradient of the discriminator, they take samples of the generator parameters. To approximate the gradient of the generator they take samples of the discriminator parameters but they also need to compute a gradient of the previous generator distribution. However, because this generator distribution is not available in close form they propose two simple approximations.\n\nOverall, I enjoyed reading this paper. It is well written and easy to follow. The motivation is clear, and the contribution is significant. The experiments are convincing enough, comparing their method with Saatsci's Bayesian GAN and with the state of the art of GAN that deals with mode collapse. It seems an interesting improvement over the original Bayesian GAN with theoretical guarantees and an easy implementation.\n\nSome typos:\n\n- The authors argue that compare to point mass...\n+ The authors argue that, compared to point mass...\n\n- Theorem 1 states that any the ideal generator\n+ Theorem 1 states that any ideal generator\n\n- Assume the GAN objective and the discriminator space are symmetry\n+ Assume the GAN objective and the discriminator space have symmetry\n\n- Eqn. 8 will degenerated as a Gibbs sampling\n+ Eqn. 8 will degenerate as a Gibbs sampling", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}