{"title": "experimental work now conclusive", "review": "PRIOR COMMENT:   This paper should be rejected based on the experimental work.\nExperiments need to be reported for larger datasets.  Note the MGAN\npaper reports results on STL-10 and ImageNet as well.\n\nNOTE:  this was addressed by the 27/11 revision, which included good\n   results for these other data sets, thus I now withdraw the comment\n\nNote, your results on CIFAR-10 are quite different to those in the\nMGAN paper.  Your inceptions scores are worse and FIDs are better!!  I\nexpect you have different configurations to their paper, but it would\nbe good for this to be explained.  NOTE:   explained in response!\n\nNOTE:  this was addressed by the 27/11 revision\n\nI thought the related work section was fabulous, and as an extension\nto BGAN, the paper is a very nice idea.  So I benefited a lot from reading\nthe paper.\n\nI have some comments on Bayesian treatment.  In Bayesian theory, the\ntrue distribution $p_{data}$ cannot appear in any evaluated formulas,\nas you have it there in Eqn (1) which is subsequently used in your\nlikelihood Eqn (2).  Likelihoods are models and cannot involve \"truth\".\n\nLemma 1:  Very nice observation!!  I was trying to work that out,\nonce I got to Eqn (3), and you thought of it. \n\nAlso, you do need to explain 3.2 better.  The BGAN paper, actually, is\na bit confusing from a strict Bayesian perspective, though for\ndifferent reasons.  The problem you are looking at is not a\ntime-series problem, so it is a bit confusing to be defining it as\nsuch.  You talk about an iterative Bayesian model with priors and\nlikelihoods.  Well, maybe that can be *defined* as a probabilistic\nmodel, but it is not in any sense a Bayesian model for the estimation\nof $p_{model}$.\n\nNOTE:  anonreviewer2 expands more on this\n\nWhat you do with Equation (3) is define a distribution on\n$q_g(\\theta_g)$ and $q_d(\\theta_d)$ (which, confusingly, involves the\n\"true\" data distribution ... impossible for a Bayesian formulation).\nYou are doing a natural extension of the BGAN papers formulation in\ntheir Eqs (1) and (2).  This, as is alluded to in Lemma 1.  Your\nformulation is in terms of two conditional distributions, so\nconditions should be given that their is an underlying joint\ndistribution that agrees with these.  Lemma 1 gives a negative result.\nYou have defined it as a time series problem, and apparantly one wants\nthis to converge, as in Gibbs sampling style.  Like BGAN, you have\njust arbitrarily defined a \"likelihood\".\n\nTo me, this isn't a Bayesian model of the unsupervised learning task,\nits a probabilistic style optimisation for it, in the sense that you are defining a probability\ndistribution (over $q_g(\\theta_g)$ and $q_d(\\theta_d)$) and sampling\nfrom it, but its not really a \"likelihood\" in the formal sense.  A\nlikelhood defines how data is generated.  Your \"likelihood\" is over\nmodel parameters, and you seem to have ignored the data likelihood,\nwhich you define in sct 3.1 as $p_{model}()$.\n\nAnyway, I'm happy to go with this sort of formulation, but I think you\nneed to call it what it is, and it is not Bayesian in the standard sense.  The theoretical\ntreatment needs a lot of cleaning up.  What you have defined is a\nprobabilistic time-series on $q_g(\\theta_g)$ and $q_d(\\theta_d)$.\nFair enough, thats OK.  But you need to show that it actually works in\nthe estimation of $p_{model}$.  Because one never has $p_{data}$, all\nyour Theorem 1 does is show that asymptotically, your method works.\nUnfortunately, I can say the same for many crude algorithms, and most\nof the existing published work.  Thus, we're left with requiring a\nsubstantial empirical validation to demonstrate the method is useful.\n\nNow my apologies to you: I could make somewhat related statements\nabout the theory of the BGAN paper, and they got to publish theirs at\nICLR!  But they did do more experimentation.\n\nOh, and some smaller but noticable grammar/word usage issues.\n\nNOTE:  thanks for your good explanation of the Bayesian aspects of the model ...\nyes I agree, you have a good Bayesian model of the GAN computation , but it\nis still not a Bayesian model of the unsupervised inference task.  This is a somewhat\nminor point, and should not in anyway influence worth of the paper ... but clarification\nin paper would be nice.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}