{"title": "Another empirical analysis related to instability of fully unsupervised cross-lingual word embedding models", "review": "The recent methods on learning cross-lingual word embeddings without any bilingual supervision (i.e., in a fully unsupervised way) have generated a lot of traction in NLP and beyond. Follow-up papers from Artetxe et al. (ICLR 2018), and Lample et al. (ICLR 2018, EMNLP 2018) have shown that such cross-lingual word embeddings can seed unsupervised (neural and statistical) machine translation, and other applications are also possible (e.g., unsupervised cross-lingual retrieval). \n\nThis paper in particular focuses on further empirical analyses related to previously observed instability of unsupervised mapping-based approaches for learning unsupervised cross-lingual word embeddings, but remains focused on the task of unsupervised bilingual dictionary induction (UBDI). The paper is largely a continuation of the preliminary paper which focuses on the same problem and research question: the paper of Sogaard et al. (ACL 2018). In fact, this paper reads a lot like a more elaborated (or incremental) version of the preliminary paper of Sogaard et al. (ACL 2018), with the paper structure and some descriptions borrowed from the previous work (e.g., the focus on the MUSE algorithm, the description of k-isospectrality), with two main enhancements compared to the prior work: 1) The introduction of Procrustes fit as a means to diagnose and anticipate (im)possibility of unsupervised embedding learning. Arguably, the proposed diagnostic tool is more efficient than the alternatives; 2) A very simple method for unsupervised model selection (Section 4) based on the mean cosine similarity predicted by the CSLS method.\n\nAlthough some of the listed empirical observations are very straightforward if the reader knows anything about the entire procedure, it is good to see them empirically supported and presented in a very organised fashion. However, the two main methodological contributions of the paper seem quite thin to me. First, the method for unsupervised model selection is based on the unsupervised stopping criterion of Conneau et al. (2018) and it is a very simple (even simplistic) idea which happens to work well in practice already. Second, I am not fully bought by the Procrustes fit test and its comparison to two other tests has to be further examined, especially its relation to two similarity measures from Sogaard et al. For instance, k-subgraph isospectrality in the work of Sogaard et al also displayed a very strong correlation with UBDI performance (>0.9), while the authors in this paper claim that the correlation coefficient with their data in this paper is -27% for the same method. Could the authors explain the discrepancy?\n\nThe authors claim that the Procrustes fit is much less computationally demanding as a test than the two alternative tests. However, I would like to see this claim empirically validated by running some actual tests and measuring test times. How are the two alternative tests impractical if they were used previously by Sogaard et al.?\n\nSome observations have been already confirmed in prior work. For instance, Observation 2 is already analysed in the work of Sogaard et al. Observation 3 and Observation 5 are also straightforward given the technicalities and hyper-parameter setup of the MUSE procedure. \n\nI wonder if the authors are aware of the more recent work on UBDI from Artetxe et al. (ACL 2018). A crucial analysis for this paper should focus on Artetxe's more recent work rather than the MUSE algorithm. I acknowledge the fact that this field in particular is moving rapidly, but the paper would be much more impactful by focusing on the more recent and more appropriate starting ground rather than recycling the ideas already covered in prior work (e.g., in Sogaard et al.'s paper). \n\nThe paper operates only in the fully unsupervised setting, which is, in my opinion, an artificial (even \"artistic\" setting). As Artetxe et al. (ACL 2017) showed and Sogaard et al. verified: relying on some sort of supervision (identical words, cognates, even numerals for languages with different scripts) already mitigates the problems of instability to a large extent. Why would we want to tie our analysis only to the fully unsupervised setting and go about it at length when it seems obvious that some sort of (cheap) supervision can already help the mapping-based algorithms to a large extent? I would like to see more experiments in such weakly-supervised settings. I would also like to see if the model selection criterion is applicable to this more real-life setting.\n\nAlthough I am happy to see a clear division into possible and impossible UBDI setups, the paper also does not provide any solutions (or rules of thumb) on how to proceed if we end up having an impossible setup. What are the alternatives to the fully unsupervised BDI?\n\nI am also missing explanations to some empirical observations: e.g., while the discrepancy between this work and the original work of Hoshen and Wolf has been reported, there is no insight on what causes the discrepancy and how this could be theoretically justified. In simple words, it is just observed, without getting a proper explanation supporting it.\n\nThe paper is very well written and easy to follow, but my impression is that, as an empirically driven paper, it could contribute from further experimentation as well as from linking key empirical findings to theoretical justifications. Also, the two main contributions of the work are not so substantial and overall the paper mostly proves an already established fact: that fully unsupervised models for cross-lingual word embedding learning are very unstable and their success relies on a large spectrum of design choices and hyper-parameters. I would also like to see the analysis expanded beyond UBDI to other (more downstream) tasks where such low-resource representation learning regimes might be useful.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}