{"title": "Useful taxomony of memory-based neural network", "review": "Summary\n=========\nThe paper analyses the taxonomy over memory-based neural networks, in the decreasing order of capacity: Neural RAM to Neural Stack, Neural Stack to LSTM and LSTM to vanilla RNN. The experiments with synthetic and NLP datasets demonstrate the benefits of using models that fit with task types.  \n\nComment\n========\nOverall, the paper is well written and presents interesting analysis of different memory architectures. However, the contribution is rather limited. The proposed taxonomy is not new. It is a little bit obvious and mentioned before in [1] (Unfortunately, this was not cited in the manuscript). The theorems on inclusion relationship are also obvious and the main contribution of the paper is to formally show that in mathematical forms.  The experiments on synthetic tasks give some insights into the models\u2019 operations, yet similar analyses can be found in [2, 3]. To verify the models really learn the task, the authors should include tests on unseen sequence lengths.  There remains questions unexplained in NLP tasks such as why multi-slot memory did not show more advantages in Movie Review and why Neural Stack performed worse than LSTM in bAbI data.  \n\nMinor potential errors: \n\nIn Eq. (6), r_{t-1} should be r_t   \n\nThe LSTM presented in Section 3.2 is not the common one. Normally, there should be x_t term in Eq. (3) and h_t=g_{o,t}*\\tanh(r_t) in Eq. (6). The author should follow the common LSTM formulas (which may lead to different proofs) or include reference to their LSTM version.  \n\n[1] Yogatama et al. Memory Architectures in Recurrent Neural Network Language Models. ICLR\u201918 \n\n[2] Joulin et al. Inferring algorithmic patterns with stack-augmented recurrent nets. NIPS\u201915 \n\n[3] Graves et al. Neural Turing Machines. arXiv preprint arXiv:1410.5401 (2014). ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}