{"title": "Principled optimization for GANS", "review": "Summary:\nThe authors take a variational inequality perspective to the study of the saddle point problem that defines a GAN. By doing so, they are able to profit from the corresponding literature and propose a few methods that are variants of SGD. The authors show in a simple example (a bilinear function) these exhibit better performance than Adam and a basic gradient method. After showing theoretical guarantees of these methods (linear convergence) the authors propose to combine them with existing techniques, and show in fact this leads to better results.\n\nEvaluation\nThis is a very good paper and I cannot but recommend its acceptance:\nIt is clear and well written. \nIt has the right level of balance between theory and experiments. \nTheoretical results are far from trivial. \nI haven't seen something similar.\nThe authors's do not make overstatements: they do not claim to have solved the GAN problem, but they do report improvements which are due to a thorough analysis (see above points). These results are much appreciated.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}