{"title": "A paper on mapping distributions to distributions using recurrent nets", "review": "This paper proposed a method for creating neural nets that maps historical distributions onto distributions.  The scheme relies on a recurrent net that takes a short list of distributions (3-5) and outputs a new distribution.  The authors apply this method to several distribution prediction tasks.  \n\nI have the following criticisms of this work:\n\n1) The test problems feel a bit like toy problems.  The distributions studied here are generally simple enough to be represented as a Gaussian mixture model (in fact, the experiments use input data distributions that are obtained via a GMM or kernel density estimation).  One would hope that the power of neural nets could be used for more complex distributions.  It would be interesting to see how this method performs on more complex / higher dimensional tasks.\n\n2) The improvement over a DRN is apparent, although not always significant.  Also, I'm curious what architectures were used for the experiments.  I was surprised to see how poorly an RNN performed, and I'm curious how much architecture search went into designing this network?\n\n3) In general, I'd like to see more meaty details about how networks propagate information between layers, how the input distributions are represented, how loss functions are calculated, and what network architecture look like.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}