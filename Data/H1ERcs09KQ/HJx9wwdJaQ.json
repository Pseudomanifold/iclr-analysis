{"title": "Interesting and probably useful, but presentation needs work and there are some technical issues", "review": "This paper proposes using a variant of the nested CRP as a prior on the latent space of a variational autoencoder. The authors demonstrate that this approach is able to simultaneously learn a meaningful latent representation of high-dimensional data (text and images) and do hierarchical clustering in that space.\n\nPros:\n* The high-level idea is compelling.\n* The empirical results are compelling, and the evaluation is thorough.\n\nCons:\n* The prose is pretty rough. The paper is full of sentences like \"VAE-nCRP trade-off is the direct dependency modeling among clusters against the mean-field variational approach\" that don't convey their intended meaning (at least to me).\n* The random variable \u03b7 seems completely superfluous. It only affects the likelihood through the level indicator l, but the marginal distribution p(l) = \u222b_\u03b7 p(\u03b7, l)d\u03b7 \\propto \u03b1 is tractable, since only one level is drawn per observation. (This is not the case for the traditional nCRP as used in topic modeling, since there a different level is chosen for each word.)\n* The novelty over the nCRP-VAE approach of Goyal et al. (2017) is pretty minor. The main difference seems to be that the model can select clusters at different levels, but I didn't quite get the intuition for why this should be desirable. In topic modeling, higher-level clusters tend to contain less-specialized words, and each document is a mix of specialized and general topics. But in this model, only one level is used to explain an entire image or document, and the idea that an entire image or document is much \"more specialized\" than another doesn't seem very intuitive to me.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}