{"title": "Interesting ideas and observations;  very early work", "review": "== Paper overview ==\nGiven a latent variable model (deep generative model), the paper ask how we should interpolate in the latent space. The key idea is to derive a natural interpolant from the prior distribution p(z), where z is the latent variable. The idea is that the interpolation function you apply to a variable z should not change the distribution of z of the start and end points of the interpolation curve are identically distribution. Example: consider two unit-length points drawn from a standard Gaussian, then linear interpolation of these points will result in points of smaller norm and hence different distribution. Differerent priors and corresponding interpolants are demonstrated and discussed. Empirical results are more of an illustrative nature.\n\n== Pros/cons ==\n+ The paper contribute a new and relevant point to an ongoing discussion on the geometry of the latent space.\n+ The key point is well-articulated and relevant mathematical details are derived in detail along the way.\n\n- I have some concerns about the idea itself (see below); yet, while I disagree with some of the presented view points, I don't think that diminishes the contribution.\n- The empirical evaluation hardly qualifies as such. A few image interpolations are shown, but it is unclear what conclusions can really be drawn from this. In the end, it remains unclear to me which approach to interpolation is better.\n\n== Concerns / debate ==\nI have some concerns about the key idea of the paper (in essence, I find it overly simplistic), but I nonetheless find that the paper brings an interesting new idea to the table.\n\n1) In section 1.1, the authors state \"one would expect the latent space to be organized in a way that reflects the internal structure of the training dataset\". My simple counter-question is: why? I know that this is common intuition, but I don't see anything in the cost functions of e.g. VAEs or GANs to make the statement true. Generative models, as far as I can see, only assume that the latent variables are somehow compressed versions of the data points; no assumptions on structure seems to be made.\n\n2) Later in the same section, the authors state \"In absence of any additional knowledge about the latent space, it feels natural to use the Euclidean metric\". Same question: why? Again, I know that this is a common assumption, but, again, there is nothing in the models that seem to actually justify such an assumption. I agree that it would be nice to have a Euclidean latent space, but doesn't make it so.\n\n3) In practice, we often see \"holes\" in the \"cloud\" of latent variables, that is regions of the latent space where only little data resides. I would argue that a good interpolant should not cross over a hole in the data manifold; none of the presented interpolants can satisfy this as they only depend no the start and end points, but not on the actual distribution of the latent points. So if the data does not fit the prior or are not iid, then the proposed interpolants will most likely perform poorly. A recent arXiv paper discuss one way to deal with such holes: https://arxiv.org/abs/1806.04994", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}