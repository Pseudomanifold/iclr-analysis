{"title": "This paper discusses an interesting topic without any clear conclusions.", "review": "The paper discusses linear interpolations in the latent space, which is one of the common ways used nowadays to evaluate a  quality of implicit generative models. More precisely, what researchers often do in this field is to (a) take a trained model (which often comes with a \"decoder\" or a \"generator\", that is a function mapping a noise sampled from a prior distribution Pz defined over the latent space Z to the data space), (b) sample two independent points Z1 and Z2 from Pz, and (c) report images obtained by decoding linear interpolations between Z1 and Z2 in the latent space. Researchers often tend to judge the quality of the model based on these interpolations, concluding that the model performs poorly if the interpolations don't look realistic and vice versa. The authors of the paper argue that this procedure has drawbacks, because in typical modern use cases (Gaussian / uniform prior Pz) the aforementioned interpolations are not distributed according to Pz anymore, and thus are likely to be out of the domain where decoder was actually trained. \n\nI would say the main contributions of the paper are:\n(1) The sole fact that the paper highlights the problems of linear interpolation based evaluation is already important.\n(2) Observation 2.2, stating that if (a) Pz has a finite mean and (b) aforementioned linear interpolations are still distributed according to Pz, then Pz is a Dirac distribution (a point mass).\n(3) The authors notice that Cauchy distribution satisfies point (a) from (2), but as a result does not have a mean. The authors present some set of experiments, where DCGAN generator is trained on the CelebA dataset with the Cauchy prior. The interpolations supposedly look nice but the sampling gets problematic, because a heavy tailed Cauchy often results in the Z samples with excessively large norm, where generator performs poorly.\n(4) The authors propose several non-linear ways to interpolate, which keep the prior distribution unchanged (Sections 3.4 and 3.5). In other words, instead of using a linear interpolation and Pz compatible with it (which is necessarily is heavy tailed as shown in Observation 2.2), the authors propose to use non linear interpolations which work with nicer priors Pz, in particular the ones with finite mean.\n\nI think this topic is very interesting and important, given there is still an unfortunate lack of well-behaved and widely accepted evaluation metrics in the field of unsupervised generative models. \n\nUnfortunately, I felt the exposition of the paper was rather confusing and, more importantly, I did not find a clear goal of the paper or any concrete conclusions. One possible conclusion could be that the generative modelling community should stop reporting the linear interpolations. However, I feel the paper is lacking a convincing evidence (from what I could find the authors base all the conclusions on one set of similar experiments performed with one generator architecture on one data set) in order to be viewed as a significant contribution to the generative modeling field. On the other hand, the paper has not enough insights to constitute a significant theoretical contribution (I would expect Observation 2.2 to be already known in the probability field). \n\nOverall, I have to conclude that the paper is not ready to be published but I am willing to give it a chance. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}