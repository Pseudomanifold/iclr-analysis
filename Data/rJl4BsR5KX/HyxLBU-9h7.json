{"title": "need more clear writing and strong experimental results", "review": "I had a hard time understanding this paper. The approach is clearly about combining kNN with neural networks, but it wasn\u2019t clear how it is done. After reading the whole paper, my guess is that kNN is done on raw data first, and then its results are used for training a neural network. In particular, a network is trained to predict the labels of neighboring samples, which are obtained by kNN beforehand. A simple figure explaining it in the introduction would be very helpful since the idea is not that complex. \n\nAlso, the authors also fail to give an adequate explanation on why the method works. The only reason I can think of is that this regularization forces the model to detect if a sample near a class boundary. This is because when a sample is far from boundaries and surrounded by samples of the same class, the model would simply predict that class label. The same is true when predicting out-of-sample vectors because the average position of K'th neighbor is likely to overlap with the input sample due to the randomness of sampling. \n\nI don\u2019t really see why a memory-based model is introduced. The external memory is used for holding random samples. It is not clear how the model can use such random samples for making predictions. Also, the authors give no explanation to why it should help. The results also don\u2019t show the benefit of a memory-based model. Maybe the authors should look into models that output a set instead of a sequence since neighbors are more like a set in their structure.\n\nThe experimental results show clear improvements over basic baselines, so the method is doing some regularization. However, I'm not very familiar with datasets used here and their state-of-art. They are relatively low dimensional compared to usual datasets used in deep learning. It is not clear if the method can scale to high dimensional data such as images. The vanilla neural network is not really a strong baseline here. Since the authors proposed a regularization technique, it should be compared with other regularization techniques in neural networks.\n\nPros:\n- a simple idea\n- encouraging experimental results\n\nCons:\n- confusing read\n- no clear intuition is given\n- restricted to low-dimensional datasets\n- strong baselines needed\n- the plots are too small to see (impossible to see when printed)\n\nOther comments:\n- The authors are using the term \"feature vector\" to refer to a data point. However, in the context of neural networks, \"feature vector\" often means a hidden representation of a neural network. \n- why repeat \"randomly draw B samples\" R times? why not directly sample RxB samples?\n- \"it is quite implausible that only affine ...\" any evidence to support this?\n- The model is not really \"sequence-to-sequence\" since the input is not a sequence.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}