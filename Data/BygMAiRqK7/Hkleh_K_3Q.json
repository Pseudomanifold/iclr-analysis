{"title": "Interesting attempt on theory of entropic GANs ", "review": "The contribution of the paper is to show that WGAN with entropic regularization maximize a lower bound on the likelihood of the observed data distribution. While the WGAN formulation minimizes the Wasserstein distance of the transformed latent distribution and the empirical distribution which is already a nice measure of \"progress\", having a bound on the likelihood can be interesting.\n\nPros:\n+ I like the entropic GAN formulation and believe it is very interesting as it gives access to the joint distribution of latent and observed variables. \n+ While there are some doubtful statements, overall the paper is well written and easy to read.\n\nCons:\n- The assumption of injectivity of the generator could be problematic, as it might not be fulfilled due to mode collapse.\n- I feel the theory is not very deep. Since one has a closed form of the transportation map (Eq. 3.7), the likelihood of the data is obtained by marginalizing out the latent space. However, this assumes that the inner dual maximization problem is solved to stationarity so that Eq 3.7 holds, which is not the case in practice (5 discriminator updates).\n- Thus in Sec. 4.1 for the likelihood at various points in training it is not clear what is actually happening.\n- Sec 4.3 for unregularized GANs might be problematic. In general, the transportation plan is not a density function, so I'm not certain whether Theorem 1 / Corollary 2 still hold. Furthermore, the heuristic for \"inverting\" G^* is very crude. \n\n- There are also some minor problematic statements in the paper. While they can be easily fixed, they give me doubts:\n  * The original VAE paper is not cited in the introduction for VAEs\n  * The 2013 paper by Cuturi cited on page 2 has nothing to do with \"computational aspects of GANs\". It is about fast computation of approximate OT between two discrete prob. measures. \n  * First-order / second-order Wasserstein distance is I think a bit unusual name for W_1, W_2\n  * On pg. 4, the point of the entropy term is to make the objective strongly convex. Strict convexity has no computational benefits.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}