{"title": "Convincing paper about of a potentially not-too-widespread technical issue.", "review": "The paper studies the problem of distilling a student probabilistic model (that\nis easy to sample from) from a complex teacher model (for which sampling is\nslow).  The authors identify a technical issue with a recent distillation\ntechnique, namely that positive gradient signals become increasingly unlikely\nas the dimensionality of the teacher model increases.  They then propose two\nalternative technique that sidestep this issue.\n\nThe topic is definitely relevant.  The paper focus on a single method for\nprobability distillation, which limits the significance of the contribution.\n\nThe paper is very well written and well structured.  Section 4 is may be a bit\ntoo dense for the uninitiated; it may make sense to clarify that calT and calS\nrefer to the teacher and student models---it is only obvious while reading this\nsection for the second time around.\n\nAll contributions seem novel.  The fact that the (reverse) KL can lead to bad\nmodels is known; the issue identified in this paper, however, seems novel.\n\nI could not spot any major flaws with the paper.\n\nThe evaluation is satisfactory.  The issue of KL-based training is very clear,\nas is the advantage of the encoder-decoder alternatives.\n\nI especially appreciated the link between distillation and encoder-decoder\narchitectures.\n\nDetailed comments:\n\n1 - How widespread is the issue identified in this paper?  In other words, is\nreverse KL realistically used in applications other than probability\ndistillation?\n\n2 - It is unclear to me why Proposition 2 is important.  This should be\nexplicitly stated.\n\n3 - It would make sense to add a forward pointer to Figure 3c in Section 3.1,\nto provide another example of mode-seeking.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}