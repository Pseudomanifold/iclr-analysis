{"title": "Interesting ideas but the paper has some issues", "review": "This paper proposes new methods for distilling a feed-forward generative model (student) from an autoregressive generative model (teacher) as an alternative to the reverse-KL divergence. The first part of the paper analyses optimization issues with the reverse KL divergence while in the second part of the paper alternatives are proposed (x-reconstruction and z-reconstruction).\n\nDetailed comments:\n\n1.\nIn abstract and other places: \"sparse gradient signal from the teacher\".\nSparsity implies that many of the values are exactly zero, while Section 3.1 seems to imply that some of the values might be small (or pointing towards the origin).\n\n2.\nIn Section 3.1 and 3.2 the authors discuss a potential failure mode of the reverse KL:\n\nBut, proposition 1 boils down to the fact that if the student's mass is more spread out than the teacher is some direction, that it should shrink that mass closer to zero as well.\n\nIn the example of the paper: if an eigenvalue of T is smaller than 1, it would mean that the student which is spherical Gaussian, would adjust its probability mass to also be smaller in that eigenvector's direction.\n\nAs training progresses, the students mass would be much closer to the teacher and the probability of 'pointing away' from the origin would be about as likely as pointing towards.\n\nSo it's not clear at all that the described property is problematic for optimization, as it could as well be interpreted as the student trying to fit the teacher's distribution better.\n\n3.\nWas the KL between P_S(x_i | z_<i) and P_T(x_i | x_<i) computed analytically? If these conditional distributions are Gaussian (which they are in many of the examples) this should be trivial.\n\n4.\nSection 4 about the neural vocoder needs to be expanded: many details are missing here and although it's one of the more important experiments in the paper it's relatively neglected compared to the other parts of the paper.\n\n5.\nIn the Section 4: the experiment with reverse-KL is a straw man comparison: For audio the reverse KL was only proposed in combination with the power loss (Oord et al). Two additional experiments would make the result a lot stronger: KL+power-loss and X-recon+power-loss. Because if the x-recon method does not work well together with the power-loss, its practical applicability seems limited.\n\n\nThe proposed methods are interesting, because they are elegant and seems to work reasonably well on the tasks tried. The first part of the paper about gradient sparsity/orientation needs to be addressed. Section 4 should be expanded and an additional comparison should be made.\n\nI would change my rating if these issues were addressed.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}