{"title": "review", "review": "This is a very well-written paper which proposed a way to accelerate the value-iteration of MDP. The method is the so-called \"Anderson-Mixing\" method. It replaces the policy evaluation step by solving a smaller linear equation: find a linear combination of a few historical values to represent the value of the current policy. The paper also presents a very nice explanation of why such a modification of VI accelerates VI. The paper also extends the method to DQN and shows a very nice acceleration. The experiments are convincing and interesting.\n\nI only have two concerns: \n\n1) In section 4, the convergence proof is shown but the contraction is only gamma. This is the same as the original VI. Of course, this is the worst case best bound. Is it possible to show a result that the modified-VI is always better than the original VI?\n\n2) In section 4, the dependence on k has not been studied. But k actually critically affects the time complexity. Is it possible to obtain convergence proof depending on k?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}