{"title": "Intriguing results. But don't similar methods achieve similar things with similar mechanisms?", "review": "The results are intriguing. However, similar methods like BN-LSTM [3] and Variational RNNs [4] achieve arguably the same with very similar mechanisms. We do not think they can be considered as orthogonal. This should be addressed by the authors. Also, hard long-term experiments like sequentially predicting pixels (like through MDLSTM-based PixelRNN) or language modelling should be favoured over short sentence image captions. \n\nIt is possible that we will improve our ratings once our concerns are addressed.\n\nPaper Summary:\n\nThe authors claim that the gradient along the computational path that goes through the cell state (the linear temporal path or A gradient) of an LSTM carries information about long-term dependencies. Those gradients can be corrupted by the gradient of all other computational paths (i.e. the B gradient). They claim that this makes it hard to learn long-term dependencies and has, therefore, significant negative effects on the convergence speed, training stability, and generalisation performance. They propose a method called h-detach and run experiments on the delayed copy task, sequential MNIST, permuted sequential MNIST (pMNIST), and caption generation on the MS COCO dataset. All show either somewhat improved performance or much more stable learning curves. At every step, h-detach randomly drops all gradients that flow through the h of the standard LSTM, the B gradients, and only keeps the ones from the linear temporal path, the A gradients. Experiments also suggest that the A gradients carry more long-term information than B gradients and that LSTMs with h-detach do not need gradient clipping for successful training.\n\nPositive:\n\nThe paper is written clearly. It is well structured and well motivated. H-detach is simple, effective, and somewhat novel (see below). Experiments indicate that its main benefit is training stability as well as minor performance improvements.\n\nNegative:\n\nWe are not sure how significant these results are for the following reasons:\n\n- MS COCO image caption generation is the only more challenging dataset, but it seems a bit misplaced as it has very short sentences, while the authors motivate their work through a focus on long-term dependencies. Why not apply h-detach to a language model such as [1] with official online implementations, e.g., [2]. A setting with PixelRNN [6] based on MD-LSTM [7] would also be a great testbed for h-detach.\n\n- The purpose of h-detach is to scale down the B gradients. However, methods which apply e.g. BatchNorm to the hidden state learn a scale parameter which could be learned by the network explicitly. For the backward pass, this has the effect of scaling down the B gradient. Consider e.g. [3] which also achieves similar training stability on sequential MNIST and pMNIST with little overhead. \n\n- Another very related method is [4] which properly applies a random dropout mask over the recurrent inputs that is shared across timesteps of an RNN. We think that h-detach is essentially achieving the same in a similar way.\n\nProblems with Introduction and Related Work Section:\n\n- The vanishing gradient problem was first described by Hochreiter in 1991 [5] (not by Bengio in 1994). \n\n- Intro mentions GRU as if it was separate from LSTM. Clarify that GRU is essentially a variant of vanilla LSTM with forget gates [8]. Since one gate is missing, GRU is less powerful than the original LSTM [9]. \n\n[1] Zaremba et al. \"Recurrent neural network regularization.\" arXiv:1409.2329 (2014).\n[2] https://www.tensorflow.org/tutorials/sequences/recurrent\n[3] Cooijmans et al. \"Recurrent batch normalization.\" arXiv:1603.09025 (2016).\n[4] Gal et al. \"A theoretically grounded application of dropout in recurrent neural networks.\" NIPS 2016.\n[5] Hochreiter, Sepp. \"Untersuchungen zu dynamischen neuronalen Netzen.\" Diploma thesis, TUM (1991)\n[6] Oord et al. \"Pixel recurrent neural networks.\" arXiv preprint arXiv:1601.06759 (2016).\n[7] Graves et al. \"Multi-Dimensional Recurrent Neural Networks\" arXiv preprint arXiv:0705.2011 (2011).\n[8] Gers et al. \u201cLearning to Forget: Continual Prediction with LSTM.\u201c Neural Computation, 12(10):2451-2471, 2000. \n[9] Weiss et al. On the Practical Computational Power of Finite Precision RNNs for Language Recognition. arXiv:1805.04908.\n\n\nComments after rebuttal:\n\nThe  paper has clearly improved. \n\nIt leaves a few questions open though. For example, it is surprising that h-detach doesn't work on language modelling since Dropout-LSTM and BN-LSTM clearly improve over vanilla LSTM in this case (if not every case). In the new version, the authors only reference it in one or two sentences but don't discuss this in detail. \n\nWhen dropout is mentioned, one should also mention that dropout is a variant of the old stochastic delta rule:\n\nHanson, S. J. (1990). A Stochastic Version of the Delta Rule, PHYSICA D,42, 265-272.  See also arXiv:1808.03578 \n\nNevertheless, we now think that this is a very interesting LSTM regularization paper that people who study this field should probably know. We are increasing the score by 2 points!\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}