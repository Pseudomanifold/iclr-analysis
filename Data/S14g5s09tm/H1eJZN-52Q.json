{"title": "Contribution of temporal attention is not evaluated on all tasks - decisions in experiments not justified throughout and take-home message missing", "review": "The paper attempts multimodal representation of video and text through an attention layer that allows weighted temporal pooling. The approach was tested on a collection of datasets including a newly introduced dataset, with the embedding and evaluated on three tasks: zero-shot classification, activity clustering and captioning.\n\nThe paper is easy to read in general and the approach is scientifically sound. The need for an autoencoder in multimodal embedding has been proven for a variety of modalities including image-text, video-text, image-audio and video-audio. The contribution here is thus focused on temporal pooling through a learnt attention layer.\n\nHowever, the paper has a mix of tasks (3 tasks tested), without a conclusive understanding of the effect of the various loss functions on the learnt space. As the importance of various losses changes per task and dataset, the take-away message from the work is not obvious. Additionally, using unpaired data, proposed through a large-scale dataset is not obvious. The paper concludes that related data is required but how related data can be collected remains unexplored.\n\nThe evaluation for the unsupervised discovery seems biased \u2013 1NearestNeighbour is used as opposed to the more balanced mAP on ranking all test sequences as opposed to top-1. \n\nThe collected dataset, which is a contribution of the paper is also poorly explained. The authors collect \u2018dense annotations\u2019 but it is not clear how many annotators were used, and what instructions they were given. The paper does not give examples of the collected annotations and how these differ from previous annotations available with the dataset (Fig 4).\n\nAppendix 1 concludes with sentences proposed to annotate UCF. These seem to apply per action and it\u2019s not clear how they scale to the different instances, e.g. Action Surfing (85) is assigned to a male caption \u2018a man is\u2019, action 100 to a woman and action 96 to groups of people \u2018people are riding\u2019. This distinction is not obvious in all the instances of the dataset and such captioning might have significantly biased the results.\n\nOverall, there is little explanation of the decisions made to produce the comparative results. The novelty is limited to the attention pooling, which is not evaluated on all the three tasks. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}