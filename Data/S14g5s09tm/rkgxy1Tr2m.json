{"title": "limited novelty and wrong experimental settings", "review": "This paper proposes a joint embedding model that aligns video sequences with sentences describing the context (caption) in a shared embedding space. With the space, various tasks such as zero-shot activity recognition and unseen video captioning can be performed. The problem tackled in this paper is interesting. However, the approach proposed is limited in novelty and there are some serious flaws in the experimental settings. So overall, this paper is not yet ready for publication. \n\nPros:\n\n\u2022\tThe overall bidirectional encoder-decoder architecture for learning a shared embedding space is sensible. It is also interesting that adversarial training is introduced so that unlabelled data can be utilized. \n\u2022\tAdditional annotations are provided to two activity recognition datasets, creating new benchmarks.\nCons\n\u2022\tMy first concern is the limited novelty of the work. Although I am not aware of a joint embedding learning model that has exactly the same architecture and formulation, the model is closely related to many existing ones both in zero-shot learning and beyond. More specifically,\no\tThe overall framework is similar to \u201ccorrelational neural networks\u201d, Neural Computation, 2016 by Chandar et al. This should be acknowledged.\no\tThe connections to CyclyGan and its variants for image-to-image style transfer is obvious, as pointed out by the authors.\no\tMore importantly, there are quite a few closely related zero-shot learning (ZSL) papers published recently. Although they focus on static images and class name, rather that image sequences and sentences, I don\u2019t see any reason why these models cannot be applied to solve the same problem tackled in this paper. In particular, the autoencoder architecture was first used in ZSL in E. Kodirov, T. Xiang and S. Gong, \"Semantic Autoencoder for Zero-Shot Learning\", in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, Hawaii, July 2017. This work is further extended in Chen et al, \u201cZero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Network\u201d, cvpr18, now with adversarial learning. Similarly, variational autoencoder is adopted in Wang et al, Zero-Shot Learning via Class-Conditioned Deep Generative Models, AAAI 2018.  Note that the joint embedding spaces in these studies are the semantic spaces \u2013 attribute or word vector spaces representing the classes. In contrast, since the semantic modality is a variable-length word sequences, this is not possible, so a third space (other than the visual feature space or semantic space) is used as the embedding space. Beyond these autoencoder based models, there are also a number of recent ZSL works that use a conditional generative model with adversarial loss. Instead of learning a joint embedding space where the visual and text modalities are aligned and compared for recognition, these works use the text modality as condition to the generative model to synthesize visual features for the unseen classes followed by a conventional supervised classifier. The representative one of this line of work is Xian et al, \u201cFeature Generating Networks for Zero-Shot Learning, cvpr18\u201d.\no\tIn summary, there are too many existing works that are similar to the proposed one in one or more aspects. The authors failed to acknowledge most of them; moreover, it is not argued theoretically or demonstrated empirically, why combining different approaches together is necessary/making fundamental differences.\n\n\u2022\tMy second main concern is the experiment setting. This paper adopts a conventional ZSL setting in two aspects: (1) the visual features are obtained by a video CNN, I3D, which is pretrained on the large (400 or 600 classes depending on which version is used) Kinetics dataset. This dataset have classes overlapping with those in ActivityNet, HMDB and UCF101. So if these overlapped classes are used in the unseen class partition, then the ZSL assumption (the target classes are \u2018unseen\u2019) is violated. (2): The test data only contains unseen class samples. In practice, one will face a test set composed of a mix of seen and unseen classes. Under this more realistic setting (termed generalized ZSL in the ZSL community), a ZSL must avoid the bias towards the seen classes which provide the only visual data available during training. The two problems have been identified in the ZSl community when static images are considered. As a result, the conventional setting has been largely abandoned in the last two years and the \u2018pure\u2019 and \u2018generalized\u2019 settings become the norm; that is, there is no overlapping classes between the test classes and classes used to pretrain the visual feature extraction network; and both seen and unseen class samples are used for testing. The ZSL evaluation is only meaningful under this more rigorous and realistic setting. In summary, the experimental results presented in this paper are obtained under the wrong setting and the proposed model is not compared with a number of closely related ZSL models listed above, so it is not possible to judge how effective the proposed model is. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}