{"title": "Interesting observations!", "review": "The authors of this paper studied the popular belief that deep neural networks do information compression for supervised tasks. They studied this compression behavior with tanh and ReLU (and it's variants) activation functions which are saturating and non saturating in nature respectively. \n\nThe compression score is computed using Mutual Information Estimation which when computed are usually infinite. For finite mutual information values, noise can be added to hidden activations. For this purpose, two approaches namely Entropy Based Binning(EBAB) and adaptive Kernel Density Estimation(aKDE) were explored. EBAB adds noise to the hidden activations by binning and aKDE by Gaussian noise. Their results show that both EBAB and aKDE exhibit compression in case of ReLU, although this behavior is the strongest in tanh. \n\nFinally, When compression score was plotted against accuracy, higher rates of compression did not show significant correlation with generalization. Hence showing evidence that generalization(or good performance) can be achieved even without information bottleneck(information compression).\n\nQualms:\n1. Figure 7's description that ELU, Swish and centered softplus functions doing compression is not very apparent. \n2. Figure 9b: Regression line between compression score and accuracy shows a positive correlation between them. This seems contradictory to the inference.\n3. The experiments were done on a 5-layer network with 10-7-5-4-3 nodes respectively on a toy data of 12-bit binary vectors. The study could have included bigger networks with popular datasets which would give substantial support to the trend observed on toy data.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}