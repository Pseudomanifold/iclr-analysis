{"title": "promising approach, but the problem formulation has some flaws", "review": "The paper studies the behavior of two-timescale stochastic approximation methods in the two fundamental approximate dynamic programming problems of policy evaluation and optimal control. Specifically, the authors pose the problem of Bellman residual minimization as a saddle-point problem and apply stochastic gradient updates for the primal and dual variables, and show that this procedure converges to a certain fixed point under rather general conditions. Notably, the results are claimed to hold even when the objective function is not convex-concave. The authors also highlight that the eventual solution may not necessarily be sensible in general, and also analyze a variant in which the dual variables can be optimized with bounded error in each iteration. In this setup, the main result is showing that the proposed algorithm converges to a neighborhood of the optimal solution of the original problem, and the size of the neighborhood is controlled by the optimization error in the dual objective.\n\nI enjoyed reading the paper for the most part: the proposed approach is simple and natural (closely resembling GTD algorithms), and the core technical results seem plausible (although I didn't have the capacity to check the details this time). I appreciated the openness of the authors to discuss the limitations of their result in Theorem 4.3. The writing is generally good, up to some minor typos.\n\nThat said, I do have some relatively serious concerns about the nature of some of the results. Specifically, the objective function considered for optimal control seems fundamentally flawed and can be shown to lead to meaningless results even when optimized to zero error. To see this, observe that the objective (3.1) involves an expectation taken under the behavior policy \\pi_b which may be arbitrarily far away from the optimal policy. This can lead to easily-seen problems in extreme cases, e.g., when pi_b only visits a recurrent class of states that are not visited at all by the optimal policy, and all rewards are zero in this recurrent class and its neighboring states. Under these conditions, the constant zero function trivially minimizes the Bellman error, even though being a potentially terrible solution. This is a well-known issue with this objective, and it can be solved by making some stronger assumptions on the behavior policy and using importance weighting (as done by, e.g., Antos et al. 2008). Incorporating these elements in the present analysis seems nontrivial, although certainly not impossible.\n\nAnother concern is that the optimization oracle required for Theorem 4.5 is unrealistically strong, and not just due to computational issues. The problem is that the optimization target for the oracle considered in this theorem is not an observable quantity, but rather an expectation over random variables with unknown distribution (that can only be estimated with an infinite sample size). Making such an assumption effectively eliminates the main technical challenges related to the double sampling issue, so it feels a bit like cheating. In fact, one can use such an assumption to completely circumvent the need for a saddle-point formulation and prove convergence results for Q-learning-style methods directly in a single-timescale SA framework. Right now, this aspect remains hidden due to the writing---the authors should discuss this assumption more honestly in the paper.\n\nIn order to resolve the above issues, I suggest that the authors limit their presentation to policy evaluation problems where at least some of these technical problems are not present. I believe that a paper focusing only on policy evaluation should have sufficient merit for publication---but then again, rewriting the paper accordingly should be a bit too much to ask for in a conference revision. In either case, I am looking forward to hearing the opinion of the authors on the issues raised above.\n\nDetailed comments\n=================\n- pp.1, intro: \"the statistical and computational properties of the RL algorithms are well-understood under these settings\"---well, let's just say that *some* of these properties are well understood. E.g., I don't think that many people would consider \"concentrability coefficients\" to be really well-understood :)\n- pp.2, top: \"Bellman residue\" -> \"Bellman residual\" (many times in the paper)\n- pp.2, related work: \"our study\" -> \"we study\"\n- pp.2, bottom: \"could stringent\" -> \"could be stringent\"\n- pp.3, below (2.2): \"applying the Bellman optimality operator repeatedly gives the classical Q-learning algorithm\"---this is actually value iteration, Q-learning also uses other ideas related to stochastic approximation. Please clarify a bit.\n- pp.3, above Sec 3: \"saddle framework\" -> \"saddle-point framework\"\n- pp.4, 2nd paragraph: \"when the capacity of S is large\"---what does this mean?\n- pp.4, Eq.(3.3): extra \\cdot\n- pp.5, below (3.7): \"since the saddle point problem is nonconvex, we project the iterates\"---can you explain what this projection has to do with nonconvexity?\n- pp.5, below (3.7): \"such a condition on the learning rates ensures that the dual iterates asymptotically track the sequence [...]\"---this is technically only true if the faster-timescale can be first shown to converge at all, which is not trivial!\n- pp.6, Assumption 4.1: missing comma between Q_max and \\|\\nabla_\\theta...\\|. It would also be worth noting that this assumption technically excludes neural nets with RELU activations, so I'm not sure if your theory actually applies to the setup in the experiments.\n- pp.6, below (4.1): \"most online TD-learning algorithms uses [sic] linear function approximation\"---this is a quite weak argument given that the main contribution of the paper is precisely going beyond these settings; this claim suggests that linear FA methods are OK after all.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}