{"title": "an interesting approach to meta-learning, clear accept", "review": "This paper introduces a novel meta-learning approach to unsupervised representation learning where an update rule for a base model (i.e., an MLP) is meta-learned using a supervised meta-objective (i.e., a few-shot linear regression from the learned representation to classification GTs). Unlike previous approaches, it meta-learns an update rule by directly optimizing the utility of the unsupervised representation using the meta-objective. In the phase of unsupervised representation learning, the learned update rule is used for optimizing a base model without using any other base model objective. Experimental evaluations on few-shot classification demonstrate its generalization performance over different base architectures, datasets, and even domains.  \n\n+  Novel and interesting formulation of meta-learning by learning an unsupervised update rule for representation learning. \n+  Technically sound, and well organized overall with details documented in appendixes. \n+  Clearly written overall with helpful schematic illustrations and, in particular, a good survey of related work. \n+ Good generalization performance over different (larger and deeper) base models, activation functions, datasets, and even a different modality (text classification).\n\n-  Motivations are not very clear in some parts. E.g., the reason for learning backward weights (V), and the choice of meta-objective.  \n- Experimental evaluation is limited to few-shot classification, which is very close to the meta-learning objective used in this paper. \n- The result of text classification is interesting, but not so informative given no further analysis. E.g., why domain mismatch does not occur in this case?\n\nI enjoyed reading this paper, and happy to recommend it as a clear accept paper. The idea of meta-learning update networks looks a promising direction worth exploring, indeed. \nI hope the authors to clarify the things I mentioned above. Experimental results are enough considering the space limit, but not great. Since the current evaluation task is quite similar to the meta-objective, evaluations on more diverse tasks would strengthen this paper. \n\nFinally, this paper aims at unsupervised representation learning, but it\u2019s not clear from the current title, which is somewhat misleading. I think that's quite an important feature of this paper, so I highly recommend the authors to consider a more informative title, e.g., `Learning Rules for Unsupervised Representation Learning\u2019 or else. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}