{"title": "substantial step towards good unsupervised and local learning", "review": "The paper describes unsupervised learning as a meta-learning problem: the observation is that unsupervised learning rules are effectively supervised by the quality of the representations that they yield relative to subsequent later semi-supervised (or RL) learning. The learning-to-learning algorithm allows for learning network architecture parameters, and also 'network-in-networks' that determine the unsupervised learning signal based on pre and post activations. \n\nQuality \nThe proposed algorithm is well defined, and it is compared against relevant competing algorithms on relevant problems. \nThe results show that the algorithm is competitive with other approaches like VAE (very slightly outperforms).\n\nClarity\nThe paper is well written and clearly structured. The section 5.4 is a bit hard to understand, with very very small images. \n\nOriginality\nThere is an extensive literature on meta-learning, which is expanded upon in Appendix A. The main innovation in this work is the parametric update rule for outer loop updates, which does have some similarity to the old work by Bengio in 1990 and 1992. \n\nSignificance\n- pros clear and seemingly state-of-the-art results, intuitive approach, \n-cons only very modestly better than other methods. I would like to get a feel for why VAE is so good tbh (though the authors show that VAE has a problem with objective function mismatch).\n\nOne comment: the update rule takes as inputs pre and post activity and a backpropagated error; it seems natural to also use the local gradient of the neuron's transfer function here, as many three or four factor learning rules do. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}