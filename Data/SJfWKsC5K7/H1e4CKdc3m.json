{"title": "Interesting area but quite misleading on explanation vs. interpretation", "review": "\nSummary:\n=========\nThe paper proposes a technique to explain the embedding space quantitatively and semantically by distilling learned representations into an additive explainable model. Explanation generation for deep neural network models is a growing research area especially for mission critical applications that decision without explanation is unacceptable such as security and medical applications. More importantly, the focus on semantic explanations instead of (in addition to) the usual \u2018visual attention or pixel level correlation as explanation\u2019 is interesting. Although, the proposed method is interesting, I have several concerns. I have laid out my comments on the strength of the paper and my concerns as follows.\n\nStrength:\n======== \n- Distilling the representation into a separate model to explain rather than incorporating the explanation generation into a single model helps minimize performance degradation due to the additional constraint of generating explanation.\n\n- The introduction of semantic explanations is interesting, although the \u2018explanation\u2019s are not human-like explanations. See below for further discussion on this.\n\nWeakness:\n=========\n- The abstract could use a bit simplification and clarification to make it concise at the same time be inclusive of the key points of the proposed method. More specifically, the second sentence could be simplified to make it easy to follow.\n\n- The need for explanation was not strongly motivated in the introduction in the context of existing literature. Existing explanation generating and/or interpretable networks should be discussed and the proposed method should be presented with what gap in existing literature it is trying to address in a more concise form. It is a bit lengthy in current form.\n\n- Need to cite related literature on making claims such as \u201c\u2026Distilling knowledge from a pre-trained neural network into an additive model usually suffers from the problem of bias-interpreting.\u201d\n\n- The paper mixes interpretation with explanation. Interpretability and explainability are two different things. Invoking attributes of explanations in human cognition, generated explanations should mimic human explanations in that they should be human-understandable, concise, and at the right level of detail. What the introduction promised as semantic explanation was not delivered in the experiments. Explanations for why the network predicts a particular face as attractive are not laundry list of other attributes with positive and negative correlations. Explanations are composed. The authors are learning the inter-attribute relationship (correlations) and this is presented as \u2018explanation\u2019. This is disappointing. The title and other earlier discussions need to be revised not to confuse simple interpretation with explanation.\n\n- The \u2018explanations\u2019 are not as generic as they were advertised earlier. They are in fact specific to specific tasks. It is not obvious from the experiments on object parts and facial attributes tasks how this interpretation using correlation could be generalized to any task learned through a CNN.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}