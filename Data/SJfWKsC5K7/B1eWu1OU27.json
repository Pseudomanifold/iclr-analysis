{"title": "The content must be re-organized and a deeper evaluation is needed", "review": " \n=============================\nSUMMARY\n=============================\n\nThe paper proposes a method to address the task of model explanation for deep convolutional neural networks (CNNs). More specifically, given a (performer) model to be explained, it proposes to use pre-trained detectors of visual concepts as means to justify the the predictions made by the performer through an additive (explainer) model. This additive model operates on top of the response, i.e. the score, produced by the concept detectors and its weights are learned also through a deep model, i.e. a ResNet-152. \n\n The proposed method is evaluated on two specific explanation tasks. The first task is related to explaining the occurrence of an object in an image (object recognition) based on object parts. Experiments on this task are conducted on the Pascal-Parts dataset and cover various standard CNN architectures including AlexNet and various variants of VGG (VGG-S, VGG-M and VGG-16). The second task is related to the prediction of face attributes based on other attributes. Experiments on this task are conducted on the CelebFaces Attributes dataset and are based on the VGG-16 CNN architecture.\n Performance on both experiments is reported in terms of prediction accuracy and relative deviation, which compare the performance of the performer model with that of the explainer model.\n \n=============================\nSTRENGHTS\n=============================\n\nThe manuscript proposes to produce explanations derived from semantic concepts. Adding semantic concepts to visual explanations is a desirable characteristic in order to reduce possible ambiguity that may occur when highlighting image regions with strong influence on the decision made by the model. \n\nThe related work section of the manuscript seems is quite detailed and covers a good amount of work from the literature.\n\n=============================\nWEAKNESSES\n=============================\n\nThe content of the manuscript is not properly balanced. On the one hand, the starting sections are quite detailed and extended. On the other hand, the last sections of the are quite reduced. This affects the flow of the manuscript, which has a good start but results in an abrupt end.\n\nSimilar to the network dissection method (Bau et al, CVPR'17), the proposed method has the requirement of a set of visual concepts to be pre-defined. A suboptimal selection of these concepts will result in a set not representative or not sufficiently general concepts which will produce meaningless explanations. That is the possible set of explanations is bounded by the set of selected concepts.\nIn addition, the proposed has the strong requirement of pre-trained detector for each of these visual concepts.\n\nIn several parts of the manuscript, i.e Section 1 and 5, it is claimed that the proposed method allows to provide a semantic explanation of the logic followed by the CNNs in order to make predictions. At this point it is not clear to me how the proposed method effectively mimics the decision-making process followed by the performer models to reach a particular decision.\n\nIn the experiment from Section 4.1 it is stated that several CNN types (AlexNet, VGG-S, VGG-M and VGG-16) are considered. However, besides depth and spatial resolution of some of the layers, all these models are feed-forward CNN architectures. I would be more convinced if CNNs and perhaps another type of architecture, e.g. a RNN, were considered.\n\nIn Section 3.1, details regarding the computation of prior weights are provided. However, a closer analysis of this Section in conjunction with Section 4 suggest that the computation of this priors is tailored for the specific explanation tasks covered in the experiments conducted in Sections 4.1 and 4.2.\nIn addition, for the explanation tasks of Sections 4.1 and 4.2, a very suitable set of visual concepts were pre-defined.\nPerhaps I am missing something, but these two points make wonder about the generability of the proposed method.\n\nIn the quantitative evaluation from Section 4.3 the proposed method is only compared against a single baseline, i.e. the distillation loss to learn the explainer. \nThis evaluation seems quite limited when taking into account the significant amount of visual explanation methods that have been proposed recently and the fact that distillation loss used in the compared baseline is not necessarily designed for model explanation.\nIn addition, the discussion of results covered in Section 4.3 is quite reduced. The manuscript would definitively benefit from an extended discussion of its inner-workings (an ablation  study) and an extended comparison w.r.t. recent methods.\nFurthermore, I would recommend complementing the presented experiments with evaluations following protocols proposed in existing work, e.g. a) occlusion analysis (Zeiler et al., ECCV 2014, Samek et al.,2017), a pointing experiment (Zhang et al., ECCV 2016), or c) a measurement of explanation accuracy by feature coverage (Oramas et al. arXiv:1712.06302).\n\nFinally, there are several parts of the manuscript pointing towards the appendix. Some of these pointers, e.g. Sec. 4.3, link toward important content that should be on the main manuscript. Having inspected the appendix, it seems a good amount of experiments were conducted and significant amount of qualitative results are available for the proposed method. In my opinion the manuscript would benefit significantly from moving some of this content to the main manuscript and discussing it adequately.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}