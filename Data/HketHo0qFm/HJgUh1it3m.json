{"title": "Interesting idea that may only work in some specific cases", "review": "Given an MDP <S, A, T, R>, the paper suggests to learn both the optimal Q function of that MDP (denoted Q^+), but also that of the MDP <S, A, T, -R> (denoted Q^-). The basic idea is that min_a Q^-(s, a) could be a good action for the initial MDP. Based on this idea, the authors propose to combine Q^+ and Q^- with a linear combination in order to obtain what they call a hybrid policy. \n\nThe proposed idea is indeed interesting and I find the experimental results surprising. It is not clear to me why the policy obtained from Q^- does better than Q^+. Theoretically, this should not happen: if we have the exact optimal Q function, Q^-, for <S, A, T, -R>, the policy defined by argmin_a Q^-(s, a) in every state s may be suboptimal in <S, A, T, R>. Is there a good conjecture/explanation for why the policy induced by Q^- works so well in 2(a) and (b)?\n\nThe authors chose to report the results using off-line training, which seem to favor their proposition. What are the results for on-line training?\n\nIn the experimental part, I think the authors should also report the results of the method that consists in learning two Q^+ and combining them with an average. This baseline would help understand if the good performance of hybrid policies really comes from learning Q^-.\n\nObtaining hybrid policies faces one important issue, which is the need to perform two actions in the environment in a given state, one for Q^+ and the other for Q^-. Therefore, the proposition seems to be doable only when one has access to a simulator.\n\nThe writing is generally clear, but the paper should be checked for typos.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}