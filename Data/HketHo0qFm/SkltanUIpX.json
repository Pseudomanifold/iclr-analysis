{"title": "Interesting idea, but requires more rigorous evaluation", "review": "The paper proposes a method for improving the stability of reinforcement learning with value function approximation, e.g., deep Q-learning. The key idea is fitting a Q function to rewards, fitting another Q function to negative rewards, then estimating Q values using a linear combination of the two Q functions. The method is applied to DQN, double DQN, and on-policy actor-critic on the CartPole, Mountain Car, and Pendulum tasks in OpenAI Gym.\n\nThe writing isn't clear, especially in the introduction. Phrases like \"risky\", \"badness of a state\", and \"inverse policy\" are used without definition.\n\nThe experiments only test one value of \\lambda. Since \\lambda is the one hyperparameter that controls the degree to which the inverse rewards influence the Q value estimates, I think it is critical to test the performance of the proposed method under various values of \\lambda (e.g., by sweeping the unit interval in increments of 0.1).\n\nOne of the central claims is that the proposed Q value estimator gives more accurate estimates of returns than the estimators used in previous deep Q-learning methods. However, the experiments never compare the predicted Q values to the true values, as is done in [3].\n\nThe experiments only evaluate the proposed method on the CartPole, Mountain Car, and Pendulum tasks, which have very small action spaces. I suspect the benefit of the proposed method will be smaller in environments with a larger number of possible actions, since the inverse policy may fail to accurately estimate the values of actions that are neither the best nor the worst at any given time. \n\nOne of the central claims is that the proposed method improves the stability of Q-learning, but it is unclear how many random seeds were tested in Figure 2 and Table 2. It appears that only the data from one training run was used, and the reported standard deviations are computed using the last 10% of episodes in that single training run. Furthermore, the curves are smoothed using a moving average with a window size of 100 episodes. Together, these two details make it extremely difficult to evaluate the claim that the proposed algorithm is more stable, and also makes it difficult to evaluate the significance of the differences between the method's performance and the baselines. [1] shows how results on a small number of random seeds tend to not be reproducible.\n\n[1] https://arxiv.org/pdf/1709.06560.pdf\n[2] http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf\n[3] https://arxiv.org/pdf/1509.06461.pdf", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}