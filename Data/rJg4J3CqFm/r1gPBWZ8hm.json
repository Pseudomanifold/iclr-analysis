{"title": "A very nice and original work.", "review": "The paper \u2018Learning Discrete Wasserstein Embeddings' describes a new embedding method that,\ncontrary to usual embedding approaches, does not try to embed (complex, structured) data into an \nHilbertian space where Euclidean distance is used, but rather to the space of probability measures\nendowed with the Wasserstein distance. As such, data are embed on an empirical \ndistribution supported by Diracs, which locations can be determined by a map that is learnt from data.\nInterestingly, authors note a 'potential universality' for W_p(R^3) (from a result of Andoni et al., 2015), \nsuggesting that having Diracs in R^3 could embed potentially any kind of metric on symbolic data. \n\nExperimental validations are presented on graph and word embedding, and a discussion on visualization of \nthe embedding is also proposed (since the Diracs are located in a low dimensional space).   \n\nAll in all the paper is very clear and interesting. The idea of embedding in a Wasserstein space is \noriginal (up to my knowledge) and well described. I definitely believe that this work should be presented\nat ICLR. I have a couple of questions and remarks for the authors:\n - It is noted in section 3.2 that both Diracs location and associated weights could be optimized. Yet the authors \n   chose to only optimize locations. Why not only optimizing the weights (as in an Eulerian view of probability\n   distributions) ? The sentence involving works of Brancolini and Claici 2018 is not clear to me. Why weighting \n   does not improve asymptotically the approximation quality ? \n - Introducing the entropic regularization is mainly done for being able to differentiate the Wasserstein \n   distance. However, few is said on the embeddability of metrics in W^\\lambda_p(R). Is using an entropic \n   version of W moderating the capacity of embedding ? At least experimentally, a discussion could be made  \n   on the choice of the regularization parameter, at least in section 4.1. In eq. (9), it seems that it is not \n   the regularized version of W. ? \n - I assume that the mapping is hard to invert, but did the authors tried to experiment reconstructing an object \n   of interest by following a geodesic in the Wasserstein space ?  \n - It seems to me that authors never give generalization results. What is the performance of the metric approximation \n   when tested on unseen graphs or words ? This point should be clarified in the experiment.        \n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}