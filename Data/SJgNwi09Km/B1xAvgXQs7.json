{"title": "review", "review": "Revision post-discussion: The paper's notation and model has been clarified, and my concerns about the paper have been addressed. Proposing a latent tree structure on the latent space of generative models is a strong contribution, the model performs well and seems to find meaningful and interpretable structure in the latent space.\n\n\nThe paper proposes a latent tree superstructure for the latent space of VAE\u2019s. The idea itself is novel and interesting, and could have major impact in learning structured manifolds.\n\nThe overall presentation of the method is direct but slightly confusing. It seems that the zb grouping corresponds to different dimensions of the full z_i-vector of a single data point x_i. This should be made more explicit. \n\nThe method itself has three levels of groupings: the zb\u2019s, the conditioned variables Yb, and the connections between the Y\u2019s. The method is also called a  Bayesian Network, but the paper seems to avoid defining it as a BN. I wonder if the method could be presented in a simpler form, if all the structure is necessary, and if the method could be defined directly as a BN. For instance, why do the Y\u2019s have to have a hierarchical tree structure, wouldn\u2019t a \u201cflat\u201d grouping into zb's be sufficient? \n\nIn eq 2 the p(z) is defined as a mixture of Y-conditioned Gaussians, while in eq 4 its defined in the conventional encoder form N(z ; mu_x, sigma_x). These forms don\u2019t seem to be compatible with each other. The term H seems to be entropy, but its not explained. It can\u2019t be computed if we use the eq 2 definition of p(z). The interplay between these two structures is unclear. Furthermore, in fig 1 the tree is showed as a network (no arrows), while in fig 2 its a tree. I can\u2019t find the definition for the dependencies P(Y | Y\u2019), are these simply conditional density tables, or are they implicit? I also can\u2019t see how are the \\Sigma_{yb} defined. Are they of full rank? What is their dimension?\n\nThe inference sections are well motivated and efficient techniques are used. \n\nThe synthetic experiment has 4 dimensional \u201cz\u201d, but the \u201cW\u201d matrix is 10x2, these do not match. What is the connection between Y_1 and Y_2 (in fig4 there is a dependency between)? Why is the dependency undirected if the model is a tree? The fig4b does not show ground truth to assess how well the model fits. The experiment should also include comparisons to the mentioned earlier works, and show how they perform. Why is there an arrow from the green scatter to the z3/z4? The main problem of the synthetic example is that it does not demonstrate why the tree structure learning is useful. The experiment should highlight a case where there is a natural latent tree structure corresponding to some realistic phenomena in real datasets.\n\nThe section 4.3. shows that the proposed method does find better representations of the MNIST than VAE, but does not mention that there are numerous extended VAE methods (and others) that would perform better than the LTVAE here. Those should be at least acknowledged, and preferably compared to.\n\nThe main results of the paper are very good with great performance in clustering, and the facets and clusters look great. The system has clearly learnt meaningful latent structures.\n\nThere are no learning curves or running time analyses. One would expect the proposed method to be slow with multiple levels of inference (tree structure, tree parameters, AE networks), and this should be discussed. How large datasets can it handle?\n\nOverall the paper proposes a BN-style structure on VAE latent space with great performance, but somewhat incomplete experimental section, and some presentation issues.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}