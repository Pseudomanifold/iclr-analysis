{"title": "A good paper ", "review": "The authors provide a good idea to solve Rubik\u2019s Cube using an approximate policy iteration method, which they call it as Autodidactic iteration. The method overcomes the problem of sparse rewards by creating its own rewards system. Autodidactic iteration starts with solved cube and then propagate backwards to the state. \n\nThe testing results are very impressive. Their algorithm solves 100% of randomly scrambled(1000 times) cubes and has a median solve length of 30 moves. The God\u2019s number is 26 in the quarter turn metric, while their median moves 30 is only 4 hands away from the God\u2019s number. I appreciate the non-human domain knowledge part most because a more general algorithm can be used to other area without  enough pre-knowledges. \n\nThe training conception to design rewards by starting from solved state to expanded status is smart, but I am not very clear how to assign the rewards based on the stored states? Only pure reinforcement learning method applied sounds simple, but performance is great. The results are good enough with the neural network none-random search guidance. Do you have solving time comparison  between your method and other approximate methods? \n\nPros: -  solved nearly 100% problems with reasonable  moves.\n          -  a more general algorithm solving unknown states value problems.\n\nCons: - the Rubik\u2019s cube problem has been solved with other optimal approaches in the past. This method is not as competitive as other optimal solution solver within similar running time for this particular game.\n           - to solve more dimension cubes, this method might be out of time.  \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}