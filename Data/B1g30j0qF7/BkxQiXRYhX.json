{"title": "BAYESIAN CONVOLUTIONAL NEURAL NETWORKS WITH MANY CHANNELS ARE GAUSSIAN PROCESSES", "review": "Overall Score: 7/10.\nConfidence Score: 3/10. (This paper includes so many ideas that I have not been able to prove that are right due to\nmy limited knowledge, but I think that there are correct).\n\nSummary of the main ideas: This paper establishes a theoretical correspondence between BCNN with many channels and GP and\npropsoes a Monte Carlo method to estimate the GP corresponding to a NN architecture. It is a very strong and complete\npaper since its gives theoretical contents and experiments content. I think that it is a really good result that should\nbe read by anyone interested in Neural Network and GP equivalences, and that Machine Learning in general needs these kind\nof papers that establish this complicated equivalences.\n\nRelated to: The work by Lee and G. Matthews (2018) regarding equivalence between Deep Neural Networks and GPs and the\nConvolutional Neural Network framework.\n\nStrengths:\nTheoretical content, Experiments and methodology content (even a Monte Carlo approach) makes it a very complete paper.\nHaving been able to establish complicated and necessary equivalences.\n\nWeaknesses:\nVery difficult for newcomers or non expert technical readers.\n\nDoes this submission add value to the ICLR community? : Yes, it adds, and a lot.\n\nQuality:\nIs this submission technically sound?: Yes it is, it is a necessary step in GP-NN equivalence research.\nAre claims well supported by theoretical analysis or experimental results?: Yes, quite sure.\nIs this a complete piece of work or work in progress?: Complete piece of work.\nAre the authors careful and honest about evaluating both the strengths and weaknesses of their work?: Yes, they are.\n\nClarity:\nIs the submission clearly written?: Yes, but I suggest giving formal introductions to some concepts in the introduction\nand include a figure with the ideas given or the equivalences.\nIs it well organized?: Yes, although sometimes section feel a little but put one after the another. More cohesion would be\nadded if they are introduce before.\nDoes it adequately inform the reader?: Yes.\n\nOriginality:\nAre the tasks or methods new?: The monte carlo is new, the other methods not but the task of the equivalence is new.\nIs the work a novel combination of well-known techniques?: It is kind of a combination, but the proposed ideas are new, it is very theoretical.\nIs it clear how this work differs from previous contributions?: Yes, authors bother in explaining it clearly.\nIs related work adequately cited?: Yes, this is a huge positive point of the paper.\n\nSignificance:\nAre the results important?: From my point of view, yes they are.\nAre others likely to use the ideas or build on them?: I think so, because the topic is hot right now.\nDoes the submission address a difficult task in a better way than previous work?: It is a new task.\nDoes it advance the state of the art in a demonstrable way?: Yes, clearly.\nDoes it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?: Yes, the theoretical approach is sound.\n\n\nArguments for acceptance: It is a paper that provides theory, methodology and experiments regarding a very difficult and challenging task that add value to the community and makes progress in the area of the equivalence between NN and GPs.\n\nArguments against acceptance: I do not have.\n\nTypos:\n\n-> Define the channel concept in introduction.\n-> Put in bold best results of the experiments.\n-> Why not put \"deep\" in the title?\n-> In the introduction, introduce formally a CNN. (brief)\n-> Define the many channel limit.\n-> Put a figure with the equivalences and with the contents of the paper explaining a bit.\n\n\n\nAfter rebuttal:\n=============\n\nAuthors have addressed many topics that not only I but rev 3 address and hence I score this paper with a 7 and recommend it for publication.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}