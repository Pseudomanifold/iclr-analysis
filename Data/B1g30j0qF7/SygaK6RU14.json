{"title": "REVIEW OF DEEP BAYESIAN CONVOLUTIONAL NETWORKS WITH MANY CHANNELS ARE GAUSSIAN PROCESSES", "review": "The paper establishes a connection between  infinite channel Bayesian convolutional neural network and Gaussian processes. The authors prove that taking the number of channels in a Bayesian CNN to infinite leads to a GP with a specific Kernel (GP-CNN) and provide a Monte Carlo approach to evaluate the kernels when it is intractable. They show that without pooling the kernel fails to maintain the equivariance property that is achievable with a CNN without pooling.  GP-CNN with pooling maintains the invariance property. They make extensive  experimental comparison with CNN, demonstrating that as the number of channels become large, CNN achieve performance close to a GP-CNN. A discussion on reasons for best CNN to give a performance better than GP-CNN (especially with pooling), and a experimental comparison with finite width Bayesian CNN would have made the paper more concrete.  The paper has both strong theoretical and experimental contribution, and is also very relevant to the ICLR conference.\n\nQuality\n\nThe paper provides a theoretical connection between Bayesian CNN with infinite wide channels and Gaussian processes with a recursive kernel (GP-CNN). The derivations and arguments seem correct. The experiments are conducted comparing the performance of SGD trained CNN with  GP-CNN, and other models on mainly on CIFAR-10 data set. \nHowever, some discussion and clarity on the following points will be  useful to improve the paper.\n\n- (Page 5) on convergence of K^l :  From Equations (3) and (4), it can be seen that  K^l converges to  C(K^{l-1}), with C(K^{l-1}) defined slightly different from the paper, in that the expectation over z  is taken w.r.t z~ N(0;A(K)) instead of z ~ N(0; K). Is this equivalent to the expressions (7) and (8) described in the paper for a non-linear function \\phi ?\n- Experimental comparison with Bayesian CNN, demonstrating the effect of increasing the number of channels.\n- (Page 7) GP-CNN with pooling :  Paper proposes subsampling one particular pixel to improve computational efficiency. Has some experiments been performed to evaluate the performance of this approach ? How accurate is this approach ?\n- Discussion on the positive semi-definiteness of the recursive GP-CNN kernel\n- More explanations on why the best SGD-trained CNN gives a better performance than GP-CNN, especially with pooling. Does the Monte-Carlo approximation of GP-CNN kernel  computation could impact this performance? I suppose hyper-parameters of the GP-CNN kernel are not learnt from the data, could this result in a lower accuracy  ?\n- Discussion on learning the hyper-parameters of the GP-CNN kernel and its impact on the performance of the model. \n- Demonstrate  through some sample figures that GP-CNN with pooling achieves invariance while GP-CNN with out pooling fail to capture it.\n- Is the best result on CIFAR-10  achieved using the proposed method ? See Deep convolutional Gaussian processes by Kenneth Blomqvist, Samuel Kaski, Markus Heinonen\n- Include the results with CNN-GP both with pooling and without pooling in Table 1 and Table 2. \n- Provide the results of best SGD trained CNN against CNN-GP, both with pooling, as in Figure 3.c. Is the same trend observed in this case also ?\n- Experimental comparison and results on other Image datasets, specifically MNIST. Does the same observations hold on MNIST too ? \n\nClarity\n\nThe paper is relatively well written and clearly provides main ideas leading to the results. However, notations could have made more succinct, and figures could have been more legible( Axis labels are missing for some figures in Figure 3, and provide legends wherever possible). The is also an ambiguity in what CNN-GP refers to, with pooling to without pooling.\n- The term CNN-GP is overloaded in many places in the experimental section. I guess in Table 1, its CNN-GP without pooling, while in Table 2, its CNN-GP with pooling. Kindly make the distinction clear in the nomenclature itself, by calling one of them by a different name. Its also not clear when they mention SGD trained CNN, if it is with pooling or without pooling.  \n- What is the difference between the top and bottom pair of figures in Figure 3 (b). Why is the GP performance different in top and bottom cases.?\n- What does 10, 100, 1000 correspond to in Figure 3 ? Please explain it in caption.\n\nOriginality\n\nPrevious works of  Lee and G. Matthews (2018) had shown the equivalence between Deep Neural Networks and GPs. This paper has extended it  to deep convolutional  neural network setting, but is interesting in its own way. The have come up with an equivalent kernel corresponding to infinite wide Bayesian convolution neural network and provided a monte-carlo approach to compute it. Along with the theoretical contribution, they have also provided extensive experimental comparison. \n\nSignificance\n\nThe paper has made significant contributions connecting the Bayesian convolutional neural networks with Gaussian processes, in deriving the equivalent kernel for GPs, and in demonstrating the performance of the proposed approach on Image datasets", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}