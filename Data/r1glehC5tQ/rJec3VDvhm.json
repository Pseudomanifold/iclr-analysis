{"title": "Interesting experiments but with major questions on defensive distinction. ", "review": "In this paper, the authors proposed 'defensive distinction' to address questions: Are adversarial examples distinguishable from natural examples? Are adversarial examples generated by different methods distinguishable from each other?\n\nI have some major concerns about this submission.\n\n1) The presentation of this work should further be improved. It contains many vague sentences. For example, \"Unfortunately, even state-of-the-art defense approaches such as adversarial training and defensive distillation still suffer from major limitations and can be circumvented.\" I really hope I can see some justifications based on authors' approach for this argument. Also, the definition of 'AdvGen-Model' is not clear. Do you mean Adversarial attack generator knows the network model (i.e., white-box attack)? It is also not clear that how representative scenarios and cases in Table 1 affect the implementation of the proposed experiments (implementation details rather than results). \n\n2) The technical contribution of this paper is weak, and the experiments are not enough to support its main claim. MNIST is a simple dataset, please try larger and more complex datasets. The contribution of the current version is limited. \n\n ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}