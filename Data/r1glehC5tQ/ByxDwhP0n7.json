{"title": "Not enough depth", "review": "Defensive Distinction (DD) is an interesting model for detecting adversarial examples. However, it leaves some key aspects of defense and distinction out. Firstly, one can argue that if you know the adversaries of your model you can simply regularize the model for them. Even if regularization doesn't work fully, the DD model still suffers since it can have its own adversarial examples. From distinction perspective, it would be hard to believe that every single adversarial example will be detected, at least not without some solid theoretical background. It seems that  and natural examples are being thrown at the DD model without an elegant approach. \n\nI have the following concerns about the visualization and understanding of what DD does, which I believe should have been the focus of this paper. It was not immediately clear, what the message of the paper is or the claimed message was too weak: detecting adversarial examples using a classifier. It was not immediately clear why this is a good idea (since an adversarial example can be an adversary of both original network and DD) or what the DD learns.\n\nFurthermore, from experimental perspective, it is not sufficient to just perform experiments on one dataset, specially if the claim is big. You should consider running your model on multiple datasets and reporting what each DD learned. Furthermore, you should establish better comparison and back your claims with proper references. Some claims were too strong to believe without reference. \n\nI do look forward to seeing more about the visualization and intriguing properties which may arise from continuation of your studies. In the current state, I vote to reject until a more clear demonstration of your work comes out. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}