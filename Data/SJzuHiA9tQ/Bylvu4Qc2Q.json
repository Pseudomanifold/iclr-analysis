{"title": "Not enough ", "review": "This paper connects continual learning and GAN training together, and propose to use standard continual learning schemes (EWC etc) to improve GAN training.\n\nContinual learning for GANs is certainly an important problem to look at. Even though I like the problem, I'm not convinced with the solution provided by the paper. \n\nThe paper in it's current form, in terms of technical contributions and experimental analyses presented to support the hypotheses it started with, is not good enough to be accepted in ICLR. My comments:\n\n1) Catastrophic forgetting of discriminator: Interesting view about mode collapse. I have following concerns:\n(a) I like the view in which the training is shown as a sequential process. I wonder if we could solve the issue of catastrophic forgetting of discriminator by storing sufficient fake examples from previously generated samples from the generator. Storing previous generations has already been explored, however, just to prove the point that forgetting is an issue, it would be interesting to store enough samples for the mixture of Gaussian setting and analyse.\n\n(b) Why not thinking of catastrophic forgetting of generator? What if we constrain the generator to not-to-forget about previously generated samples by Fisher or something similar? In this case, every new task in the training process will have sufficient fake samples from all the modes.\n\n2) Lack of technical contributions: The approach, in which EWC or IS is being used to regulate the discriminator's parameters, seems to be straightforward. The issue of multiple parameters is being resolved by storing one Fisher/Score vector using moving average type scheme. This, to me, is almost same as Online EWC or EWC++. Both these papers have already addressed this issue and discussed them in great detail. How is this approach different?\n\n3) Not sure what exactly Fig 2 is conveying as D_1^{gen}, \\cdots, D_T^{gen} should almost be the same, so training on one and testing forgetting on other doesn't actually say much. I think we can't conclude anything from this figure, at least using MNIST experiments.\n\nMinor:\n4) I'm assuming that you call your method EWC/IS GAN (I think it wasn't explicitly mentioned in the paper). Why don't you call Seff et al. 2018 (they used EWC with GAN for the first time) work EWC-GAN? I personally feel that it's important to give acronyms so that it doesn't undermine previous works. Just to clarify, I'm not advocating the work by Seff et al..\n\n5) Please provide citations for mode collapse\n\nOnline EWC: Progress & compress: A scalable framework for continual learnin, ICML 2018.\nEWC++: Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence, ECCV 2018.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}