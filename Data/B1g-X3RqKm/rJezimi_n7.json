{"title": "ambitious goal and lack of approach", "review": "The paper provides empirical evidence that the generalization error scales inversely proportional to the log of number of training samples.\n\nThe motivation of the paper is well explained. A large amount of effort is put into experiments. The conclusion is consistent throughout.\n\nIt is a little unclear about the definition of s(m). From the definition at the end of Section 2, it is unclear what it means to fit the training data. It can mean reaching zero on the task loss (e.g., the zero-one loss) or reaching zero on the surrogate loss (e.g., the cross entropy). I assume models larger than a certain size should have no trouble fitting the training set, so I'm not sure if the curve, say in Figure 2., is really plotting the smallest model that can reach zero training error or something else.\n\nVarying the size of the network is also tricky. Most papers, including this one, seem to be confined by the concept of layers. Increasing the number of filters and increasing the number of hidden units are actually two very structured operations. We seldom investigate cases to break the symmetry. For example, what if the number of hidden units is increased in one layer while the number is decreased in another layer? What if the number of hidden units is increased for the forward LSTM but not the backward? Once we break the symmetry, it becomes unclear whether the size of the network is really the right measure.\n\nSuppose we agree on the measure of network size that the paper uses. It is nice to have a consistent theory about the network size and the generalization error. However, it does not provide any reason, or at least rule out any reason, as to why this is the case. For example, say if I have a newly proposed model, the paper does not tell me much about the potential curve I might get.\n\nThe paper spends most of the time discussing the relationship between the network size and the generalization error, but it does not have experiments supporting the hypothesis that harder problems are more difficult to fit or to generalize (in the paper's terminology, large beta_g and large beta_p). For example, a counter argument would be that the community hasn't found a good enough inductive bias for the tasks with large beta_g and beta_p. It is very hard to prove or disprove these statements from the results presented in the paper.\n\nThis paper also sends a dangerous message that image classification and speech recognition are inherently simpler than language modeling and machine translation. A counter argument for this might be that the speech and vision community has spent too much optimizing models on these popular data sets to the point that the models overfit to the data sets. Again these statements can be argued either way. It is hard to a scientific conclusion.\n\nAs a final note, here are the quotes from the first two paragraphs.\n\n\"In undergraduate classes on Algorithms, we are taught how to reduce one problem to another, so we can make claims about time and space complexity that generalize across a wide range of problems.\"\n\n\"It would be much easier to make sense of the deep learning literature if we could find ways to generalize more effectively across problems.\"\n\nAfter reading the paper, I still cannot see the relationships among language modeling, machine translation, speech recognition, and image classification.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}