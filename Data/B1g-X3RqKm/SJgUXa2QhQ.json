{"title": "Interesting paper, but requires more work", "review": "The authors propose to measure the power-law exponent to sort natural language processing, speech and vision problems by the degree of their difficulty. The main idea is that, while in general model performance goes up for most tasks if more training data becomes available or for bigger model sizes, some tasks are more effective at leveraging more data. Those tasks are supposed to be easier on the proposed scale.\n\nThe main idea of the paper is to consider the bigger picture of deep learning research and to put different results on different tasks into an overall context. I think this is an exciting direction and I strongly encourage the authors to continue their research. However, the paper in its current state seems not quite ready to me. The write-up is repetitive at times, i.e., 'There is not data like more data.' appears 7 times in the paper. Also, some parts are very informal, e.g., the use of 'can't' instead of 'cannot'. Also, the sentence 'It would be nice if our particular proposal is adopted, but it is more important to us that the field agree on a satisfactory solution than that they adopt our particular proposal.', though probably correct, makes the reader wonder if the authors do not trust their proposal, and it would better be replaced by alternative suggestions or deleted. Also, the claim 'This may help explain why there is relatively more excitement about deep nets in speech and vision (vs. language modeling).' seems strange to me - deep nets are the most commonly used model type for language modeling at the moment.\n\nFurthermore, I believe that drawing conclusions about tasks with the proposed approach is an over-simplification. The authors should probably talk about difficulties of datasets, since even for the same task, datasets can be of varying difficulty. Similarly, it would have been nice to see more discussion on what conclusions can be drawn from the obtained results; the authors say that they hope that 'such a hierarchy can serve as a guide to the data and computational requirements of open problems', but, unless I missed this, it is unclear from the paper how this should be done.", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}