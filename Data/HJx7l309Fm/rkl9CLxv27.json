{"title": "Interesting new method, though more thorough experiments are needed", "review": "This paper introduces a new method for multi-agent reinforcement learning. The proposed algorithm -- which uses shared critics at training time but individual policies at test time -- makes use of a specialised attention mechanism. The benefits include better scalability (as the dependency of the inputs is linear in the number of agents, rather than quadratic), and also being more amenable to diverse reward and action structures than the previous work. \n\n---------Quality and clarity---------\nThe paper is nicely written, and the ideas are developed in a clear fashion, if slightly verbose (the first 3 pages, though informative, might have been condensed a bit to make more room for the new algorithm). The problem is well-motivated and the benefits of the new algorithm are well showcased.\n\nOne negative point that does stick out is the bibliography, where papers that have been published for years (e.g. the Adam paper) are still referenced as arXiv preprints.\n\n---------Originality and significance----------\nAlthough attentive mechanisms have been around for a while, their use in this specific setting (learning shared critics for multi-agent RL) is, and yields desirable properties. The new algorithm opens the door for training in more complex environments, with a larger number of agents (although the number is still limited in the presented experiments).\n\nThe main issue I do see with the paper is its experimental section. \nThe two tasks are picked to showcase the benefits of the new approach. This does mean that the competing algorithms have to undergo significant changes (at least in the case of the DDPG-based methods), which takes away from the validity of the comparison. \n\nIdeally, there would be at least one other task on which the other algorithms have been trained on by their respective authors. As mentioned right before Section 4, MAAC can be used on continuous action spaces at the price of increased computational cost, so this should be doable.\n\n\nOverall, this is a nicely written paper which introduces an interesting new method for multi-agent RL, with promising initial results. A more thorough experimental section with slightly fairer comparisons would increase its quality significantly.\n\nPros\n- clear paper, easy to read\n- interesting application of attention mechanism to multi-agent RL\n- promising initial results\n\nCons\n- no comparison to related algorithms on tasks where they have already been evaluated externally\n- the amount of workers is still quite limited in the experiments", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}