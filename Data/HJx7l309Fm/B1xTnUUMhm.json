{"title": "Simple method, but gives insufficient insight in model behavior and how it could generalize", "review": "Summary\n\nAuthors present a decentralized policy, centralized value function approach (MAAC) to multi-agent learning. They used an attention mechanism over agent policies as an input to a central value function. \n\nAuthors compare their approach with COMA (discrete actions and counterfactual (semi-centralized) baseline) and MADDPG (also uses centralized value function and continuous actions)\n\nMAAC is evaluated on two 2d cooperative environments, Treasure Collection and Rover Tower. MAAC outperforms baselines on TC, but not on RT. Furthermore, the different baselines perform differently: there is no method that consistently performs well.\n\nPro\n- MAAC is a simple combination of attention and a centralized value function approach.\n\nCon\n- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents. \n- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.\n- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems. \n- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP). It is unclear how the model actually operates and uses attention during execution.\n\nReproducibility\n- It seems straightforward to implement this method, but I encourage open-sourcing the authors' implementation.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}