{"title": "Interesting direction but some important prior work is missing and evaluation does not yet convincingly support claims", "review": "In this work the authors propose and analyse generative models as defences against adversarial examples. In addition, three detection methods are introduced and an extension to deep features is suggested.\n\nMy main concerns are as follows (details below):\n* Important prior work is not mentioned.\n* Evaluation with direct attacks is only based on (very few) gradient-based techniques, many results are not reliable.\n* There are signs of gradient masking (the common problem of robustness evaluation, in particular of only gradient-based techniques are used).\n* The way detection rates are taken into account in the perfect knowledge scenario is confusing.\n\n### Style\nI like the idea of testing many different factorisation structures. However, that comes with the drawback that one needs to constantly check back what the abbreviations mean. Together with the three detection methods, the manuscript is quite confusing at times and should definitely be streamlined. One suggestion: remove the detection methods: I did not find any real conclusion about them but they are definitely side-tracking users away from the main results.\n\n### Prior work\nThere is at least one closely related prior work not mentioned here: the analysis by synthesis model [1]. This model uses a variational auto encoder to learn class conditional distributions and shows high robustness on MNIST. Please make clear what your contribution is over this paper (other than testing several other factorisations).\n\n### Evaluation problems\nThe robustness of models should be evaluated on different direct attacks ranging from gradient-based to score-based (e.g. NES [2]) to decision-based attacks [3]. Please take a look at [1] to see how a very extensive evaluation might look like. The results can be astonishingly different for different attacks, and so basing conclusion on only one or two attacks is dangerous (in particular if you only use gradient-based ones). One can also see that in your results, just check the variations you get between MIM and PGD. Also, rather then discussing (and showing in detail) results for individual attacks, the minimum adversarial distance for a given sample that can be found by any attack is much more comparable between models (which can also streamline the manuscript).\n\nOne can see signs of gradient masking in your results. For example, in Figure 3 the MIM attacks levels out at 20% for the DBX model. That can happen for iterative attacks if the gradient is masked. Similarly, in Figure 5 DBX-ZK (zero knowledge) is better in both accuracy and detection rate than DBX-PKK (which takes the KL-detection method into account and should thus either be better in accuracy or detection rate).\n\nMore generally, the perfect knowledge case, in which the attacker knows about the detector, should only count samples as adversarials which evade the detector and change the model decision. Thus, the detection rate should be zero. Otherwise I have no idea what trade-off between accuracy and detection rate you are actually targeting and how to compare the results.\n\nAlso, some intermediate results are conflicting with each other. E.g. in 4.1 you state \u201cthe usage of bottleneck is beneficial for better robustness\u201d, but for L2 this is not true.\n\nAlso, I am not sure how conclusive the grey-box and black-box scenarios really are: since the substitute is basically a DFX or DFZ, it\u2019s unsurprising that adversarials transfer best to those two models.\n\n### Minor\n * In 4.1 you say \u201cas they fail to find near manifold adversarials\u201d, but I don\u2019t see how there can be L-infty adversarials on MNIST that are on-manifold (remember, MNIST pixel values are basically binary). Plus, in the zero-knowledge scenario there is nothing that enforces staying on this manifold.\n * Result presentation (Figure 3/5 & Table 1) is very different for different attack scenarios, which makes them hard to compare. Please unify.\n * Is the L2 distance you report in Table 1 the mean (or median) distance to adversarial examples. If so, GBZ (for which you state that C&W \u201cfailed on attacking\u201d has actually a smaller mean adversarial distance than some other models (for which C&W is actually quite successful).\n * Grey-box scenario doesn\u2019t make a lot of sense: since the substitute is basically a DFX or DFZ, it\u2019s unsurprising that adversarials transfer best to those two models. A similar confounder makes the black-box results difficult to interpret.\n* Also, taking into account that the paper is two pages longer and thus calls for higher standards\n\nTaken together, I find the general direction of the paper very interesting and I\u2019d definitely encourage the authors to go further. At the current stage, however, I feel that (1) contributions are not sufficiently delineated to prior work, (2) the evaluation is not convincingly supporting the claims and that  (3) the manuscript needs to be streamlined (both in terms of text and figures).\n\n[1] Schott et al. (2018) \u201cTowards the first adversarially robust neural network model on MNIST\u201d (https://arxiv.org/abs/1805.09190)\n[2] Ilyas et al. (2018) \u201cBlack-box Adversarial Attacks with Limited Queries and Information\u201d ( [https://arxiv.org/abs/1804.08598)](https://arxiv.org/abs/1804.08598)) \n[3] Brendel et al. (2018) \u201cDecision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\u201d (https://arxiv.org/abs/1712.04248)", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}