{"title": "Missing many technical details and more thorough evaluation", "review": "This work replaces first convolutional layers with combination of Gabor and Schmidt filters with learnable parameters. However it does not provide enough technical details which would allow this work to be reproducible and the experimental section does not verify almost any of the design choices which would allow the reader to asses the main factors which lead to the obtained network performance .\n\nI do not believe that this paper is of sufficient technical quality for this conference, thus I would not recommend this paper for acceptance. I believe that the amount of changes required for addressing the following issues would require a resubmission.\n\nMainly, the text misses many technical details of the work, for example:\n- How are the derivatives of the Gabor/Schmidt filter parameters computed? Automatic differentiation? What are the resulting distributions of these parameters, considering that some of them have to be non-negative? (e.g. \\sigma for Gabor filters). How does they differ between each other after training? How are they initialised? Are the learning rates the same for each of the GO parameters?\n- What is the hit on performance when the filter parameters are initialised by hand and not trained? (e.g. the ones visualised in Figure 2 in appendix, which span the spatial frequency spectrum). What is the performance of a random initialisation?\n- What is the processing speed of the network, e.g. training time versus training time of the vanilla CNN?\n- What are the confidence intervals for the adversial stability experiment? (Table 3). The difference between the proposed algorithm and vanilla CNN are so small it might be easily a result of for example a favourable draw of the random rotations.\n- How does this method performs against other methods addressing the Conv1 approximation with fixed bases [1] or bases with learnable parameters? [2]\n\nAdditionally, it is not clear what the proof is proving as it is clear that a CNN has the ability to learn the Gabor/Schmidt filters exactly. However,  for this to hold the other way around and for the universal approximator theorem to hold a sufficient number of predefined filters would have to be used where the sufficient number of filters has no upper bound, thus reducing practicality of this proof. More trivially, without any upper bound on the number of predefined filters, one can simply use any bases (which are by definition injective) for generating the exact CNN filters which lead to a good performance. More interesting would be to show (empirically) that the predefined Gabor/Schmidt bases are more efficient for a sparse coding the manifold of CNN filters with good performance, which would have some practical implications regarding the aptness of these predefined filters.\n\n[1] Yao, Hu, et al. \"Gabor feature based convolutional neural network for object recognition in natural scene.\" Information Science and Control Engineering (ICISCE), 2016 3rd International Conference on. IEEE, 2016.\n[2] Qiu, Qiang, et al. \"DCFNet: Deep Neural Network with Decomposed Convolutional Filters.\" arXiv preprint arXiv:1802.04145 (2018).", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}