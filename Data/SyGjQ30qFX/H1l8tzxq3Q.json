{"title": "not convincing", "review": "This paper proposes a new framework for topic modeling, which consists of two main steps: generating bag of words for topics and then using RNN to decode a sequence text. \n\nPros:\nThe author draws lessons from the infoGAN and designed a creative object function with reconstruction loss and categorical loss. As a result, this paper achieved impressive outcome for topic modeling tasks.\n\nComments:\n1. High-level language is used to describe how to train two parts of the model, which is not technically clear. It would be better describe the algorithms in more details by listing steps for your algorithm in the section 3.3.\n\n2. For text generation experiments, why didn\u2019t you compare your model with any other related model such as SeqGAN or TextGAN? It is not so convincing to just use VAE+Wgan-gp as a baseline model.\n\n3. For qualitative analysis part, you just listed some of your generated sentences for proving the fluency and relevance. Why didn\u2019t you use some standard metrics for evaluating the quality of the text? I cannot judge the quality of your model through these randomly selected sentences.\n\n4. As you mentioned in this paper \u201cyour model can be easily combined with any current text generation models\u201d, have you done any experiments for demonstrating the original text generation model will get better performance after applying your framework? \n\nMinor comments:\n1. On page 2 and page 4, you mentioned \u201cthe third term in (2)\u201d. According to my understanding, this should be equation 1 instead. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}