{"title": "Good paper, some things are oversold", "review": "This paper attempts to learn layers of NNs greedily one at a time by using kernel machines as nodes instead of standard nonlinearities. The paper is well-written and was an interesting read, despite being notation heavy. \n\nI think the interpretability claims have some merits but are over-stated. Furthermore, the expressive power of universal approximation through kernels holds only asymptotically. So I am not sure if the authors can claim equivalence in expressive powers to more traditional NNs theoretically. I have some additional questions about the paper, and I am reserving my recommendation on this paper till the authors answer them. \n\n1) Since individual node is simply a hyperplane in the induced kernel space, why not just specify the cost function as the risk + \\tau * norm(weights) ?  What is the benefit of explicitly talking about gaussian complexities and delineating Theorem 4.2 when the same can be achieved by writing a much simpler form? Lemmas 4.4 and 4.5 should be straightforward extensions too if just used in this form since Lemma C.1 follows easily, and again could be simplified a lot by just using the regularized cost function. Am I missing something here?\n\n\n2) Lemma 4.3 assumes separability (since c should be > a for \\tau to be positive) of classes, and also balanced classes (since number of positives = number of negatives). Why are these assumptions reasonable ? I understand that the empirical evaluation presented do justify the methodology, but I am wondering if based on these assumptions the theoretical results are of any use in the way they are currently presented. \n\nMinor :\nBelow Def 4.1  \"to a standard normal distribution \" should be \"according to P\".\nSome typos, please proof read e.g. spelling error \"represnetation \". ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}