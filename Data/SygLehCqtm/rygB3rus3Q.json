{"title": "Promising idea but falls short in write-up / evaluation", "review": "Thanks for the detailed responses. After reading the author response and the updated paper, I am satisfied on several of my concerns, many of which were due to the writing in the earlier submission. The updated results on various comparisons are also good.  I have updated my score accordingly. Some qualitative analysis of the results would have been nice -- examples of protein pairs where they do well and other methods have difficulty as they don't use the structural similarity info / global sequence info used by this paper. But maybe those can be in a journal submission.\nMy only remaining concern is on the lack of reporting average performance on the test data (which used to be the norm until recently for papers submitted to ML conferences).\n\n\nSummary:\nThis paper proposes an approach for embedding proteins using their amino-acid sequences, with the goal that embeddings of proteins with similar structure are encouraged to be close in the embedded space. A stacked 3-layer Bi-directional LSTM is used for embedding the proteins. The structure information is obtained from the SCOP database which is used in an ordinal regression framework, where the output is the structural similarity and inputs are the embeddings. Along with the ordinal regression, another loss term to incorporate contacts of amino-acid residues is used. Results are shown on structural similarity prediction and secondary structure prediction.\n\nClarity:\n1. The introduction of the paper is not very well written and it takes some time to figure out the exact problem being addressed. Is it learning sequence embeddings, or predicting structure from sequence or searching for similar structures in a database. Defining a clear goal -- input/output of their pipeline is important before describing the applications of the method, such as predicting structural similarity. \n2. Due to the write-up, the method comes across as having too many modeling components without a very clear motivation for why these help the problem at hand. Where is the alignment part?\n3. Why is each sequence embedded as a matrix? What is the motivation for a vector representation at each amino-acid position?\n4. The authors need to explain the particular choice of 3 layers of bi-directional LSTMs. Why three? And why Bi-LSTM and not LSTMs? \n\nQuality:\n1. While the problem being addressed is interesting, the work lacks a clear reasoning behind the choice of modeling components which makes it seem ad-hoc.\n2. Structural similarity is defined using the hierarchy of protein structure classes and the numbers seemed a bit arbitrary to me. Why not have a vector to encode the different aspects of structure? Have they looked at prior work?\n3. How does the pre-trained language model on Pfam sequences help? Why is the output from it concatenated; have other composition functions been considered? \n\nOriginality:\nThe various components of the model are not novel, but the particular framework of putting them together is novel.\n\nResults:\n1. While the authors claim that some prior methods only work with high sequence similarity, their own evaluation only considers pairs of sequences with 95% identity. HHalign for instance, considers sequences with ~20% identity.\n2. Why weren't several train/test splits of the data tried, so that performance can be reported with std. error bars?\n3. Methods against which they compare have not been described properly.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}