{"title": "Are protein sequence emebeddings learned from structure?", "review": "This work learns embeddings for proteins. They use techniques from deep learning on natural langauge that are typically applied to scentences and words, and apply them correspondinly to proteines and amino acids. Thus they learn a vector representation using a bi-directional LSTM for amino acids by training the amino acid equivalent of a language model.\n\nThe authors then multitask 2 models using the embeddings that perform contact prediction (using an mlp and CNN) and structural class similarity model, which appear to perform very well.\n\nTheir SSA - soft symmetric alignment mechanism is neat and gives a single scalar value for a pair of proteins (by comparing their strings of emebedded amino acids by L1 distance), and it is descriptive enough feature for a simple ordinal regression to output a set structural similarity scores via a linear classifier (one for each strength of similarity re. the SCOP classification hierachy). It seems to work well, but I am unable to judge how good this is with respect to more recent work in this field. I would suspect being able to backprop to the embedding LSTMs through the SSA at this point would give much better results. \n\nAuthors only give 2 recent refeneces for protein embedding work [12,13] but should also take a look at this work: Melvin et. al, PLOS Computational Biology, this work uses structural class labels from SCOP to supervise the embedding. Although they do mention profile HMM in 'related work' which was used to create features in that work.\n\nThese authors, as far as I can tell do not \"backprop\" to the amino acid embeddings (and the LSTMs) from the contact or similarty loss. So the bi-LSTM-produced feature vectors, although trained unsupervised from many proteins, are not trained with structural supervision as claimed in the title (they state this in last paragraph of 3.1) and so the embeddings are not related to structural similarity directly. They do, however, seem to produce good features for the tasks they then tackle.\nThey say in the conclusion that the SSA model is fully differentiable, but I don't see where they \"backprop\" through it.\n\nI would say (if this assessment is correct) then the title is very misleading, although the work and  final results look good.\n\n\nupdate: the authors have assured me in comments that the model is trained end to end - changing rating to good..\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}