{"title": "Review", "review": "Paper summary: This paper focuses on the case where the finiteness of trajectories will make the underlying process to lose the Markov property and investigates the claims theoretically for a one-dimensional random walk and Wiener process, and empirically on a number of simple environments. \n\nComments: The language and the structure of the paper are not on a very good scientific level. The paper should be proofread as it contains a lot of grammatical mistakes. \n\nGiven the assumption that every state's reward is fixed, the theoretical analysis is trivial.\n\nThe comparison of policy gradient methods is too old. The authors should look for more advanced methods to compare.\n\nThe experimental environment is very simple in reinforcement learning tasks, and the authors should look for more complex environments for comparison. The experiment results are hard to interpret. \n\n\n\nQ1: In the theoretical analysis, why should the rewards for each state be fixed?\n\nQ2:Why use r_t \u2013 (V(s_t)-\\gammaV(s_{t+1})) as the advantage function?\n\nQ3: What does the \u201cvariant\u201d mean in all figures? \n\nTypos: with lower absolute value -> with lower absolute values \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}