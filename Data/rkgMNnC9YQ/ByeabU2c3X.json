{"title": "Interesting approach, incomplete work.", "review": "Summary:\nThe authors propose a framework for training an external observer that tries explain the behavior of a prediction function using the minimal description principle. They extend this idea by considering how a human with domain knowledge might have different expectations for the observer. The authors test this framework on a multi-variate time series medical data (MIMIC-III) to show that, under their formulation, the external observer can learn interpretable embeddings.\n\nPros:\n- Interesting approach: Trying to develop an external observer based on information theoretic perspective. \n- Considering the domain knowledge of the human subject can potentially be an important element when we want to use interpretable models in practice.\n\nIssues:\n(1) In 2.4: So between M and M^O, which one is a member of M_reg? \n(2) On a related note to issue (1): In 2.4.1, \"Clearly, for each i, (M(X))_i | M^O(X) follows also a Gaussian distribution: First of all, I'm not sure if that expression is supposed to be  p(M(X)_i | M^O(X)) or if that was intended. But either way, I don't understand why that would follow a normal distribution. Can you clarify this along with issue (1)?\n(3) In 2.4.2: The rationale behind using attention & compactness to estimate the complexity of M^O is weak. Can you elaborate this in the future version?\n(4) What do each figure in Figure 4 represent?\n(5) More of a philosophical question: the authors train M and M^O together, but it seems more appropriate to train an external observer separately. If we are going to propose a framework to train an agent that tries to explain a black box function, then training the black-box function together with the observer can be seen as cheating. It can potentially make the job of the observer easier by training the black box function to be easily explainable. It would have been okay if this was discussed in the paper, but I can't find such discussion.\n(6) The experiments can be made much stronger by applying this approach to a specific prediction task such as mortality prediction. The current auto-encoding task doesn't seem very interesting to apply interpretation.\n(7) Most importantly: I like the idea very much, but the paper clearly needs more work. There are broken citations and typos everywhere. I strongly suggest polishing this paper as it could be an important work in the model interpretability field.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}