{"title": "deep learning architecture for graph semi-supervised learning", "review": "This paper presents an interesting idea for the following task: given a graph and a subset of labelled nodes, infer the labels on the remaining nodes. Here the authors will make prediction for absent labels based on local averages on the graph of the neighbouring soft labels. The main originality is that the local average is weighted and the weights are learnt. \n\nI had trouble understanding the details of the algorithm and the authors should be more careful in their description of the algorithm. Some points to clarify:\n- section 3.1, I am not sure to understand the 'dynamic weights'. The main point here seems to be the use of an attention mechanism (which does not vary in time) applied to inputs varying in time?\n- section 3.2, I do not understand equation (13). What is \\theta^\\tau, it does not appear in the right-hand term?\n\nI think that using the term time is misleading. Time might refer to epochs in an optimization process, whereas time in Section 3 seems to refer to a number of layers as described in equation (6).\n\nPlease, be more explicit on the use of raw features. How are the similarities described in appendix B incorporated in the loss?\n\nOverall, I think this paper requires a lot of clarification before being published.", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}