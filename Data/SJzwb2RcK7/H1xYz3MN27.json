{"title": "Interesting method, but some important questions are not answered, and experiments didn't clearly justify the contributions", "review": "This paper proposed a method to decompose text representation into two vectors for meaning and form respectively. The method used an adversarial discriminator to eliminate form information in meaning vectors and used a motivator on form vectors. The authors evaluated the model on a form transfer task and a downstream task of paraphrase detection.\n\nPros of the paper:\n1. The paper proposed to use a motivator to encourage the model to keep more form information in form vectors.\n\n2. Learned meaning embedding gives a better performance than other unsupervised method on a downstream paraphrasing detection task.\n\nCons of the paper, and questions:\n\n1. One big concern about the method is why f (form vector) would contain only form information, while m (meaning vector) would contain all semantic information. I might miss something, but it seems totally possible to me that some semantic information will shift to f rather than to m, because f has access to the whole input, and that's why previous work used a categorial vector. The authors didn't explain how they addressed this issue.\n\n2. In the form transfer experiments, the proposed model had lower content preservation scores. This probably means semantic meaning will shift after switching f, so the issue mentioned above does exist, right? The authors should show more results and analysis on how well meaning is preserved.\n\n3. The procedure of constructing the opposite form vector in Section 5.1.1 is a bit ad-hoc, and it doesn't really motivate the use of continuous form representation. It would be nice if the authors could show other use cases where continuous form representation is clearly a better choice.\n\n4. Why not report experiments on sentiment data? I think the model is generic enough and should be able to handle sentiment decomposition and transfer. I would be curious to see the results on sentiment data.\n\n5. The perplexity change from 6.89 to 9.74 seems huge to me. Even though the absolute change seems small, the relative change is huge (~30%) according to 1-billion-words-benchmark of language model. It would be useful to show more evidence that this change doesn't impact the fluency.\n\nOverall the originality of the method is marginal. Some questions about the method need to be answered. Evaluations are a bit weak and they don't clearly justify the contributions of the paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}