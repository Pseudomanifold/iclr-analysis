{"title": "Neat little contribution.", "review": "The paper presents a method for style transfer using latent vectors that are trained to separately capture meaning and form (style). \n\nThe main claims are that (i) the form aspect is modeled in a continuous fashion as opposed to prior work, and (ii) the use of a motivator (in addition to a discriminator) to encourage packing information about the form. \n\nThe paper presents experiments that compare how controllable the generation is under this model. \n\nHere are my concerns with the paper:\n\n1. Why is decomposing information from a packed sentence embedding better than say separately tracking meaning and form related information using two separate LSTMs? It is not obvious why one is better than the other. -- One can argue that there are more parameters with two LSTMs. But then how does this compare with the parameters of the 2-layer MLP? \n\n2. One of the main distinctions being claimed is that this model treats form as truly continuous. But the discriminator seems to be trained to predict the form of the sentence -- where form is one of the two classes (middle vs. modern)? \n-- If this is indeed the case then how is this treatment different from prior work (e.g. Hu et al. (2017) which treat the latent variable as a binary feature but with continuous values). The mechanism is different but the treatment seems to be the same. Is it?\n-- If this is not the case then how does the discriminator know what the continuous form vector should be for each sentence?\n\n3. As far as I can tell the evaluations don\u2019t seem to test the benefit of the form being truly continuous in the sense of controllable generation. \nIs there a way to say generate this sentence in a high degree of middle english? I cannot imagine why/how this would be useful as a task.\n\n4. There is one evaluation in Figure 2, which suggests that the ADNet model generated sentences have higher transfer strength but this evaluation is not conclusive in the sense that you have too much drop in content. It would be compelling to have a manual evaluation to supplement this evidence. \n\n5. Table 1 examples look good but Table 2 results seem to indicate that the model doesn\u2019t do as well for headlines to scientific article titles. Why is this the case?\n\n6. Table 3 results are encouraging. It would be useful to also include a result where you use both the form and meaning vectors (say concatenated) as a control test to see if it is indeed the meaning vectors that are useful or if it is this kind of separation that is somehow useful. \n\n7. The intro has a sentence that says the method decomposes the sentence embedding into several vectors -- this has only been tested for form and meaning. There is nothing in the model that says it can't have more than two vectors but just that the empirical evidence is only for form and meaning. \n\nSome typos:\nbetter decomposition \u2192 better decompose\ndissociation meaning and form \u2192 dissociating meaning from form\ncomplimentary \u2192 complementary\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}