{"title": "An interesting word representation model with good ablation experiments", "review": "This paper presents an approach to creating word representations that operate at both the sub-word level and generalise across languages. The paper presents soft decoupled encoding as a method to learn word representations from weighted bags of character-n grams, a language specific transformation layer, and a \"latent semantic embedding\" layer. The experiments are conducted over low-resource languages from the multilingual TED corpus. The experiments show consistent improvements compared to existing approaches to training translation models with sub-word representations. The ablation studies in Section 4.4 are informative about the relative importance of different parts of the proposed model.\n\nCan you comment on how your model is related to the character-level CNN of Lee et al. (TACL 2017)?\n\nIn the experiments, do you co-train the LRLs with the HRLs? This wasn't completely clear to me from the paper. In Section 4.2 you use phrases like \"concatenated bilingual data\" but I couldn't find an explicit statement that you were co-training on both language pairs.\n\nWhat does it mean for the latent embedding to have a size of 10,000? Does that mean that W_s is a 10,000 x D matrix?\n\nIs Eq (4) actually a residual connection, as per He et al. (CVPR 2016)? It looks more like a skip connection to me.\n\nWhy do you not present results for all languages in Section 4.6?\n\nWhat is the total number of parameters in the SDE section of the encoder? The paper states that you encode 1--5 character n-grams, and presumably the larger the value of N, the sparser the data, and the larger the number of parameters that you need to estimate.\n\nFor which other tasks do you think this model would be useful?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}