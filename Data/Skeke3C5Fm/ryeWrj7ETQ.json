{"title": "Interesting and clean idea for multilingual lexicon sharing. ", "review": "Overall:\nThis paper proposed soft decoupled encoding (SDE), a special multilingual lexicon encoding framework which can share lexical-level information without requiring heuristic preprocessing. Experiments for low-resource languages show consistent improvements over strong multilingual NMT baselines.\n\nGeneral Comments:\nTo me this paper is very interesting and is nicely summarized and combined previous efforts in two separated directions for sharing multilingual lexicons: based on the surface similarity (how the word is spelled, e.g. subword/char-level models), and based on latent semantic similarity (e.g. Gu et.al. 2018). However, in terms of the proposed architecture, it seems to lack some novelty. Also, more experiments are essential for justification.\n\nI have some questions:\n(1) One of the motivation proposed by Gu et.al. 2018 is that spelling based sharing sometimes is difficult/impossible to get (e.g. distinct languages such as French and Korean), but monolingual data is relatively easy to obtain. Some languages such as Chinese is not even \u201cspelling\u201d based. Will distinct languages still fit in the proposed SDE? In my point of view, it will break the \u201cquery\u201d vector to attention to the semantic embeddings.\n(2) How to decide the number of core semantic concepts (S) in the latent semantic embeddings? Is this matrix jointly trained in multilingual setting?\n(3) Is the latent semantic embeddings really storing concepts for all the languages? Say would you pick words in different languages with similar meanings, will the they naturally get similar attention weights? In other words, do multiple languages including very low resource languages learn to naturally align together to the semantic embeddings during multilingual training? I am a bit doubtful especially for the low resource languages.\n(4) It seems that the language specific transformation does not always help. Is it because there is not enough data to learn this matrix well?\n(5) During multilingual training, how you balance the number of examples for low and high resource languages?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}