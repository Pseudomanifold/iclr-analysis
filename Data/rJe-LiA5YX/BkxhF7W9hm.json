{"title": "A lot of sophisticated mathematical concepts, not much explained, hard to understand.", "review": "\nI found the paper poorly written, with many typos and incorrect formulations. It contains several sophisticated mathematical concepts (probably from differentiable geometry, dynamical systems and differential equations) that I believe a majority of ICLR audience would not be so familiar with; and these notions are not defined nor explained (e.g Riemannian metrics/manifold, Krylov subspaces). Section 2 presents some theoretical results and formal definition of exponentially decaying flows but I could not see the intuition or the goal of these results. In Section 3, for the main\noptimization problem (13), it is assumed that \"there exists no stationary point except for a minima.\" and so, I understand that there is only \"a minima\" or at least that all the local minima are assumed to have the same value, which not realistic for deep learning.\n\n\nComments and questions\n\nSection 1:\n\nAfter speaking of gradient descent methods, the authors say \"Another class of methods... is adaptive methods such as AdaGrad and Adam\": These are all based on stochastic gradient descent algorithm, so this distinction is surprising.\n\nSection 2:\n\n\"Jacobian with variable w\": does not make sense (although I can guess the author mean w.r.t.)\n\n\"J is regular and the equation has a unique solution\": what is regular here? No equation has been mentioned before\n\n\"...becomes a Riemannian manifold under some appropriate conditions\": which conditions?\n\nSection 3:\n\n\"Applying Theorem 2.2, we obtain the differential equation\": I can't see how applying Theorem 2.2 leads to a differential equation.\n\nSection 4:\n\n\"we consider a projection which maps r to a vector P_k(A, v) in the k-th order Krylov subspace such that r = P_\\infty(A, v)\": I don't understand.\n\n\"Particularly, in the case that \u03c7 = \u22121, we set \u03bb = akJ \u03c6 T F k b with a, b > 0, so that the convergence rate of (24) stays exponential\": why is the case \u03c7 = \u22121 relevant to point out?\n\n\"Finally, to accelerate the EDF-based methods, it is sometimes effective to change equation (18) into a second-order differential equation\": is it an empirical observation? or is there an explanation?\n\nSection 5:\n\nAgain, so many notations (not all conventional: e.g. \\theta denotes the softmax function), just to present a standard loss for a deep learning model.\n\nSection 6:\n\n\"As has been found, second-order-methods on full-batch training converge to the solution within a few iterations.\":  Reference?\n\nThe method is compared to Nesterov accelerated gradient (NAG) for the data-fitting problem (fig 1) but not on the other tasks (figs 2 to 5).\n\nIn the end, it was still unclear to me what the training consists of with the introduced exponentially decaying flows. I understand that a differential equation was formulated but then how is it solved iteratively? What is the cost of solving it? In Section 6, different algorithms are compared in terms of number of steps. What about the computational cost?\n\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}