{"title": "Official Review", "review": "Summary: This paper proposes a \u201cmeta-learning\u201d approach for zero-shot learning. There is a Task Module that works in a conventional zero-shot way: Training to predict a class prototype using the auxiliary/text data description of that task. The new part is the added Correction Module that inputs both the target/zero-shot task description, the training task description, and the current prediction of the task module, and then outputs a correction vector that is added to the output of the task-module to produce the final output. The resulting system achieves state of the art results on zero-shot fine-grained classification (CUB and NAB).\n\nAssessment: Overall this might be a good idea worthy of publication at some point. But despite the good results, the current realisation is not well analysed about exactly how and why it works, with no insight being provided; and leaves some doubt about the validity of the comparative experiments. The writing is also very rushed. It is not ICLR standard yet.\n\nStrengths:\n+ Interesting idea overall.\n+ Good results.\nWeaknesses:\n- Poor clarity. \n- Some experimental evaluation questions. \n- Poor analysis.\n\nComments:\n1. The correction module inputs the full set of training features T_s (Alg1-L13). However the training dataset is fixed, therefore this input is effectively a constant. So its not clear how a constant input can possibly be useful. \n1.1 Possibly this has something to do with the episodic training, but this is exactly the kind of thing that should be analysed and explained, but is not discussed at all.\n2. The paper is sold as a meta-learning paper, but it\u2019s not clearly explained what is the \u201cmeta\u201d part of the algorithm.\n3. Its not explained anywhere how exactly the T_s, T_s^u, etc are fed into the correction network. Is it average pooling? It seems that simple average pooling is unlikely to be adequate given the large number (150) of classes in CUB.\n4. There are no experimental details such as hyper parameters, network architecture, etc.\n5. Based on the ablation study (Tab 2), the baseline task network without correction network already achieves state of the art results. Conceptually the task-network alone is a very standard \u201cregression\u201d based approach to ZSL of the type that people tried almost 10 years ago. So what is the explanation for why its so good? This makes the comparison to all the competitors in Tab1 suspect. If there is some reason (E.g., better image feature extractor or pre/post-processing) that makes the ultra simple baseline there already outperform SotA, then you have to ask how all the prior methods would perform if they were run with the same tweak. \n6. Overall no insight provided about what kind of corrections are made, when they are useful, etc. This is important to provide insight about how/why correcting outputs can work.\n7. There is nothing particularly unique about this setup for ZSL. It could equally be applied to correct outputs in the case of few-shot learning (CF: Prototype Networks). It would be more convincing if it was applied to both settings and analysed better for both.\n8. The writing is very rushed. There are lots of writing and editorial errors. To name a few: P4 Extra \u201cTask module is trained to minimise.\u201d P4 \u201c\\mu_u\u201d Is repeated. Citation style \u201cMohamed Elhoseiny & Elgammal\u201d is wrong, check the bibtex.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}