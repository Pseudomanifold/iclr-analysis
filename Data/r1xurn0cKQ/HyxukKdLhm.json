{"title": "Original approach with strong results, but lacks many details", "review": "=== Post-rebuttal update ===\n\nThe authors' rebuttal provided many of the details I was seeking. I asked a few additional questions which were also recently addressed, and I encourage the authors to include these clarifications into the final draft of the paper.\n\nHence, I've increased my score for this paper.\n\n=== Pre-rebuttal review ===\nThis paper presents a meta-learning approach to zero-shot learning. The idea is to train a correction module which is trained to produce a correction to the output of a previously trained task module. The hypothesis is that the correction should depend on the nature of the training data of the task module, and so the correction module receives as input a representation of the training data of the task module. An episodic approach is then used for training the correction module, whereby many different task modules are trained on various subsets of the total training data, the rest being used as unseen data for the correction module.\n\nThe proposed idea is original and the results are strong. Generally, I'd be inclined to see this paper published.\n\nHowever right now, the paper lacks A LOT of details on how the experiments were run. I would like to see these answered in the rebuttal, before I consider raising my rating for this paper:\n- What are the architectures used for M_T and M_C?\n- What distance functions was used for training?\n- What optimizer was used for training?\n- How was convergence established in the inner and outer while loops of algorithm 1?\n- Text mentions that before evaluation, M_T is trained on all data in D_S. How is this done exactly (e.g. how is convergence assessed)?\n- How is the T_S computed exactly?\n- How expensive is it to run Algorithm 1 (i.e. to train the correction module)? Since a new task module M_T needs to be trained for each subset S^s, it seems like it might be expensive to run... if not, why?\n\nI would also strongly suggest the authors release their code if this paper ends up being published.\n\nIn summary:\n\nPros\n- Claims SOTA results on two good benchmarks for zero-shot learning\n- Approach is original\n\nCons\n- Paper lacks a lot of methodological and experimental details\n\nSome minor details:\n\n- \"We found the task module performance improves slightly when the output of the task module is feed into a classifier with a single hidden layer that is also trained to classify samples from the task model\u2019s training dataset.\" => I don't understand what this means. Isn't the output of the task module already trained to classify samples from its training dataset? So why is this additional single hidden layer needed?\n- Typos:\n  - on few shot learn => on few shot learning\n  - but needs not => but need not\n  - image image classification => image classification\n  - the the compatibility => the compatibility\n  - psuedo => pseudo\n  - \"The task module is trained to minimize\" => that reads like an unfinished sentence\n  - \\hat{\\mu}_U \\hat{\\mu}_U => \\hat{\\mu}_U \n  - inputted => input\n  - FOr => For\n  - it's inputs => its inputs\n  - otherhand => other hand", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}