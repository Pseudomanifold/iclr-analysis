{"title": "On the adaptive CW attack", "review": "This work introduces a novel defense method \"Neural Fingerprinting\" against adversarial examples.\nIn the training process, this method embeds a set of characteristic labeled samples so that responses of the model around real data show a specific pattern.  The defender can detect if a given query is adversarial or not by checking the pattern at test time.\n\nStrong point:\nThe strong point is that the proposed method seems to be appropriate and technically original. The performance is well investigated and compared with several competitors.  The organization is good and the idea is clearly stated. \n  \nWeak point:\nOne question is that why the proposed method can be protective against the adaptive CW attack. In the public discussion, the authors mention that the defense works successfully because the landscape of the fingerprinting loss is non-convex and no gradient method is guaranteed to find a suitable solution. If this is correct, did you repeatedly try the gradient-based attack with changing random seeds? By doing so, the attack might work successfully with a certain probability.\n\nComments:\nThe presented method seems to have a certain similarity with digital watermarking of deep neural networks, for example:\nhttps://gzs715.github.io/pubs/WATERMARK_ASIACCS18.pdf\nIt would be interesting to mention to these methods in the related work section.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}