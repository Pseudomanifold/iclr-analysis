{"title": "Good paper, need more justification on the learned features by TGM", "review": "This paper introduces a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer, and present how it can be used for activity recognition. The kernels of the new layer are controlled by a set of (temporal) Gaussian distribution parameters, which significantly reduce learnable parameters. The results are complete on four benchmarks and show consistent improvement. I just have minor comments. \n\n1. I am curious what the learned feature look like. As the author mentioned, \"The motivation is to make each temporal Gaussian distribution specify (temporally) \u2018where to look\u2019 with respect to the activity center, and represent the activity as a collection/mixture of such temporal Gaussians convolved with video features.\" So does the paper achieve this goal? \n\nAnother thing is, can the authors extract the features after TGM layer, and maybe perform action recognition on UCF101 to see if the feature really works? I just want to see some results or visualizations to have an idea of what TGM is learning. \n\n2. How long can the method actually handle? like hundreds of frames? Since the goal of the paper is to capture long term temporal information. \n\n3. It would be interesting to see an application to streaming video. For example, surveillance monitoring of human activities. \n\n\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}