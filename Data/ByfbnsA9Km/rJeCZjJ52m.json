{"title": "The technical results can be obtained by a simple combination of previous work.", "review": "Summary: \nThis paper investigates the properties of minimizing cross-entropy of linear functions over separable data (looks like logistic loss). The authors show a simple example where the minimizer of the cross-entropy loss leads to maximum margin hyperplane where the bias term is regarded as an extra dimension, which is different from the standard max. margin solution of  SVMs with bias not regarded as an extra dimension. The authors then propose a method to obtain the latter solution by minimizing the cross-entropy loss.\n\n\nComments:\n\nThere is a previously known result quite related to this paper: \n\nIshibashi, Hatano and Takeda: Online Learning of Approximate Maximum p-Norm Margin Classifiers with Bias, COLT2008. \n\nTheorem 2 of Ishibashi et al. shows that the hard margin optimization with linear classifier with bias is equivalent to those without bias over pairs of positive and negative instances. \n\nCombined with Theorem 3 of (Soudry et al., 2018)), I am afraid that the main result Theorem 5 can be readily derived. \n\nFor this reason, I am afraid that the main technical result is quite weak.\n\nAfter Rebuttal:\nI read the authors' comments. I understand more the technical contribution of the paper and raised my score. But I also agree with Reviewer 3.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}