{"title": "The topic is interesting but the novelty is incremental", "review": "In this paper, an efficient SLU model, called as TSSM, is proposed to tackle the problem of insufficient training data for the task of spoken language understanding. TSSM considers the intent and slot detection as a unified multi-objective optimization problem which is addressed by a meta-learning scheme. The model is pre-trained on a large dataset and then fine-tuned on a small target dataset. Thus, the proposed TSSM can improve the model performance on a small datatset in new domains.\n\nPros:\n1)\tThe transfer learning of spoken language understanding is very interesting.\n2)\tThe proposed TSSM can integrate the task of intents and slots and take the relationship between intents and slots into consideration.\n3)\tFive datasets are used to evaluate the performance of the method.\n\nCons:\nOverall, the novelty of this paper is incremental and some points are not clear. My main concerns are listed as follows.\n1)\tThe authors state that the knowledge transfer is the main contribution of this paper. However, as introduced in 3.5, the transfer scheme in which the model is first pre-trained on a large dataset and then fine-tuned on a small target dataset is very straightforward. For example, currently, almost all methods in the area of object recognition are pre-trained on ImageNet and then fine-tuned on a small dataset for particular tasks.\n2)\tAuthors also state that improvements for transferring from Restaurant, Laptop, TV, Atis to Hotel is not obvious. I think the results also need to be reported and the reasons why the improvement is not obvious should be provided and discussed.\n3)\tThe paper needs more proofreading and is not ready to be published, such as \u201cA survey fnor transfer\u201d and \u201ca structured multi-objective optimization problems\u201d.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}