{"title": "Good idea, but paper suffers on many important points", "review": "This paper trains an information retrieval (IR) model by contrasting the joint query-document distributions, p(q, d) with negative samples drawn from a resampling of the product of marginals, p(q) x p(d). They use a second discriminator to provide the re-weighting (I believe picking to top negative sample from the other model) and train this other model in a way that mirrors the first. They also attempt to point out some theoretical problems with a competing model, IRGAN, which uses a generator that is trying to model the joint.\n\nWhile I like the proposal idea, I think the paper has too many problems to warrant publication. First, the story is very disappointing. The authors phrase most of the paper as a critique of IRGAN, but this critique falls short. Really this is more of a paper about where to get negative samples when training a model of the joint (or the log-ratio in this case). Using negative samples from real data with noise contrastive estimation [1] is found in numerous works in NLP [2][3], and has gained some recent attention in the context of representation learning [4][5]. The first algorithm proposed is essentially doing a sort of ranking loss on negative samples, which mirrors similar works [6]. In fact, the generator in IRGAN could be viewed as just a parametric / adaptive negative sampling distribution in the context of NCE for the ultimate purpose of learning an estimate of the log-ratio. The most interesting thing I think of this work here is the co-training, i.e., using another model to help re-sample, and I think this idea should be explored in more detail.\n\nSecond, the paper spends far too much time revisiting prior work than addressing their own model, doing more analysis, providing more insight.\n\nThird, the paper is just poorly written. The notation is confusing, some of the equations are unclear (I have no idea how \"r\" is used in any of this), and the arguments of the baseline in IRGAN don't really doesn't make any sense.\n\nNotes:\nP1\nI don't really follow why IRGAN is so central to this work. Good ideas aren't difficult to motivate, especially if empirically everything works out.\nP2\nI'm having trouble with claims, especially more recently, about GAN instability, particularly since numerous approaches [7][8] seem to have more or less solved the problem.\n\nThe use of \"|\" in G is awfully confusing.\nP3\nAlmost 2 pages of unnecessary background\n\nP4\nWhy are we using \"|\" in functions? What's wrong with \",\"?\ntheta = \\theta\nI don't understand the point of the quote (in italics).\nWhat happened to \"r\" in all of this?\nThe last two equations and their relationship could be more clear.\n\nYou use italics, so is this supposed to be a quote? But then you have a section which attempts to show this.\nP5\nI have no idea what's supposed to be going on in 5). The samples from the real joint don't factor in the generator gradient, or at least it's absolutely not clear that this pops out of the baseline? Then you switch from log (1 - x) to - log x and there's some claim about this violating the adversarial objective?\n\nIt took me more than a few reads to figure out what the equation at the bottom of P5 is doing: is this resampling? It's fairly unclear.\n\n[1] Gutmann, Michael U., and Aapo Hyv\u00e4rinen. \"Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics.\"\n[2] Mnih, Andriy, and Koray Kavukcuoglu. \"Learning word embeddings efficiently with noise-contrastive estimation.\" \n[3] Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\"\n[4] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" \n[5] Hjelm, R. Devon, et al. \"Learning deep representations by mutual information estimation and maximization.\"\n[6] Faghri, Fartash, et al. \"VSE++: Improving Visual-Semantic Embeddings with Hard Negatives.\"\n[7] Miyato, Takeru, et al. \"Spectral normalization for generative adversarial networks.\"\n[8] Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. \"Which Training Methods for GANs do actually Converge?.\" ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}