{"title": "Interesting idea for quantitatively estimating the robustness of a network. Would like to see more comprehensive large-scale experiments. ", "review": "Given a network and input model for generating adversarial examples, this paper presents an idea to quantitatively evaluate the robustness of the network to these adversarial perturbations. Although the idea is interesting, I would like to see more experimental results showing the scalability of the proposed method and for evaluating defense strategies against different types of adversarial attacks.  Detailed review below:\n- How does the performance of the proposed method scale wrt scalability? It will be useful to do an ablation study, i.e. keep the input model fixed and slowly increase the dimension. \n- Did you experiment with other MH proposal beyond a random walk proposal? Is it possible to measure the diversity of the samples using techniques such as the effective sample size (ESS) from the SMC literature?\n- What is the performance of the proposed method against \"universal adversarial examples\"?\n- The most interesting question is whether this method gives reasonable robustness estimates even for large networks such as AlexNet?\n- Please provide some intuition for this line in Figure 3: \"while the robustness to perturbations of size \u000f = 0:3 actually starts to decrease after around 20 epochs.\"\n- A number of attack and defense strategies have been proposed in the literature. Isn't it possible to use the proposed method to quantify the increase in the robustness towards an attack model using a particular defense strategy? If it is possible to show that the results of the proposed method match the conclusions from these papers, then this will be an important contribution. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}