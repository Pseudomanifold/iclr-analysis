{"title": "Interesting work but some points need clarification.", "review": "This paper proposes a new method for speeding up convolutional neural networks. Different from previous work, it uses the idea of early terminating the computation of convolutional layers. The method itself is intuitive and easy to understand. By sorting the parameters in a descending order and early stopping the computation in a filter, it can reduce the computation cost (MAC) while preserving accuracy.\n\n1. The networks used in the experiments are very simple. I understand that in the formulation part the assumption is that ReLU layer is put directly after convolutional layer. However, in state-of-the-art network, batch normalization layer is put between convolutional layer and ReLU non-linearity. It would add much value if the authors could justify the use cases of the proposed method on the widely adopted networks such as ResNet. \n\n2. I notice that there is a process that sort the parameters in the convolutional layers. However, the authors do not give any time complexity analysis about this process. I would like see how weight sorting influences the inference time.\n\n3. The title contains the word \u201cdynamic\u201d. However, I notice that the parameter e used in the paper is predefined (or chosen from a set predefined of values). So i am not sure it is appropriate to use the word \u201cdynamic\u201d here. Correct me if i am wrong here.\n\n4. In the experiment part, the authors choose two baselines: FPEC [1]. However, to my knowledge, their methods are performed on different networks. Also, the pruned models from their methods are carefully designed using sensitivity analysis. So I am curious how are the baselines designed in your experiments.\n\nOverall this paper is well-written and points a new direction to speeding up neural networks. I like the analysis in section 3.\n\nI will consider revising the score if the authors can address my concerns.\n\n\n[1] Pruning Filters for Efficient ConvNets. Li et al., ICLR 2017.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}