{"title": "official review", "review": "The authors proposed meta domain adaptation to address domain shift scenario in meta learning setup. The proposed model combines few shot meta-learning with the adversarial domain adaptation to demonstrate performance improvements in several experiments.\n\nPros:\n1. A new few shot learning with domain shift problem is studied in the paper.\n2. A new model combining prototypical network with GAN and cycle-consistency loss for addressing meta-learning domain shift scenario. The experimental improvements on omniglot seem quite substantial. \n\nCons:\n1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline? It seems that both are using meta-learning with domain adaptation technique. What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?  I feel the baseline in domain adaptation area is a bit limited.\n2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)? \n3. It seems the domain shift in the paper is less dramatic. i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.\n4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.\n\nMinor:\n1. Where is L_da in Figure 2? In Figure 2, what\u2019s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?\n2. In the caption of figure 2, there should be a space after `\":\".", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}