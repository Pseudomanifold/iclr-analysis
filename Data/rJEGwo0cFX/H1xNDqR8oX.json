{"title": "Original problem and method but lacks solid experimental results.", "review": "This work proposes an attention-based method for the stated task of dynamic network discovery. Please find below some of the pros and cons of the paper.\n\nPros:\n\n1. The authors investigate a method which can handle data with temporal and spatial dependencies which is an important problem. \n\n2. Showed experiments on several synthetic datasets as well as multiple real-world datasets.\n\nCons:\n\n1. The paper in its current form isn't very polished yet and clarity can still be improved in several ways:\n   * There were more than a few spelling and grammatical errors, please proofread the work and improve the writing. For example, some spelling errors are: Tab. 1 \"Sequence lenght,\" last sentence pg. 10 \"whit respect,\" and in the discussion prior to Eq. 5 \"average polling.\"\n   * The paper's stated goal seems to be studying the problem of identifying the latent interaction graph of a system. However, after reading the introduction and looking at the example in Fig. 1 I don't really get the sense as to why this is important. Is it for anomaly detection? Or for urban planning (assuming we are studying traffic flow? If yes, how? The approach needs to be motivated better.\n\n2. From reading the abstract and the introduction, it seems to me that the main problem that is studied is that of identifying the latent dynamic networks from spatio-temporal data. However, most of the experiments in my opinion don't help in showing why the proposed method is useful for such task.\n   * The network discovery problem is a well-studied problem. In one application area which is that of functional brain network analysis [1,2], the task is to discover the latent functional network (in this case just a single network). In these studies they usually test whether the approach can retrieve the well-known ground-truth default mode functional network and also synthetically generated latent networks (with known correlations/weights). Apart from a few rather limited experiments in the appendix, the authors don't seem to do experiments on network discovery and they certainly don't do any analysis to retrieve ground-truth networks.\n   * For instance, can the retrieved dynamic networks be useful for anomaly detection? \n   * Also, why do the authors make the assumption that the neighbors of a node are fixed. If we are looking at just traffic networks we may assume that a node is only connected to other spatially close nodes but won't it be interesting also to model indirect influence (maybe an accident in a node further away affecting flow later on).\n   \n3. The authors test the performance of the proposed model on the forecasting task on 3 real-world datasets. If the goal of this experiment is to show the usefulness of modeling spatio-temporal dependencies using their approach for forecasting, then the authors should at least evaluate against a stronger set of baselines. They test against several methods but most don't seem to be designed for this. For instance a quick search of related literature brings up the following related deep methods for traffic flow forecasting (setting might vary a little) which considers both spatial and temporal information [3,4]. \n   * Also, please show the SD in Table 3.\n   * Are you aware of [5] which studies a warp-invariant RNN?\n\nThe problem which the authors study and the proposed method are both quite novel. However, the paper clarity can be improved. Also, the experimental results are not enough to back up the paper's main claims. \n\n[1] Mining brain region connectivity for alzheimer's disease study via sparse inverse covariance estimation. Sun et al. In Proc. of KDD '09.\n[2] Unsupervised Network Discovery for Brain Imaging Data. Bai et al. In Proc. of KDD '17.\n[3] Deep Spatio-Temporal Residual Networks\nfor Citywide Crowd Flows Prediction. Zhang et al. In AAAI '17.\n[4] Deep learning for short-term traffic flow prediction. Polson et al. In Trans. Research Part C: Emerging Technologies. 2017.\n[5] Can Recurrent Neural Networks Warp Time? Tallec and Ollivier. In ICLR '18.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}