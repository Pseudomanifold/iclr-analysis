{"title": "Good efforts in approximation properties of deep generative networks; the results are weakly related to GANs", "review": "This paper claims three contributions. \n1. We show that GANs perform better than other standard methods of estimating probability\nmeasures when the measure satisfies the disconnected support property.\n\nI prefer to state the contribution as \"We show that deep generative networks perform better than other standard methods of estimating probability measures when the measure satisfies the disconnected support property.\" This claim is essentially from Lemma 3, and it is a property of deep generative networks instead of GANs. If we have another way to train deep generative networks (say, variational auto-encode), we still get the same good approximation error, just linearly dependent of the number of disconnected pieces. The proof of Lemma 3 is mainly from the definition of locally smoothness and the results in Petersen & Voigtlaender 2017. It's a nice effort to leverage the result in Petersen & Voigtlaender 2017 to prove approximation properties of deep generative models. The flaw of this part is that the claims of other standard methods (Proposition 1) is very hand-wavy and floppy. The proof of Proposition 1 has lots of typos. For example, the definition of S_1 and S_2. And this sentence \"Then, by the proof of Lemma 1, p2(x) on S2 is a quadratic function with respect to z1 is 1-times differentiable but not twice-differentiable at the boundary ...\" I guess that the authors want to argue that traditional function approximation methods (like Kernel methods, polynomial approximation) all have rate n^{-1/(2+D)} approximation rate in the L^2 norm when the function to approximate has discontinuities... However, the authors fail to make this point clear. Moreover, if we use a mixture model of traditional approximations, the rate will not be deteriorated.  And we will get similar results in Lemma 3. Then, even the claim \"deep generative networks perform better than other standard methods of estimating probability measures when the measure satisfies the disconnected support property\" is not that grounded. \n\n\n2. We provide a new generalization error bound under a general formulation of GANs by analyzing an approximation error. The result is thus applicable to a wide range of variations of GANs.\n\nThis corresponding to the results in Theorem 1. The authors may want to write the assumption \"all the discriminators are L_1 Lipschitz continuous\" in the Theorem 1 explicitly, because this is an important assumption to get the results. The analysis in i & iii is standard. The analysis is the main contribution of this paper, and is from Lemma 3. \n\n3. Based on the generalization bound, we provide a theoretical guideline for selecting architectures\nof generators and discriminators. \n\nI think the authors mean Equation (6) in this claim. However, this practical guidance is not practical, because (1) both \\beta and \\kappa are unknown in practice, especially \\beta, (2) I can hardly image the number of connections (non-zero weights) will be my model design guidance instead of the model architecture. \n\nIn the numerical experiment, \"We use d_F to evaluate errors by GANs, and a root of the expected squared errors with the L2-norm for the other methods.\" With different metrics, is this a fair comparison?\n\nFinally, there are lots of typos in the paper and appendix. \"refers to a probability measures\", \"The property makes a probability measure be complex\", \"with disconnected support\", \"Theorem 1, Corollary 1 and ??\", \n\nAmong others, the original GAN(Goodfellow et al., 2014) is realized if F contains a logarithm of density ratio. The f-GAN (Nowozin et al., 2016) also belongs to this class. Equation (1) and (2) only includes Integral Probability Metrics, not divergences in Goodfellow et al., 2014 or Nowozin et al., 2016.\n\n\"smoothness and a dimension of data are sufficient to characterize an optimal convergence of generalization errors.\" If we allow mixture models, that's a different story.\n\n\"A boundary of S is J combination of \u000b-smooth hyper surfaces\" Definition of S_{\\alpha, J} is not clear. The exact definition in appendix is based on the definition of the horizon function, which include \"x_d \\pm h\". Really confused about this \\pm. The original definition in Petersen & Voigtlaender 2017 does not have this \\pm. \n\n\u201can ordinary density function cannot be defined.\u201d Can an ordinary density function be defined by setting the value outside its support to be 0?\n\n\u201can empirical norm \\| \\|\u201d What is the empirical norm?\n\nIn Proof outline of Theorem 1, we have the decomposition of i, ii and iii. Should P_0 in iii be P^{*}?\n\n\"We compare the numerical performance of GANs and the other methods with toy data with.\"\n\nMore typos in the proofs, especially in proof of Lemma 3.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}