{"title": "The paper suggests using Z-scores for comparing ID and OOD samples. Simple Idea, and much related work. Needs to address some issues. ", "review": "There has been recent interest in using statistics and information summary measures to evaluate what deep nets are trying to do. Following the line of work, the paper suggests to use mean and variance of Z-scores accumulated across all layers/channels as features to distinguish ID and OOD samples. Simple idea but needs some work in its current format. \n\nFirstly, the bulk of content in Sections 2 and 3 can be reduced/shortened since the importance of normalized statistics to understand learning models is well known, and not novel. \n\n1) The choice of datasets/netowrks needs to be understood here. How is the OOD summary changing as more layers are added into computing the score (since the score is basically averaging all layers'/channels contribution)? \n2)  What happens if we split the ID itself into two datasets and train on one, while using the other as OOD?\n3)  (r) is random and (c) is not is it for the TinyImages? Seems to be the other way around. \n4) What is the influence of the dataset? Since the summaries are first order statistics, there can be significant dependance of the 'coverage' of training data (i.e., how many and how good of instances are present for each class)? This is purely a sampling problem and it may reciprocate in the OOD scores (back to first order statistics). This needs to be tested. \n5) Statistical tests of significance needs to be reported for the performance summaries shown in the Tables. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}