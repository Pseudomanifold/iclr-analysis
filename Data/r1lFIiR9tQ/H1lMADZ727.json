{"title": "The research direction itself is interesting, but further experimental validation is needed", "review": "\\clarity & quality\nThe paper is easy to follow and self-contained. \nHowever, the motivation for minimizing the upper bound is not so clear for me. \nAs far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. One is that by using the reverse KL, we can obtain sharper outputs, and the second one is that the optimization process will be stable compared to that of the lower bound. \nIn the introduction, the author just mentioned that \"the f-divergence is generally computationally intractable for such complex models. The main contribution of our paper is the introduction of an upper bound on the f-divergence.\"\nFor me, this seems that the author just introduced the new fancy objective. I think the motivation to introduce the new objective function should be stated clearly.\n\n\\originality & significance\nAlthough the upper bound of the f-divergence is the trivial extension, the idea to optimize the upper bound for the latent model seems new and interesting.\n\nHowever, it is hard to evaluate the usefulness of the proposed method from the current experiments.\nIt seems that there are two merits about the proposed method as above.\nThe only evidence that the learning tends to be stable is the Fig.8 in the appendix, but this is just the fitting of univariate Gaussian to a mixture of Gaussians, thus it is too weak as the evidence.\nAbout the sharp output, there are already many methods to overcome the blurred output of the usual VAE. No comparison is done in the paper.\nSo I cannot tell whether the proposed objective is really useful to learn the deep generative models.\nI think further experimental results are needed to validate the proposed method.\n\n\\Question\nIn page 4,  the variance of the p(y|x) and p_\\theta(y|z) are set to be the same. What is the intuition behind this trick? \nSince this p(y|x) is used as the estimator for the log p(y) as the smoothed delta function whose Gaussian window width (the variance), and the Gaussian window width is crucial to this kind of estimator, I know why the author used this trick.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}