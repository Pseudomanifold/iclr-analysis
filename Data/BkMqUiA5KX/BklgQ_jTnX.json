{"title": " ", "review": "The paper proposes to resolve the issue about a variational auto-encoder ignoring the latent variables. The paper presents a variational auto-encoder with repeated likelihoods, which results in a 1+m factor in front of the log-likelihood term. This paper justifies this model by defining a model as a combination of a variational auto-encoder and a stochastic auto-encoder.\n\nThis paper tries to re-interpret the VAE models with weighting factors such as beta-VAE as a model of repeated likelihoods/views. From the Bayesian modelling perspective, it is a bit problematic as the same observed variables appear multiple times in the model. The model assumes two independent observation of the sam latent variable, however, it is actually given the same observation twice. This introduces a bias in posterior. On the other hand, a weighting factor in front of likelihood is not new. This trick has been used in multi-view learning or imblanced classification as a practical solution to balance the views or the classes.\n\nThe derivations in Section 2 is hard to follow. It is unclear how to Equation 7 is derived from Equation 6.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}