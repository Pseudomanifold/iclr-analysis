{"title": "simple yet effective", "review": "The authors present a method on prediction of frames in a video, with the key contribution being that the target prediction is floating, resolved by a minimum on the error of prediction. The authors show the merits of the approach on a synthetic benchmark of object manipulation with a robotic arm. \n\nQuality: this paper appears to contain a lot of work, and in general is of high quality. \n\nClarity: some sections of the paper were harder to digest, but overall the quality of the writing is good and the authors have made efforts to present examples and diagrams where appropriate. Fig 1, especially helps one get a quick understanding of the concept of a `bottleneck` state. \n\nOriginality: To the extent of my knowledge, this work is novel. It proposes a new loss function, which is an interesting direction to explore.\n\nSignificance: I would say this work is significant. There appears to be a significant improvement in the visual quality of predictions. In most cases, the L1 error metric does not show such a huge improvement, but the visual difference is remarkable, so this goes to show that the L1 metric is perhaps not good enough at this point. \n\nOverall, I think this work is significant and I would recommend its acceptance for publication at ICLR. There are some drawbacks, but I don\u2019t think they are major or would justify rejection (see comments below). \n\n\nI\u2019m curious as to why you called the method in section 3.2 the \u201cGeneralized minimum\u201d? It feels more like a weighted (or preference weighted) minimum to me and confused me a few times as I was reading the paper (GENerative? GENeralized? what\u2019s general about it?). Just a comment.\n\nWhat results does figure 4 present? Are they only for the grasping sequence? Please specify.  \n\nIn connection with the previous comment, I think the results would be more readable if the match-steps were normalized for each sequence (at least for Figure 4). There would be a clearer mapping between fixT methods and the normalized matching step (e.g., we would expect fix0.75 to achieve a matching step of 0.75 instead of 6 / ? ).\n\nSection 4, Intermediate prediction. The statement \u201cthe genmin w(t) preference is bell-shaped\u201d is vague. Do you mean a Gaussian? If so, you should say \u201ca Gaussian centered at T/2 and tuned so that \u2026\u201d\n\nSection 4, Bottleneck discovery frequency. I am not entirely convinced by the measuring of bottleneck states. You say that a distance is computed between the predicted object position and the ground-truth object position. If a model were to output exactly the same frame as given in context, would the distance be zero? If so, doesn\u2019t that mean that a model who predicts a non-bottleneck state before or after the robotic arm moves the pieces is estimated to have a very good bottleneck prediction frequency? I found this part of the paper the hardest to follow and the least convincing. Perhaps some intermediate results could help prospective readers understand better and be convinced of the protocol\u2019s merits.\n\n\n\nTypos:\n\nAppendix E, 2nd paragraph, first sentence: \u201c... generate an bidirectional state\u201d --> \u201cgenerate A bidirectional state\u201d \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}