{"title": "Proposing a new setting, interesting for ICLR and has some proof-of-concept results", "review": "The paper proposes a novel adversarial attack on deep neural networks. It departs from the mainstream literature in two points: \n1. A 'federated' learning setting is considered, meaning that we optimize a DNN in parallel (imagine a map-reduce approach, where each node performs SGD and then a central server (synchronously) updates the global parameters by averaging over the results of the nodes) and an attacker has control over one of the nodes.\n2. The treat model is not the common data poisoning setting, but 'model poisoning' (the attacker can send an arbitrary parameter vector back to the server).\n\nThe paper, which is well written, starts with proposing a couple of straightforward (naive) attacks, which are subsequently used as a baseline. Since there (apparently) is no direct related work, these baselines are used in the experimental comparisons. Then the authors propose a more sophisticated attacks, based on alternatingly taking a step into the attack direction (to get an effective attack) and minimizing the loss (to Camouflage the attack), respectively. They add also the feature of restricting the solution being not to far away from the usual benign SGD step.\n\nAll in all, I am acknowledging that his paper introduces the federated learning paradigm to 'adversarial examples' subcommunity of ICLR and would make for good discussions at a potential poster. I find the used method slightly oversimplistic, but this is maybe fine for a proof of concept paper. \n\nFinal judgement: For me this paper is a 6-7 rating paper; a nice addition to the program, but not a must-have.\n\nA have a question to the authors that is important to me: it seems that the baseline attack could be very very simply detected by checking on the server the norm of the update vector of the attacked node. Since the vector has been boosted, the norm will be large. While your distance-based regularization somewhat takes that effect away, it remains unclear to what amount. Can you give me some (empirical) details on this issue? / or clarify if I am completely off here?  thank you", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}