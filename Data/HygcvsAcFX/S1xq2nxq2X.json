{"title": "PAC-Bayesian analysis for DNNs", "review": "This paper presents a PAC-Bayesian bound for a margin loss.\n\nTheorem 1 seems specific to ReLU activations. I wonder whether this theorem holds for other activations since most deep neural networks can use different activations at different layers instead of only the ReLU activation for all the layers. In Section 3, only Definition 3 is related to the activation. Can an activation satisfying Definition 3 have a similar bound to Theorem 1? Moreover, since the convolutional layer is a simplified case of the fully connected layer discussed in Section 3, does the convolutional layer simplify the bound in Theorem 1?\n\nThere are some typos in this paper.\n\u201cTo derive a expected risk bound\u201d: a -> an\n\u201cused to formalize error-resilience in Arora et al. (2018) as following:\u201d: following: -> follows.\n\u201cthe deep network from layer i to layer j\u201d, \u201cinjected before level i\u201d: i,j should be in the math mode.\n\u201cdependent on the network structure .\u201d there is an additional blank space after \u2018structure\u2019.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}