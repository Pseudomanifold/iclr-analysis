{"title": "Important benchmark for an interesting problem. Some key analysis missing", "review": "Summary:\nThis paper introduces a large-scale benchmark for the task of image captioning in which the goal is for the caption to be as engaging as humans as possible. The dataset consists of 200K + captions spanning 200+ personality traits. The dataset includes traits like \u201cKind\u201d, \u201cMellow\u201d, \u201cCreative\u201d, \u201cContradictory\u201d etc.  The authors also analyze the performance of models built on top of recent approaches in image and sentence representations on both caption generation and caption retrieval task.  The paper consists of two main experiments: (1) they evaluate their architecture on MS-COCO and Flickr30K dataset to assess the model\u2019s ability to describe the image factually and in a neutral tone. They show similar performance to existing state-of-the-art approaches on caption-generation task and show best retrieval numbers on Flickr30K dataset. \n(2) They also perform human evaluation of personality-captions and found that humans tend to prefer these captions over the neutral captions\n\nPros:\n- The paper introduces a first large-scale dataset for personality captions that has 215 traits and ~200,000 images. It is significantly bigger than past datasets which consist of a only a few thousand captions covering a small set of categories.\n- The analysis of models is done not only on the personality-captions dataset but these models are also evaluated on the traditional COCO dataset. \n- The paper evaluates both retrieval and caption generation models on both these datasets and also perform human evaluation for completeness. \n- The paper is well written and is backed-up well by details mentioned in the supplementary material.\n\nCons:\n- The paper doesn\u2019t analyze and compare the benchmark with existing datasets. For instance, readers might be interested in  statistics of the dataset compared to existing datasets: \n    - Does it produce more diverse captions (are the captions longer) \n    - Is the caption vocabulary bigger than existing dataset? What\u2019s the overlap with existing dataset. \n    - Does it have more unique words per sentence position) \n- It has been shown in the past that collecting multiple captions for the same image leads to more robust evaluation using automatic metrics. However, the dataset contains just one caption per image. \n- The paper also didn\u2019t perform ablation studies on their model to know the source of performance gains. For instance, it would be interesting to see the performance of captioning model trained on ResNeXT-IG-3.5B and COCO dataset. What is the contribution of supervision of more captions coming from the new dataset vs (better?) image features extracted from ResNeXT-IG-3.5B \n- Although the paper does mention human evaluation of which caption is more engaging, there is no human baseline to compare the models to for personality-caption test set using automatic metrics. \n\n\nSome other questions/remarks:\n- During caption collection, caption-writers were asked to write \u201ca comment\u201d instead of a \u201ccaption\u201d. I wonder why this instruction was added? \n- Compared to the StyleNet paper by Gan et al (2017), it seems like the instructions were quite different to collection captions. They observed that collecting captions by just mentioning the style resulted in captions that were not visually grounded. Therefore, they setup the interface such that factual captions are changed to match the style. Can the authors comment on their choice of the interface as compared to the one described in the StyleNet paper?\n- There is also some value in evaluating whether the caption conformed to the personality mentioned while generating caption. Although there is a bit of discussion about this in the diversity of captions in the supplementary, I would be curious to see a bit more analysis on this. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}