{"title": "This paper is well-writen except a few flaws (see below). The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.", "review": "This paper presents Riemannian versions of adaptive optimization methods, including ADAGRAD, ADAM, AMSGRAD and ADAMNC. There are no natural coordinates on a manifold. Therefore, the authors resort to product of manifolds and view each manifold component as a coordinate. Convergence analyses for those methods are given. The the theoretical results and their Euclidean versions coincide. An experiment of embedding a tree-like graph into a Poincare model is used to show the performance of the Riemannian versions of the four methods.\n\nThis paper is well-written except a few flaws (see below). I do not have time to read the proofs carefully. The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.\n\nRemarks:\n*) P1, line 2: it particular -> in particular.\n*) P3, line 9: Is R_x(v) = x + v most often chosen? A manifold is generally nonlinear. A simple addition would not give a point in the manifold.\n*) P5, in Assumptions and notations paragraph: what are T and [T]? Is T the number of total iterations or the number of functions in the function family. The subscript of the function f_t seems to be an index of the functions. But its notation is also related to the number of iterations, see (8) and the algorithms in Figure 1.\n*) P5, Figure 1: does a loop for the index $i$ missing?\n*) Section 5: it would be clearer if the objective function is written as L:(D^n)^m \\to R: \\theta-> , where m is the number of nodes. Otherwise, it is not obvious to see the domain. \n*) P7, last paragraph: Tables 2 and 3 -> Figures 2 and 3.\n*) Besides the application in the experiments, it would be nice if more applications, at least references, are added.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}