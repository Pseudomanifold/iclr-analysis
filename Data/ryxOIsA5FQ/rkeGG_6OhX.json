{"title": "Recommend rejection of the paper due to the limited contributions and preliminary experiments.", "review": "The authors proposed a stacking method for both instance-based and feature-based transfer learning based on a two-phase strategy. It first introduced some self-defined parameters to diversify the data or the model, and then adopted some existing transfer learning to train the model.\n\nStrength:\n1) A stacking method for both instance-based and feature-based transfer learning.\n\nWeakness:\n1) Incremental contributions and limited novelty.\n2) Some claims are not well supported.\n3) Experimental results are preliminary.\n\nThe technical contribution of this work is limited. The main difference between the proposed instance-based stacking method and TrAdaboost are twofold: 1) using a self-defined threshold to select a subset of source samples for training; 2) using stacking instead of TrAdaboost to get the final output. The improvements upon TrAdaboost are marginal. Also, for the proposed feature-based stacking method, it just used the different kernel parameter values to diversify the model. The novelty is trivial. \n\nSeveral claims in the paper are not well discussed and/or evidence-supported, such as\uff1a\n1) \u201cWhen the similarity between domains is low, the final estimator can still achieve good performance on target training set.\u201d \n2) \u201cWhen source domain is not related enough, stacking performs better.\u201d\n3) \u201cWe reduce the risk of negative transfer in a simple and effective way without a similarity measure.\u201d\nMore discussions should be given, for example, how to measure the domain similarity and how to reduce the risk of negative transfer. Also, there are no experimental results to support the claims.\n\nFor the evaluation, it is inappropriate to choose Randomforest as the weak leaners since Randomforest is an ensemble method. It is better to give some explanation on how the data distribution and similarity between domains change with the kernel parameters in Figure 5.\n\nFor the algorithm comparison, only TrAdaboost is used as the baseline to compare with the proposed instance-based stacking method. The results could be more convincing if some recent ensemble-based transfer learning methods are included for comparison. For the evaluation of the proposed feature-based stacking method, the authors should at least compare their method with BDA since BDA is used as its base algorithm.\n\nSome symbols used in equations are not defined, such as h_t and c in Equation (2).\n\nThe paper needs a careful proofreading to correct the grammar errors and typos, such as:\n1) Line 1 of page 7: moreover -> Moreover?\n2) Line 1 of Abstract: overtting -> overfitting?\n\nIn summary, the paper has to make significant improvements before it can meet the bar of ICLR.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}