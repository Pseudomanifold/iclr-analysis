{"title": "lacking technical novelty ", "review": "This paper proposes to use a GPLVM to model financial returns. The resulting covariance matrix is used for portfolio optimization, and other applications. \n\nThe weakness of this paper is that there is no technical innovation, and the inference of the GPLVM is unclear. As far as I understand, the inference is exact over the GP, and VI is used for the latent variable locations (though the VI posterior is not specified). This is not the inference used in Titsias and Lawrence 2010 (where sparse VI is used for the GP), which makes the inference confusing as  Titsias and Lawrence 2010 is cited in the VI inference section. \n\nAn important aspect is not made clear: how are predictions made at test points? In Titsias and Lawrence 2010 a further optimization is performed with a partial objective (using the \u2018observed\u2019 dimensions only. \n\nThe results come across as disjointed as it is not made clear exactly what problem the model is intending to solve. There are some potentially interesting things here, but in 4.3.1 the difference in the methods appears very small (only a small fraction of a standard deviation), and in Figure 3 there is no comparison to other approaches so it is not clear how useful this apparent clustering might be. \n\n\nFurther comments (numbering is section, paragraph, line)\n\n0,1,5 \u2018readily interpretable\u2019 does not seem to me justified\n\n0,1,-4 \u2018naturally shrinks the sample covariance matrix\u2019 This claim is not revisited \n\n1,1,2:3: w and K not defined\n\n1,1,-4 \u2018Bayesian machine learning methods are used more and more in this domain\u2019 What evidence is there for this claim?\n\n1,1,-1 \u2018one can easily[\u2026] suitable priors\u2019. This seems to me a confusing statement as in under the Bayesian paradigm priors are necessary for inference, not additional.\n\n1,3,1 The \u2018Bayesian covariance estimator\u2019 here is a bit confusing, as the covariance itself is not the primary object being modelled, but is being estimated as part of a GP model. \n\n2.2,1,2 \u2018without losing too much information\u2019 This needs clarification. \n\n2.2,1,: The prior for X is not stated.\n\n2.3,:,: It seems unnecessary to give such a long introduction to VI but then have no details whatsoever on the actual variational posterior being used. It is extremely unclear what inference is being used here. In Lawrence 2005 the form of (8) is used with a MAP estimate for X. The introduction of VI in this subsection suggests that inference is going to follow Titsias and Lawrence 2010, but in this case there is much to say about the variational distribution over the GP, inducing points, kernel expectations, etc.\n\n2.3,-1,1 \u2018So, GP-LVM is an algorithm\u2019 I would not describe it as such. I would say that it is a model. \n\n3, 1,0 \u2018Now we have a procedure to estimate the covariance matrix\u2019 This procedure has not been made clear, in my view. As I understand it, a (potentially Bayesian) GP-LVM is fitted (potentially following  Titsias and Lawrence 2010, but perhaps following Lawrence 2005), and the K matrix is formed. \n\n3.1,-1,-6 \u2018known, that\u2019 perhaps this comma is unintended?\n\n3.1,-1,-2 is B is what was previously known as X?\n\n3.2,1,1:2 \u2018, for which he received a Nobel Price in economics\u2019 What is the purpose of this information?\n\n3.2,1,2 \u2018how good a given portfolio is\u2019. What does \u2018good\u2019 mean, in this context?\n\n3.2,2,1 \u2018w\u2019 has not been defined. \n\n3.2,2,4 \u2018primarily interested in the estimation of the covariance matrix\u2019. This seems to be a circular justification. \n\n4.1,2,: I would suggest defining the kernels with a variance parameter, to save the cumbersome Sigma notation \n\n4.1,-1,: What variational posteriors are used? \n\n4.2,2,1 The ELBO is only a lower bound to the marginal likelihood, so cannot always be used reliably for model comparison (though in practice it may be a good proxy)\n\n4.2,2,-3 \u2018which is a sign of overfitting\u2019 I\u2019m not sure this claim is true. A lower ELBO only indicates that the complexity penalty isn\u2019t being compensated by a better model fit term. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}