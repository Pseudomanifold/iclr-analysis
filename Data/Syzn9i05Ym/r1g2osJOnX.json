{"title": "Incremental Contribution", "review": "The paper proposes the inclusive neural random field model. Compared the existing work, the model is different because of the use of the inclusive-divergence minimization for the generative model and the use of stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian Monte Carlo  (SGHMC) for sampling. Experimental results are reported for unsupervised, semi-supervised, and supervised learning problems on both synthetic and real-world datasets. Specific comments follow:\n\n1. A major concern of the reviewer is that, given the related work mentioned in Section 3, whether the proposed method exerts substantial enough contribution to be published at ICLR. The proposed method seems like an incremental extension of existing works.\n\n2. A major claim by the authors is that the proposed techniques can help explore various modes in the distribution. However, this claim can only seem easily substantiated by experiments on synthetic data. It is unclear whether this claim is true in principle or in reality.\n\nOther points:\n3. the experimental results of the proposed method seems marginally better or comparable to existing methods, which call in question the necessity of the proposed method.\n\n4. more introduction to the formulation of the inclusive-divergence minimization problem could be helpful. The presentation should be self-contained.\n\n5. what makes some of the statistics in the tables unobtainable or unreported?\n\n\n============= After Reading Response from Authors ====================\n\nThe reviewer would like to thank the authors for their response. However, the reviewer is not convinced by the authors\u2019 argument. \n\n\u201cThe target NRF model, the generator and the sampler are all different.\u201d\nIt is understandable that modeling continuous data can be substantially different from modeling discrete data. Therefore, it is non-surprising that the problem formulations are different.\n\nAs for SGLD/SGHMC and the corresponding asymptotic theoretical guarantees, this reviewer agrees with reviewer 2\u2019s perspective that it is a contribution made by this paper. But this reviewer is not sure whether such a contribution is substantial enough to motivate acceptance.\n\nThe explanation for better mode exploration of the proposed method given by the authors are the sentences from the original paper. The reviewer is aware of this part of the paper but unconvinced.\n\nIn terms of experiments, sample generation quality seems to be marginally better. Performances in multiple learning settings are comparable to existing methods.\n\nA general advice on future revision of this paper is to be more focus, concrete, and elaborative about the major contribution of the paper. The current paper aims at claiming many contributions under many settings. But the reviewer did not find any of them substantial enough.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}