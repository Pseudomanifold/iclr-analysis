{"title": "Nice Work", "review": "The authors splitted the features of multimodal representations to \"common\" (multimodal discriminative) and \"specific\" (modality-specific generative) factors. In this framework, their MFM can capture more detailed features. \n\nPros:\n(*) Learning the feature representations from two perspectives. \n\n(*) Even missing one modality, MFM can still achieve acceptable performance. \n\n(*) Using mutual information and gradient-based method to interpret their method. \n\nCons:\n(*) The work has some similarity to Hsu & Glass (2018), but the comparison between this work is only on CMU-MOSI.\n\n(*) In Table. 3, it shows that language is the most informative feature for prediction. However, in Table. 2, it can be seen that if audio is missing, the result it the worse compared to the other two cases. It seems the interpretation is not convincing to me. Can you give us more explanation about this phenomenon? \n\nComments:\n(*) The details of SVHN-MNIST experiment are missing. Appendix B gave some information about models but specified the targeted datasets.\n\n(*) The appendix is not clear, e.g. In Appendix B, it is said \"subsection 3.3\" but there is no section 3.3.  \n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}