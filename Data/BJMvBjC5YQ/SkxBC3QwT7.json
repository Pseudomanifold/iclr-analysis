{"title": "Incremental improvements. Latency overhead is not reported. ", "review": "\nThis paper introduces a re-forwarding method to cut the memory footprint for training neural networks. It improved on top of previous work that did re-forwarding for linear computation graph, and this work also supports non-linear computation graph. \n\nPros:\n(1) This paper is solving an important problem saving the training memory of deep neural networks. GPU price and GPU memory increase non-proportionally, and many machine learning tasks require large GPU memory to train.  Though, there's rich literature in the area of saving the GPU memory for training. \n\n(2) This paper evaluated a large number of neural networks (Alexnet, VGG, ResNet, DenseNet). The evaluation is extensive. \n\nCons: \n(1) this paper has incremental contribution compared with Chen et al. (2016). Chen et al worked on linear computation graph. Extending the algorithm from linear graph to non-linear graph is important but the technical contribution is incremental and thin. \n\n(2) the improvement over previous work is quite marginal. For example, the memory saving went from 2894MB to 2586MB for VGG16, from 2332Mb to 1798MB for ResNet-50, just to pick a few widely used architectures. \n\n(3) The paper only reported the memory saving, but didn't report the latency overhead. It's important to know how much latency overhead is brought by this algorithm. If we save 2x memory but the training time also get much longer, users will simply reduce the virtual batch size by half and do forward the backward twice to simulate the same physical batch size. So, the authors should be clear how much training time is brought by such algorithm and report the *end-to-end training time* in Table 1, with and without re-forwarding. ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}