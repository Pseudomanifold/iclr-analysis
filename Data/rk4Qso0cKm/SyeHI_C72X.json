{"title": "Interesting contribution", "review": "After feedback: I would like to thank the authors for careful revision of the paper and answering and addressing most of my concerns. From the initial submission my main concern was clarity and now the paper looks much more clearer. \n\nI believe this is a strong paper and it represents an interesting contribution for the community.\n\nStill things to fix:\na) a dataset used in 4.2 is not stated\nb) missing articles, for example, p.5 \".In practice, however, we need a weaker regularization for A small dataset or A large model\"\nc) upper case at the beginning of a sentence after question: p.8 \"Is our Adv-BNN model susceptible to transfer attack? we answer\" - \"we\" -> \"We\"\n====================================================================================\n\nThe paper proposes a Bayesian neural network with adversarial training as an approach for defence against adversarial attacks. \n\nMain pro:\nIt is an interesting and reasonable idea for defence against adversarial attacks to combine adversarial training and randomness in a NN (bringing randomness into a new level in the form of a BNN), which is shown to outperform both adversarial training and random NN alone.\n\nMain con:\nClarity. The paper does not crucially lack clarity but some claims, general organisation of the paper and style of quite a few sentences can be largely improved.\n\nIn general, the paper is sound,  the main idea appears to be novel and the paper addresses the very important and relevant problem in deep learning such as defence against adversarial attacks. Writing and general presentation can be improved especially regarding Bayesian neural networks, where some clarity issues almost become quality issues. Style of some sentences can be tuned to more formal.\n\nIn details:\n1. The organisation of Section 1.1 can be improved: a general concept \"Attack\" and specific example \"PGD Attack\" are on the same level of representation, while it seems more logical that \"PGD Attack\" should be a subsection of \"Attack\". And while there is a paragraph \"Attack\" there is no paragraph \"Defence\" but rather only specific examples\n2. The claim \u201cwe can either sample w \u223c p(w|x, y) efficiently without knowing the closed-form formula through the method known as Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011)\u201d sounds like SGLD is the only sampling method for BNN, which is not true, see, e.g., Hamiltonian Monte Carlo (Neal\u2019s PhD thesis 1994). It is better to be formulated as \"through, for example, the method ...\"\n3. Issues regarding eq. (7):\n   a) Why there is an expectation over (x, y)? There should be the joint probability of all (x, y) in the evidence.\n   b) Could the authors add more details about why it is the ELBO given that it is unconventional with adversarial examples added?\n   c)  It seems that it should be log p(y | x^{adv}, \\omega) rather than p(x^{adv}, y | \\omega). \n   d) If the authors assume noise component, i.e., y = f(x; \\omega) + \\epsilon, then they do not need to have a compulsory Softmax layer in their network, which is important, for example, for regression models. Then the claim \u201cour Adv-BNN method trains an arbitrary Bayesian neural network\u201d would be more justified\n4. It would make the paper more self-contained if the Bayes by Backprop algorithm would be described in more details (space can be taken from the BNN introduction). And it seems to be a typo that it is Bayes by Backprop rather than Bayes by Prop\n5. There are missing citations in the text:\n    a) no models from NIPS 2017 Adversarial Attack and Defence competition (Kurakin et al. 2018) are mentioned\n    b) citation to justify the claim \u201cC&W attack and PGD attack (mentioned below) have been recognized as\ntwo state-of-the-art white-box attacks for image classification task\u201d\n    c) \u201cwe can approximate the true posterior p(w|x, y) by a parametric distribution q_\u03b8(w), where the unknown parameter \u03b8 is estimated by minimizing KL(q_\u03b8(w) || p(w|x, y)) over \u03b8\u201d - there are a variety of works in approximate inference in BNN, it would be better to cite some of them here\n    d) citation to justify the claim \"although in these cases the KL-divergence of prior and posterior is hard to compute and practically we replace it with the Monte-Carlo estimator, which has higher variance, resulting in slower convergence rate.\u201d\n6. The goal and result interpretation of the correlation experiment is not very clear\n7. From the presentation of Figure 4 it is unclear that this is a distribution of standard deviations of approximated posterior.\n8. \u201cTo sum up, our Adv-BNN method trains an arbitrary Bayesian neural network with the adversarial examples of the same model\u201d \u2013 unclear which same model is meant\n9. \"Among them, there are two lines of work showing effective results on medium-sized convolutional networks (e.g., CIFAR-10)\" - from this sentence it looks like CIFAR-10 is a network rather than a dataset\n10. In \"Notations\" y introduction is missing\n11. It is better to use other symbol for perturbation rather than \\boldsymbol\\delta since \\delta is already used for the Dirac delta function\n12. \u201cvia tuning the coefficient c in the composite loss function\u201d \u2013 the coefficient c is never introduced\n\nMinor:\n1. There are a few missing articles, for example, in Notations, \u201cIn this paper, we focus on the attack under THE norm constraint\u2026\u201d\n2. Kurakin et al. (2017) is described in the past tense whereas Carlini & Wagner (2017a) is described in the present tense\n3. Inner brackets in eq. (2) are bigger than outer brackets\n4. In eq. (11) $\\delta$ is not bold\n5. In eq. (12) it seems that the second and third terms should have \u201c-\u201d rather than \u201c+\u201d\n6. Footnote in page 6 seems to be incorrectly labelled as 1 instead of 2\n\n\n\n\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}