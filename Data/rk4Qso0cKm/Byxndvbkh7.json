{"title": "A nice paper that bridges adversarial training and Bayesian neural nets", "review": "The paper extends the PGD adversarial training method (Madry et al., 2017) to Bayesian Neural Nets (BNNs). \nThe proposed method defines a generative process that ties the prediction output and the adversarial input \npattern via a set of shared neural net weights. These weights are then assinged a prior and \nthe resultant posterior is approximated by variational inference.\n\nStrength:\n  * The proposed approach is incremental, but anyway novel.\n  * The results are groundbreaking.\n  * There are some technical flaws in the way the method has been presented, \nbut the rest of the paper is very well-written.\n\nMajor Weaknesses:\n\n  * Equation 7 does not seem to be precise. First, the notation p(x_adv, y | w) is severely misleading. If x_adv is also an input, no matter if stochastic or deterministic, the likelihood should read p(y | w, x_adv). Furthermore, if the resultant method is a BNN with an additional expectation on x_adv, the distribution employed on x_adv resulting from the attack generation process should also be written in the form of the related probability distribution (e.g. N(x_adv|x,\\sigma)).\n\n  * Second, the constraint that x_adv should lie within the \\gamma-ball of x has some implications on the validity of\nthe Jensen's inequality, which relates Equation 7 to proper posterior inference.\n\n  * Blundell et al.'s algorithm should be renamed to \"Bayes-by-BACKprop\". This is also an outdated inference technique for quite many scenarios including the one presented in this paper. Why did not the authors benefit from the local reparametrization trick that enjoy much lower estimator variance? There even emerge sampling-free techniques that nullify this variance altogether and provide much more stable training experience.\n\nAnd Some Minor Issues:\n\n  * The introduction part of paper is unnecessarily long and the method part is in turn too thin. As a reader, I would prefer getting deeper into the proposed method instead of reading side material which I can also find in the cited articles.\n\n  * I do symphathize and agree that Python is a dominant language in the ML community. Yet, it is better scientific writing practice to provide language-independent algorithmic findings as pseudo-code instead of native Python.\n\nOverall, this is a solid work with a novel method and very strong experimental findings. Having my grade discounted due to the technical issues I listed above and the limitedness of the algorithmic novelty, I still view it as an accept case.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}