{"title": "Interesting approach, not fully mature yet", "review": "The paper addresses the interesting problem of generating a small number of synthetic examples that can be used to train a classifier, replacing a larger dataset. \n\nThe paper is clearly written, the approach makes sense, and the experiments are interesting. \nMy major concerns are regarding to previous literature, analysis of the algorithm, and details of the experiment. Overall, I expect an ICLR paper to go deeper (rather than wide). I recommend presenting strong convincing evidence on one front. \n\n\nSpecific comments: \n(1)  I'm missing analysis of the proposed procedure. It wasn't fully clear which Loss it minimizes and if it indeed guaranteed to converge to the minimum of that loss. \n\n(2)  The topic of learning from few samples is presented as completely new. It is well known that for classical linear algorithms like the Perceptron and SVM, the weights are a weighted sum of (label-weighted) samples, hence by definition of these algorithm, there is a single sample that can be used to \"train\" the model in one step. I'd expect some discussion of how the proposed approach relate to these classical approaches. \nThere is also existing literature on a related problem of selecting samples (Teaching dimension Goldman&Kearns) that could be somewhat relevant here. \n\n(3) Motivation. The paper provide several motivations for dataset distillation. I support the first motivation of scientific understanding what data is actually needed for a classifier, and this means that deeper analysis is needed. The practical motivations are less convincing, because (a) domain adaptation experiments are not compared with real baselines (b) robustness of poisoning with a single sample is not studied/discussed.\n\n(4) experiments: The intro states that training with 10 images reaches 94% accuracy, but this does not seem consistent with the results in Table 1. The caption of figure 2 suggests that accuracy is between 12% and 94% which means the stated 94% is not representative or typical. Could you clarify? \nFor domain adaptation. The baseline (random images) are very weak, and still perform almost   comparably to the proposed approach. More robust experiments are needed here: stronger baselines, decent hyper-parameter search etc.\n\n(5) Writing and exposition: The paper addresses two issues: (a) learning with few synthetic samples, and (b) learning with few gradient steps. The intro tends to mix the two, and it is not clear why learning with a single gradient step is important. I recommend to separate the two topics more clearly. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}