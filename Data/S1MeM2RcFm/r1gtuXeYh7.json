{"title": "review", "review": "Strengths:\n\nWell written paper, covers most of the relevant related work\nTechnique is conceptually easy to understand (~ adversarial training)\n\nWeaknesses:\n\nUnclear set of desiderata properties for a watermarking technique\nNo formal guarantees are verified, the mechanism is only tested\nAttacks tested are not tailored to the technique proposed\n\nFeedback and rebuttal questions:\n\nThis submission is easy to read and follow, and motivates the problem of watermarking well in light of intellectual property concerns. The technique proposed exploits unused capacity in the model to train it to associate specific inputs (computed adversarially) to specific outputs (the keys). Watermarking succeeds when the bit error rate between the predicted signature and the expected one is zero. This approach is conceptually easy to understand. \n\nThe experimental setup used to evaluate the approach is however limited. First, it is unclear why desiderata stated in Section 3.1 and summarized in Table 1 are necessary and sufficient. Would you be able to justify their choice in your rebuttal? For instance, the \u201csecurity\u201d requirement in Table 1 overlaps with \u201cfidelity\u201d. Similarly, the property named \u201cintegrity\u201d really refers to only a subset of what one would typically describe as integrity. It basically calls for a low false positive or high precision. \n\nThe attack model described in Section 3.2 only considers three existing attacks: model fine-tuning, parameter pruning and watermark overwriting. These attacks do not consider how the adversary could adapt and they are not optimal strategies for attacking the specific defensive mechanism put in place here. For instance, could you explain in your rebuttal why pruning the smallest weights in the architecture in the final architecture would help with removing adversarial examples injected to watermark the model? Similarly, given that adversarial subspaces have large volumes, it makes sense that multiple watermarks could be inserted simultaneously and thus watermark overwriting attacks would fail.\n\nIf the approach is based on exploring unused capacity in the model, the adversary could in fact attempt to use a compression technique to preserve the model\u2019s behavior on the task and remove the watermarking logic. For instance, the adversary could use an unlabeled set of inputs and have them labeled by the watermarked model. Because these inputs will not be \u201cadversarial\u201d, the watermarked model\u2019s decision surface used to encode the signatures will remain unexplored during knowledge transfer and the resulted compressed or distilled model would solve the original task without being watermarked. Is this an attack you have considered in your experiments and if not could you elaborate why one may exclude it in your rebuttal?\n\nMinor comments: \n\nP3: Typo \u201cVerifiabiity\u201d\nP5: Could you add a reference or additional experimental results that justify why transferable keys would be located near the decision boundaries? \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}