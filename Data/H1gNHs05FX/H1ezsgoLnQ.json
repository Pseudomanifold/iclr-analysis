{"title": "Well-motivated and innovative approach to construct intensity functions using wavelets. But the overall quality is not good enough due to large amount of unclear important content (in both method and experiments).", "review": "\n[PROS]\n\n[originality]\n\nThe paper proposed to construct the intensity function of a point process using wavelet, in order to improve its expressiveness, e.g. allowing non-additivity.  \n\nThe authors did extensive experiments to investigate their model performance compared to many appropriate baselines, on both synthetic and real-world datasets. \n\n[CONS]\n\n[clarity]\n\nThe major drawback of this submission is its clarity. The paper is vague at various important points in both method and experiments, thus leaving their correctness and soundness undetermined. \n\nIn the method section, the authors did not specify a well-defined and self-consistent notation system. This makes the paper really hard to understand. For example, one may be easily confused with things like: \n1) How q, q(g(t), t_i), g(t), g_{es}, g_{d} are connected and distinguished? \n2) The function g_{es} maps from t to R, then is t a space or a variable? \n3) The state s seems crucial in the function g_{es}, but why it is only mentioned one time in the paper? How it is defined and how it is used?\n4) How the Hadamard product is applied to two matrices of different sizes? \n5) j=1 is time dimension and j=2 is time and value dimension, then why j=1 is not part of j=2? Time is needed in both cases and it seems natural that the associated parameters are shared. \n6) Figure-3 has t_i in figure, t\u2019 in caption, but the text in main paper mentions t_0 for Figure-3. How are they related? Are they actually the same?\n\nThe most confusing part is the censoring distance c. Its introduction around eqn-(2) suggests that c > 0 and the censoring section clearly mentions that. But c is also used to denote the forecast distance, which is clearly < 0 according to Figure-6 and Figure-7. What\u2019s worse, there is also a term called forecast censoring distance. What are the relationships among these terms? If they are all the same, then is the c actually a model parameter or an evaluation control knob? Such things are very important to clarify. \n\nMoreover, the paper did not clearly explain how the model is trained in each case, especially for (multi-)forecasting. In details:\n1) What is the training objective?.\n2) What is the optimization method for this objective?\n3) How is it implemented and would the code be released?\n\nIt is good that the experimental section lists many appropriate baseline models and multiple evaluation metrics, but it is not clear how they are used. For example:\n1) Fourier methods and Hawkes process do not deal with the value v, then how are they fairly compared to the proposed model which takes v into account as in eqn-(2)? \n2) How is the Goodman-Kruskal gamma exactly computed? On all the instances of the held-out set? What is exactly the rank in this case?\n3) The authors also leave out the positiveness constraints of a Hawkes process to incorporate inhibitory interactions, but how the positivity of the intensity function is ensured in this case? \n\n[quality and significance]\n\nThe method is well-motivated and innovative. But details of the model and experiments are very unclear, so its overall soundness is hard to judge. For example: \n1) The authors claim that, compared to neural models, their model has the advantage of the interpretability (for small datasets), but they also have neural components in their model. So why their model is more interpretable than others (e.g. Mei and Eisner 2017 as they cited) is not clear to me. \n2) It is not clear why the interpretability is associated with the size of the dataset (quote `remains interpretable for small data sets\u2019). What\u2019s worse, the interpretability seems the only advantage of the model over other neural models (please correct me if I am wrong). If this edge could not scale up to large datasets, then does it mean on large datasets, a neural model should always be preferred over this model, because they are supposed to be more expressive? \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}