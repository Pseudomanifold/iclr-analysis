{"title": "ODIN: outlier detection in neural networks", "review": "This paper studies how to better recognise classes that are found in the training set, and identify those that are not in the training set, using neural network methods. They call this as \"outlier detection in neural networks\".  I have several questions about this paper.\n\nFirst, the definition of outlier is ambiguous. The classes that are not in the training set do not necessarily belong to outliers. In this respect, I felt the evaluations are problematic, as the unused data have been considered as outliers. The performance metrics used are therefore not entirely appropriate, as they are used to evaluate how good is the classifier. Even you define the unused data as outliers, such outliers cannot be directly considered as outliers in the network, a drift of the training model from the actual physical model should be quantified with different inputs to the system. \n\nSecond, the actual contribution is the incorporation of equation (7) and (8) into the network layers, based on a linear approximation - something like applying PCA type of operation on the activations in different layers. There is no validation on the assumptions made, or any empirical study on the assumptions made about the \"disentangle manifolds\" and \"approximately linear\". \n\nThird, the paper lacks proofread and there are some typos and grammtical issues visual at several places, such as \"systems raises\", \"that uses reconstruction residual\", \"multitide\"  \"see 3.1\", \"CNN:s\", etc.\n\nFourth, in the experiments, the linear operation is applied to different layers and different networks, what's the reasoning behind this?\n\nSome specific comments:\nHow t_new is obtained in equation (4)?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}