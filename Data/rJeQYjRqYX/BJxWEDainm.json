{"title": "No clear contribution showed without experiment comparison with previous work. Also the motivation could be more clear.", "review": "The authors propose the notion of effective path, for the purpose of identifying neurons that contributes to the predictions and being able to detect adversarial images in the context of image classification.\nOverall the paper is well written except that the authors are mixing two highly related but still different topics: explanation and adversary detection so that the motivation is confusing.\nThe experimental results indeed show promises that effective path can help understand class similarities and network efficiencies but doesn\u2019t really show how the proposed work is adding value to the field.\nIt lacks the experimental comparison with previous methods but only include discussion in texts.\nThis paper could turn out to be a stronger paper but it is not ready yet.\n\nBelow are some more detailed comments.\n1) The authors motivate by stating that the vulnerability of NN to input perturbations is due to the lack of interpretability (Section Introduction & Abstract). I can understand that we want more interpretability, and we want less vulnerability, but I can\u2019t agree that vulnerability is caused by lack of interpretability. Also, the authors are trying to accomplish both tasks, interpretability and adversary detection, by showing data analysis of how the findings coincide with prior knowledge (eg. Class of digit 1 is the most different from other classes in MNIST task), and by showing detecting adversary images. However, neither has valid quantitative comparison with previous work; actually for the interpretability topic, the authors didn\u2019t really provide a tool or a generalizable method. Thus, I would suggest to choose one of the two topics (ie. adversarial image detection) and focus on it by adding thorough comparison with other methods; in the discussion and result section, include the interpretability analysis to justify why the proposed adversary detection method is behaving in certain ways.\n\n2) One topic that is missing from the paper is the time complexity of the proposed method. At a na\u00efve estimate, it would require tracking and finding the minimum set of effective neurons with threshold \\theta and thus per instance, at least O(m log m) is required at prediction phase, where m is the number of features; for n instances, the asymptotic complexity is O(nm log m) How does it compare to the other adversary detection methods?\n\n3) Page 3 mentions that the work for critical routing path (Wang et al. 2018) requires re-training for every image; this statement is not really true without more context. Also authors discuss this work again very briefly in Page 8 due to the high similarity in methods and motivation with the proposed method, but the authors don\u2019t show any quantitative comparison. After all, both methods are trying to identify neurons that contribute the most to the prediction, some more concrete comparison would be nice.\n\n4) Page 3 mentions that the derived overall effective path is highly sparse compared to the original network and the effective path density for five trained models ranges from 13% to 42% which conforms with the \u201c80%\u201d claim from another paper. Together with the other similar statements, it would be really nice to note what \\theta is used for such statements; how does such statement change with different \\theta. Also some discussion would be nice about what such sparsity implies. Specifically, does the sparsity suggest the opportunity for feature selection, or does it suggest a way for detecting overfitting?\n\n5) Page 5 shows the path similarity between the normal and the adversary examples; from the figure 5a and 5b, we can see the on the first layer, the mean deviate between normal and others but why the last layer they almost reach to the same point? It seems it is the middle layer that distinguish the normal from the adversary examples the most. Some more discussion would be good.\n\n6) Some justification of why \\theta=0.5 is chosen would be good on Page 6.\n\n7) On Page 7, the authors are discussing the performance of the proposed method, however, there is no really comparison with other methods. But rather, the authors stated \u201cbetter accuracy\u201d, \u201cAUC\u2026 is better\u2026\u201d by comparing different evaluation scenarios. I don\u2019t find such discussion helpful in showing the contribution of the proposed method. Also in the parameter sensitivity, it would be nice to add the analysis for the effective path density and see if it still conforms with the \u201c80%\u201d claim with different \\theta.\n\n8) Page 1, need to add citations for the statement \u201c\u2026 and even outperformed human beings.\u201d\n\n9) Minor issue: Page 1 \u201csuch computer vision\u2026\u201d should be \u201csuch as computer vision\u2026\u201d.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}