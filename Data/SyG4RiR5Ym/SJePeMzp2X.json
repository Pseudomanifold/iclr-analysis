{"title": "Re: Neural Distribution Learning for generalized time-to-event prediction", "review": "The authors propose a parametric framework (HazardNet) for survival analysis with deep learning where they mainly focus on the discrete-time case. The framework allows different popular architectures to learn the representation of the past events with different explicit features. Then, it considers a bunch of parametric families and their mixtures for the distribution of inter-event time. Experiments include a comprehensive comparison between HazardNet and different binary classifiers trained separately at each target time duration. \n\nOverall, the paper is well-written and easy to follow. It seeks to build a strong baseline for the deep survival analysis, which is an hot ongoing research topic recently in literature. However, there are a few weaknesses that should be addressed. \n\n1. In the beginning, the paper motivates the mixtures of distributions from MDN. Because most existing work focuses on the formulation of the intensity function, it is very interesting to approach the problem from the cumulative intensity function instead. Originally, it looks like the paper seeks to formulate a general parametric form based on MDN. However, it is disappointing that in the experiments, it still only considers a few classic parametric distributions. There is lack of solid technical connection between Sec 3.1, 3.2 and Sec 4.\n\n2. The discretization discussion of Sec 3.4 is not clear. Normally, the major motivation for discretization is application-driven, say, in hospital, the doctor regularly triggers the inspection event. However, how to optimally choose a bin-size and how to aggregate the multiple events within each bin is still not clear, which is not sufficiently discussed in the paper. Why is taking the summation of the events in a bin a proper way of aggregation? What if we have highly skewed bins?\n\n3. Although the comparison and experimental setting in Figure 4 is comprehensive, the paper misses a very related work \"Deep Recurrent Survival Analysis, https://arxiv.org/abs/1809.02403\", which also considers the discrete-time version of survival analysis. Only comparing with the binary classifiers is not quite convincing without referring to other survival analysis work.\n\n4. Finally, the authors state that existing temporal point process work \"have little meaning without taking into account censored data\". However, if inspecting the loss function of these work closely, we can see there is a survival term exactly the same as the log-cumulative hazard in Equation 3 that handles the censored case.\n\n5. A typo on the bottom of page 3, should be p(t) = F(t + 1) - F(t)\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}