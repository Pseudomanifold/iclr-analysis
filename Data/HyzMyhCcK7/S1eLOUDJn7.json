{"title": "Interesting idea but novelty may not be enough", "review": "After the rebuttal:\n\n1.  Still, the novelty is limited. The authors want to tell a more motivated storyline from Nestrove-dual-average, but that does not contribute to the novelty of this paper. The real difference to the existing works is \"using soft instead of hard constraint\" for BNN. \n\n2. The convergence is a decoration. It is easy to be obtained from existing convergence proof of proximal gradient algorithms, e.g. [accelerated proximal gradient methods for nonconvex programming. NIPS. 2015].\n\n---------------------------\nThis paper proposes solving binary nets and it variants using proximal gradient descent. To motivate their method, authors connect lazy projected SGD with straight-through estimator. The connection looks interesting and the paper is well presented. However, the novelty of the submission is limited.\n\n1. My main concern is on the novelty of this paper. While authors find a good story for their method, for example,\n- A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training\n- Training Ternary Neural Networks with Exact Proximal Operator\n- Loss-aware Binarization of Deep Networks\n\nAll above papers are not mentioned in the submission. Thus, from my perspective, the real novelty of this paper is to replace the hard constraint with a soft (penalized) one (section 3.2). \n\n2. Could authors perform experiments with ImageNet?\n\n3. Could authors show the impact of lambda_t on the final performance? e.g., lambda_t = sqrt(t) lambda, lambda_t = sqrt(t^2 lambda", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}