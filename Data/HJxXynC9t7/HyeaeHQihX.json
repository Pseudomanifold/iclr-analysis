{"title": "Review", "review": "# Summary\nThis paper proposes a simple regularizer for RL which encourages the state representations learned by neural networks to be more discriminative across different observations. The main idea is to (implicitly) measure the rank of the matrix which is constructed from a sequence of observations and state feature vectors and encourage the rank to be high. This paper introduces three different objectives to implement the same idea (increasing the rank of the matrix). The experimental results on Atari games show that this regularizer improves A3C on most of the games and show that the learned representations with the proposed regularizer has a high rank compared to the baseline. \n\n[Pros]\n- Makes an interesting point about the correlation between RL performance and state representations. \n- Proposes a simple objective function that gives strong empirical results.\n\n[Cons]\n- Needs more empirical evidences to better support the main hypothesis of the paper.\n\n# Novelty and Significance\n- This paper makes an interesting observation about the correlation between the RL performance and the how discriminative the learned features.\n- To verify it, this paper proposes a new and simple regularizer which improves the performance across many Atari games. \n\n# Quality and Experiment\n- The main hypothesis of this paper is that the expressiveness of the features (specifically the rank of the matrix that consists of a sequence of features) and its RL performance is highly correlated. Although this paper showed some plots (Figure 2, 6) to verify this, a more extensive statistical test or experiments would be more convincing to show the hypothesis. Examples would be:\n1) Measuring the correlation between the two across all Atari games.\n2) An ablation study on the hyperparameter (alpha). \n3) Learning state representations just from the reconstruction task (without RL) with/without the proposed regularizer and separately learning policies on top of that (with fixed representations). It would be much more convincing if the regularizer helps even in this setup, because this would show the general effect of the expressiveness term (by removing the effect of RL algorithm on the representation).\n- This paper only reports \"relative\" performances to the baseline. Though it looks strong, it is also important to report the absolute performances in Atari games (e.g., median human-normalized score, etc) to show how significant the performance gap is. \n- The results with DQN are not convincing because the agents are trained only for 20M frames (compared to 200M frames in many other papers). It is not much meaningful to compare performances on such a short training regime. I would suggest running longer or removing this result from the paper and focusing on more analysis.\n\n# Clarity and Presentation\n- Figure 1a is not much insightful. It is not surprising that the representations that led to a poor policy (which achieves 0 reward) are much less discriminative given five situations with five distinct optimal actions, because the the policy has no idea which action is better than the other in such situations. It would be more informative to pick N consecutive frames and show how scattered they are in the embedding space. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}