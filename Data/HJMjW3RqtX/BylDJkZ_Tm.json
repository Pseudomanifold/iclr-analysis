{"title": "Interesting idea to extend DDPGfD to use only state trajectories, but needs a further experimental validation. ", "review": "**Summary**\n\nThe paper looks at the problem of one-shot imitation with high accuracy of imitation. The main contributions: \n1. learning technique for high fidelity one-shot imitation at test time. \n2. Policies to improve the expert performance through RL.  \n\nThe main improvements of this method is that demo action and rewards are not needed only state trajectories are sufficient. \n\n\n** Comments **\n- The novelty of algorithm block\nThe main method is very similar to D4PG-fd. The off-policy method samples from a replay buffer which comprises of both the demos and the agent experience from the previous learner iterates. \n\n1. From a technical perspective, what is the advantage of training an imitation learner from a memory buffer of the total experience? \nIf the task reward is not accessed, then when the imitation learner is training, then the data should not be used for training the task policy learner. On the other hand if task reward is indeed available then what is the advantage of not using it. \n\n2. A comparison with a BC policy to generate more experience data for the task policy agent/learning might also be useful. \n\n* Improved Comparisons\n- Compare with One-Shot Performance\nSince this is one of the main contributions, explicit comparison with other one-shot imitation papers needs to be quantified with a clearly defined metric for generalization. \n\nThis comparison should be both for short-term tasks such as block pick and place (Finn et al, Pathak et al, Sermanet et al.) and also for long-term tasks as shown in (Duan et al. 2017 and also in Neural Task Programming/Neural Task Graph line of work from 2018)\n\n- Compare High-Fidelity Performance\nIt is used as a differentiator of this method but without experimental evidence.\nThe results showing imitation reward are insufficient. The metric should be independent of the method. An evaluation might compare trajectory tracking error: for objects, end-effector, and joint positions. This is available as privileged information since the setup is in a simulation.\n\nFurthermore, a comparison with a model-based trajectory tracking with a learned or fitted model of dynamics is also very useful.\n\n- Compare Policy Learning Performance\nIn addition to D4PG variants, performance comparison with GAIL will ascertain that unconditional imitation is better than SoTA. \n\n\n* Tracking a reference (from either sim or demos) is a good idea that has been explored in sim2real literature[2,3] and imitation learning [4]. It is not by itself novel. The authors fail to acknowledge any work in this line as well as provide insight why is this good and when is this valid. For instance, with highly stochastic dynamics this may not work!\n\n\n- \"Diverse Novel Skills\" \nThe experiments are limited to a rather singular pick and place task with a 3-step structured reward model. It is unfair to characterize this domain as very diverse or complex from a robotics perspective. More experiments on continuous control would help.\n\n- Bigger networks\n\"In fig. 3 we demonstrate that indeed a large ResNet34-style network (He et al., 2016) clearly outperforms\" -- but Fig 3 is a network architecture diagram. It is probably fig 6!\n\n- The authors are commended for presenting a broad overview of imitation based methods in table 2\n\n** Questions **\n\n1.  How different if the imitation learner (trained with imitation reward) from a Behaviour Cloning Policy. \n\n2. How is the local context considered in action generation in sec 2.1. \nThe authors reset the simulation environment to o_1 = d_1. \nThen actions are generated with  \\pi_{theta} (o_t, d_{t+1}). \na. Is the environment reset every time step?\nb. If not how is the deviation of the trajectory handled over time? \nc. how is the time horizon for this open loop roll out chosen. \n\n3. How is this different for a using a tracking based MPC with the same horizon? The cost can be set the same the similarity metric between states. \n\n4. The architecture uses a deep but simplistic model. When the major attribution of the model success is to state similarity -- especially image similarity -- why did the authors not use image comparators something like the Siamese model?\n\nSuggestion:\nThe whole set of experiments are in a simulation. \nThe authors go above and beyond in using Mitsuba for rendering images. But the images used are Mujoco rendered default. It would nice if the authors were more forthcoming about this. All image captions should clearly state -- Simulated robot results, show images used for agent training. The Mitsuba renders are only used for images but nowhere in the algorithm. So why do this at all, and if it has to be used please do it with a disclaimer. Right now this detail is rather buried in the text. \n\nReferences:\n1. Neural Task Programming, Xu et al. 2018 (https://arxiv.org/abs/1710.01813)\n2. Preparing for the Unknown: Learning a Universal Policy with Online System Identification (https://arxiv.org/abs/1702.02453)\n3. Adapt: zero-shot adaptive policy transfer for stochastic dynamical systems (https://arxiv.org/abs/1707.04674)\n4. A survey of robot learning from demonstration, Argall et al. 2009\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}